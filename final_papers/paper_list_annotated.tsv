title	year	url	semanticscholar_tldr	use	citation_count	abstract	semanticscholar_id	bibtex
Zero-Shot Opinion Summarization with GPT-3	2022	http://www.semanticscholar.org/paper/f9a6f76a20475602e5facaa019f3e4f078cedead	This paper explores several pipeline methods for applying GPT-3 to summarize a large collection of user reviews in a zero-shot fashion, and evaluates against several new measures targeting faithfulness, factuality, and genericity to contrast these different methods.	maybe	0	Very large language models such as GPT-3 have shown impressive performance across a wide variety of tasks, including text summarization. In this paper, we show that this strong performance extends to opinion summarization. We explore several pipeline methods for applying GPT-3 to summarize a large collec-tion of user reviews in a zero-shot fashion, no-tably approaches based on recursive summarization and selecting salient content to summarize through supervised clustering or extraction. On two datasets, an aspect-oriented summarization dataset of hotel reviews and a generic summarization dataset of Amazon and Yelp reviews, we show that the GPT-3 models achieve very strong performance in human evaluation. We argue that standard evaluation metrics do not reﬂect this, and evaluate against several new measures targeting faithfulness, factuality, and genericity to contrast these different methods.	f9a6f76a20475602e5facaa019f3e4f078cedead	@['JournalArticle', 'Review']{bhaskar-etal-2022-zero,  author = {Adithya Bhaskar and Alexander R. Fabbri and Greg Durrett},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Zero-Shot Opinion Summarization with GPT-3},  volume = {abs/2211.15914},  year = {2022} }
Your fairness may vary: Pretrained language model fairness in toxic text classification	2021	http://www.semanticscholar.org/paper/fb1810f403288c40291fbe65fdfa4adf55c466e7	It is demonstrated that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics, and it is shown that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models.	maybe	5	The popularity of pretrained language models in natural language processing systems calls for a careful evaluation of such models in down-stream tasks, which have a higher potential for societal impact. The evaluation of such systems usually focuses on accuracy measures. Our findings in this paper call for attention to be paid to fairness measures as well. Through the analysis of more than a dozen pretrained language models of varying sizes on two toxic text classification tasks (English), we demonstrate that focusing on accuracy measures alone can lead to models with wide variation in fairness characteristics. Specifically, we observe that fairness can vary even more than accuracy with increasing training data size and different random initializations. At the same time, we find that little of the fairness variation is explained by model size, despite claims in the literature. To improve model fairness without retraining, we show that two post-processing methods developed for structured, tabular data can be successfully applied to a range of pretrained language models. Warning: This paper contains samples of offensive text.	fb1810f403288c40291fbe65fdfa4adf55c466e7	@['JournalArticle']{baldini-etal-2021-your,  author = {Ioana Baldini and Dennis Wei and K. Ramamurthy and Mikhail Yurochkin and Moninder Singh},  booktitle = {Findings},  pages = {2245-2262},  title = {Your fairness may vary: Pretrained language model fairness in toxic text classification},  year = {2021} }
You Can't Learn What's Not There: Self Supervised Learning and the Poverty of the Stimulus	2021	http://www.semanticscholar.org/paper/27f06ede6bdabca875c3a2ebb600094639bd5b49		maybe	0		27f06ede6bdabca875c3a2ebb600094639bd5b49	@['JournalArticle', 'Conference']{veres-sampson-2021-you,  author = {C. Veres and Jennifer Sampson},  booktitle = {International Conference on Applications of Natural Language to Data Bases},  pages = {3-14},  title = {You Can't Learn What's Not There: Self Supervised Learning and the Poverty of the Stimulus},  year = {2021} }
You Are What You Write: Preserving Privacy in the Era of Large Language Models	2022	http://www.semanticscholar.org/paper/597cad6c7b9de94eecc153c7cdcaf824905fe915	This paper presents the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender).	maybe	0	Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e.g. through adversarial attacks. We present an empirical investigation into the extent of the personal information encoded into pre-trained representations by a range of popular models, and we show a positive correlation between the complexity of a model, the amount of data used in pre-training, and data leakage. In this paper, we present the ﬁrst wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender). The results show since larger and more complex models are more prone to leaking private information, use of privacy-preserving methods is highly desirable. We also ﬁnd that highly privacy-preserving technologies like differential privacy (DP) can have serious model utility effects, which can be ameliorated using hybrid or metric-DP techniques.	597cad6c7b9de94eecc153c7cdcaf824905fe915	@['JournalArticle']{plant-etal-2022-you,  author = {Richard Plant and V. Giuffrida and Dimitra Gkatzia},  booktitle = {ArXiv},  journal = {ArXiv},  title = {You Are What You Write: Preserving Privacy in the Era of Large Language Models},  volume = {abs/2204.09391},  year = {2022} }
Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning	2022	http://www.semanticscholar.org/paper/2002afb402fb55bd42108e3cabffe2996bdf5e37	Y -Tuning is proposed, an efﬁcient yet effective paradigm to adapt frozen large-scale PTMs to speciﬂc downstream tasks and achieves performance more than 96% of full tuning on GLUE Benchmark with only 2% tunable parameters and much fewer training costs.	maybe	2	With the success of large-scale pre-trained models (PTMs), how efﬁciently adapting PTMs to downstream tasks has attracted tremendous attention, especially for PTMs with billions of parameters. Previous work focuses on designing parameter-efﬁcient tuning paradigms but needs to save and compute the gradient of the whole computational graph. In this paper, we propose Y -Tuning, an efﬁcient yet effective paradigm to adapt frozen large-scale PTMs to speciﬁc downstream tasks. Y -tuning learns dense representations for labels Y deﬁned in a given task and aligns them to ﬁxed feature representation. Without computing the gradients of text encoder at training phrase, Y -tuning is not only parameter-efﬁcient but also training-efﬁcient. Experimental results show that for DeBERTa XXL with 1.6 billion parameters, Y -tuning achieves performance more than 96% of full ﬁne-tuning on GLUE Benchmark with only 2% tunable parameters and much fewer training costs.	2002afb402fb55bd42108e3cabffe2996bdf5e37	@['JournalArticle']{liu-etal-2022-y,  author = {Yitao Liu and Chen An and Xipeng Qiu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Y-Tuning: An Efficient Tuning Paradigm for Large-Scale Pre-Trained Models via Label Representation Learning},  volume = {abs/2202.09817},  year = {2022} }
XplaiNLI: Explainable Natural Language Inference through Visual Analytics	2020	http://www.semanticscholar.org/paper/109e3f6c0860a8daf05f2263665f13a9a58998e9	XplaiNLI is proposed, an eXplainable, interactive, visualization interface that computes NLI with different methods and provides explanations for the decisions made by the different approaches.	maybe	2	Advances in Natural Language Inference (NLI) have helped us understand what state-of-the-art models really learn and what their generalization power is. Recent research has revealed some heuristics and biases of these models. However, to date, there is no systematic effort to capitalize on those insights through a system that uses these to explain the NLI decisions. To this end, we propose XplaiNLI, an eXplainable, interactive, visualization interface that computes NLI with different methods and provides explanations for the decisions made by the different approaches.	109e3f6c0860a8daf05f2263665f13a9a58998e9	@['JournalArticle', 'Conference']{kalouli-etal-2020-xplainli:,  author = {A. Kalouli and R. Sevastjanova and Valeria de Paiva and Dick Crouch and Mennatallah El-Assady},  booktitle = {International Conference on Computational Linguistics},  pages = {48-52},  title = {XplaiNLI: Explainable Natural Language Inference through Visual Analytics},  year = {2020} }
XAI for Transformers: Better Explanations through Conservative Propagation	2022	http://www.semanticscholar.org/paper/577a350ade92578913245e2d474aeabcb576e6d6	This proposal, which can be seen as a proper extension of the well-established LRP method to Transformers, is shown both theoretically and empirically to overcome the deﬁciency of a simple gradient-based approach, and achieves state-of-the-art explanation performance on a broad range of Transformer models and datasets.	maybe	7	Transformers have become an important workhorse of machine learning, with numerous applications. This necessitates the development of reliable methods for increasing their transparency. Multiple interpretability methods, often based on gradient information, have been proposed. We show that the gradient in a Transformer reﬂects the function only locally, and thus fails to reliably identify the contribution of input features to the prediction. We identify Attention Heads and LayerNorm as the main reasons for such unreliable explanations and propose a more stable way for propagation through these layers. Our proposal, which can be seen as a proper extension of the well-established LRP method to Transformers, is shown both theoretically and empirically to overcome the deﬁciency of a simple gradient-based approach, and achieves state-of-the-art explanation performance on a broad range of Transformer models and datasets.	577a350ade92578913245e2d474aeabcb576e6d6	@['JournalArticle', 'Conference']{ali-etal-2022-xai,  author = {Ameen Ali and Thomas Schnake and Oliver Eberle and G. Montavon and Klaus-Robert Müller and Lior Wolf},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {XAI for Transformers: Better Explanations through Conservative Propagation},  volume = {abs/2202.07304},  year = {2022} }
Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences	2020	http://www.semanticscholar.org/paper/1e26ff02f330fcf198bea8db0510d295c07ff908	A newly introduced problem concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations, framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform.	maybe	8	Understanding human preferences, along with cultural and social nuances, lives at the heart of natural language understanding. Concretely, we present a new task and corpus for learning alignments between machine and human preferences. Our newly introduced problem is concerned with predicting the preferable options from two sentences describing scenarios that may involve social and cultural situations. Our problem is framed as a natural language inference task with crowd-sourced preference votes by human players, obtained from a gamified voting platform. We benchmark several state-of-the-art neural models, along with BERT and friends on this task. Our experimental results show that current state-of-the-art NLP models still leave much room for improvement.	1e26ff02f330fcf198bea8db0510d295c07ff908	@['JournalArticle', 'Conference']{tay-etal-2020-would,  author = {Yi Tay and Donovan Ong and Jie Fu and Alvin Chan and Nancy Chen and A. Luu and C. Pal},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5369-5373},  title = {Would you Rather? A New Benchmark for Learning Machine Alignment with Cultural Values and Social Preferences},  year = {2020} }
Wordcraft: Story Writing With Large Language Models	2022	http://www.semanticscholar.org/paper/fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20	This work built Wordcraft, a text editor in which users collaborate with a generative language model to write a story, and shows that large language models enable novel co-writing experiences.	maybe	12	The latest generation of large neural language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work, we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a generative language model to write a story. We evaluated Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences. For example, the language model is able to engage in open-ended conversation about the story, respond to writers’ custom requests expressed in natural language (such as ”rewrite this text to be more Dickensian”), and generate suggestions that serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.	fb5c11bbf63884f75d2da615fbf37a3bcfa2bd20	@['JournalArticle', 'Book', 'Conference']{yuan-etal-2022-wordcraft:,  author = {Ann Yuan and Andy Coenen and Emily Reif and Daphne Ippolito},  booktitle = {International Conference on Intelligent User Interfaces},  journal = {27th International Conference on Intelligent User Interfaces},  title = {Wordcraft: Story Writing With Large Language Models},  year = {2022} }
Word-order Typology in Multilingual BERT: A Case Study in Subordinate-Clause Detection	2022	http://www.semanticscholar.org/paper/cd48780a4bb7513e6b81cf3bb993b4f4fb962e13		maybe	0	The capabilities and limitations of BERT and similar models are still unclear when it comes to learning syntactic abstractions, in particular across languages. In this paper, we use the task of subordinate-clause detection within and across languages to probe these properties. We show that this task is deceptively simple, with easy gains offset by a long tail of harder cases, and that BERT’s zero-shot performance is dominated by word-order effects, mirroring the SVO/VSO/SOV typology.	cd48780a4bb7513e6b81cf3bb993b4f4fb962e13	@['JournalArticle']{nikolaev-padó-2022-word,  author = {Dmitry Nikolaev and Sebastian Padó},  booktitle = {SIGTYP},  journal = {ArXiv},  title = {Word-order Typology in Multilingual BERT: A Case Study in Subordinate-Clause Detection},  volume = {abs/2205.11987},  year = {2022} }
Word Sense Distance in Human Similarity Judgements and Contextualised Word Embeddings	2020	http://www.semanticscholar.org/paper/95c664af5ec33be07cba1031b331841acfc69144	A recent model of polyseme sense clustering proposed by Ortega-Andres & Vicente (2019) is investigated through analysing empirical evidence of word sense grouping in human similarity judgements and the evaluation of context-sensitive word embedding systems is extended.	maybe	5	Homonymy is often used to showcase one of the advantages of context-sensitive word embedding techniques such as ELMo and BERT. In this paper we want to shift the focus to the related but less exhaustively explored phenomenon of polysemy, where a word expresses various distinct but related senses in different contexts. Specifically, we aim to i) investigate a recent model of polyseme sense clustering proposed by Ortega-Andres & Vicente (2019) through analysing empirical evidence of word sense grouping in human similarity judgements, ii) extend the evaluation of context-sensitive word embedding systems by examining whether they encode differences in word sense similarity and iii) compare the word sense similarities of both methods to assess their correlation and gain some intuition as to how well contextualised word embeddings could be used as surrogate word sense similarity judgements in linguistic experiments.	95c664af5ec33be07cba1031b331841acfc69144	@['Conference']{haber-poesio-2020-word,  author = {J. Haber and Massimo Poesio},  booktitle = {Passive and Active Network Measurement Conference},  title = {Word Sense Distance in Human Similarity Judgements and Contextualised Word Embeddings},  year = {2020} }
Word Order Matters when you Increase Masking	2022	http://www.semanticscholar.org/paper/ca0f43ae0e80deeac39b71822e63928b56031132	It is found that the ne-cessity of position information increases with the amount of masking, and that masked language models without position encodings are not able to reconstruct this information on the task.	maybe	0	Word order, an essential property of natural languages, is injected in Transformer-based neural language models using position encoding. However, recent experiments have shown that explicit position encoding is not always useful, since some models without such feature managed to achieve state-of-the art performance on some tasks. To understand better this phenomenon, we examine the effect of removing position encodings on the pre-training objective itself (i.e., masked language modelling), to test whether models can reconstruct position information from co-occurrences alone. We do so by controlling the amount of masked tokens in the input sentence, as a proxy to affect the importance of position information for the task. We find that the necessity of position information increases with the amount of masking, and that masked language models without position encodings are not able to reconstruct this information on the task. These findings point towards a direct relationship between the amount of masking and the ability of Transformers to capture order-sensitive aspects of language using position encoding.	ca0f43ae0e80deeac39b71822e63928b56031132	@['JournalArticle', 'Conference']{lasri-etal-2022-word,  author = {Karim Lasri and Alessandro Lenci and T. Poibeau},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Word Order Matters When You Increase Masking},  volume = {abs/2211.04427},  year = {2022} }
Word Order Does Matter and Shuffled Language Models Know It	2022	http://www.semanticscholar.org/paper/d2c730502ab895c18bb241261f1efc4268008767	Surprisingly, even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities.	maybe	6	Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models’ good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain a notion of word order information. We show this is in part due to a subtlety in how shuffling is implemented in previous work – before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.	d2c730502ab895c18bb241261f1efc4268008767	@['JournalArticle', 'Conference']{ravishankar-etal-2022-word,  author = {Vinit Ravishankar and Mostafa Abdou and Artur Kulmizev and Anders Søgaard},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {6907-6919},  title = {Word Order Does Matter and Shuffled Language Models Know It},  year = {2022} }
Word meaning in minds and machines	2020	http://www.semanticscholar.org/paper/19f4404f640f850521cc97bc75ddbe8f482fc85e	It is argued that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects, and that they will be more successful, with a more human-like, conceptual basis for word meaning.	maybe	40	Machines have achieved a broad and growing set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Psychologists have shown increasing interest in such models, comparing their output to psychological judgments such as similarity, association, priming, and comprehension, raising the question of whether the models could serve as psychological theories. In this article, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are fairly successful models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people express through words. Word meanings must also be grounded in perception and action and be capable of flexible combinations in ways that current systems are not. We discuss promising approaches to grounding NLP systems and argue that they will be more successful, with a more human-like, conceptual basis for word meaning. (PsycInfo Database Record (c) 2021 APA, all rights reserved).	19f4404f640f850521cc97bc75ddbe8f482fc85e	@['JournalArticle']{lake-murphy-2020-word,  author = {B. Lake and G. Murphy},  booktitle = {Psychology Review},  journal = {Psychological review},  title = {Word meaning in minds and machines},  year = {2020} }
Word Frequency Does Not Predict Grammatical Knowledge in Language Models	2020	http://www.semanticscholar.org/paper/fcae937363a9e0804d8a255401deb55483c89677	Focusing on subject-verb agreement and reflexive anaphora, it is found that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models.	maybe	5	Neural language models learn, to varying degrees of accuracy, the grammatical properties of natural languages. In this work, we investigate whether there are systematic sources of variation in the language models’ accuracy. Focusing on subject-verb agreement and reflexive anaphora, we find that certain nouns are systematically understood better than others, an effect which is robust across grammatical tasks and different language models. Surprisingly, we find that across four orders of magnitude, corpus frequency is unrelated to a noun’s performance on grammatical tasks. Finally, we find that a novel noun’s grammatical properties can be few-shot learned from various types of training data. The results present a paradox: there should be less variation in grammatical performance than is actually observed.	fcae937363a9e0804d8a255401deb55483c89677	@['JournalArticle', 'Conference']{yu-etal-2020-word,  author = {Charles Yu and Ryan Sie and N. Tedeschi and Leon Bergen},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Word Frequency Does Not Predict Grammatical Knowledge in Language Models},  volume = {abs/2010.13870},  year = {2020} }
Word Acquisition in Neural Language Models	2021	http://www.semanticscholar.org/paper/18e736f814c6d64a27dc9ff088fb288bcac39002	It is found that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition.	maybe	8	We investigate how neural language models acquire individual words during training, extracting learning curves and ages of acquisition for over 600 words on the MacArthur-Bates Communicative Development Inventory (Fenson et al., 2007). Drawing on studies of word acquisition in children, we evaluate multiple predictors for words’ ages of acquisition in LSTMs, BERT, and GPT-2. We find that the effects of concreteness, word length, and lexical class are pointedly different in children and language models, reinforcing the importance of interaction and sensorimotor experience in child language acquisition. Language models rely far more on word frequency than children, but, like children, they exhibit slower learning of words in longer utterances. Interestingly, models follow consistent patterns during training for both unidirectional and bidirectional models, and for both LSTM and Transformer architectures. Models predict based on unigram token frequencies early in training, before transitioning loosely to bigram probabilities, eventually converging on more nuanced predictions. These results shed light on the role of distributional learning mechanisms in children, while also providing insights for more human-like language acquisition in language models.	18e736f814c6d64a27dc9ff088fb288bcac39002	@['JournalArticle']{chang-bergen-2021-word,  author = {Tyler A. Chang and B. Bergen},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1-16},  title = {Word Acquisition in Neural Language Models},  volume = {10},  year = {2021} }
Will You Find These Shortcuts? A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification	2021	http://www.semanticscholar.org/paper/1818e24137383ece9b2f379a63674b2632e16d04	This work proposes a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking and does an in-depth analysis of four standard salience method classes on a range of datasets and lexical shortcuts for BERT and LSTM models.	maybe	19	Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model’s prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared.Focusing on text classification and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and lexical shortcuts for BERT and LSTM models. We demonstrate that some of the most popular method configurations provide poor results even for simple shortcuts while a method judged to be too simplistic works remarkably well for BERT.	1818e24137383ece9b2f379a63674b2632e16d04	@['JournalArticle', 'Conference']{bastings-etal-2021-“will,  author = {Jasmijn Bastings and Sebastian Ebert and Polina Zablotskaia and Anders Sandholm and Katja Filippova},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {“Will You Find These Shortcuts?” A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification},  volume = {abs/2111.07367},  year = {2021} }
WhyGen: Explaining ML-powered Code Generation by Referring to Training Examples	2022	http://www.semanticscholar.org/paper/0c6c5e0fb38c28e50909cd5e165737636abf804b	This work introduces a tool, named WhyGen, to explain the generated code by referring to training examples, and introduces a data structure, named inference fingerprint, to represent the decision process of the model when generating a prediction.	maybe	1	Deep learning has demonstrated great abilities in various code generation tasks. However, despite the great convenience for some developers, many are concerned that the code generators may recite or closely mimic copyrighted training data without user awareness, leading to legal and ethical concerns. To ease this problem, we introduce a tool, named WhyGen, to explain the generated code by referring to training examples. Specifically, we first introduce a data structure, named inference fingerprint, to represent the decision process of the model when generating a prediction. The fingerprints of all training examples are collected offline and saved to a database. When the model is used at runtime for code generation, the most relevant training examples can be retrieved by querying the fingerprint database. Our experiments have shown that WhyGen is able to precisely notify the users about possible recitations and highly similar imitations with a top-10 accuracy of 81.21%. The demo video can be found at https://youtu.be/EtoQP6850To.	0c6c5e0fb38c28e50909cd5e165737636abf804b	@['Book', 'JournalArticle', 'Conference']{yan-li-2022-whygen:,  author = {Weixiang Yan and Yuanchun Li},  booktitle = {2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},  journal = {2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)},  pages = {237-241},  title = {WhyGen: Explaining ML-powered Code Generation by Referring to Training Examples},  year = {2022} }
Why Machine Reading Comprehension Models Learn Shortcuts?	2021	http://www.semanticscholar.org/paper/476afc913d63f3ba1882f6419f718984379b2380	It is argued that larger proportion of shortcut questions in training data make models rely on shortcut tricks excessively, and two new methods are proposed to quantitatively analyze the learning difficulty regarding shortcut and challenging questions, and revealing the inherent learning mechanism behind the different performance between the two kinds of questions.	maybe	15	Recent studies report that many machine reading comprehension (MRC) models can perform closely to or even better than humans on benchmark datasets. However, existing works indicate that many MRC models may learn shortcuts to outwit these benchmarks, but the performance is unsatisfactory in real-world applications. In this work, we attempt to explore, instead of the expected comprehension skills, why these models learn the shortcuts. Based on the observation that a large portion of questions in current datasets have shortcut solutions, we argue that larger proportion of shortcut questions in training data make models rely on shortcut tricks excessively. To investigate this hypothesis, we carefully design two synthetic datasets with annotations that indicate whether a question can be answered using shortcut solutions. We further propose two new methods to quantitatively analyze the learning difficulty regarding shortcut and challenging questions, and revealing the inherent learning mechanism behind the different performance between the two kinds of questions. A thorough empirical analysis shows that MRC models tend to learn shortcut questions earlier than challenging questions, and the high proportions of shortcut questions in training sets hinder models from exploring the sophisticated reasoning skills in the later stage of training.	476afc913d63f3ba1882f6419f718984379b2380	@['JournalArticle']{lai-etal-2021-why,  author = {Yuxuan Lai and Chen Zhang and Yansong Feng and Quzhe Huang and Dongyan Zhao},  booktitle = {Findings},  pages = {989-1002},  title = {Why Machine Reading Comprehension Models Learn Shortcuts?},  year = {2021} }
Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?	2022	http://www.semanticscholar.org/paper/eaee0b647d336c6fc8b844812675ec35cddf14a1		maybe	0	This work presents a detailed linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and ﬁt to reading times for the more recently released ﬁve GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to ‘memo-rize’ sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.	eaee0b647d336c6fc8b844812675ec35cddf14a1	@['JournalArticle']{oh-schuler-2022-why,  author = {Byung-Doh Oh and William Schuler},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Why Does Surprisal From Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?},  volume = {abs/2212.12131},  year = {2022} }
Why Do Neural Language Models Still Need Commonsense Knowledge to Handle Semantic Variations in Question Answering?	2022	http://www.semanticscholar.org/paper/3b44755822ab3ee23e0802f34f3ef5debdc6f0c8	The possibility to overcome the limitations of the MNLM-based RC models by enriching text with the required knowledge from an external commonsense knowledge repository in controlled experiments is exemplified.	maybe	0	Many contextualized word representations are now learned by intricate neural network models, such as masked neural language models (MNLMs) which are made up of huge neural network structures and trained to restore the masked text. Such representations demonstrate superhuman performance in some reading comprehension (RC) tasks which extract a proper answer in the context given a question. However, identifying the detailed knowledge trained in MNLMs is challenging owing to numerous and intermingled model parameters. This paper provides new insights and empirical analyses on commonsense knowledge included in pretrained MNLMs. First, we use a diagnostic test that evaluates whether commonsense knowledge is properly trained in MNLMs. We observe that a large proportion of commonsense knowledge is not appropriately trained in MNLMs and MNLMs do not often understand the semantic meaning of relations accurately. In addition, we ﬁnd that the MNLM-based RC models are still vulnerable to semantic variations that require commonsense knowledge. Finally, we discover the fundamental reason why some knowledge is not trained. We further suggest that utilizing an external commonsense knowledge repository can be an effective solution. We exemplify the possibility to overcome the limitations of the MNLM-based RC models by enriching text with the required knowledge from an external commonsense knowledge repository in controlled experiments.	3b44755822ab3ee23e0802f34f3ef5debdc6f0c8	@['JournalArticle']{kwon-etal-2022-why,  author = {Sunjae Kwon and Cheongwoong Kang and Jiyeon Han and Jaesik Choi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Why Do Neural Language Models Still Need Commonsense Knowledge to Handle Semantic Variations in Question Answering?},  volume = {abs/2209.00599},  year = {2022} }
Why Do Masked Neural Language Models Still Need Common Sense Knowledge?	2019	http://www.semanticscholar.org/paper/e9062386b2f56b9b5bb37b3dda7855481b6bea6b	A test that measures what types of common sense knowledge do pretrained MNLMs understand is proposed and it is experimentally demonstrated that existing MNLM-based models can be elevated by combining knowledge from an external common sense repository.	maybe	12	Currently, contextualized word representations are learned by intricate neural network models, such as masked neural language models (MNLMs). The new representations significantly enhanced the performance in automated question answering by reading paragraphs. However, identifying the detailed knowledge trained in the MNLMs is difficult owing to numerous and intermingled parameters. This paper provides empirical but insightful analyses on the pretrained MNLMs with respect to common sense knowledge. First, we propose a test that measures what types of common sense knowledge do pretrained MNLMs understand. From the test, we observed that MNLMs partially understand various types of common sense knowledge but do not accurately understand the semantic meaning of relations. In addition, based on the difficulty of the question-answering task problems, we observed that pretrained MLM-based models are still vulnerable to problems that require common sense knowledge. We also experimentally demonstrated that we can elevate existing MNLM-based models by combining knowledge from an external common sense repository.	e9062386b2f56b9b5bb37b3dda7855481b6bea6b	@['JournalArticle']{kwon-etal-2019-why,  author = {Sunjae Kwon and Cheongwoong Kang and Jiyeon Han and Jaesik Choi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Why Do Masked Neural Language Models Still Need Common Sense Knowledge?},  volume = {abs/1911.03024},  year = {2019} }
Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers	2022	https://www.semanticscholar.org/paper/482b42574aa2108ab8d847bd1aa26d246f51ec48	Inspired by the understanding of meta-optimization, a momentum-based attention is designed by analogy with the momentum- based gradient descent algorithm and its consistently better performance over vanilla attention supports the understanding from another aspect.	maybe	3	Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit ﬁnetuning. Theoretically, we ﬁgure out that the Transformer attention has a dual form of gradient descent based optimization. On top of it, we understand ICL as follows: GPT ﬁrst produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of ICL and explicit ﬁnetuning based on real tasks to provide empirical evidence that supports our understanding. The results prove that ICL behaves similarly to explicit ﬁnetuning at the prediction level, the representation level, and the attention behavior level. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention by analogy with the momentum-based gradient descent algorithm. Its consistently better performance over vanilla attention supports our understanding again from another aspect, and more impor-tantly, it shows the potential to utilize our understanding for future model designing.	482b42574aa2108ab8d847bd1aa26d246f51ec48	@['JournalArticle']{dai-etal-2022-why,  author = {Damai Dai and Yutao Sun and Li Dong and Y. Hao and Zhifang Sui and Furu Wei},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},  volume = {abs/2212.10559},  year = {2022} }
Why Attentions May Not Be Interpretable?	2020	http://www.semanticscholar.org/paper/5a5cb1d36d3ab64fc1941d05ec4387c4e62feac7	It is demonstrated that one root cause of this phenomenon is the combinatorial shortcuts, which means that, in addition to the highlighted parts, the attention weights themselves may carry extra information that could be utilized by downstream models after attention layers.	yes	12	Attention-based methods have played important roles in model interpretations, where the calculated attention weights are expected to highlight the critical parts of inputs (e.g., keywords in sentences). However, recent research found that attention-as-importance interpretations often do not work as we expected. For example, learned attention weights sometimes highlight less meaningful tokens like "[SEP]", ",", and ".", and are frequently uncorrelated with other feature importance indicators like gradient-based measures. A recent debate over whether attention is an explanation or not has drawn considerable interest. In this paper, we demonstrate that one root cause of this phenomenon is the combinatorial shortcuts, which means that, in addition to the highlighted parts, the attention weights themselves may carry extra information that could be utilized by downstream models after attention layers. As a result, the attention weights are no longer pure importance indicators. We theoretically analyze combinatorial shortcuts, design one intuitive experiment to show their existence, and propose two methods to mitigate this issue. We conduct empirical studies on attention-based interpretation models. The results show that the proposed methods can effectively improve the interpretability of attention mechanisms.	5a5cb1d36d3ab64fc1941d05ec4387c4e62feac7	@['JournalArticle', 'Book', 'Conference']{bai-etal-2020-why,  author = {Bing Bai and Jian Liang and Guan Zhang and Hao Li and Kun Bai and Fei Wang},  booktitle = {Knowledge Discovery and Data Mining},  journal = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining},  title = {Why Attentions May Not Be Interpretable?},  year = {2020} }
Why Attention is Not Explanation: Surgical Intervention and Causal Reasoning about Neural Models	2020	http://www.semanticscholar.org/paper/973b72dc176244194ee4fb426ba25e2a6a588b22	The state-of-the-art in explanation for neural models for NLP tasks is studied from the viewpoint of philosophy of science, and the impossibility of causal explanations from attention layers over text data is asserted.	maybe	26	As the demand for explainable deep learning grows in the evaluation of language technologies, the value of a principled grounding for those explanations grows as well. Here we study the state-of-the-art in explanation for neural models for NLP tasks from the viewpoint of philosophy of science. We focus on recent evaluation work that finds brittleness in explanations obtained through attention mechanisms. We harness philosophical accounts of explanation to suggest broader conclusions from these studies. From this analysis, we assert the impossibility of causal explanations from attention layers over text data. We then introduce NLP researchers to contemporary philosophy of science theories that allow robust yet non-causal reasoning in explanation, giving computer scientists a vocabulary for future research.	973b72dc176244194ee4fb426ba25e2a6a588b22	@['JournalArticle']{grimsley-etal-2020-why,  author = {Christopher Grimsley and Elijah Mayfield and Julia R. S. Bursten},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {1780-1790},  title = {Why Attention is Not Explanation: Surgical Intervention and Causal Reasoning about Neural Models},  year = {2020} }
Who’s on First?: Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains	2021	http://www.semanticscholar.org/paper/6d8943bd6edf34b022862be286cfd11fff16c89d	It is shown that language models of three different architectures can answer questions about world states using only verb-like encodings of activity, which is extensible to new language models and additional question-answering tasks.	maybe	1	The capabilities of today’s natural language processing systems are typically evaluated using large datasets of curated questions and answers. While these are critical benchmarks of progress, they also suffer from weakness due to artificial distributions and incomplete knowledge. Artifacts arising from artificial distributions can overstate language model performance, while incomplete knowledge limits fine-grained analysis. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces (SPLAT). SPLATs are corpora of language encodings of activity in some closed domain (we study traces from chess and baseball games in this work). SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains. We show that language models of three different architectures can answer questions about world states using only verb-like encodings of activity. Our approach is extensible to new language models and additional question-answering tasks.	6d8943bd6edf34b022862be286cfd11fff16c89d	@['JournalArticle']{demeter-downey-2021-who’s,  author = {David Demeter and Doug Downey},  booktitle = {Conference on Computational Natural Language Learning},  pages = {210-222},  title = {Who’s on First?: Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains},  year = {2021} }
Who is GPT-3? An Exploration of Personality, Values and Demographics	2022	https://www.semanticscholar.org/paper/73b54046259f84a0c4e9409714d6a71c41981b57		maybe	0	Language models such as GPT-3 have caused a furore in the research community. Some studies found that GPT-3 has some creative abilities and makes mistakes that are on par with human behaviour. This paper answers a related question: Who is GPT-3? We administered two validated measurement tools to GPT-3 to assess its personality, the values it holds and its self-reported demographics. Our results show that GPT-3 scores similarly to human samples in terms of personality and - when provided with a model response memory - in terms of the values it holds. We provide the first evidence of psychological assessment of the GPT-3 model and thereby add to our understanding of this language model. We close with suggestions for future research that moves social science closer to language models and vice versa.	73b54046259f84a0c4e9409714d6a71c41981b57	@['JournalArticle']{miotto-etal-2022-who,  author = {Marilu Miotto and Nicola Rossberg and Bennett Kleinberg},  booktitle = {NLPCSS},  journal = {ArXiv},  title = {Who is GPT-3? An exploration of personality, values and demographics},  volume = {abs/2209.14338},  year = {2022} }
Who did what to Whom? Language models and humans respond diversely to features affecting argument hierarchy construction	2022	http://www.semanticscholar.org/paper/3e6a9d19604b15ddb64f6be36e6e4bd62950edfe		maybe	0	Pre-trained transformer-based language models have achieved state-of-the-art performance in many areas of NLP. It is still an open question whether the models are capable of integrating syntax and semantics in language processing like humans. This paper investigates if models and humans construct argument hierarchy similarly with the effects from telicity, agency, and individuation, using the Chinese structure “NP1+BA/BEI+NP2+VP”. We present both humans and six transformer-based models with prepared sentences and analyze their preference between BA (view NP1 as an agent) and BEI (NP2 as an agent). It is found that the models and humans respond to (non-)agentive features in telic context and atelic feature very similarly. However, the models show insufficient sensitivity to both pragmatic function in expressing undesirable events and different individuation degrees represented by human common nouns vs. proper names. By contrast, humans rely heavily on these cues to establish the thematic relation between two arguments NP1 and NP2. Furthermore, the models tend to interpret the subject as an agent, which is not the case for humans who align agents independently of subject position in Mandarin Chinese.	3e6a9d19604b15ddb64f6be36e6e4bd62950edfe	@['JournalArticle']{xu-chen-2022-who,  author = {Xiaonan Xu and Haoshuo Chen},  booktitle = {AACL},  pages = {254-265},  title = {Who did what to Whom? Language models and humans respond diversely to features affecting argument hierarchy construction},  year = {2022} }
Which Shortcut Solution Do Question Answering Models Prefer to Learn?	2022	http://www.semanticscholar.org/paper/ad7c788824aba70deb141617410dd70b390c4ae8	It is claimed that the learnability of shortcuts should be considered when designing mitigation methods, and the more learnable a shortcut is, the smaller the proportion of anti-shortcut examples required to achieve comparable per- formance on shortcut and anti- shortcut examples.	maybe	0	Question answering (QA) models for reading comprehension tend to learn shortcut solutions rather than the solutions intended by QA datasets. QA models that have learned short- cut solutions can achieve human-level performance in shortcut examples where shortcuts are valid, but these same be- haviors degrade generalization potential on anti-shortcut examples where shortcuts are invalid. Various methods have been proposed to mitigate this problem, but they do not fully take the characteristics of shortcuts themselves into account. We assume that the learnability of shortcuts, i.e., how easy it is to learn a shortcut, is useful to mitigate the problem. Thus, we ﬁrst examine the learnability of the representative shortcuts on extractive and multiple-choice QA datasets. Be- havioral tests using biased training sets reveal that shortcuts that exploit answer positions and word-label correlations are preferentially learned for extractive and multiple-choice QA, respectively. We ﬁnd that the more learnable a shortcut is, the ﬂatter and deeper the loss landscape is around the shortcut solution in the parameter space. We also ﬁnd that the availability of the preferred shortcuts tends to make the task easier to perform from an information-theoretic viewpoint. Lastly, we experimentally show that the learnability of shortcuts can be utilized to construct an effective QA training set; the more learnable a shortcut is, the smaller the proportion of anti-shortcut examples required to achieve comparable per- formance on shortcut and anti-shortcut examples. We claim that the learnability of shortcuts should be considered when designing mitigation methods.	ad7c788824aba70deb141617410dd70b390c4ae8	@['JournalArticle']{shinoda-etal-2022-which,  author = {Kazutoshi Shinoda and Saku Sugawara and Akiko Aizawa},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Which Shortcut Solution Do Question Answering Models Prefer to Learn?},  volume = {abs/2211.16220},  year = {2022} }
Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers	2022	http://www.semanticscholar.org/paper/0ad58485bb1b0ff3cdcf0d2a087ce8c40de529e8	This work proposes an alternative method that asks where the task-relevant information emerges in the model, consisting of a family of metrics that explicitly model local information gain relative to the previous layer and each layer's contribution to the model’s overall performance.	maybe	0	Probing studies have extensively explored where in neural language models linguistic information is located. The standard approach to interpreting the results of a probing classifier is to focus on the layers whose representations give the highest performance on the probing task. We propose an alternative method that asks where the task-relevant information emerges in the model. Our framework consists of a family of metrics that explicitly model local information gain relative to the previous layer and each layer’s contribution to the model’s overall performance. We apply the new metrics to two pairs of syntactic probing tasks with different degrees of complexity and find that the metrics confirm the expected ordering only for one of the pairs. Our local metrics show a massive dominance of the first layers, indicating that the features that contribute the most to our probing tasks are not as high-level as global metrics suggest.	0ad58485bb1b0ff3cdcf0d2a087ce8c40de529e8	@['JournalArticle', 'Conference']{kunz-kuhlmann-2022-where,  author = {Jenny Kunz and Marco Kuhlmann},  booktitle = {International Conference on Computational Linguistics},  pages = {4664-4676},  title = {Where Does Linguistic Information Emerge in Neural Language Models? Measuring Gains and Contributions across Layers},  year = {2022} }
When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment	2022	http://www.semanticscholar.org/paper/1c1ca2392155ddf30408a442e6b504b5d60d4f2a	This paper presents a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule- Breaking – inspired by recent moral psychology studies and proposes a novel moral chain of thought prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments.	maybe	1	AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the ﬂexibility of the human moral mind — the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of rule-breaking question answering (RBQA) of cases that involve potentially permissible rule-breaking – inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (M ORAL C O T) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. M ORAL C O T outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the ﬂexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using RBQA. 1	1c1ca2392155ddf30408a442e6b504b5d60d4f2a	@['JournalArticle']{jin-etal-2022-when,  author = {Zhijing Jin and Sydney Levine and Fernando Gonzalez and Ojasv Kamal and Maarten Sap and Mrinmaya Sachan and Rada Mihalcea and J. Tenenbaum and B. Schölkopf},  booktitle = {ArXiv},  journal = {ArXiv},  title = {When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment},  volume = {abs/2210.01478},  year = {2022} }
When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories	2022	http://www.semanticscholar.org/paper/7b0f98f51040700aae3cd9f0e3432dedcd69fb30	It is found that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail, and a simple, yet effective, method for powerful andcient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary is devised.	maybe	0	Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs’ strengths and limitations in memorizing factual knowledge, by con-ducting large-scale knowledge probing experiments of 10 models and 4 augmentation methods on P OP QA, our new open-domain QA dataset with 14k questions. We ﬁnd that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those ﬁndings, we devise a simple, yet effective, method for powerful and efﬁcient retrieval-augmented LMs, which retrieves non-parametric memories only when necessary. Experimental results show that this signiﬁcantly improves models’ performance while reducing the inference costs. 1	7b0f98f51040700aae3cd9f0e3432dedcd69fb30	@['JournalArticle']{mallen-etal-2022-when,  author = {Alex Mallen and Akari Asai and Victor Zhong and R. Das and Hannaneh Hajishirzi and Daniel Khashabi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories},  volume = {abs/2212.10511},  year = {2022} }
When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems	2022	http://www.semanticscholar.org/paper/1a5fcd44ebba0aaecc0397b26957fcc5e5476033	This work reveals a troubling quirk in building (broad-coverage) NLU systems: as the training dataset grows, more data is needed to learn new symbols, forming a vicious cycle that is closely associated with an effect the authors call source signal dilution.	maybe	1	In natural language understanding (NLU) production systems, users’ evolving needs necessitate the addition of new features over time, indexed by new symbols added to the meaning representation space. This requires additional training data and results in ever-growing datasets. We present the first systematic investigation into this incremental symbol learning scenario. Our analysis reveals a troubling quirk in building broad-coverage NLU systems: as the training dataset grows, performance on a small set of new symbols often decreases. We show that this trend holds for multiple mainstream models on two common NLU tasks: intent recognition and semantic parsing. Rejecting class imbalance as the sole culprit, we reveal that the trend is closely associated with an effect we call source signal dilution, where strong lexical cues for the new symbol become diluted as the training dataset grows. Selectively dropping training examples to prevent dilution often reverses the trend, showing the over-reliance of mainstream neural NLU models on simple lexical cues.	1a5fcd44ebba0aaecc0397b26957fcc5e5476033	@['JournalArticle', 'Conference']{eskin-etal-2022-when,  author = {Elias Stengel-Eskin and Emmanouil Antonios Platanios and Adam Pauls and Sam Thomson and Hao Fang and Benjamin Van Durme and J. Eisner and Yu Su},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {When More Data Hurts: A Troubling Quirk in Developing Broad-Coverage Natural Language Understanding Systems},  volume = {abs/2205.12228},  year = {2022} }
When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes	2022	http://www.semanticscholar.org/paper/2b144c53f24531170f4d3f902ec094b2db144fee	It is demonstrated that models do encode syntactic information redundantly and a new probe design is introduced that guides probes to consider all syntactic Information present in embeddings, allowing for the use of syntax in models where prior methods did not.	yes	2	Recent causal probing literature reveals when language models and syntactic probes use similar representations. Such techniques may yield “false negative” causality results: models may use representations of syntax, but probes may have learned to use redundant encodings of the same syntactic information. We demonstrate that models do encode syntactic information redundantly and introduce a new probe design that guides probes to consider all syntactic information present in embeddings. Using these probes, we find evidence for the use of syntax in models where prior methods did not, allowing us to boost model performance by injecting syntactic information into representations.	2b144c53f24531170f4d3f902ec094b2db144fee	@['JournalArticle', 'Conference']{tucker-etal-2022-when,  author = {Mycal Tucker and Tiwalayo Eisape and Peng Qian and R. Levy and Julie Shah},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {5393-5408},  title = {When Does Syntax Mediate Neural Language Model Performance? Evidence from Dropout Probes},  year = {2022} }
When Do You Need Billions of Words of Pretraining Data?	2020	http://www.semanticscholar.org/paper/31392ad8722d9c66181b621936e2013199e02edc	While the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.	yes	46	NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models.	31392ad8722d9c66181b621936e2013199e02edc	@['JournalArticle', 'Conference']{zhang-etal-2020-when,  author = {Yian Zhang and Alex Warstadt and Haau-Sing Li and Samuel R. Bowman},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1112-1125},  title = {When Do You Need Billions of Words of Pretraining Data?},  year = {2020} }
When Combating Hype, Proceed with Caution	2021	http://www.semanticscholar.org/paper/1b94ebedacda0c21a4b8a40a5a40afcea4cc719a	This paper urges researchers to be careful about false claims about the capabilities of state-of-the-art language technology and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.	maybe	7	In an effort to avoid reinforcing widespread hype about the capabilities of state-of-the-art language technology, researchers have developed practices in framing and citation that serve to deemphasize the field’s successes. Though well-meaning, these practices often yield misleading or even false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It limits our ability to mitigate shortterm harms from NLP deployments and it limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.	1b94ebedacda0c21a4b8a40a5a40afcea4cc719a	@['JournalArticle']{bowman-2021-when,  author = {Samuel R. Bowman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {When Combating Hype, Proceed with Caution},  volume = {abs/2110.08300},  year = {2021} }
When classifying arguments, BERT doesn’t care about word order...except when it matters	2022	http://www.semanticscholar.org/paper/d0bc90a811318c0f253dbbdea9870a984f4f8fbe		maybe	2	While contextual embedding models are often praised for capturing rich grammatical structure, a spate of recent work has shown that they are surprisingly invariant to scrambling word order (Sinha et al., 2021; Hessel and Schofield, 2021; Pham et al., 2020; Gupta et al., 2021; O’Connor and Andreas, 2021) and that grammatical knowledge like part of speech, often attributed to contextual embeddings, is actually also captured by fixed embeddings (Pimentel et al., 2020). These results point to a puzzle: how can syntactic contextual information be important for language understanding when the words themselves, not their order, are what matter? We argue that this apparent paradox arises because of the redundant structure of language itself. Lexical distributional information alone captures a great deal of meaning (Erk, 2012; Mitchell and Lapata, 2010), and the local coherence of words is crucial for constructing meaning in both humans (Mollica et al., 2020) and machines (Clouatre et al., 2021). Viewing this redundancy from the perspective of grammatical role (whether a noun is the subject or the object of a clause), most clauses are prototypical: in a sentence like “the chef cut the onion”, the grammatical roles of chef and onion are clear to humans from the words alone, without word order or context (Futrell et al., 2019, experiments in English and Russian). This means syntactic word order is redundant with lexical semantics. Whether handconstructed or corpus-based, most studies probing contextual representations have used prototypical sentences as input, where syntactic context does not have much information to contribute to core meaning beyond the words themselves. Yet human language can use syntax to deviate from the expectations generated by lexical items alone: we can also understand the absurd meaning of a rare non-prototypical sentence like “The 0.00 0.25 0.50 0.75 1.00	d0bc90a811318c0f253dbbdea9870a984f4f8fbe	@None{papadimitriou-mahowald-2022-when,  author = {Isabel Papadimitriou and Kyle Mahowald},  booktitle = {SCIL},  title = {When classifying arguments, BERT doesn’t care about word order...except when it matters},  year = {2022} }
When Choosing Plausible Alternatives, Clever Hans can be Clever	2019	http://www.semanticscholar.org/paper/a2f5f74a6730b67675543ed89420f14cc234dd72	Balanced COPA, an extension of COPA that does not suffer from easy-to-exploit single token cues, is introduced, finding that BERT relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that Bert learns the task to a certain degree when forced to.	maybe	19	Pretrained language models, such as BERT and RoBERTa, have shown large improvements in the commonsense reasoning benchmark COPA. However, recent work found that many improvements in benchmarks of natural language understanding are not due to models learning the task, but due to their increasing ability to exploit superficial cues, such as tokens that occur more often in the correct answer than the wrong one. Are BERT’s and RoBERTa’s good performance on COPA also caused by this? We find superficial cues in COPA, as well as evidence that BERT exploits these cues.To remedy this problem, we introduce Balanced COPA, an extension of COPA that does not suffer from easy-to-exploit single token cues. We analyze BERT’s and RoBERTa’s performance on original and Balanced COPA, finding that BERT relies on superficial cues when they are present, but still achieves comparable performance once they are made ineffective, suggesting that BERT learns the task to a certain degree when forced to. In contrast, RoBERTa does not appear to rely on superficial cues.	a2f5f74a6730b67675543ed89420f14cc234dd72	@['JournalArticle', 'Conference']{kavumba-etal-2019-when,  author = {Pride Kavumba and Naoya Inoue and Benjamin Heinzerling and Keshav Singh and Paul Reisert and Kentaro Inui},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {When Choosing Plausible Alternatives, Clever Hans can be Clever},  volume = {abs/1911.00225},  year = {2019} }
When Can Self-Attention Be Replaced by Feed Forward Layers?	2020	http://www.semanticscholar.org/paper/f64301c8e2f37299a9a18460976ef18a2a27d3e9	The experiments offer insights to how self-attention layers process the speech signal, leading to the conclusion that the lower self-ATTention layers of the encoder encode a sufficiently wide range of inputs, hence learning further contextual information in the upper layers is unnecessary.	maybe	1	Recently, self-attention models such as Transformers have given competitive results compared to recurrent neural network systems in speech recognition. The key factor for the outstanding performance of self-attention models is their ability to capture temporal relationships without being limited by the distance between two related events. However, we note that the range of the learned context progressively increases from the lower to upper self-attention layers, whilst acoustic events often happen within short time spans in a left-to-right order. This leads to a question: for speech recognition, is a global view of the entire sequence still important for the upper self-attention layers in the encoder of Transformers? To investigate this, we replace these self-attention layers with feed forward layers. In our speech recognition experiments (Wall Street Journal and Switchboard), we indeed observe an interesting result: replacing the upper self-attention layers in the encoder with feed forward layers leads to no performance drop, and even minor gains. Our experiments offer insights to how self-attention layers process the speech signal, leading to the conclusion that the lower self-attention layers of the encoder encode a sufficiently wide range of inputs, hence learning further contextual information in the upper layers is unnecessary.	f64301c8e2f37299a9a18460976ef18a2a27d3e9	@['JournalArticle']{zhang-etal-2020-when,  author = {Shucong Zhang and Erfan Loweimi and P. Bell and S. Renals},  booktitle = {ArXiv},  journal = {ArXiv},  title = {When Can Self-Attention Be Replaced by Feed Forward Layers?},  volume = {abs/2005.13895},  year = {2020} }
When BERT Plays the Lottery, All Tickets Are Winning	2020	https://www.semanticscholar.org/paper/91ac65431b2dc46919e1673fde67671c29446812	It is shown that the "bad" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the "good" ones, indicating that most weights in the pre-trained BERT are potentially useful.	seed	100	Much of the recent success in NLP is due to the large Transformer-based models such as BERT (Devlin et al, 2019). However, these models have been shown to be reducible to a smaller number of self-attention heads and layers. We consider this phenomenon from the perspective of the lottery ticket hypothesis. For fine-tuned BERT, we show that (a) it is possible to find a subnetwork of elements that achieves performance comparable with that of the full model, and (b) similarly-sized subnetworks sampled from the rest of the model perform worse. However, the "bad" subnetworks can be fine-tuned separately to achieve only slightly worse performance than the "good" ones, indicating that most weights in the pre-trained BERT are potentially useful. We also show that the "good" subnetworks vary considerably across GLUE tasks, opening up the possibilities to learn what knowledge BERT actually uses at inference time.	91ac65431b2dc46919e1673fde67671c29446812	@['JournalArticle', 'Conference']{prasanna-etal-2020-when,  author = {Sai Prasanna and Anna Rogers and Anna Rumshisky},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {When BERT Plays the Lottery, All Tickets Are Winning},  volume = {abs/2005.00561},  year = {2020} }
When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions	2020	https://www.semanticscholar.org/paper/c8b00d4706fc8979a9c5f410addccbcfe1c0d894	The inability to infer behavioral conclusions from probing results is pointed out, and an alternative method which is focused on how the information is being used is offered, rather than on what information is encoded is offered.	seed	24	A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method which is focused on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention which removes it from the representation. Equipped with this new analysis tool, we can now ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.	c8b00d4706fc8979a9c5f410addccbcfe1c0d894	@['JournalArticle']{elazar-etal-2020-when,  author = {Yanai Elazar and Shauli Ravfogel and Alon Jacovi and Yoav Goldberg},  booktitle = {ArXiv},  journal = {ArXiv},  title = {When Bert Forgets How To POS: Amnesic Probing of Linguistic Properties and MLM Predictions},  volume = {abs/2006.00995},  year = {2020} }
When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it	2022	http://www.semanticscholar.org/paper/2bbf022d9a61f124a01da306a7ddd49d1b93abae	This work adapts the psycholinguistic assessment of language models paradigm to higher-level linguistic phenomena and introduces an English evaluation suite that targets the knowledge of the interactions between sentential operators and indefinite NPs, finding that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.	maybe	6	Understanding longer narratives or participating in conversations requires tracking of discourse entities that have been mentioned. Indefinite noun phrases (NPs), such as ‘a dog’, frequently introduce discourse entities but this behavior is modulated by sentential operators such as negation. For example, ‘a dog’ in ‘Arthur doesn’t own a dog’ does not introduce a discourse entity due to the presence of negation. In this work, we adapt the psycholinguistic assessment of language models paradigm to higher-level linguistic phenomena and introduce an English evaluation suite that targets the knowledge of the interactions between sentential operators and indefinite NPs. We use this evaluation suite for a fine-grained investigation of the entity tracking abilities of the Transformer-based models GPT-2 and GPT-3. We find that while the models are to a certain extent sensitive to the interactions we investigate, they are all challenged by the presence of multiple NPs and their behavior is not systematic, which suggests that even models at the scale of GPT-3 do not fully acquire basic entity tracking abilities.	2bbf022d9a61f124a01da306a7ddd49d1b93abae	@['JournalArticle', 'Conference']{schuster-linzen-2022-when,  author = {Sebastian Schuster and Tal Linzen},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {969-982},  title = {When a sentence does not introduce a discourse entity, Transformer-based models still sometimes refer to it},  year = {2022} }
Whatcha lookin' at? DeepLIFTing BERT's Attention in Question Answering	2019	https://www.semanticscholar.org/paper/304b7c87e5c6e76ffcfdaa59fbd0656f9dab47d8	This paper investigates one such model, BERT for question-answering, with the aim to analyze why it is able to achieve significantly better results than other models.	seed	7	There has been great success recently in tackling challenging NLP tasks by neural networks which have been pre-trained and fine-tuned on large amounts of task data. In this paper, we investigate one such model, BERT for question-answering, with the aim to analyze why it is able to achieve significantly better results than other models. We run DeepLIFT on the model predictions and test the outcomes to monitor shift in the attention values for input. We also cluster the results to analyze any possible patterns similar to human reasoning depending on the kind of input paragraph and question the model is trying to answer.	304b7c87e5c6e76ffcfdaa59fbd0656f9dab47d8	@['JournalArticle']{arkhangelskaia-dutta-2019-whatcha,  author = {Ekaterina Arkhangelskaia and Sourav Dutta},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Whatcha lookin' at? DeepLIFTing BERT's Attention in Question Answering},  volume = {abs/1910.06431},  year = {2019} }
What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models	2020	http://www.semanticscholar.org/paper/a8216d9fc9740772a44570281e7df5770978ad8d	A Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks are probed and it is shown that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems.	maybe	31	Peeking into the inner workings of BERT has shown that its layers resemble the classical NLP pipeline, with progressively more complex tasks being concentrated in later layers. To investigate to what extent these results also hold for a language other than English, we probe a Dutch BERT-based model and the multilingual BERT model for Dutch NLP tasks. In addition, through a deeper analysis of part-of-speech tagging, we show that also within a given task, information is spread over different parts of the network and the pipeline might not be as neat as it seems. Each layer has different specialisations, so that it may be more useful to combine information from different layers, instead of selecting a single one based on the best overall performance.	a8216d9fc9740772a44570281e7df5770978ad8d	@['JournalArticle']{vries-etal-2020-what’s,  author = {Wietse de Vries and Andreas van Cranenburgh and M. Nissim},  booktitle = {Findings},  journal = {ArXiv},  title = {What’s so special about BERT’s layers? A closer look at the NLP pipeline in monolingual and multilingual models},  volume = {abs/2004.06499},  year = {2020} }
What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?	2020	https://www.semanticscholar.org/paper/167f52d369b0979f27282af0f3a1a4be9c9be84b	A simple method is provided that ensembles predictions from multiple replacements while jointly modeling the uncertainty of type annotations and label predictions and shows that this method enhances robustness and increases accuracy on both natural and adversarial datasets.	seed	15	We evaluate named entity representations of BERT-based NLP models by investigating their robustness to replacements from the same typed class in the input. We highlight that on several tasks while such perturbations are natural, state of the art trained models are surprisingly brittle. The brittleness continues even with the recent entity-aware BERT models. We also try to discern the cause of this non-robustness, considering factors such as tokenization and frequency of occurrence. Then we provide a simple method that ensembles predictions from multiple replacements while jointly modeling the uncertainty of type annotations and label predictions. Experiments on three NLP tasks shows that our method enhances robustness and increases accuracy on both natural and adversarial datasets.	167f52d369b0979f27282af0f3a1a4be9c9be84b	@['JournalArticle']{balasubramanian-etal-2020-what’s,  author = {S. Balasubramanian and Naman Jain and G. Jindal and Abhijeet Awasthi and Sunita Sarawagi},  booktitle = {Workshop on Representation Learning for NLP},  pages = {205-214},  title = {What’s in a Name? Are BERT Named Entity Representations just as Good for any other Name?},  year = {2020} }
What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning	2019	http://www.semanticscholar.org/paper/4a4646a5ce6b57e369403e4efea1a2e4559fe9f1	This paper examines two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability, and shows that only a fourth of the final layers need to be fine-tuned to achieve 90% of the original quality.	maybe	46	Pretrained transformer-based language models have achieved state of the art across countless tasks in natural language processing. These models are highly expressive, comprising at least a hundred million parameters and a dozen layers. Recent evidence suggests that only a few of the final layers need to be fine-tuned for high quality on downstream tasks. Naturally, a subsequent research question is, "how many of the last layers do we need to fine-tune?" In this paper, we precisely answer this question. We examine two recent pretrained language models, BERT and RoBERTa, across standard tasks in textual entailment, semantic similarity, sentiment analysis, and linguistic acceptability. We vary the number of final layers that are fine-tuned, then study the resulting change in task-specific effectiveness. We show that only a fourth of the final layers need to be fine-tuned to achieve 90% of the original quality. Surprisingly, we also find that fine-tuning all layers does not always help.	4a4646a5ce6b57e369403e4efea1a2e4559fe9f1	@['JournalArticle']{lee-etal-2019-what,  author = {Jaejun Lee and Raphael Tang and Jimmy J. Lin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning},  volume = {abs/1911.03090},  year = {2019} }
What Transformers Might Know About the Physical World: T5 and the Origins of Knowledge	2021	http://www.semanticscholar.org/paper/72d22b5a2d549aeb8d1b8c192750a54abdec8615	As the size of the models was increased from 60M to 11B parameters, it was found that the fit to human judgments improved dramatically, suggesting that the difference between humans and these learning systems might ultimately disappear as the parameter sizes grow even larger.	maybe	1	Features of the physical world may be acquired from the statistical properties of language. Here we investigate how the Transformer Language Model T5 is able to gain knowledge of the visual world without being able to see or feel. In a series of four studies, we show that T5 possesses an implicit understanding of the relative sizes of animals, their weights, and their shapes, but not their colors, that aligns well with that of humans. As the size of the models was increased from 60M to 11B parameters, we found that the fit to human judgments improved dramatically, suggesting that the difference between humans and these learning systems might ultimately disappear as the parameter sizes grow even larger. The results imply that knowledge of the perceptual world—and much of semantic memory—might be acquired in dis-embodied learning systems using real-time inferential processes.	72d22b5a2d549aeb8d1b8c192750a54abdec8615	@None{haohan-wolff-2021-what,  author = {Haohan and Wolff},  title = {What Transformers Might Know About the Physical World: T5 and the Origins of Knowledge},  year = {2021} }
What Happens To BERT Embeddings During Fine-tuning?	2020	https://www.semanticscholar.org/paper/b2fd96a52ded7a64f60c1e54f5bb488c787629c0	It is found that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks, whereas SQuAD and MNLI involve much shallower processing.	maybe	73	While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques—supervised probing, unsupervised similarity analysis, and layer-based ablations—we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.	b2fd96a52ded7a64f60c1e54f5bb488c787629c0	@['JournalArticle']{merchant-etal-2020-what,  author = {Amil Merchant and Elahe Rahimtoroghi and Ellie Pavlick and Ian Tenney},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {33-44},  title = {What Happens To BERT Embeddings During Fine-tuning?},  year = {2020} }
What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge	2019	https://www.semanticscholar.org/paper/5a9001cdccdb8b1de227a45eccc503d32d1a2464	A methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation, and confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge.	seed	35	Abstract Open-domain question answering (QA) involves many knowledge and reasoning challenges, but are successful QA models actually learning such knowledge when trained on benchmark QA tasks? We investigate this via several new diagnostic tasks probing whether multiple-choice QA models know definitions and taxonomic reasoning—two skills widespread in existing benchmarks and fundamental to more complex reasoning. We introduce a methodology for automatically building probe datasets from expert knowledge sources, allowing for systematic control and a comprehensive evaluation. We include ways to carefully control for artifacts that may arise during this process. Our evaluation confirms that transformer-based multiple-choice QA models are already predisposed to recognize certain types of structural linguistic knowledge. However, it also reveals a more nuanced picture: their performance notably degrades even with a slight increase in the number of “hops” in the underlying taxonomic hierarchy, and with more challenging distractor candidates. Further, existing models are far from perfect when assessed at the level of clusters of semantically connected probes, such as all hypernym questions about a single concept.	5a9001cdccdb8b1de227a45eccc503d32d1a2464	@['JournalArticle']{richardson-sabharwal-2019-what,  author = {Kyle Richardson and Ashish Sabharwal},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {572-588},  title = {What Does My QA Model Know? Devising Controlled Probes Using Expert Knowledge},  volume = {8},  year = {2019} }
What Does it Mean for a Language Model to Preserve Privacy?	2022	http://www.semanticscholar.org/paper/62d17b6f6ad77fd71ef9954c7784700d5e316f1f	It is argued that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models, and it is concluded that language models should be trained on text data which was explicitly produced for public use.	maybe	19	Natural language reflects our private lives and identities, making its privacy concerns as broad as those of real life. Language models lack the ability to understand the context and sensitivity of text, and tend to memorize phrases present in their training sets. An adversary can exploit this tendency to extract training data. Depending on the nature of the content and the context in which this data was collected, this could violate expectations of privacy. Thus, there is a growing interest in techniques for training language models that preserve privacy. In this paper, we discuss the mismatch between the narrow assumptions made by popular data protection techniques (data sanitization and differential privacy), and the broadness of natural language and of privacy as a social norm. We argue that existing protection methods cannot guarantee a generic and meaningful notion of privacy for language models. We conclude that language models should be trained on text data which was explicitly produced for public use.	62d17b6f6ad77fd71ef9954c7784700d5e316f1f	@['JournalArticle', 'Book']{brown-etal-2022-what,  author = {Hannah Brown and Katherine Lee and FatemehSadat Mireshghallah and R. Shokri and Florian Tramèr},  booktitle = {Conference on Fairness, Accountability and Transparency},  journal = {2022 ACM Conference on Fairness, Accountability, and Transparency},  title = {What Does it Mean for a Language Model to Preserve Privacy?},  year = {2022} }
What Does BERT Look at? An Analysis of BERT’s Attention	2019	https://www.semanticscholar.org/paper/95a251513853c6032bdecebd4b74e15795662986	It is shown that certain attention heads correspond well to linguistic notions of syntax and coreference, and an attention-based probing classifier is proposed and used to demonstrate that substantial syntactic information is captured in BERT’s attention.	seed	922	Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.	95a251513853c6032bdecebd4b74e15795662986	@['JournalArticle']{clark-etal-2019-what,  author = {Kevin Clark and Urvashi Khandelwal and Omer Levy and Christopher D. Manning},  booktitle = {BlackboxNLP@ACL},  pages = {276-286},  title = {What Does BERT Look at? An Analysis of BERT’s Attention},  year = {2019} }
What Does BERT Learn about the Structure of Language?	2019	https://www.semanticscholar.org/paper/335613303ebc5eac98de757ed02a56377d99e03a	This work provides novel support for the possibility that BERT networks capture structural information about language by performing a series of experiments to unpack the elements of English language structure learned by BERT.	seed	743	BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT’s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures.	335613303ebc5eac98de757ed02a56377d99e03a	@['JournalArticle', 'Conference']{jawahar-etal-2019-what,  author = {Ganesh Jawahar and Benoît Sagot and Djamé Seddah},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3651-3657},  title = {What Does BERT Learn about the Structure of Language?},  year = {2019} }
What does BERT know about distributivity?	2022	http://www.semanticscholar.org/paper/10c8078293b0e37a623fae85794c315ed87d60c7	The goal is to test whether transformer models are able to encode event structural information in their representations of sentences and it is found that BERT may encode information about distributivity in later layers but it can still find a signal as early as layer 1.	maybe	0	Our goal is to test whether transformer models are able to encode event structural information in their representations of sentences. Using [1]’s event classification dataset, we seek to evaluate whether and how BERT may encode information about distributivity. We train classifiers to predict whether a predicate is distributive or collective using each of BERT’s hidden layers contextualized representations of a complete predicate, a predicate span, and an argument span. In line with previous research that suggest that semantic information is encoded in the top layers, we find that BERT may encode information about distributivity in later layers but it can still find a signal as early as layer 1. We also report that while all three types of representation perform in a similar fashion, argument span representations may better encode information about distributivity.	10c8078293b0e37a623fae85794c315ed87d60c7	@None{lu-2022-what,  author = {Jiayi Lu},  title = {What does BERT know about distributivity?},  year = {2022} }
What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation	2020	http://www.semanticscholar.org/paper/cc4db47a416aba22d1073e59a6867b6997928b7c	Overall, the experiments show that: (i) BERT has knowledge stored in its parameters about the content of books, movies and music; (ii) it has more content-based knowledge than collaborative-basedknowledge; and (iii) fails on conversational recommendation when faced with adversarial data.	maybe	29	Heavily pre-trained transformer models such as BERT have recently shown to be remarkably powerful at language modelling, achieving impressive results on numerous downstream tasks. It has also been shown that they implicitly store factual knowledge in their parameters after pre-training. Understanding what the pre-training procedure of LMs actually learns is a crucial step for using and improving them for Conversational Recommender Systems (CRS). We first study how much off-the-shelf pre-trained BERT “knows” about recommendation items such as books, movies and music. In order to analyze the knowledge stored in BERT’s parameters, we use different probes (i.e., tasks to examine a trained model regarding certain properties) that require different types of knowledge to solve, namely content-based and collaborative-based. Content-based knowledge is knowledge that requires the model to match the titles of items with their content information, such as textual descriptions and genres. In contrast, collaborative-based knowledge requires the model to match items with similar ones, according to community interactions such as ratings. We resort to BERT’s Masked Language Modelling (MLM) head to probe its knowledge about the genre of items, with cloze style prompts. In addition, we employ BERT’s Next Sentence Prediction (NSP) head and representations’ similarity (SIM) to compare relevant and non-relevant search and recommendation query-document inputs to explore whether BERT can, without any fine-tuning, rank relevant items first. Finally, we study how BERT performs in a conversational recommendation downstream task. To this end, we fine-tune BERT to act as a retrieval-based CRS. Overall, our experiments show that: (i) BERT has knowledge stored in its parameters about the content of books, movies and music; (ii) it has more content-based knowledge than collaborative-based knowledge; and (iii) fails on conversational recommendation when faced with adversarial data.	cc4db47a416aba22d1073e59a6867b6997928b7c	@['Book', 'JournalArticle']{penha-hauff-2020-what,  author = {Gustavo Penha and C. Hauff},  booktitle = {ACM Conference on Recommender Systems},  journal = {Proceedings of the 14th ACM Conference on Recommender Systems},  title = {What does BERT know about books, movies and music? Probing BERT for Conversational Recommendation},  year = {2020} }
What do you mean, BERT? Assessing BERT as a Distributional Semantics Model	2019	https://www.semanticscholar.org/paper/4bff291cf7fa02a0dbac767aba55d43ad8c59055	This work focuses on BERT, a deep neural network that produces contextualized embeddings and has set the state-of-the-art in several semantic tasks, and studies the semantic coherence of its embedding space, finding that BERT does not fully live up to the natural expectations for a semantic vector space.	seed	32	Contextualized word embeddings, i.e. vector representations for words in context, are naturally seen as an extension of previous noncontextual distributional semantic models. In this work, we focus on BERT, a deep neural network that produces contextualized embeddings and has set the state-of-the-art in several semantic tasks, and study the semantic coherence of its embedding space. While showing a tendency towards coherence, BERT does not fully live up to the natural expectations for a semantic vector space. In particular, we find that the position of the sentence in which a word occurs, while having no meaning correlates, leaves a noticeable trace on the word embeddings and disturbs similarity relationships.	4bff291cf7fa02a0dbac767aba55d43ad8c59055	@['JournalArticle']{mickus-etal-2019-what,  author = {Timothee Mickus and Denis Paperno and Mathieu Constant and Kees van Deemter},  booktitle = {ArXiv},  journal = {ArXiv},  title = {What do you mean, BERT? Assessing BERT as a Distributional Semantics Model},  volume = {abs/1911.05758},  year = {2019} }
What do you mean, BERT?	2020	http://www.semanticscholar.org/paper/a05a614dc4c39a15f708149aa653fa1b696e928d	This work focuses on BERT, a deep neural network that produces contextualized embeddings and has set the state of the art in several semantic tasks, and probes its embedding space for semantic coherence, finding that BERT does not fully live up to the natural expectations for a semantic vector space.	maybe	8	Contextualized word embeddings, i.e. vector representations for words in context, are naturally seen as an extension of previous noncontextual distributional semantic models. In this work, we focus on BERT, a deep neural network that produces contextualized embeddings and has set the state of the art in several semantic tasks, and probe its embedding space for semantic coherence. While showing a tendency towards coherence, BERT does not fully live up to the natural expectations for a semantic vector space. In particular, we find that the position of the sentence in which a word occurs, while having no meaning correlates, leaves a noticeable trace on the word embeddings and disturbs similarity relationships.	a05a614dc4c39a15f708149aa653fa1b696e928d	@None{mickus-etal-2020-what,  author = {Timothee Mickus and Denis Paperno and M. Constant and Kees van Deemter},  booktitle = {SCIL},  title = {What do you mean, BERT?},  year = {2020} }
What do you learn from context? Probing for sentence structure in contextualized word representations	2019	https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee	A novel edge probing task design is introduced and a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline are constructed to investigate how sentence structure is encoded across a range of syntactic, semantic, local, and long-range phenomena.	seed	583	Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.	e2587eddd57bc4ba286d91b27c185083f16f40ee	@['JournalArticle']{tenney-etal-2019-what,  author = {Ian Tenney and Patrick Xia and Berlin Chen and Alex Wang and Adam Poliak and R. Thomas McCoy and Najoung Kim and Benjamin Van Durme and Samuel R. Bowman and Dipanjan Das and Ellie Pavlick},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {What do you learn from context? Probing for sentence structure in contextualized word representations},  volume = {abs/1905.06316},  year = {2019} }
What do tokens know about their characters and how do they know it?	2022	http://www.semanticscholar.org/paper/9a1a9ae2fc2911f1f275702bef38aa8f79c86986	The mechanisms through which PLMs acquire English-language character information during training are investigated and it is argued that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.	maybe	3	Pre-trained language models (PLMs) that use subword tokenization schemes can succeed at a variety of language tasks that require character-level information, despite lacking explicit access to the character composition of tokens. Here, studying a range of models (e.g., GPT- J, BERT, RoBERTa, GloVe), we probe what word pieces encode about character-level information by training classifiers to predict the presence or absence of a particular alphabetical character in a token, based on its embedding (e.g., probing whether the model embedding for “cat” encodes that it contains the character “a”). We find that these models robustly encode character-level information and, in general, larger models perform better at the task. We show that these results generalize to characters from non-Latin alphabets (Arabic, Devanagari, and Cyrillic). Then, through a series of experiments and analyses, we investigate the mechanisms through which PLMs acquire English-language character information during training and argue that this knowledge is acquired through multiple phenomena, including a systematic relationship between particular characters and particular parts of speech, as well as natural variability in the tokenization of related strings.	9a1a9ae2fc2911f1f275702bef38aa8f79c86986	@['JournalArticle', 'Conference']{kaushal-mahowald-2022-what,  author = {Ayush Kaushal and Kyle Mahowald},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2487-2507},  title = {What do tokens know about their characters and how do they know it?},  year = {2022} }
What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code	2022	http://www.semanticscholar.org/paper/9e92039847c5ce7186ca579dc863e887601b38ca	A thorough structural analysis is conducted aiming to provide an interpretation of pre-trained language models for source code from three distinctive perspectives: attention analysis, probing on the word embedding, and syntax tree induction.	yes	12	Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.	9e92039847c5ce7186ca579dc863e887601b38ca	@['JournalArticle', 'Book', 'Conference']{wan-etal-2022-what,  author = {Yao Wan and Wei Zhao and Hongyu Zhang and Yulei Sui and Guandong Xu and Hairong Jin},  booktitle = {International Conference on Software Engineering},  journal = {2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)},  pages = {2377-2388},  title = {What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code},  year = {2022} }
What do pre-trained code models know about code?	2021	http://www.semanticscholar.org/paper/70087677fd1a6309829b42968934575d05a95f92	Four probing tasks are constructed (probing for surface-level, syntactic, structural, and semantic information) for pre-trained code models to identify whether models are deficient in (understanding) certain code properties, characterize different model layers, and get insight into the model sample-efficiency.	maybe	19	Pre-trained models of code built on the transformer architecture have performed well on software engineering (SE) tasks such as predictive code generation, code summarization, among others. However, whether the vector representations from these pre-trained models comprehensively encode characteristics of source code well enough to be applicable to a broad spectrum of downstream tasks remains an open question.One way to investigate this is with diagnostic tasks called probes. In this paper, we construct four probing tasks (probing for surface-level, syntactic, structural, and semantic information) for pre-trained code models. We show how probes can be used to identify whether models are deficient in (understanding) certain code properties, characterize different model layers, and get insight into the model sample-efficiency.We probe four models that vary in their expected knowledge of code properties: BERT (pre-trained on English), CodeBERT and CodeBERTa (pre-trained on source code, and natural language documentation), and GraphCodeBERT (pre-trained on source code with dataflow). While GraphCodeBERT performs more consistently overall, we find that BERT performs surprisingly well on some code tasks, which calls for further investigation.	70087677fd1a6309829b42968934575d05a95f92	@['Book', 'JournalArticle', 'Conference']{karmakar-robbes-2021-what,  author = {Anjan Karmakar and R. Robbes},  booktitle = {International Conference on Automated Software Engineering},  journal = {2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},  pages = {1332-1336},  title = {What do pre-trained code models know about code?},  year = {2021} }
What do Large Language Models Learn beyond Language?	2022	http://www.semanticscholar.org/paper/5efab88c0cdb11c795fa8f44a5d31b40e2a1c261	It is found that pretrained models outperform comparable non-pretrained neural models on a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings, and suggest a deep connection between pretraining and inductive learning abilities of language models.	maybe	0	Large language models (LMs) have rapidly become a mainstay in Natural Language Processing. These models are known to acquire rich linguistic knowledge from training on large amounts of text. In this paper, we investigate if pre-training on text also confers these models with helpful ‘inductive biases’ for non-linguistic reasoning. On a set of 19 diverse non-linguistic tasks involving quantitative computations, recognizing regular expressions and reasoning over strings. We ﬁnd that pretrained models signiﬁcantly outperform comparable non-pretrained neural models. This remains true also in experiments with training non-pretrained models with fewer parameters to account for model regularization effects. We further explore the effect of text domain on LMs by pretraining models from text from different domains and provenances. Our experiments surprisingly reveal that the positive effects of pre-training persist even when pretraining on multi-lingual text or computer code, and even for text generated from synthetic languages. Our ﬁndings suggest a hith-erto unexplored deep connection between pretraining and inductive learning abilities of language models 1 .	5efab88c0cdb11c795fa8f44a5d31b40e2a1c261	@['JournalArticle']{madasu-srivastava-2022-what,  author = {Avinash Madasu and Shashank Srivastava},  booktitle = {ArXiv},  journal = {ArXiv},  title = {What do Large Language Models Learn beyond Language?},  volume = {abs/2210.12302},  year = {2022} }
What do Large Language Models Learn about Scripts?	2021	http://www.semanticscholar.org/paper/1ea78b1684371de0e859edf759e5e659152d31e3	This work introduces the task of generating full event sequence descriptions (ESDs) given a scenario as a natural language prompt and proposes a pipeline-based script induction framework (SIF) which can generate good quality ESDs for unseen scenarios.	maybe	1	Script Knowledge (Schank and Abelson, 1975) has long been recognized as crucial for language understanding as it can help in filling in unstated information in a narrative. However, such knowledge is expensive to produce manually and difficult to induce from text due to reporting bias (Gordon and Van Durme, 2013). In this work, we are interested in the scientific question of whether explicit script knowledge is present and accessible through pre-trained generative language models (LMs). To this end, we introduce the task of generating full event sequence descriptions (ESDs) given a scenario as a natural language prompt. Through zero-shot probing, we find that generative LMs produce poor ESDs with mostly omitted, irrelevant, repeated or misordered events. To address this, we propose a pipeline-based script induction framework (SIF) which can generate good quality ESDs for unseen scenarios (e.g., bake a cake). SIF is a two-staged framework that fine-tunes LM on a small set of ESD examples in the first stage. In the second stage, ESD generated for an unseen scenario is post-processed using RoBERTa-based models to filter irrelevant events, remove repetitions, and reorder the temporally misordered events. Through automatic and manual evaluations, we demonstrate that SIF yields substantial improvements (1-3 BLEU points) over a fine-tuned LM. However, manual analysis shows that there is great room for improvement, offering a new research direction for inducing script knowledge.	1ea78b1684371de0e859edf759e5e659152d31e3	@['JournalArticle']{sancheti-rudinger-2021-what,  author = {Abhilasha Sancheti and Rachel Rudinger},  booktitle = {STARSEM},  pages = {1-11},  title = {What do Large Language Models Learn about Scripts?},  year = {2021} }
What do Bias Measures Measure?	2021	http://www.semanticscholar.org/paper/bdd8530d72f51661f24247c9b5cd936ddb8e5c59	This work presents a comprehensive survey of existing bias measures in NLP as a function of the associated NLP tasks, metrics, datasets, and social biases and corresponding harms and proposes a documentation standard for bias measures to aid their development, categorization, and appropriate usage.	maybe	7	Natural Language Processing (NLP) models propagate social biases about protected attributes such as gender, race, and nationality. To create interventions and mitigate these biases and associated harms, it is vital to be able to detect and measure such biases. While many existing works propose bias evaluation methodologies for different tasks, there remains a need to cohesively understand what biases and normative harms each of these measures captures and how different measures compare. To address this gap, this work presents a comprehensive survey of existing bias measures in NLP as a function of the associated NLP tasks, metrics, datasets, and social biases and corresponding harms. This survey also organizes metrics into different categories to present advantages and disadvantages. Finally, we propose a documentation standard for bias measures to aid their development, categorization, and appropriate usage.	bdd8530d72f51661f24247c9b5cd936ddb8e5c59	@['JournalArticle', 'Review']{dev-etal-2021-what,  author = {Sunipa Dev and Emily Sheng and Jieyu Zhao and Jiao Sun and Yu Hou and M. Sanseverino and Jiin Kim and Nanyun Peng and Kai-Wei Chang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {What do Bias Measures Measure?},  volume = {abs/2108.03362},  year = {2021} }
What Context Features Can Transformer Language Models Use?	2021	http://www.semanticscholar.org/paper/e1bc348fd7da000da6585e82994ecfedcecb5a4c	It is suggested that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.	yes	34	Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.	e1bc348fd7da000da6585e82994ecfedcecb5a4c	@['JournalArticle', 'Conference']{o'connor-andreas-2021-what,  author = {J. O'Connor and Jacob Andreas},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {851-864},  title = {What Context Features Can Transformer Language Models Use?},  year = {2021} }
What changed? Investigating Debiasing Methods using Causal Mediation Analysis	2022	http://www.semanticscholar.org/paper/145e1755ed68a73c373375f3bb1fd12c2e9f1366	The internal mechanisms of debiasing language models with respect to gender are decompose by applying causal mediation analysis to understand the influence ofdebiasing methods on toxicity detection as a downstream task.	maybe	1	Previous work has examined how debiasing language models affect downstream tasks, specifically, how debiasing techniques influence task performance and whether debiased models also make impartial predictions in downstream tasks or not. However, what we don’t understand well yet is why debiasing methods have varying impacts on downstream tasks and how debiasing techniques affect internal components of language models, i.e., neurons, layers, and attentions. In this paper, we decompose the internal mechanisms of debiasing language models with respect to gender by applying causal mediation analysis to understand the influence of debiasing methods on toxicity detection as a downstream task. Our findings suggest a need to test the effectiveness of debiasing methods with different bias metrics, and to focus on changes in the behavior of certain components of the models, e.g.,first two layers of language models, and attention heads.	145e1755ed68a73c373375f3bb1fd12c2e9f1366	@['JournalArticle']{jeoung-diesner-2022-what,  author = {Su-Ha Jeoung and Jana Diesner},  booktitle = {GEBNLP},  journal = {ArXiv},  title = {What changed? Investigating Debiasing Methods using Causal Mediation Analysis},  volume = {abs/2206.00701},  year = {2022} }
What Can a Generative Language Model Answer About a Passage?	2021	http://www.semanticscholar.org/paper/28885f6f67b8537eea390de2e62f7e06b844c9bd	This paper keeps the passage fixed, test with a wide variety of question types, exploring the strengths and weaknesses of the GPT-3 language model, and provides the passage and test questions as a challenge set for other language models.	maybe	4	Generative language models trained on large, diverse corpora can answer questions about a passage by generating the most likely continuation of the passage followed by a question/answer pair. However, accuracy rates vary depending on the type of question asked. In this paper we keep the passage fixed, and test with a wide variety of question types, exploring the strengths and weaknesses of the GPT-3 language model. We provide the passage and test questions as a challenge set for other language models.	28885f6f67b8537eea390de2e62f7e06b844c9bd	@None{stay-etal-2021-what,  author = {Douglas Summers-Stay and Claire Bonial and Clare R. Voss},  booktitle = {Workshop on Machine Reading for Question Answering},  journal = {Proceedings of the 3rd Workshop on Machine Reading for Question Answering},  title = {What Can a Generative Language Model Answer About a Passage?},  year = {2021} }
What BERTs and GPTs know about your brand? Probing contextual language models for affect associations	2021	http://www.semanticscholar.org/paper/bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7	It is believed that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for innovative products.	maybe	0	Investigating brand perception is fundamental to marketing strategies. In this regard, brand image, defined by a set of attributes (Aaker, 1997), is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional approaches (e.g., surveys) to monitor brand perceptions are time-consuming and inefficient. In the era of digital marketing, both brand managers and consumers engage with a vast amount of digital marketing content. The exponential growth of digital content has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions (i.e., brand and image attribute associations) these language models encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for innovative products. We use synthetic data and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART.	bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7	@['JournalArticle', 'Review']{srivastava-etal-2021-what,  author = {Vivek Srivastava and Stephen Pilli and S. Bhat and N. Pedanekar and Shirish S. Karande},  booktitle = {Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},  pages = {119-128},  title = {What BERTs and GPTs know about your brand? Probing contextual language models for affect associations},  year = {2021} }
What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models	2019	https://www.semanticscholar.org/paper/a0e49f65b6847437f262c59d0d399255101d0b75	A suite of diagnostics drawn from human language experiments are introduced, which allow us to ask targeted questions about information used by language models for generating predictions in context, and the popular BERT model is applied.	seed	343	Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.	a0e49f65b6847437f262c59d0d399255101d0b75	@['JournalArticle']{ettinger-2019-what,  author = {Allyson Ettinger},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {34-48},  title = {What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models},  volume = {8},  year = {2019} }
What BERT Based Language Model Learns in Spoken Transcripts: An Empirical Study	2021	http://www.semanticscholar.org/paper/3f627da72237ef55134daeaeae493ee1d628bea9	This work probes BERT based language models trained on spoken transcripts to investigate its ability to understand multifarious properties in absence of any speech cues, and establishes the efficacy and transferability of the mentioned properties on two benchmark datasets: Switchboard Dialog Act and Disfluency datasets.	maybe	3	Language Models (LMs) have been ubiquitously leveraged in various tasks including spoken language understanding (SLU). Spoken language requires careful understanding of speaker interactions, dialog states and speech induced multimodal behaviors to generate a meaningful representation of the conversation. In this work, we propose to dissect SLU into three representative properties: conversational (disfluency, pause, overtalk), channel (speaker-type, turn-tasks) and ASR (insertion, deletion, substitution). We probe BERT based language models (BERT, RoBERTa) trained on spoken transcripts to investigate its ability to understand multifarious properties in absence of any speech cues. Empirical results indicate that LM is surprisingly good at capturing conversational properties such as pause prediction and overtalk detection from lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR errors predictions. Additionally, pre-training the LM on spoken transcripts restrain its linguistic understanding. Finally, we establish the efficacy and transferability of the mentioned properties on two benchmark datasets: Switchboard Dialog Act and Disfluency datasets.	3f627da72237ef55134daeaeae493ee1d628bea9	@['JournalArticle']{kumar-etal-2021-what,  author = {Ayush Kumar and Mukuntha Narayanan Sundararaman and J. Vepa},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {322-336},  title = {What BERT Based Language Model Learns in Spoken Transcripts: An Empirical Study},  year = {2021} }
What are the Goals of Distributional Semantics?	2020	http://www.semanticscholar.org/paper/db875a1636ec0ae9b8e791ed8dad8ac96f9ed190	A broad linguistic perspective is taken, looking at how well current models can deal with various semantic challenges in distributional semantic models, and concludes that future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.	maybe	16	Distributional semantic models have become a mainstay in NLP, providing useful features for downstream tasks. However, assessing long-term progress requires explicit long-term goals. In this paper, I take a broad linguistic perspective, looking at how well current models can deal with various semantic challenges. Given stark differences between models proposed in different subfields, a broad perspective is needed to see how we could integrate them. I conclude that, while linguistic insights can guide the design of model architectures, future progress will require balancing the often conflicting demands of linguistic expressiveness and computational tractability.	db875a1636ec0ae9b8e791ed8dad8ac96f9ed190	@['JournalArticle', 'Conference']{emerson-2020-what,  author = {Guy Edward Toh Emerson},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {What are the Goals of Distributional Semantics?},  volume = {abs/2005.02982},  year = {2020} }
Weight Poisoning Attacks on Pretrained Models	2020	http://www.semanticscholar.org/paper/0d360a1256ccdfca58cf98d12243df8407fd442d	It is shown that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword.	maybe	150	Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct “weight poisoning” attacks where pre-trained weights are injected with vulnerabilities that expose “backdoors” after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.	0d360a1256ccdfca58cf98d12243df8407fd442d	@['JournalArticle', 'Conference']{kurita-etal-2020-weight,  author = {Keita Kurita and Paul Michel and Graham Neubig},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Weight Poisoning Attacks on Pretrained Models},  volume = {abs/2004.06660},  year = {2020} }
Visualizing Transformers for NLP: A Brief Survey	2020	http://www.semanticscholar.org/paper/f0f81bda8974900a46d19ac9882cdeaa3dccf458	A survey on explaining Transformer architectures through visualizations, which examines the various Transformer facets that can be explored through visual analytics and proposes a set of requirements for future Transformer visualization frameworks.	maybe	19	The introduction of Transformer neural networks has changed the landscape of Natural Language Processing during the last three years. While models inspired by it have managed to lead the boards for a variety of tasks, some of the mechanisms through which these performances were achieved are not necessarily well-understood. Our survey is focused mostly on explaining Transformer architectures through visualizations. Since visualization enables some degree of explainability, we have examined the various Transformer facets that can be explored through visual analytics. The field is still at a nascent stage and is expected to witness dynamic growth in the near future, since the results are already interesting and promising. Currently, some of the visualizations are relatively close to their original models, whereas others are model-agnostic. The visualizations designed to explore the Transformer architectures enable some additional features, like exploration of all neuronal cells or attention maps, therefore providing an advantage for this particular task. We conclude by proposing a set of requirements for future Transformer visualization frameworks.	f0f81bda8974900a46d19ac9882cdeaa3dccf458	@['JournalArticle', 'Conference', 'Review']{brasoveanu-andonie-2020-visualizing,  author = {Adrian Brasoveanu and Razvan Andonie},  booktitle = {International Conference on Information Visualisation},  journal = {2020 24th International Conference Information Visualisation (IV)},  pages = {270-279},  title = {Visualizing Transformers for NLP: A Brief Survey},  year = {2020} }
Visualizing Attention in Transformer-Based Language Representation Models	2019	https://www.semanticscholar.org/paper/beb051c652f02c2d5829d783fbc4f3acce99bc3c	An open-source tool for visualizing multi-head self-attention in Transformer-based language representation models and three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior are presented.	seed	52	We present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior.	beb051c652f02c2d5829d783fbc4f3acce99bc3c	@['JournalArticle']{vig-2019-visualizing,  author = {Jesse Vig},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Visualizing Attention in Transformer-Based Language Representation Models},  volume = {abs/1904.02679},  year = {2019} }
Visualizing and Understanding the Effectiveness of BERT	2019	https://www.semanticscholar.org/paper/d3cacb4806886eb2fe59c90d4b6f822c24ff1822	It is found that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch, and the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.	seed	100	Language model pre-training, such as BERT, has achieved remarkable results in many NLP tasks. However, it is unclear why the pre-training-then-fine-tuning paradigm can improve performance and generalization capability across different tasks. In this paper, we propose to visualize loss landscapes and optimization trajectories of fine-tuning BERT on specific datasets. First, we find that pre-training reaches a good initial point across downstream tasks, which leads to wider optima and easier optimization compared with training from scratch. We also demonstrate that the fine-tuning procedure is robust to overfitting, even though BERT is highly over-parameterized for downstream tasks. Second, the visualization results indicate that fine-tuning BERT tends to generalize better because of the flat and wide optima, and the consistency between the training loss surface and the generalization error surface. Third, the lower layers of BERT are more invariant during fine-tuning, which suggests that the layers that are close to input learn more transferable representations of language.	d3cacb4806886eb2fe59c90d4b6f822c24ff1822	@['JournalArticle', 'Conference']{hao-etal-2019-visualizing,  author = {Y. Hao and Li Dong and Furu Wei and Ke Xu},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4141-4150},  title = {Visualizing and Understanding the Effectiveness of BERT},  year = {2019} }
Visualizing and Measuring the Geometry of BERT	2019	https://www.semanticscholar.org/paper/afd110eace912c2b273e64851c6b4df2658622eb	This paper describes qualitative and quantitative investigations of one particularly effective model, BERT, and finds evidence of a fine-grained geometric representation of word senses in both attention matrices and individual word embeddings.	maybe	258	Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.	afd110eace912c2b273e64851c6b4df2658622eb	@['JournalArticle']{coenen-etal-2019-visualizing,  author = {Andy Coenen and Emily Reif and Ann Yuan and Been Kim and Adam Pearce and F. Viégas and M. Wattenberg},  booktitle = {Neural Information Processing Systems},  journal = {ArXiv},  title = {Visualizing and Measuring the Geometry of BERT},  volume = {abs/1906.02715},  year = {2019} }
VisBERT: Hidden-State Visualizations for Transformers	2020	http://www.semanticscholar.org/paper/f1922dc3bf70a8c26fe2eb280a37164b6cf0106c	VisBERT is presented, a tool for visualizing the contextual token representations within BERT for the task of (multi-hop) Question Answering that allows users to identify distinct phases in BERT’s transformations that are similar to a traditional NLP pipeline and offer insights during failed predictions.	maybe	16	Explainability and interpretability are two important concepts, the absence of which can and should impede the application of well-performing neural networks to real-world problems. At the same time, they are difficult to incorporate into the large, black-box models that achieve state-of-the-art results in a multitude of NLP tasks. Bidirectional Encoder Representations from Transformers (BERT) is one such black-box model. It has become a staple architecture to solve many different NLP tasks and has inspired a number of related Transformer models. Understanding how these models draw conclusions is crucial for both their improvement and application. We contribute to this challenge by presenting VisBERT, a tool for visualizing the contextual token representations within BERT for the task of (multi-hop) Question Answering. Instead of analyzing attention weights, we focus on the hidden states resulting from each encoder block within the BERT model. This way we can observe how the semantic representations are transformed throughout the layers of the model. VisBERT enables users to get insights about the model’s internal state and to explore its inference steps or potential shortcomings. The tool allows us to identify distinct phases in BERT’s transformations that are similar to a traditional NLP pipeline and offer insights during failed predictions.	f1922dc3bf70a8c26fe2eb280a37164b6cf0106c	@['JournalArticle', 'Book', 'Conference']{aken-etal-2020-visbert:,  author = {Betty van Aken and Benjamin Winter and Alexander Löser and Felix Alexander Gers},  booktitle = {The Web Conference},  journal = {Companion Proceedings of the Web Conference 2020},  title = {VisBERT: Hidden-State Visualizations for Transformers},  year = {2020} }
Varying Abstractions: a conceptual vs. distributional view on prepositional polysemy	2021	http://www.semanticscholar.org/paper/32e0e0851c2f0a7517225cc1062ff1812f733c99	This paper investigates whether the sense network that emerges from the principled polysemy model of over as proposed by Tyler & Evans (2003; 2001) can be reconstructed by the neural language model BERT.	maybe	2	The term ‘meaning’, as it is presently employed in Linguistics, is a polysemous concept, covering a broad range of operational definitions. Focussing on two of these definitions, meaning as ‘concept’ and meaning as ‘context’ (also known as ‘distributional semantics’), this paper explores to what extent these operational definitions lead to converging conclusions regarding the number and nature of distinct senses a polysemous form covers. More specifically, it investigates whether the sense network that emerges from the principled polysemy model of over as proposed by Tyler & Evans (2003; 2001) can be reconstructed by the neural language model BERT. The study assesses whether the contextual information encoded in BERT embeddings can be employed to succesfully (i) recognize the abstract sense categories and (ii) replicate the relative distances between the senses of over proposed in the principled polysemy model. The results suggest that, while there is partial convergence, the two models ultimately lead to different global abstractions because the imagistic information that plays a key role in conceptual approaches to prepositional meaning may not be encoded in contextualized word embeddings.	32e0e0851c2f0a7517225cc1062ff1812f733c99	@None{fonteyn-2021-varying,  author = {Lauren Fonteyn},  booktitle = {Glossa},  journal = {Glossa: a journal of general linguistics},  title = {Varying Abstractions: a conceptual vs. distributional view on prepositional polysemy},  year = {2021} }
Variation and generality in encoding of syntactic anomaly information in sentence embeddings	2021	https://www.semanticscholar.org/paper/508ccd3c2dd053e39dbd8f0e6fe98ddafb226ae1	Follow-up analyses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position information is likely also a contributor to the observed anomaly detection.	maybe	2	While sentence anomalies have been applied periodically for testing in NLP, we have yet to establish a picture of the precise status of anomaly information in representations from NLP models. In this paper we aim to fill two primary gaps, focusing on the domain of syntactic anomalies. First, we explore fine-grained differences in anomaly encoding by designing probing tasks that vary the hierarchical level at which anomalies occur in a sentence. Second, we test not only models’ ability to detect a given anomaly, but also the generality of the detected anomaly signal, by examining transfer between distinct anomaly types. Results suggest that all models encode some information supporting anomaly detection, but detection performance varies between anomalies, and only representations from more re- cent transformer models show signs of generalized knowledge of anomalies. Follow-up analyses support the notion that these models pick up on a legitimate, general notion of sentence oddity, while coarser-grained word position information is likely also a contributor to the observed anomaly detection.	508ccd3c2dd053e39dbd8f0e6fe98ddafb226ae1	@['JournalArticle']{wu-ettinger-2021-variation,  author = {Qinxuan Wu and Allyson Ettinger},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {250-264},  title = {Variation and generality in encoding of syntactic anomaly information in sentence embeddings},  year = {2021} }
Validating Large Language Models with ReLM	2022	http://www.semanticscholar.org/paper/089481199b10c0090d89d371fb4219e9d5fcea8a	This work introduces ReLM, a system for validating and querying LLMs using standard regular expressions, which formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries.	maybe	0	Although large language models (LLMs) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language. Unfortunately, the complexity and generation capacities of LLMs make validating (and correcting) such concerns difficult. In this work, we introduce ReLM, a system for validating and querying LLMs using standard regular expressions. ReLM formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. Our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that ReLM achieves up to 15× higher system efficiency, 2.5× data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers a competitive and general baseline for the increasingly important problem of LLM validation.	089481199b10c0090d89d371fb4219e9d5fcea8a	@['JournalArticle']{kuchnik-etal-2022-validating,  author = {Michael Kuchnik and V. Smith and George Amvrosiadis},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Validating Large Language Models with ReLM},  volume = {abs/2211.15458},  year = {2022} }
Using surprisal and fMRI to map the neural bases of broad and local contextual prediction during natural language comprehension	2021	http://www.semanticscholar.org/paper/9f8f19ebfe3b760967445ed23ad144f90faf4316	Different neuro-anatomical correlates suggest that local and broad contextual cues during sentence processing recruit different brain regions and that those regions of the language network functionally contribute to processing different dimensions of contextual information during comprehension.	maybe	0	Context guides comprehenders’ expectations during language processing, and informationtheoretic surprisal is commonly used as an index of cognitive processing effort. However, prior work using surprisal has considered only within-sentence context, using n-grams, neural language models, or syntactic structure as conditioning context. In this paper, we extend the surprisal approach to use broader topical context, investigating the influence of local and topical context on processing via an analysis of fMRI time courses collected during naturalistic listening. Lexical surprisal calculated from ngram and LSTM language models is used to capture effects of local context; to capture the effects of broader context a new metric based on topic models, topical surprisal, is introduced. We identify distinct patterns of neural activation for lexical surprisal and topical surprisal. These differing neuro-anatomical correlates suggest that local and broad contextual cues during sentence processing recruit different brain regions and that those regions of the language network functionally contribute to processing different dimensions of contextual information during comprehension. More generally, our approach adds to a growing literature using methods from computational linguistics to operationalize and test hypotheses about neuro-cognitive mechanisms in sentence processing.	9f8f19ebfe3b760967445ed23ad144f90faf4316	@['JournalArticle']{bhattasali-resnik-2021-using,  author = {Shohini Bhattasali and P. Resnik},  booktitle = {Findings},  pages = {3786-3798},  title = {Using surprisal and fMRI to map the neural bases of broad and local contextual prediction during natural language comprehension},  year = {2021} }
Using Roark-Hollingshead Distance to Probe BERT’s Syntactic Competence	2022	http://www.semanticscholar.org/paper/b85c476fb1218f927aa6edc3e5ceed6512c068f5		maybe	0		b85c476fb1218f927aa6edc3e5ceed6512c068f5	
Using Natural Sentence Prompts for Understanding Biases in Language Models	2022	http://www.semanticscholar.org/paper/ea0e62db166caa46de8821acce1afe1b8793025b	A prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia is created to understand the differences between using template-based prompts and natural sentence prompts when studying gender-occupation biases in language models.	maybe	2	Evaluation of biases in language models is often limited to synthetically generated datasets. This dependence traces back to the need of prompt-style dataset to trigger specific behaviors of language models. In this paper, we address this gap by creating a prompt dataset with respect to occupations collected from real-world natural sentences present in Wikipedia.We aim to understand the differences between using template-based prompts and natural sentence prompts when studying gender-occupation biases in language models. We find bias evaluations are very sensitiveto the design choices of template prompts, and we propose using natural sentence prompts as a way of more systematically using real-world sentences to move away from design decisions that may bias the results.	ea0e62db166caa46de8821acce1afe1b8793025b	@['JournalArticle', 'Conference']{alnegheimish-etal-2022-using,  author = {Sarah Alnegheimish and Alicia Guo and Yi Sun},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2824-2830},  title = {Using Natural Sentence Prompts for Understanding Biases in Language Models},  year = {2022} }
Using Large Language Models to Simulate Multiple Humans	2022	http://www.semanticscholar.org/paper/a93b9a710a5347d414fb3ddc7e2dc454f1ad5cfe	It is shown that it is possible to simulate re- sponses of different people and that their responses are consistent with prior human studies from the literature, suggesting a trend that future language models may be used for even more faithful simulations of human responses.	maybe	5	We propose a method for using a large language model, such as GPT-3, to simulate responses of different humans in a given context. We test our method by attempting to repro- duce well-established economic, psycholinguistic, and social experiments. The method requires prompt templates for each experiment. Simulations are run by varying the (hypotheti-cal) subject details, such as name, and analyzing the text gen- erated by the language model. To validate our methodology, we use GPT-3 to simulate the Ultimatum Game , garden path sentences , risk aversion , and the Milgram Shock experiments. In order to address concerns of exposure to these studies in training data, we also evaluate simulations on novel variants of these studies. We show that it is possible to simulate re- sponses of different people and that their responses are consistent with prior human studies from the literature. Across all studies, the distributions generated by larger language models better align with prior experimental results, suggesting a trend that future language models may be used for even more faithful simulations of human responses. Our use of a lan- guage model for simulation is contrasted with anthropomor-phic views of a language model as having its own behavior.	a93b9a710a5347d414fb3ddc7e2dc454f1ad5cfe	@['JournalArticle']{aher-etal-2022-using,  author = {Gati Aher and RosaI. Arriaga and A. Kalai},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Using Large Language Models to Simulate Multiple Humans},  volume = {abs/2208.10264},  year = {2022} }
Using Integrated Gradients and Constituency Parse Trees to explain Linguistic Acceptability learnt by BERT	2021	http://www.semanticscholar.org/paper/a4ded38a93d3de39bdf171728368ea76937f69c4	The decision-making process of BERT in distinguishing between Linguistic Acceptable sentences (LA) and Linguistically Unacceptable sentences (LUA) is understood and Layer Integrated Gradients Attribution Scores (LIG) are leveraged to explain the L linguistic Acceptability criteria that are learnt by BERT on the Corpus of Linguists Acceptability (CoLA) (Warstadt et al., 2018).	maybe	0	Linguistic Acceptability is the task of determining whether a sentence is grammatical or ungrammatical. It has applications in several use cases like Question-Answering, Natural Language Generation, Neural Machine Translation, where grammatical correctness is crucial. In this paper we aim to understand the decision-making process of BERT (Devlin et al., 2019) in distinguishing between Linguistically Acceptable sentences (LA) and Linguistically Unacceptable sentences (LUA).We leverage Layer Integrated Gradients Attribution Scores (LIG) to explain the Linguistic Acceptability criteria that are learnt by BERT on the Corpus of Linguistic Acceptability (CoLA) (Warstadt et al., 2018) benchmark dataset. Our experiments on 5 categories of sentences lead to the following interesting findings: 1) LIG for LA are significantly smaller in comparison to LUA, 2) There are specific subtrees of the Constituency Parse Tree (CPT) for LA and LUA which contribute larger LIG, 3) Across the different categories of sentences we observed around 88% to 100% of the Correctly classified sentences had positive LIG, indicating a strong positive relationship to the prediction confidence of the model, and 4) Around 43% of the Misclassified sentences had negative LIG, which we believe can become correctly classified sentences if the LIG are parameterized in the loss function of the model.	a4ded38a93d3de39bdf171728368ea76937f69c4	@['JournalArticle']{nayak-timmapathini-2021-using,  author = {Anmol Nayak and Hariprasad Timmapathini},  booktitle = {ICON},  pages = {80-85},  title = {Using Integrated Gradients and Constituency Parse Trees to explain Linguistic Acceptability learnt by BERT},  year = {2021} }
Using Dynamic Embeddings to Improve Static Embeddings	2019	https://www.semanticscholar.org/paper/1257f59bd9b6bc3f3823125408c7b6e63db4a158	This work uses contextualized embeddings to facilitate training of static embedding lookup tables, and shows that the resultingembeddings outperform existingstatic embedding methods on various lexical semantics tasks.	seed	5	How to build high-quality word embeddings is a fundamental research question in the field of natural language processing. Traditional methods such as Skip-Gram and Continuous Bag-of-Words learn {\it static} embeddings by training lookup tables that translate words into dense vectors. Static embeddings are directly useful for solving lexical semantics tasks, and can be used as input representations for downstream problems. Recently, contextualized embeddings such as BERT have been shown more effective than static embeddings as NLP input embeddings. Such embeddings are {\it dynamic}, calculated according to a sentential context using a network structure. One limitation of dynamic embeddings, however, is that they cannot be used without a sentence-level context. We explore the advantages of dynamic embeddings for training static embeddings, by using contextualized embeddings to facilitate training of static embedding lookup tables. Results show that the resulting embeddings outperform existing static embedding methods on various lexical semantics tasks.	1257f59bd9b6bc3f3823125408c7b6e63db4a158	@['JournalArticle']{wang-etal-2019-using,  author = {Yile Wang and Leyang Cui and Yue Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Using Dynamic Embeddings to Improve Static Embeddings},  volume = {abs/1911.02929},  year = {2019} }
Using Distributional Principles for the Semantic Study of Contextual Language Models	2021	http://www.semanticscholar.org/paper/597b1387f721fa8f7524b257aca920c7d373ed48	This article focuses on semantic similarity properties for English by exploiting the distributional principle of substitution as a probing mechanism in the controlled context of SemCor and WordNet paradigmatic relations and proposes to adapt the same method to a more open setting for characterizing the differences between static and contextual language models.	maybe	0	Many studies were recently done for investigating the properties of contextual language models but surprisingly, only a few of them consider the properties of these models in terms of semantic similarity. In this article, we first focus on these properties for English by exploiting the distributional principle of substitution as a probing mechanism in the controlled context of SemCor and WordNet paradigmatic relations. Then, we propose to adapt the same method to a more open setting for characterizing the differences between static and contextual language models.	597b1387f721fa8f7524b257aca920c7d373ed48	@['JournalArticle']{ferret-2021-using,  author = {Olivier Ferret},  booktitle = {Pacific Asia Conference on Language, Information and Computation},  pages = {189-200},  title = {Using Distributional Principles for the Semantic Study of Contextual Language Models},  year = {2021} }
Using Computational Models to Test Syntactic Learnability	2022	http://www.semanticscholar.org/paper/ccca3fe638773f3dbfc64ec87a33169df4638d7b	This work evaluates a model’s acquisition of island constraints by demonstrating that its expectation for a filler–gap contingency is attenuated within an island environment, and provides empirical evidence against the Argument from the Poverty of the Stimulus for this particular structure.	maybe	4	We study the learnability of English filler–gap dependencies and the “island” constraints on them by assessing the generalizations made by autoregressive (incremental) language models that use deep learning to predict the next word given preceding context. Using factorial tests inspired by experimental psycholinguistics, we find that models acquire not only the basic contingency between fillers and gaps, but also the unboundedness and hierarchical constraints implicated in the dependency. We evaluate a model’s acquisition of island constraints by demonstrating that its expectation for a filler–gap contingency is attenuated within an island environment. Our results provide empirical evidence against the Argument from the Poverty of the Stimulus for this particular structure.	ccca3fe638773f3dbfc64ec87a33169df4638d7b	@None{wilcox-etal-2022-using,  author = {Ethan Gotlieb Wilcox and Richard Futrell and R. Levy},  booktitle = {Linguistic Inquiry},  journal = {Linguistic Inquiry},  title = {Using Computational Models to Test Syntactic Learnability},  year = {2022} }
Using Commonsense Knowledge to Answer Why-Questions	2022	http://www.semanticscholar.org/paper/7bb907e754942b832bacf7889ba1d6bd72945ca0	This work analyzes the effects of model size and methods of injecting knowledge into large language models using COMET as a source of relevant commonsense relations and develops an ontology of knowledge types, finding that smaller models benefit more from larger amounts of knowledge.	yes	0	Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the T ELL M E W HY dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size ( T5 variants and GPT-3 ) along with methods of injecting knowledge ( COMET ) into these models. Results show that the largest models, as expected, yield substantial improvements over base models and injecting external knowledge helps models of all sizes. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories. 1	7bb907e754942b832bacf7889ba1d6bd72945ca0	@None{lal-2022-using,  author = {Yash Kumar Lal},  title = {Using Commonsense Knowledge to Answer Why-Questions},  year = {2022} }
Using cognitive psychology to understand GPT-3	2022	https://www.semanticscholar.org/paper/6a53eeada90d83b9508e7e451d62fdc9d2476350	GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities are assessed on a battery of canonical experiments from the literature to enrich the understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artiﬁcial agents.	maybe	12	We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.	6a53eeada90d83b9508e7e451d62fdc9d2476350	@['JournalArticle']{binz-schulz-2022-using,  author = {Marcel Binz and Eric Schulz},  booktitle = {Proceedings of the National Academy of Sciences of the United States of America},  journal = {Proceedings of the National Academy of Sciences of the United States of America},  pages = {           e2218523120         },  title = {Using cognitive psychology to understand GPT-3},  volume = {120 6},  year = {2022} }
User Generated Data: Achilles' heel of BERT	2020	http://www.semanticscholar.org/paper/51782ae310e404090dad8828ef5c11846f7fd257	This work systematically shows that BERT's performance on fundamental tasks like sentiment analysis and textual similarity drops significantly as the authors introduce noise in data in the form of spelling mistakes and typos.	maybe	5	Owing to BERT's phenomenal success on various NLP tasks and benchmark datasets, industry practitioners have started to experiment with incorporating BERT to build applications to solve industry use cases. Industrial NLP applications are known to deal with much more noisy data as compared to benchmark datasets. In this work we systematically show that when the text data is noisy, there is a significant degradation in the performance of BERT. While this work is motivated from our business use case of building NLP applications for user generated text data which is known to be very noisy, our findings are applicable across various use cases in the industry. Specifically, we show that BERT's performance on fundamental tasks like sentiment analysis and textual similarity drops significantly as we introduce noise in data in the form of spelling mistakes and typos. For our experiments we use three well known datasets - IMDB movie reviews, SST-2 and STS-B to measure the performance. Further, we identify the shortcomings in the BERT pipeline that are responsible for this drop in performance.	51782ae310e404090dad8828ef5c11846f7fd257	@['JournalArticle', 'Review']{kumar-etal-2020-user,  author = {Ankit Kumar and Piyush Makhija and Anuj Gupta},  booktitle = {ArXiv},  journal = {ArXiv},  title = {User Generated Data: Achilles' heel of BERT},  volume = {abs/2003.12932},  year = {2020} }
Upstream Mitigation Is   Not  All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models	2022	http://www.semanticscholar.org/paper/8d863cafea3493fb033fcdcf9f272a1a4912628b	The bias transfer hypothesis is investigated: the theory that social biases internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning.	maybe	5	A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.	8d863cafea3493fb033fcdcf9f272a1a4912628b	@['JournalArticle', 'Conference']{steed-etal-2022-upstream,  author = {Ryan Steed and Swetasudha Panda and Ari Kobren and Michael L. Wick},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3524-3542},  title = {Upstream Mitigation Is   Not  All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models},  year = {2022} }
Unsupervised Transfer Learning via BERT Neuron Selection	2019	http://www.semanticscholar.org/paper/b76f4257a466c279adc04754233bf7f9deb6ea36	A method for selecting the most important neurons to solve a specific classification task and it is found that the source and target data sources with higher degrees of similarity between their task-specific fingerprints demonstrate a better transferability property.	maybe	4	Recent advancements in language representation models such as BERT have led to a rapid improvement in numerous natural language processing tasks. However, language models usually consist of a few hundred million trainable parameters with embedding space distributed across multiple layers, thus making them challenging to be fine-tuned for a specific task or to be transferred to a new domain. To determine whether there are task-specific neurons that can be exploited for unsupervised transfer learning, we introduce a method for selecting the most important neurons to solve a specific classification task. This algorithm is further extended to multi-source transfer learning by computing the importance of neurons for several single-source transfer learning scenarios between different subsets of data sources. Besides, a task-specific fingerprint for each data source is obtained based on the percentage of the selected neurons in each layer. We perform extensive experiments in unsupervised transfer learning for sentiment analysis, natural language inference and sentence similarity, and compare our results with the existing literature and baselines. Significantly, we found that the source and target data sources with higher degrees of similarity between their task-specific fingerprints demonstrate a better transferability property. We conclude that our method can lead to better performance using just a few hundred task-specific and interpretable neurons.	b76f4257a466c279adc04754233bf7f9deb6ea36	@['JournalArticle']{valipour-etal-2019-unsupervised,  author = {M. Valipour and E. Lee and Jaime R. Jamacaro and C. Bessega},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Unsupervised Transfer Learning via BERT Neuron Selection},  volume = {abs/1912.05308},  year = {2019} }
Unsupervised Domain Clusters in Pretrained Language Models	2020	http://www.semanticscholar.org/paper/95856e0789481eedc2cedc413581a0a819ef8fc8	It is shown that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data and proposing domain data selection methods based on such models, which require only a small set of in-domain monolingual data.	maybe	114	The notion of “in-domain data” in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision – suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.	95856e0789481eedc2cedc413581a0a819ef8fc8	@['JournalArticle', 'Conference']{aharoni-goldberg-2020-unsupervised,  author = {Roee Aharoni and Yoav Goldberg},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7747-7763},  title = {Unsupervised Domain Clusters in Pretrained Language Models},  year = {2020} }
Unsupervised Distillation of Syntactic Information from Contextualized Word Representations	2020	https://www.semanticscholar.org/paper/c84e20bc2b8f8e4bc1de218f9ca8a9511ea4ada1	This work aims to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information in the vectors, and automatically generates groups of sentences which are structurally similar but semantically different.	maybe	6	Contextualized word representations, such as ELMo and BERT, were shown to perform well on various semantic and syntactic task. In this work, we tackle the task of unsupervised disentanglement between semantics and structure in neural language representations: we aim to learn a transformation of the contextualized vectors, that discards the lexical semantics, but keeps the structural information. To this end, we automatically generate groups of sentences which are structurally similar but semantically different, and use metric-learning approach to learn a transformation that emphasizes the structural component that is encoded in the vectors. We demonstrate that our transformation clusters vectors in space by structural properties, rather than by lexical semantics. Finally, we demonstrate the utility of our distilled representations by showing that they outperform the original contextualized representations in a few-shot parsing setting.	c84e20bc2b8f8e4bc1de218f9ca8a9511ea4ada1	@['JournalArticle']{ravfogel-etal-2020-unsupervised,  author = {Shauli Ravfogel and Yanai Elazar and J. Goldberger and Yoav Goldberg},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {91-106},  title = {Unsupervised Distillation of Syntactic Information from Contextualized Word Representations},  year = {2020} }
Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology	2022	http://www.semanticscholar.org/paper/2faed6139986c213de2f2dbdfd1f897457edb50f	This work proposes a fully unsupervised method to detect bias in contextualized embeddings and introduces the concept of an ideological subspace, shows how it can be found by applying the method to online discussion forums, and presents techniques to probe it.	maybe	0	We propose a fully unsupervised method to detect bias in contextualized embeddings. The method leverages the assortative information latently encoded by social networks and combines orthogonality regularization, structured sparsity learning, and graph neural networks to ﬁnd the embedding subspace capturing this information. As a concrete example, we focus on the phenomenon of ideological bias: we introduce the concept of an ideological subspace, show how it can be found by applying our method to online discussion forums, and present techniques to probe it. Our experiments suggest that the ideological subspace encodes abstract evaluative semantics and reﬂects changes in the political left-right spectrum during the presidency of Donald Trump.	2faed6139986c213de2f2dbdfd1f897457edb50f	@['JournalArticle', 'Conference']{hofmann-etal-2022-unsupervised,  author = {Valentin Hofmann and J. Pierrehumbert and Hinrich Schütze},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Unsupervised Detection of Contextualized Embedding Bias with Application to Ideology},  volume = {abs/2212.07547},  year = {2022} }
Unraveling the Mystery of Artifacts in Machine Generated Text	2022	http://www.semanticscholar.org/paper/b3da1b63ad766763b425e04e4b8d552188f93459	This work proposes to systematically study the forms and scopes of artifacts by corrupting texts, replacing them with linguistic or statistical features, and applying the interpretable method of Integrated Gradients.	maybe	1	As neural Text Generation Models (TGM) have become more and more capable of generating text indistinguishable from human-written ones, the misuse of text generation technologies can have serious ramifications. Although a neural classifier often achieves high detection accuracy, the reason for it is not well studied. Most previous work revolves around studying the impact of model structure and the decoding strategy on ease of detection, but little work has been done to analyze the forms of artifacts left by the TGM. We propose to systematically study the forms and scopes of artifacts by corrupting texts, replacing them with linguistic or statistical features, and applying the interpretable method of Integrated Gradients. Comprehensive experiments show artifacts a) primarily relate to token co-occurrence, b) feature more heavily at the head of vocabulary, c) appear more in content word than stopwords, d) are sometimes detrimental in the form of number of token occurrences, e) are less likely to exist in high-level semantics or syntaxes, f) manifest in low concreteness values for higher-order n-grams.	b3da1b63ad766763b425e04e4b8d552188f93459	@['JournalArticle']{pu-etal-2022-unraveling,  author = {Jiashu Pu and Z. Huang and Yadong Xi and Guandan Chen and Weijie Chen and Rongsheng Zhang},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {6889-6898},  title = {Unraveling the Mystery of Artifacts in Machine Generated Text},  year = {2022} }
UNQOVERing Stereotypical Biases via Underspecified Questions	2020	http://www.semanticscholar.org/paper/f72983cef733670d6915e37383257f548b5a3365	UNQOVER, a general framework to probe and quantify biases through underspecified questions, is presented, showing that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence.	maybe	34	While language embeddings have been shown to have stereotyping biases, how these biases affect downstream question answering (QA) models remains unexplored. We present UNQOVER, a general framework to probe and quantify biases through underspecified questions. We show that a naive use of model scores can lead to incorrect bias estimates due to two forms of reasoning errors: positional dependence and question independence. We design a formalism that isolates the aforementioned errors. As case studies, we use this metric to analyze four important classes of stereotypes: gender, nationality, ethnicity, and religion. We probe five transformer-based QA models trained on two QA datasets, along with their underlying language models. Our broad study reveals that (1) all these models, with and without fine-tuning, have notable stereotyping biases in these classes; (2) larger models often have higher bias; and (3) the effect of fine-tuning on bias varies strongly with the dataset and the model size.	f72983cef733670d6915e37383257f548b5a3365	@['JournalArticle']{li-etal-2020-unqovering,  author = {Tao Li and Daniel Khashabi and Tushar Khot and Ashish Sabharwal and Vivek Srikumar},  booktitle = {Findings},  pages = {3475-3489},  title = {UNQOVERing Stereotypical Biases via Underspecified Questions},  year = {2020} }
Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens	2021	http://www.semanticscholar.org/paper/89e5ffd5ce92011e234f0b0ea0d6f2e43647b463	Various analyses based on word predictions of a large-scale BERT language model demonstrate that people with disabilities can be disadvantaged.	maybe	7	Much of the world’s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuat-ing ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically signiﬁcant results demonstrate that people with disabilities can be disadvantaged. Findings also explore over-lapping forms of discrimination related to in-terconnected gender and race identities.	89e5ffd5ce92011e234f0b0ea0d6f2e43647b463	@['JournalArticle', 'Conference']{hassan-etal-2021-unpacking,  author = {Saad Hassan and Matt Huenerfauth and Cecilia Ovesdotter Alm},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens},  volume = {abs/2110.00521},  year = {2021} }
Unpacking Large Language Models with Conceptual Consistency	2022	http://www.semanticscholar.org/paper/4f4a80148cb8f328aeaee68b34f9797cfb5ea150	This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are, and shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts.	maybe	1	If a Large Language Model (LLM) answers “yes” to the question “Are mountains tall?” then does it know what a mountain is? Can you rely on it responding correctly or incorrectly to other questions about mountains? The success of Large Language Models (LLMs) indicates they are increasingly able to answer queries like these accurately, but that ability does not necessarily imply a general understanding of concepts relevant to the anchor query. We propose conceptual consistency to measure a LLM’s understanding of relevant concepts. This novel metric measures how well a model can be characterized by finding out how consistent its responses to queries about conceptually relevant background knowledge are. To compute it we extract background knowledge by traversing paths between concepts in a knowledge base and then try to predict the model’s response to the anchor query from the background knowledge. We investigate the performance of current LLMs in a commonsense reasoning setting using the CSQA dataset and the ConceptNet knowledge base. While conceptual consistency, like other metrics, does increase with the scale of the LLM used, we find that popular models do not necessarily have high conceptual consistency. Our analysis also shows significant variation in conceptual consistency across different kinds of relations, concepts, and prompts. This serves as a step toward building models that humans can apply a theory of mind to, and thus interact with intuitively.	4f4a80148cb8f328aeaee68b34f9797cfb5ea150	@['JournalArticle']{sahu-etal-2022-unpacking,  author = {Pritish Sahu and Michael Cogswell and Yunye Gong and Ajay Divakaran},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Unpacking Large Language Models with Conceptual Consistency},  volume = {abs/2209.15093},  year = {2022} }
UnNatural Language Inference	2020	http://www.semanticscholar.org/paper/2e3a7760c543a181b47245ffece91bff027c43c9	It is found that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations.	maybe	42	Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based Natural Language Understanding (NLU) models indicate that they appear to understand human-like syntax, at least to some extent. We provide novel evidence that complicates this claim: we find that state-of-the-art Natural Language Inference (NLI) models assign the same labels to permuted examples as they do to the original, i.e. they are invariant to random word-order permutations. This behavior notably differs from that of humans; we struggle to understand the meaning of ungrammatical sentences. To measure the severity of this issue, we propose a suite of metrics and investigate which properties of particular permutations lead models to be word order invariant. For example, in MNLI dataset we find almost all (98.7%) examples contain at least one permutation which elicits the gold label. Models are even able to assign gold labels to permutations that they originally failed to predict correctly. We provide a comprehensive empirical evaluation of this phenomenon, and further show that this issue exists in pre-Transformer RNN / ConvNet based encoders, as well as across multiple languages (English and Chinese). Our code and data are available at https://github.com/facebookresearch/unlu.	2e3a7760c543a181b47245ffece91bff027c43c9	@['JournalArticle', 'Conference']{sinha-etal-2020-unnatural,  author = {Koustuv Sinha and Prasanna Parthasarathi and Joelle Pineau and Adina Williams},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7329-7346},  title = {UnNatural Language Inference},  year = {2020} }
Unmasking the Mask - Evaluating Social Biases in Masked Language Models	2021	http://www.semanticscholar.org/paper/437727b6c00a5eb4944600091f66f41626d1002d	The experimental results show that the proposed bias evaluation measures accurately detect different types of biases in MLMs, and unlike AUL and AULA, previously proposed measures for MLMs systematically overestimate the measured biases and are heavily influenced by the unmasked tokens in the context.	maybe	16	Masked Language Models (MLMs) have shown superior performances in numerous downstream Natural Language Processing (NLP) tasks. Unfortunately, MLMs also demonstrate significantly worrying levels of social biases. We show that the previously proposed evaluation metrics for quantifying the social biases in MLMs are problematic due to the following reasons: (1) prediction accuracy of the masked tokens itself tend to be low in some MLMs, which leads to unreliable evaluation metrics, and (2) in most downstream NLP tasks, masks are not used; therefore prediction of the mask is not directly related to them, and (3) high-frequency words in the training data are masked more often, introducing noise due to this selection bias in the test cases. Therefore, we propose All Unmasked Likelihood (AUL), a bias evaluation measure that predicts all tokens in a test case given the MLM embedding of the unmasked input and AUL with Attention weights (AULA) to evaluate tokens based on their importance in a sentence. Our experimental results show that the proposed bias evaluation measures accurately detect different types of biases in MLMs, and unlike AUL and AULA, previously proposed measures for MLMs systematically overestimate the measured biases and are heavily influenced by the unmasked tokens in the context.	437727b6c00a5eb4944600091f66f41626d1002d	@['JournalArticle', 'Conference']{kaneko-bollegala-2021-unmasking,  author = {Masahiro Kaneko and D. Bollegala},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {11954-11962},  title = {Unmasking the Mask - Evaluating Social Biases in Masked Language Models},  year = {2021} }
Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias	2020	http://www.semanticscholar.org/paper/0712334d1109248e52706f13aeff5281834727f8	It is shown that the method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German, which highlights the importance of investigating bias and mitigation techniques cross-linguistically.	maybe	46	Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically, especially in view of the current emphasis on large-scale, multilingual language models.	0712334d1109248e52706f13aeff5281834727f8	@['JournalArticle']{bartl-etal-2020-unmasking,  author = {Marion Bartl and M. Nissim and Albert Gatt},  booktitle = {GEBNLP},  journal = {ArXiv},  title = {Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias},  volume = {abs/2010.14534},  year = {2020} }
Universal Text Representation from BERT: An Empirical Study	2019	https://www.semanticscholar.org/paper/850713961a5aa20812cf952f950f09d491fae281	There is a gap between embedding-based method and in-domain fine-tuned BERT (the authors report new state-of-the-art results on two datasets), which suggests deep interactions between question and answer pairs are critical for those hard tasks.	seed	35	We present a systematic investigation of layer-wise BERT activations for general-purpose text representations to understand what linguistic information they capture and how transferable they are across different tasks. Sentence-level embeddings are evaluated against two state-of-the-art models on downstream and probing tasks from SentEval, while passage-level embeddings are evaluated on four question-answering (QA) datasets under a learning-to-rank problem setting. Embeddings from the pre-trained BERT model perform poorly in semantic similarity and sentence surface information probing tasks. Fine-tuning BERT on natural language inference data greatly improves the quality of the embeddings. Combining embeddings from different BERT layers can further boost performance. BERT embeddings outperform BM25 baseline significantly on factoid QA datasets at the passage level, but fail to perform better than BM25 on non-factoid datasets. For all QA datasets, there is a gap between embedding-based method and in-domain fine-tuned BERT (we report new state-of-the-art results on two datasets), which suggests deep interactions between question and answer pairs are critical for those hard tasks.	850713961a5aa20812cf952f950f09d491fae281	@['JournalArticle']{ma-etal-2019-universal,  author = {Xiaofei Ma and Zhiguo Wang and Patrick Ng and Ramesh Nallapati and Bing Xiang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Universal Text Representation from BERT: An Empirical Study},  volume = {abs/1910.07973},  year = {2019} }
Universal and Independent: Multilingual Probing Framework for Exhaustive Model Interpretation and Evaluation	2022	http://www.semanticscholar.org/paper/a93a11e0b6bb8b01379b8b442780f7aad82540f8	A toolkit to systematize the multilingual ﬂaws in multilingual models is proposed, providing a reproducible experimental setup for 104 languages and 80 morphosyntactic features present in the Universal Dependencies data.	maybe	1	Linguistic analysis of language models is one of the ways to explain and describe their reasoning, weaknesses, and limitations. In the probing part of the model interpretability research, studies concern individual languages as well as individual linguistic structures. The question arises: are the detected regularities linguistically coherent, or on the contrary, do they dissonate at the typological scale? Moreover, the majority of studies address the inherent set of languages and linguistic structures, leaving the actual typological diversity knowledge out of scope.In this paper, we present and apply the GUI-assisted framework allowing us to easily probe massive amounts of languages for all the morphosyntactic features present in the Universal Dependencies data. We show that reflecting the anglo-centric trend in NLP over the past years, most of the regularities revealed in the mBERT model are typical for the western-European languages. Our framework can be integrated with the existing probing toolboxes, model cards, and leaderboards, allowing practitioners to use and share their familiar probing methods to interpret multilingual models.Thus we propose a toolkit to systematize the multilingual flaws in multilingual models, providing a reproducible experimental setup for 104 languages and 80 morphosyntactic features.	a93a11e0b6bb8b01379b8b442780f7aad82540f8	@['JournalArticle']{serikov-etal-2022-universal,  author = {Oleg Serikov and Vitaly Protasov and E. Voloshina and V. Knyazkova and Tatiana Shavrina},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Universal and Independent: Multilingual Probing Framework for Exhaustive Model Interpretation and Evaluation},  volume = {abs/2210.13236},  year = {2022} }
Universal Adversarial Triggers for Attacking and Analyzing NLP	2019	https://www.semanticscholar.org/paper/18a1c21f35153c45d0ef30c564bffb7d70a13ccc		seed			18a1c21f35153c45d0ef30c564bffb7d70a13ccc	
Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis	2020	http://www.semanticscholar.org/paper/1a0e746a185ffbcbbd9e2e7f8404dc03d89852d9	It is shown that contextualized and non-contextualized embeddings representing Black women as simultaneously less feminine than White women, and less Black than Black men, aligns with intersectionality theory.	maybe	7	We present a new approach for detecting human-like social biases in word embeddings using representational similarity analysis. Specifically, we probe contextualized and non-contextualized embeddings for evidence of intersectional biases against Black women. We show that these embeddings represent Black women as simultaneously less feminine than White women, and less Black than Black men. This finding aligns with intersectionality theory, which argues that multiple identity categories (such as race or sex) layer on top of each other in order to create unique modes of discrimination that are not shared by any individual category.	1a0e746a185ffbcbbd9e2e7f8404dc03d89852d9	@['JournalArticle', 'Conference']{lepori-2020-unequal,  author = {Michael A. Lepori},  booktitle = {International Conference on Computational Linguistics},  pages = {1720-1728},  title = {Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis},  year = {2020} }
Understanding Transformer Memorization Recall Through Idioms	2022	http://www.semanticscholar.org/paper/7f939685b8f41775e6699b1cb3fec3f96bd12971	This work offers the first methodological framework for probing and characterizing recall of memorized sequences in transformer LMs, and provides a methodological basis for future studies of transformer memorization.	maybe	1	To produce accurate predictions, language models (LMs) must balance between generalization and memorization. Yet, little is known about the mechanism by which transformer LMs employ their memorization capacity. When does a model decide to output a memorized phrase, and how is this phrase then retrieved from memory? In this work, we offer the ﬁrst methodological framework for probing and characterizing recall of memorized sequences in transformer LMs. First, we lay out criteria for detecting model inputs that trigger memory recall, and propose idioms as inputs that fulﬁll these criteria. Next, we construct a dataset of English idioms and use it to compare model behavior on memorized vs. non-memorized inputs. Speciﬁcally, we analyze the internal prediction construction process by interpreting the model’s hidden representations as a gradual reﬁnement of the output probability distribution. We ﬁnd that across different model sizes and architectures, memorized predictions are a two-step process: early layers promote the predicted token to the top of the output distribution, and upper layers increase model conﬁdence. This suggests that memorized information is stored and retrieved in the early layers of the network. Last, we demonstrate the utility of our methodology beyond idioms in memorized factual statements. Overall, our work makes a ﬁrst step towards understanding memory recall, and provides a methodological basis for future studies of transformer memorization. 1	7f939685b8f41775e6699b1cb3fec3f96bd12971	@['JournalArticle']{haviv-etal-2022-understanding,  author = {Adi Haviv and Ido Cohen and Jacob Gidron and R. Schuster and Yoav Goldberg and Mor Geva},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding Transformer Memorization Recall Through Idioms},  volume = {abs/2210.03588},  year = {2022} }
Understanding the combined meaning of words	2022	http://www.semanticscholar.org/paper/2a410f1732e5ff8be8b43432773b4f5c2c6507c3		maybe	0		2a410f1732e5ff8be8b43432773b4f5c2c6507c3	@None{erk-2022-understanding,  author = {K. Erk},  booktitle = {Nature Computational Science},  journal = {Nature Computational Science},  pages = {701 - 702},  title = {Understanding the combined meaning of words},  volume = {2},  year = {2022} }
Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing	2022	http://www.semanticscholar.org/paper/ed5ebed7ff668fd7362d531a40b49b3aea33b3a9	This paper proposes a new framework for robustly measuring and quantifying biases exhibited by generative language models and uses this framework to investigate GPT-3’s occupational gender bias and propose prompting techniques for mitigating these biases without the need for tuning.	maybe	0	Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics. These ﬁndings prompted large efforts aiming to understand and measure such effects, with the goal of providing benchmarks that can guide the development of techniques mitigating these stereotypical associations. However, as recent research has pointed out, the current benchmarks lack a robust experimental setup, consequently hinder-ing the inference of meaningful conclusions from their evaluation metrics. In this paper, we extend these arguments and demonstrate that existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain from benchmarking language models based on them. Accordingly, we propose a new framework for robustly measuring and quantifying biases exhibited by generative language models. Finally, we use this framework to investigate GPT-3’s occupational gender bias and propose prompting techniques for mitigating these biases without the need for ﬁne-tuning.	ed5ebed7ff668fd7362d531a40b49b3aea33b3a9	@['JournalArticle']{mattern-etal-2022-understanding,  author = {Justus Mattern and Zhijing Jin and Mrinmaya Sachan and Rada Mihalcea and B. Schölkopf},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing},  volume = {abs/2212.10678},  year = {2022} }
Understanding Multi-Head Attention in Abstractive Summarization	2019	https://www.semanticscholar.org/paper/317d2ac530e1db49229d6c442f50722db85afbb7	This paper investigates the interpretability of multi-head attention in abstractive summarization, a sequence-to-sequence task for which attention does not have an intuitive alignment role, such as in machine translation.	seed	12	Attention mechanisms in deep learning architectures have often been used as a means of transparency and, as such, to shed light on the inner workings of the architectures. Recently, there has been a growing interest in whether or not this assumption is correct. In this paper we investigate the interpretability of multi-head attention in abstractive summarization, a sequence-to-sequence task for which attention does not have an intuitive alignment role, such as in machine translation. We first introduce three metrics to gain insight in the focus of attention heads and observe that these heads specialize towards relative positions, specific part-of-speech tags, and named entities. However, we also find that ablating and pruning these heads does not lead to a significant drop in performance, indicating redundancy. By replacing the softmax activation functions with sparsemax activation functions, we find that attention heads behave seemingly more transparent: we can ablate fewer heads and heads score higher on our interpretability metrics. However, if we apply pruning to the sparsemax model we find that we can prune even more heads, raising the question whether enforced sparsity actually improves transparency. Finally, we find that relative positions heads seem integral to summarization performance and persistently remain after pruning.	317d2ac530e1db49229d6c442f50722db85afbb7	@['JournalArticle']{baan-etal-2019-understanding,  author = {Joris Baan and Maartje ter Hoeve and M. V. D. Wees and Anne Schuth and M. de Rijke},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding Multi-Head Attention in Abstractive Summarization},  volume = {abs/1911.03898},  year = {2019} }
Understanding Language Model from Questions in Social Studies for Students	2021	http://www.semanticscholar.org/paper/77a3141060294e45bb7ce369d5342436306c3271	What the pre-trained Japanese BERT and RoBERTa language models know is discussed and masked questions for Japanese students using the deep learning artificial intelligence using these language models are solved and investigated their knowledge and the dependence of their accuracies on the domain.	maybe	0	Artificial intelligence, especially artificial intelligence based on deep neural networks, has improved significantly. In particular, great progress has been made in natural language processing and image recognition. In the natural language processing field, many techniques and methods, such as transformer, attention, and self-attention, have been proposed and have improved this field. Recently, BERT and RoBERTa are expected to be some of the most promising natural processing technologies. Through deep learning has achieved high accuracy in various fields such as natural language processing, it has been pointed out that it lacks interpretability and explainability for decision. For addressing this issue, providing interpretability and explainable AI have been studied. For BERT and RoBERTa, discussions on understanding what language models know about language have been studied. In this paper, we discuss what the pre-trained Japanese BERT and RoBERTa language models know. We solved masked questions for Japanese students using the deep learning artificial intelligence using these language models and investigated their knowledge and the dependence of their accuracies on the domain.	77a3141060294e45bb7ce369d5342436306c3271	@['JournalArticle', 'Conference']{kawashima-yamaguchi-2021-understanding,  author = {Kaito Kawashima and Saneyasu Yamaguchi},  booktitle = {2021 IEEE International Conference on Big Data (Big Data)},  journal = {2021 IEEE International Conference on Big Data (Big Data)},  pages = {5932-5934},  title = {Understanding Language Model from Questions in Social Studies for Students},  year = {2021} }
Understanding How Model Size Affects Few-shot Instruction Prompting	2022	http://www.semanticscholar.org/paper/72491b96d8a614d1a9a099707d44593d4b5a8f49	A weak inverse scaling trend is shown, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes, and it is shown that increasing the number of examples tend to disproportionately beneﬁt larger models than smaller models.	maybe	0	Large Language Models are aﬀected by the phenomena of memorizing and forgetting their training data. But how do these vary by model size? We work towards this question by investigating how the model size aﬀects the model’s ability to discriminate a word’s meaning in a given context. We introduce a dataset called DeltaWords, which evaluates a model’s ability to follow instructions to select a sentence which replaces the target word with its antonym. We show a weak inverse scaling trend, where task accuracy degrades as model size increase, under extremely few-shot prompting regimes. We show that increasing the number of examples tend to disproportionately beneﬁt larger models than smaller models.	72491b96d8a614d1a9a099707d44593d4b5a8f49	@['JournalArticle']{joaquin-haroen-2022-understanding,  author = {Ayrton San Joaquin and Ardy Haroen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding How Model Size Affects Few-shot Instruction Prompting},  volume = {abs/2212.01907},  year = {2022} }
Understanding Finetuning for Factual Knowledge Extraction from Language Models	2023	https://www.semanticscholar.org/paper/83c2bb56f58d4ce63adb2faf073fc35c3515cda8	This paper analyzes ﬁnetuned LMs for factual knowledge extraction and shows that Frequency Shock leads to a degradation in the predictions of the model and beyond a point, the harm from Frequency Shock can even outweigh the positive eﬀects of ﬂnetuning, making  aknetuning harmful overall.	maybe	0	Language models (LMs) pretrained on large corpora of text from the web have been observed to contain large amounts of various types of knowledge about the world. This observation has led to a new and exciting paradigm in knowledge graph construction where, instead of manual curation or text mining, one extracts knowledge from the parameters of an LM. Recently, it has been shown that ﬁnetuning LMs on a set of factual knowledge makes them produce better answers to queries from a diﬀerent set, thus making ﬁnetuned LMs a good candidate for knowledge extraction and, consequently, knowledge graph construction. In this paper, we analyze ﬁnetuned LMs for factual knowledge extraction. We show that along with its previously known positive eﬀects, ﬁnetuning also leads to a (potentially harmful) phenomenon which we call Frequency Shock , where at the test time the model over-predicts rare entities that appear in the training set and under-predicts common entities that do not appear in the training set enough times. We show that Frequency Shock leads to a degradation in the predictions of the model and beyond a point, the harm from Frequency Shock can even outweigh the positive eﬀects of ﬁnetuning, making ﬁnetuning harmful overall. We then consider two solutions to remedy the identiﬁed negative eﬀect: 1- model mixing and 2- mixture ﬁnetuning with the LM’s pre-training task. The two solutions combined lead to signiﬁcant improvements compared to vanilla ﬁnetuning.	83c2bb56f58d4ce63adb2faf073fc35c3515cda8	@['JournalArticle']{kazemi-etal-2023-understanding,  author = {Mehran Kazemi and Sid Mittal and Deepak Ramachandran},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding Finetuning for Factual Knowledge Extraction from Language Models},  volume = {abs/2301.11293},  year = {2023} }
Understanding Few-Shot Commonsense Knowledge Models	2021	http://www.semanticscholar.org/paper/1b0c50a9856cd64be9f31c496d689a5f3374c83a	This work investigates training commonsense knowledge models in a fewshot setting with limited tuples per commonsense relation in the graph and finds that human quality ratings for knowledge produced from a few-shot trained system can achieve performance within 6% of knowledgeproduced from fully supervised systems.	maybe	5	Providing natural language processing systems with commonsense knowledge is a critical challenge for achieving language understanding. Recently, commonsense knowledge models (Bosselut et al., 2019) have emerged as a suitable approach for hypothesizing situation-relevant commonsense knowledge on-demand in natural language applications. However, these systems are limited by the fixed set of relations captured by schemas of the knowledge bases on which they’re trained. To address this limitation, we investigate training commonsense knowledge models in a fewshot setting with limited tuples per commonsense relation in the graph. We perform five separate studies on different dimensions of few-shot commonsense knowledge learning, providing a roadmap on best practices for training these systems efficiently. Importantly, we find that human quality ratings for knowledge produced from a few-shot trained system can achieve performance within 6% of knowledge produced from fully supervised systems. This few-shot performance enables coverage of a wide breadth of relations in future commonsense systems.	1b0c50a9856cd64be9f31c496d689a5f3374c83a	@['JournalArticle']{da-etal-2021-understanding,  author = {Jeff Da and Ronan Le Bras and Ximing Lu and Yejin Choi and Antoine Bosselut},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding Few-Shot Commonsense Knowledge Models},  volume = {abs/2101.00297},  year = {2021} }
Understanding Domain Learning in Language Models Through Subpopulation Analysis	2022	https://www.semanticscholar.org/paper/18c0751db3689ffe3ea62805c131824773d454c6	It is shown that increasing the model capacity impacts how domain information is stored in upper and lower layers differently, and that larger experimental models simultaneously embed domain-speciﬁc information as if they were conjoined control models.	maybe	0	We investigate how different domains are encoded in modern neural network architectures. We analyze the relationship between natural language domains, model size, and the amount of training data used. The primary analysis tool we develop is based on subpopulation analysis with Singular Vector Canonical Correlation Analysis (SVCCA), which we apply to Transformer-based language models (LMs). We compare the latent representations of such a language model at its different layers from a pair of models: a model trained on multiple domains (an experimental model) and a model trained on a single domain (a control model). Through our method, we find that increasing the model capacity impacts how domain information is stored in upper and lower layers differently. In addition, we show that larger experimental models simultaneously embed domain-specific information as if they were conjoined control models. These findings are confirmed qualitatively, demonstrating the validity of our method.	18c0751db3689ffe3ea62805c131824773d454c6	@['JournalArticle']{zhao-etal-2022-understanding,  author = {Zheng Zhao and Yftah Ziser and Shay B. Cohen},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Understanding Domain Learning in Language Models Through Subpopulation Analysis},  volume = {abs/2210.12553},  year = {2022} }
Understanding Commonsense Inference Aptitude of Deep Contextual Representations	2019	https://www.semanticscholar.org/paper/80dc7b0e6dbc26571672d9be57a0ae589689e410		seed	4		80dc7b0e6dbc26571672d9be57a0ae589689e410	@None{da-kasai-2019-understanding,  author = {Jeff Da and Jungo Kasai},  booktitle = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},  journal = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},  title = {Understanding Commonsense Inference Aptitude of Deep Contextual Representations},  year = {2019} }
Understanding Attention in Machine Reading Comprehension	2021	http://www.semanticscholar.org/paper/0460cf6c7fd97ce806adb342aa38e06cc2dc891f	It is discovered that passage-to-question and passage understanding attentions are the most important ones, showing strong correlations to the final performance than other parts, which could be helpful to understand how these models solve the questions.	maybe	3	Achieving human-level performance on some of Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, the internal mechanism of these artifacts still remains unclear, placing an obstacle for further understanding these models. This paper focuses on conducting a series of analytical experiments to examine the relations between the multi-head self-attention and the final performance, trying to analyze the potential explainability in PLM-based MRC models. We perform quantitative analyses on SQuAD (English) and CMRC 2018 (Chinese), two span-extraction MRC datasets, on top of BERT, ALBERT, and ELECTRA in various aspects. We discover that passage-to-question and passage understanding attentions are the most important ones, showing strong correlations to the final performance than other parts. Through visualizations and case studies, we also observe several general findings on the attention maps, which could be helpful to understand how these models solve the questions.	0460cf6c7fd97ce806adb342aa38e06cc2dc891f	@['JournalArticle']{cui-etal-2021-understanding,  author = {Yiming Cui and Weinan Zhang and Wanxiang Che and Ting Liu and Zhigang Chen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Understanding Attention in Machine Reading Comprehension},  volume = {abs/2108.11574},  year = {2021} }
Understanding Attention for Text Classification	2020	http://www.semanticscholar.org/paper/94e586cd3342940422c0bd01ad7f252db9327394	A study on understanding the internal mechanism of attention by looking into the gradient update process, and proposing to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token's significance.	maybe	31	Attention has been proven successful in many natural language processing (NLP) tasks. Recently, many researchers started to investigate the interpretability of attention on NLP tasks. Many existing approaches focused on examining whether the local attention weights could reflect the importance of input representations. In this work, we present a study on understanding the internal mechanism of attention by looking into the gradient update process, checking its behavior when approaching a local minimum during training. We propose to analyze for each word token the following two quantities: its polarity score and its attention score, where the latter is a global assessment on the token’s significance. We discuss conditions under which the attention mechanism may become more (or less) interpretable, and show how the interplay between the two quantities can contribute towards model performance.	94e586cd3342940422c0bd01ad7f252db9327394	@['JournalArticle', 'Conference']{sun-lu-2020-understanding,  author = {Xiaobing Sun and Wei Lu},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3418-3428},  title = {Understanding Attention for Text Classification},  year = {2020} }
Underreporting of errors in NLG output, and what to do about it	2021	http://www.semanticscholar.org/paper/941b658b093c8d4fffeaf9c8d309f8adb0fd5291	There is a severe under-reporting of the different kinds of errors that Natural Language Generation systems make, and this position paper provides recommendations for error identification, analysis and reporting.	maybe	13	We observe a severe under-reporting of the different kinds of errors that Natural Language Generation systems make. This is a problem, because mistakes are an important indicator of where systems should still be improved. If authors only report overall performance metrics, the research community is left in the dark about the specific weaknesses that are exhibited by ‘state-of-the-art’ research. Next to quantifying the extent of error under-reporting, this position paper provides recommendations for error identification, analysis and reporting.	941b658b093c8d4fffeaf9c8d309f8adb0fd5291	@['JournalArticle']{miltenburg-etal-2021-underreporting,  author = {Emiel van Miltenburg and Miruna Clinciu and Ondrej Dusek and Dimitra Gkatzia and Stephanie Inglis and Leo Leppanen and Saad Mahamood and Emma Manning and S. Schoch and Craig Thomson and Luou Wen},  booktitle = {International Conference on Natural Language Generation},  journal = {ArXiv},  title = {Underreporting of errors in NLG output, and what to do about it},  volume = {abs/2108.01182},  year = {2021} }
Uncovering Semantic Bias in Neural Network Models Using a Knowledge Graph	2020	http://www.semanticscholar.org/paper/43e292ac6230ee1f14846910a8fc276d0e908646	This paper applies rule mining using knowledge graphs in combination with neural network explanation methods to uncover systematic preferences of trained neural models and capture them in the form of conjunctive rules.	maybe	4	While neural networks models have shown impressive performance in many NLP tasks, lack of interpretability is often seen as a disadvantage. Individual relevance scores assigned by post-hoc explanation methods are not sufficient to show deeper systematic preferences and potential biases of the model that apply consistently across examples. In this paper we apply rule mining using knowledge graphs in combination with neural network explanation methods to uncover such systematic preferences of trained neural models and capture them in the form of conjunctive rules. We test our approach in the context of text classification tasks and show that such rules are able to explain a substantial part of the model behaviour as well as indicate potential causes of misclassifications when the model is applied outside of the initial training context.	43e292ac6230ee1f14846910a8fc276d0e908646	@['JournalArticle', 'Book', 'Conference']{nikolov-d’aquin-2020-uncovering,  author = {A. Nikolov and M. d’Aquin},  booktitle = {International Conference on Information and Knowledge Management},  journal = {Proceedings of the 29th ACM International Conference on Information & Knowledge Management},  title = {Uncovering Semantic Bias in Neural Network Models Using a Knowledge Graph},  year = {2020} }
Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns	2022	http://www.semanticscholar.org/paper/d00c436974359a250f0869aa548e94324c9a62a5	It is suggested that the lack of generalization observable in the study means that the PLMs are currently not learning NLI, but rather spurious heuristics.	maybe	1	In this article, we explore the shallow heuristics used by transformer-based pre-trained language models (PLMs) that are fine-tuned for natural language inference (NLI). To do so, we construct or own dataset based on syllogistic, and we evaluate a number of models’ performance on our dataset. We find evidence that the models rely heavily on certain shallow heuristics, picking up on symmetries and asymmetries between premise and hypothesis. We suggest that the lack of generalization observable in our study, which is becoming a topic of lively debate in the field, means that the PLMs are currently not learning NLI, but rather spurious heuristics.	d00c436974359a250f0869aa548e94324c9a62a5	@['JournalArticle']{gubelmann-handschuh-2022-uncovering,  author = {Reto Gubelmann and S. Handschuh},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Uncovering More Shallow Heuristics: Probing the Natural Language Inference Capacities of Transformer-Based Pre-Trained Language Models Using Syllogistic Patterns},  volume = {abs/2201.07614},  year = {2022} }
UnCommonSense: Informative Negative Knowledge about Everyday Concepts	2022	http://www.semanticscholar.org/paper/94e5ca13e8c884509ccad233c77979923b677a3e	Intrinsic and extrinsic evaluations show that the UNCOMMONSENSE framework for materializing informative negative commonsense statements significantly outperforms the state-of-the-art.	maybe	0	Commonsense knowledge about everyday concepts is an important asset for AI applications, such as question answering and chatbots. Recently, we have seen an increasing interest in the construction of structured commonsense knowledge bases (CSKBs). An important part of human commonsense is about properties that do not apply to concepts, yet existing CSKBs only store positive statements. Moreover, since CSKBs operate under the open-world assumption, absent statements are considered to have unknown truth rather than being invalid. This paper presents the UNCOMMONSENSE framework for materializing informative negative commonsense statements. Given a target concept, comparable concepts are identified in the CSKB, for which a local closed-world assumption is postulated. This way, positive statements about comparable concepts that are absent for the target concept become seeds for negative statement candidates. The large set of candidates is then scrutinized, pruned and ranked by informativeness. Intrinsic and extrinsic evaluations show that our method significantly outperforms the state-of-the-art. A large dataset of informative negations is released as a resource for future research.	94e5ca13e8c884509ccad233c77979923b677a3e	@['Book', 'JournalArticle', 'Conference']{arnaout-etal-2022-uncommonsense:,  author = {Hiba Arnaout and S. Razniewski and G. Weikum and Jeff Z. Pan},  booktitle = {International Conference on Information and Knowledge Management},  journal = {Proceedings of the 31st ACM International Conference on Information & Knowledge Management},  title = {UnCommonSense: Informative Negative Knowledge about Everyday Concepts},  year = {2022} }
Twenty Years Beyond the Turing Test: Moving Beyond the Human Judges Too	2020	http://www.semanticscholar.org/paper/ee7084bc397dae9a9c42940f7c4e6eda66cf3a31	This paper revisits some of the key questions surrounding the Turing test, such as ‘understanding’, commonsense reasoning and extracting meaning from the world, and explores how the new testing paradigms should work to unmask the limitations of current and future AI.	maybe	3		ee7084bc397dae9a9c42940f7c4e6eda66cf3a31	@['JournalArticle']{orallo-2020-twenty,  author = {J. Hernández-Orallo},  booktitle = {Minds and Machines},  journal = {Minds and Machines},  pages = {533 - 562},  title = {Twenty Years Beyond the Turing Test: Moving Beyond the Human Judges Too},  volume = {30},  year = {2020} }
Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets	2022	http://www.semanticscholar.org/paper/5ac18af328ae469544ed41c6ee37a21df001ea00	It is shown that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties, casting doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.	yes	15	We introduce a new class of attacks on machine learning models. We show that an adversary who can poison a training dataset can cause models trained on this dataset to leak significant private details of training points belonging to other parties. Our active inference attacks connect two independent lines of work targeting the integrity and privacy of machine learning training data. Our attacks are effective across membership inference, attribute inference, and data extraction. For example, our targeted attacks can poison <0.1% of the training dataset to boost the performance of inference attacks by 1 to 2 orders of magnitude. Further, an adversary who controls a significant fraction of the training data (e.g., 50%) can launch untargeted attacks that enable 8× more precise inference on all other users' otherwise-private data points. Our results cast doubts on the relevance of cryptographic privacy guarantees in multiparty computation protocols for machine learning, if parties can arbitrarily select their share of training data.	5ac18af328ae469544ed41c6ee37a21df001ea00	@['JournalArticle', 'Book', 'Conference']{tramèr-etal-2022-truth,  author = {Florian Tramèr and R. Shokri and Ayrton San Joaquin and Hoang M. Le and Matthew Jagielski and Sanghyun Hong and Nicholas Carlini},  booktitle = {Conference on Computer and Communications Security},  journal = {Proceedings of the 2022 ACM SIGSAC Conference on Computer and Communications Security},  title = {Truth Serum: Poisoning Machine Learning Models to Reveal Their Secrets},  year = {2022} }
Truth Machines: Synthesizing Veracity in AI Language Models	2023	https://www.semanticscholar.org/paper/f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3		maybe	0	As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. But truth is highly contested, with many different definitions and approaches. This article discusses the struggle for truth in AI systems and the general responses to date. It then investigates the production of truth in InstructGPT, a large language model, highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity. It conceptualizes this performance as an operationalization of truth, where distinct, often conflicting claims are smoothly synthesized and confidently presented into truth-statements. We argue that these same logics and inconsistencies play out in Instruct’s successor, ChatGPT, reiterating truth as a non-trivial problem. We suggest that enriching sociality and thickening “reality” are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude, however, by stepping back to consider AI truth-telling as a social practice: what kind of “truth” do we as listeners desire? Keywords— truthfulness, veracity, AI, large language model, GPT-3, InstructGPT, ChatGPT 1 ar X iv :2 30 1. 12 06 6v 1 [ cs .C Y ] 2 8 Ja n 20 23 ChatGPT was released with great fanfare in December 2022. OpenAI’s latest language model appeared to be powerful and almost magical, generating news articles, writing poetry, and explaining arcane concepts instantly and on demand. But a week later, the coding site StackOverflow banned all answers produced by the model. “The primary problem,” explained the staff, “is that while the answers which ChatGPT produces have a high rate of being incorrect, they typically look like they might be good and the answers are very easy to produce” (Vincent 2022). For a site aiming to provide correct answers to coding problems, the issue was clear: the AI model was “substantially harmful.” As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. Researchers have suggested that vulnerabilities in these models could be deployed by malicious actors to produce misinformation rapidly and at scale (Dhanjani 2021; Weidinger et.al. 2022). But more concerning is the everyday impact of this dependence on automated truth claims. For instance, incorrect advice on medical symptoms and drugs can lead to patient harm or death (Bickmore et al. 2018), with one medical chatbot based on GPT-3 already advising a patient to kill themselves (Quach 2022). Whether in medicine or other domains, belief in the oftenplausible claims of these AI oracles can lead to unwarranted trust in questionable models (Passi and Vorvoreanu 2022). Such potentials increasingly proliferate with AI’s deployment across industries and social fields, testifying to the stakes of truth in AI systems. But while AI systems are increasingly given authority and invested with veracity, truth is highly contested. There are many different understandings of what truth means and how we might arrive at a truthful claim, and how truth may be verified or evaluated. No longer limited to binary notions of true or false, AI systems instead rely on degrees of truth, and may attempt to use a dataset’s implicit features, employ explicit fact checking, or appeal to authority as a method (García Lozano 2020). Osterlind (2019) suggests that quantitative methods reveal unexpected patterns, challenging old fashioned notions of fact and accuracy based on biased human assumptions. And Maruyama (2022) concludes that truth in data science may be regarded as “post-truth,” fundamentally different from truth in traditional science. Choosing an approach to truth and implementing it within a computational system is not given, but must be decided. We stress then that truth in AI is not just technical but also social, cultural, and political, drawing on particular norms and values. And yet we also recognise that the technical matters: translating truth theories into actionable architectures and processes updates them in significant ways. These disparate sociotechnical forces coalesce into a final AI model which purports to tell the truth—and in doing so, our understanding of “truth” is remade. “The ideal of truth is a fallacy for semantic interpretation and needs to be changed,” suggested two AI researchers (Welty and Aroyo 2015). This article is interested less in truth as a function of AI—how accurate a given model is, according to criteria. Rather it focuses on what the advent of AI—and specifically of language models like ChatGPT—means for the relation between truth and language. The first section discusses the contested nature of truth and the problems that it represents within AI models. The second section builds on these ideas by examining InstructGPT, an important large language model, highlighting the disparate approaches to evaluating and producing truth embedded in its social and technical layers. The third section discusses how the model synthesizes these disparate approaches into a functional machine that can generate truth claims on demand, a dynamic we term the operationalization of truth. The fourth section shows how these same logics and inconsistencies play out in Instruct’s successor, ChatGPT, reiterating once more truth as a non-trivial problem. And the fifth section suggests that enriching sociality and thickening “reality” are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude by turning to Foucault’s Discourse and Truth (2019) to reflect on the role that these verification machines should play. If truth claims emerge from a certain arrangement of social actors and associated expectations, then these questions can be posed about language models as much as human interlocutors: what is the truth we are looking for? Risking paradox, we could ask further: what is AI’s true truth? 1. AI’s Struggle For Truth The de-facto understanding of truth in AI models is centered around “ground truth.” This is often referred to as the “fundamental truth” underpinning testing and training data or the “reality” that a developer	f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3	@['JournalArticle']{munn-etal-2023-truth,  author = {Luke Munn and L. Magee and Vanicka Arora},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Truth Machines: Synthesizing Veracity in AI Language Models},  volume = {abs/2301.12066},  year = {2023} }
True Few-Shot Learning with Language Models	2021	http://www.semanticscholar.org/paper/b58d8579ece27a60432e667bfbdb750590fa65d9	This work evaluates the few-shot ability of LMs when such held-out examples are unavailable, a setting the authors call true few- shot learning, and suggests that prior work significantly overestimated thetrue few-shots ability ofLMs given the difficulty of few-Shot model selection.	maybe	137	Pretrained language models (LMs) perform well on many tasks even when learning from a few examples, but prior work uses many held-out examples to tune various aspects of learning, such as hyperparameters, training objectives, and natural language templates (“prompts”). Here, we evaluate the few-shot ability of LMs when such held-out examples are unavailable, a setting we call true few-shot learning. We test two model selection criteria, cross-validation and minimum description length, for choosing LM prompts and hyperparameters in the true few-shot setting. On average, both marginally outperform random selection and greatly underperform selection based on held-out examples. Moreover, selection criteria often prefer models that perform significantly worse than randomly-selected ones. We find similar results even when taking into account our uncertainty in a model’s true performance during selection, as well as when varying the amount of computation and number of examples used for selection. Overall, our findings suggest that prior work significantly overestimated the true few-shot ability of LMs given the difficulty of few-shot model selection.	b58d8579ece27a60432e667bfbdb750590fa65d9	@['JournalArticle']{perez-etal-2021-true,  author = {Ethan Perez and Douwe Kiela and Kyunghyun Cho},  booktitle = {Neural Information Processing Systems},  pages = {11054-11070},  title = {True Few-Shot Learning with Language Models},  year = {2021} }
True Detective: A Challenging Benchmark for Deep Abductive Reasoning in Foundation Models	2022	http://www.semanticscholar.org/paper/b1054e448186822bfe9445bb6f4533d157e3da5e	The results show that state-of-the-art GPT models perform significantly worse than human solvers on this benchmark, indicating that there is still a signiﬁcant gap in the abductive reasoning abilities of LLMs and highlights the need for further research in this area.	maybe	0	Large language models (LLMs) have demon-strated strong performance in zero-shot reasoning tasks, including abductive reasoning. This is reﬂected in their ability to perform well on current benchmarks in this area. However, to truly test the limits of LLMs in abductive reasoning, a more challenging benchmark is needed. In this paper, we present such a benchmark, consisting of 191 long-form mystery stories, each approximately 1200 words in length and presented in the form of detective puzzles. Each puzzle includes a multiple-choice question for evaluation, sourced from the "5 Minute Mystery" platform. Our results show that state-of-the-art GPT models perform signiﬁcantly worse than human solvers on this benchmark, with an accuracy of 28% com-pared to 47% for humans. This indicates that there is still a signiﬁcant gap in the abductive reasoning abilities of LLMs and highlights the need for further research in this area. Our work provides a challenging benchmark for future studies on reasoning in language models and contributes to a better understanding of the limits of LLMs’ abilities. 1	b1054e448186822bfe9445bb6f4533d157e3da5e	@['JournalArticle']{del-fishel-2022-true,  author = {Maksym Del and Mark Fishel},  booktitle = {ArXiv},  journal = {ArXiv},  title = {True Detective: A Challenging Benchmark for Deep Abductive Reasoning in Foundation Models},  volume = {abs/2212.10114},  year = {2022} }
TrojanPuzzle: Covertly Poisoning Code-Suggestion Models	2023	http://www.semanticscholar.org/paper/dae74645479f7c1fa3671066f9e24ec6c20c17ec	Two novel data poisoning attacks are demonstrated, C OVERT and T ROJAN P UZZLE, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings and have implications for how practitioners should select code used to be coded.	maybe	0	—With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model’s training or ﬁne-tuning phases by injecting malicious data. Poisoning attacks could be designed to inﬂuence the model’s suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior poisoning attacks explicitly inject the insecure code payload into the training data, making the poisoning data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel data poisoning attacks, C OVERT and T ROJAN P UZZLE , that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings. Our most novel attack, T ROJAN P UZZLE , goes one step further in generating less suspicious poisoning data by never including certain (suspicious) parts of the payload in the poisoned data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes T ROJAN P UZZLE robust against signature-based dataset-cleansing methods that identify and ﬁlter out suspicious se- quences from the training data. Our evaluation against two model sizes demonstrates that both C OVERT and T ROJAN P UZZLE have signiﬁcant implications for how practitioners should select code used to	dae74645479f7c1fa3671066f9e24ec6c20c17ec	@['JournalArticle']{aghakhani-etal-2023-trojanpuzzle:,  author = {H. Aghakhani and Wei Dai and Andre Manoel and Xavier Fernandes and Anant Kharkar and Christopher Kruegel and Giovanni Vigna and David Evans and B. Zorn and Robert Sim},  booktitle = {ArXiv},  journal = {ArXiv},  title = {TrojanPuzzle: Covertly Poisoning Code-Suggestion Models},  volume = {abs/2301.02344},  year = {2023} }
Transparency Helps Reveal When Language Models Learn Meaning	2022	http://www.semanticscholar.org/paper/66e3f91e438100cbda49dc5cbc26b595a583a5a0	System-atic experiments with synthetic data reveal that autoregressive and masked language models successfully learn to emulate semantic relations between expressions, but this ability degrades when denotations are changed to be context-dependent with the language otherwise un-modiﬁed, and shows this failure relates to the context- dependent nature of natural language form-meaning mappings.	maybe	0	Many current NLP systems are built from language models trained to optimize unsupervised objectives on large amounts of raw text. Under what conditions might such a procedure acquire meaning? Our system-atic experiments with synthetic data reveal that, with languages where all expressions have context- independent denotations (i.e., languages with strong transparency ), both autoregressive and masked language models successfully learn to emulate semantic relations between expressions. However, when denotations are changed to be context-dependent with the language otherwise un-modiﬁed, this ability degrades. Turning to natural language, our experiments with a spe-ciﬁc phenomenon—referential opacity—add to the growing body of evidence that current language models do not well-represent natural language semantics. We show this failure relates to the context-dependent nature of natural language form-meaning mappings.	66e3f91e438100cbda49dc5cbc26b595a583a5a0	@['JournalArticle']{wu-etal-2022-transparency,  author = {Zhaofeng Wu and Will Merrill and Hao Peng and Iz Beltagy and Noah A. Smith},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Transparency Helps Reveal When Language Models Learn Meaning},  volume = {abs/2210.07468},  year = {2022} }
Transient Chaos in BERT	2021	http://www.semanticscholar.org/paper/b34550b66141668f9224de21bc34197f7866c494		maybe	3	,	b34550b66141668f9224de21bc34197f7866c494	@['JournalArticle']{inoue-etal-2021-transient,  author = {Katsuma Inoue and Soh Ohara and Yasuo Kuniyoshi and K. Nakajima},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Transient Chaos in BERT},  volume = {abs/2106.03181},  year = {2021} }
Transformers Generalize Linearly	2021	http://www.semanticscholar.org/paper/c2dda1f632072a331a50398d613079b71d76ee95		yes	2	Natural language exhibits patterns of hierarchically governed dependencies, in which relations between words are sensitive to syntactic structure rather than linear ordering. While recurrent network models often fail to generalize in a hierarchically sensitive way (McCoy et al., 2020) when trained on ambiguous data, the improvement in performance of newer Transformer language models (Vaswani et al., 2017) on a range of syntactic benchmarks trained on large data sets (Goldberg, 2019; Warstadt et al., 2019) opens the question of whether these models might exhibit hierarchical generalization in the face of impoverished data. In this paper we examine patterns of structural generalization for Transformer sequenceto-sequence models and find that not only do Transformers fail to generalize hierarchically across a wide variety of grammatical mapping tasks, but they exhibit an even stronger preference for linear generalization than comparable recurrent networks.	c2dda1f632072a331a50398d613079b71d76ee95	@['JournalArticle']{petty-frank-2021-transformers,  author = {Jackson Owen Petty and R. Frank},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Transformers Generalize Linearly},  volume = {abs/2109.12036},  year = {2021} }
Transformers as Soft Reasoners over Language	2020	http://www.semanticscholar.org/paper/15ad2b27c5248e7d1db5456794ca1ca8a8198f5d	This work trains transformers to reason (or emulate reasoning) over natural language sentences using synthetically generated data, thus bypassing a formal representation and suggesting a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language.	maybe	149	Beginning with McCarthy's Advice Taker (1959), AI has pursued the goal of providing a system with explicit, general knowledge and having the system reason over that knowledge. However, expressing the knowledge in a formal (logical or probabilistic) representation has been a major obstacle to this research. This paper investigates a modern approach to this problem where the facts and rules are provided as natural language sentences, thus bypassing a formal representation. We train transformers to reason (or emulate reasoning) over these sentences using synthetically generated data. Our models, that we call RuleTakers, provide the first empirical demonstration that this kind of soft reasoning over language is learnable, can achieve high (99%) accuracy, and generalizes to test data requiring substantially deeper chaining than seen during training (95%+ scores). We also demonstrate that the models transfer well to two hand-authored rulebases, and to rulebases paraphrased into more natural language. These findings are significant as it suggests a new role for transformers, namely as limited "soft theorem provers" operating over explicit theories in language. This in turn suggests new possibilities for explainability, correctability, and counterfactual reasoning in question-answering.	15ad2b27c5248e7d1db5456794ca1ca8a8198f5d	@['JournalArticle', 'Conference']{clark-etal-2020-transformers,  author = {Peter Clark and Oyvind Tafjord and Kyle Richardson},  booktitle = {International Joint Conference on Artificial Intelligence},  pages = {3882-3890},  title = {Transformers as Soft Reasoners over Language},  year = {2020} }
Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning	2023	http://www.semanticscholar.org/paper/a131c44951b7ace0892dd830dd0a040b99ed0803	This work formalizes in-context learning as an algorithm learning problem, treating the transformer model as a learning algorithm that can be specialized via training to implement—at inference-time—another target algorithm.	maybe	0	In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-ﬂy. This implicit training is in contrast to explicitly tuning the model weights based on examples. In this work, we formalize in-context learning as an algorithm learning problem, treating the transformer model as a learning algorithm that can be specialized via training to implement—at inference-time—another target algorithm. We ﬁrst explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer, which holds under mild assumptions. Secondly, we use our abstraction to show that transformers can act as an adaptive learning algorithm and perform model selection across diﬀerent hypothesis classes. We provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d	a131c44951b7ace0892dd830dd0a040b99ed0803	@['JournalArticle']{li-etal-2023-transformers,  author = {Yingcong Li and M. E. Ildiz and Dimitris Papailiopoulos and Samet Oymak},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Transformers as Algorithms: Generalization and Implicit Model Selection in In-context Learning},  volume = {abs/2301.07067},  year = {2023} }
Transformers and the representation of biomedical background knowledge	2022	http://www.semanticscholar.org/paper/974b94cbc0990b55cb9ede765146d3455eeee33d	It is shown that these models do indeed encode biological knowledge, although some of this is lost in fine-tuning for specific tasks, and how the models behave with regard to biases and imbalances in the dataset is analysed.	maybe	1	Specialised transformers-based models (such as BioBERT and BioMegatron) are adapted for the biomedical domain based on publicly available biomedical corpora. As such, they have the potential to encode large-scale biological knowledge. We investigate the encoding and representation of biological knowledge in these models, and its potential utility to support inference in cancer precision medicine - namely, the interpretation of the clinical significance of genomic alterations. We compare the performance of different transformer baselines; we use probing to determine the consistency of encodings for distinct entities; and we use clustering methods to compare and contrast the internal properties of the embeddings for genes, variants, drugs and diseases.We show that these models do indeed encode biological knowledge, although some of this is lost in fine-tuning for specific tasks. Finally, we analyse how the models behave with regard to biases and imbalances in the dataset.	974b94cbc0990b55cb9ede765146d3455eeee33d	@['JournalArticle']{wysocki-etal-2022-transformers,  author = {Oskar Wysocki and Zili Zhou and Paul O'Regan and D. Ferreira and M. Wysocka and Dónal Landers and Andr'e Freitas Department of Computer Science and The University of Manchester and digital Experimental Cancer Medicine Team and Cancer Biomarker Centre and Cruk Manchester Institute and U. Manchester and Idiap Research Institute},  booktitle = {Computational Linguistics},  journal = {ArXiv},  title = {Transformers and the representation of biomedical background knowledge},  volume = {abs/2202.02432},  year = {2022} }
Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors	2021	http://www.semanticscholar.org/paper/64c1c87c98d237c3671ad8823d615270d8dd1c09	Through visualization, this paper demonstrates the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency.	yes	13	Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these ‘black boxes’ as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at: https://github.com/zeyuyun1/TransformerVis.	64c1c87c98d237c3671ad8823d615270d8dd1c09	@['JournalArticle']{yun-etal-2021-transformer,  author = {Zeyu Yun and Yubei Chen and B. Olshausen and Yann LeCun},  booktitle = {Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},  pages = {1-10},  title = {Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},  year = {2021} }
Transformer Language Models without Positional Encodings Still Learn Positional Information	2022	http://www.semanticscholar.org/paper/a2fc77f075f666b462d9350e7576f0ba9845c61b	It is shown that LMs without any explicit positional encoding are still competitive with standard models, and that this phenomenon is robust across different datasets, model sizes, and sequence lengths.	maybe	11	Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models, and that this phenomenon is robust across different datasets, model sizes, and sequence lengths. Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information. We conjecture that causal attention enables the model to infer the number of predecessors that each token can at-tend to, thereby approximating its absolute position. Our ﬁndings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism, but also from the effects of the causal mask.	a2fc77f075f666b462d9350e7576f0ba9845c61b	@['JournalArticle']{haviv-etal-2022-transformer,  author = {Adi Haviv and Ori Ram and Ofir Press and Peter Izsak and Omer Levy},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Transformer Language Models without Positional Encodings Still Learn Positional Information},  volume = {abs/2203.16634},  year = {2022} }
Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale	2022	http://www.semanticscholar.org/paper/024dbd9cf7f9c605cc3a99b35b578ce24993d32c	Transformer Grammars are introduced, a novel class of Transformer language models that combine the expressive power, scalability, and strong performance of Transformers and recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree.	maybe	3	Abstract We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism—one that is independent of composed syntactic representations—plays an important role in current successful models of long text.	024dbd9cf7f9c605cc3a99b35b578ce24993d32c	@['JournalArticle']{sartran-etal-2022-transformer,  author = {Laurent Sartran and Samuel Barrett and A. Kuncoro and Milovs Stanojevi'c and P. Blunsom and Chris Dyer},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1423-1439},  title = {Transformer Grammars: Augmenting Transformer Language Models with Syntactic Inductive Biases at Scale},  volume = {10},  year = {2022} }
Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space	2022	http://www.semanticscholar.org/paper/cf36236015c9f93f15bfafbf282f69e08bdc9c16	This work reverse-engineering the operation of the feed-forward network layers, one of the building blocks of transformer models, shows that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable.	yes	16	Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50%, and for improving computation efficiency with a simple early exit rule, saving 20% of computation on average.	cf36236015c9f93f15bfafbf282f69e08bdc9c16	@['JournalArticle', 'Conference']{geva-etal-2022-transformer,  author = {Mor Geva and Avi Caciularu and Ke Wang and Yoav Goldberg},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},  volume = {abs/2203.14680},  year = {2022} }
Transformer Feed-Forward Layers Are Key-Value Memories	2020	http://www.semanticscholar.org/paper/4a54d58a4b20e4f3af25cea3c188a12082a95e02	This work shows that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary.	yes	69	Feed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys’ input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model’s layers via residual connections to produce the final output distribution.	4a54d58a4b20e4f3af25cea3c188a12082a95e02	@['JournalArticle', 'Conference']{geva-etal-2020-transformer,  author = {Mor Geva and R. Schuster and Jonathan Berant and Omer Levy},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Transformer Feed-Forward Layers Are Key-Value Memories},  volume = {abs/2012.14913},  year = {2020} }
Transferability of Contextual Representations for Question Answering	2020	http://www.semanticscholar.org/paper/e15dc8c8ca010a12b8b4c2edf16b942d108d181e	This work investigates whether there is an appreciable difference in feature quality at various depths of pretrained and fine tuned BERT-models by training softmax regression probes to perform a semantic level task, question answering (QA) on SQuAD 2.0.	maybe	0	Large pretrained natural language representations such as BERT, ALBERT, and other variants (BERT-models) have achieved state of the art performance on a diverse set of NLP tasks after fine tuning. This suggests these BERT-models learn to extract signal-rich, transferable language features. We investigate whether there is an appreciable difference in feature quality at various depths of pretrained and fine tuned BERT-models by training softmax regression probes to perform a semantic level task, question answering (QA) on SQuAD 2.0. We find that the feature quality of fine tuned BERT-models improves with each successive layer, while the feature quality of pretrained BERT-models remains constant. We also find that pretrained BERT-models feature quality is relatively poor in comparison to high layers in the fine tuned models, suggesting that the fine tuning process is key for extracting high quality features. One particularly interesting finding is that the early to middle layers in fine tuned BERT-models begin to perform well on questions with answers, at the cost of performance on questions with no answer. Higher layers in fine tuned BERT-models are able to perform well on both questions with and without answers. Code available at https://github.com/travismcguire/cs224nfinalproject	e15dc8c8ca010a12b8b4c2edf16b942d108d181e	@None{mcguire-bhardwaj-2020-transferability,  author = {Travis McGuire and P. Bhardwaj},  title = {Transferability of Contextual Representations for Question Answering},  year = {2020} }
Training Trajectories of Language Models Across Scales	2022	http://www.semanticscholar.org/paper/65043331280851b767df238fabca32f6c52f1148	Analyzing the intermediate training checkpoints of differently sized OPT models shows that perplexity is more predictive of model behaviors than model size or training computation.	maybe	0	Scaling up language models has led to unprece-dented performance gains, but little is under-stood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)—from 125 M to 175 B parameters—on next-token prediction, sequence-level generation and downstream tasks. We ﬁnd that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most signiﬁcant reduction in loss, with the rest stagnating or showing double-descent behavior; 2) early in training, all models learn to re-duce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.	65043331280851b767df238fabca32f6c52f1148	@['JournalArticle']{xia-etal-2022-training,  author = {M. Xia and Mikel Artetxe and Chunting Zhou and Xi Victoria Lin and Ramakanth Pasunuru and Danqi Chen and Luke Zettlemoyer and V. Stoyanov},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Training Trajectories of Language Models Across Scales},  volume = {abs/2212.09803},  year = {2022} }
Training language models to follow instructions with human feedback	2022	http://www.semanticscholar.org/paper/d766bffc357127e0dc86dd69561d5aeb520d6f4c	The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets.	maybe	277	Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.	d766bffc357127e0dc86dd69561d5aeb520d6f4c	@['JournalArticle']{ouyang-etal-2022-training,  author = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and J. Schulman and Jacob Hilton and Fraser Kelton and Luke E. Miller and Maddie Simens and Amanda Askell and P. Welinder and P. Christiano and J. Leike and Ryan J. Lowe},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Training language models to follow instructions with human feedback},  volume = {abs/2203.02155},  year = {2022} }
Training Dynamics for Text Summarization Models	2021	http://www.semanticscholar.org/paper/23f1d4b46bc7c8f357a5a89144d5d32af7be13a5	This work analyzes the training dynamics for generation models, focusing on summarization, and finds that a propensity to copy the input is learned early in the training process consistently across all datasets studied.	maybe	10	Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training time or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on summarization. Across different datasets (CNN/DM, XSum, MediaSum) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that a propensity to copy the input is learned early in the training process consistently across all datasets studied. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, though this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly in the latter stages of the training process. We show that these simple training modifications allow us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness.	23f1d4b46bc7c8f357a5a89144d5d32af7be13a5	@['JournalArticle']{goyal-etal-2021-training,  author = {Tanya Goyal and Jiacheng Xu and J. Li and Greg Durrett},  booktitle = {Findings},  journal = {ArXiv},  title = {Training Dynamics for Text Summarization Models},  volume = {abs/2110.08370},  year = {2021} }
Training Data Influence Analysis and Estimation: A Survey	2022	http://www.semanticscholar.org/paper/6823d6273cfbbbadcf6259aba1e70a956ac37b86	This paper provides the first comprehensive survey of training data influence analysis and estimation and organizes state-of-the-art influence analysis methods into a taxonomy; it describes each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses.	maybe	0	Good models require good training data. For overparameterized deep models, the causal relationship between training data and model predictions is increasingly opaque and poorly understood. Influence analysis partially demystifies training’s underlying interactions by quantifying the amount each training instance alters the final model. Measuring the training data’s influence exactly can be provably hard in the worst case; this has led to the development and use of influence estimators, which only approximate the true influence. This paper provides the first comprehensive survey of training data influence analysis and estimation. We begin by formalizing the various, and in places orthogonal, definitions of training data influence. We then organize state-of-the-art influence analysis methods into a taxonomy; we describe each of these methods in detail and compare their underlying assumptions, asymptotic complexities, and overall strengths and weaknesses. Finally, we propose future research directions to make influence analysis more useful in practice as well as more theoretically and empirically sound. A curated, up-to-date list of resources related to influence analysis is available at https://github.com/ZaydH/influence_analysis_papers.	6823d6273cfbbbadcf6259aba1e70a956ac37b86	@['JournalArticle', 'Review']{hammoudeh-lowd-2022-training,  author = {Zayd Hammoudeh and Daniel Lowd},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Training Data Influence Analysis and Estimation: A Survey},  volume = {abs/2212.04612},  year = {2022} }
Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation	2021	http://www.semanticscholar.org/paper/9ca329408813d209b1dcb36936f7f9cba82506bd	This work shows that extrapolation can be enabled by simply changing the position representation method, though it finds that current methods do not allow for efficient extrapolation, and introduces a simpler and more efficient position method, Attention with Linear Biases (ALiBi).	maybe	62	Since the introduction of the transformer model by Vaswani et al. (2017), a fundamental question has yet to be answered: how does a model achieve extrapolation at inference time for sequences that are longer than it saw during training? We first show that extrapolation can be enabled by simply changing the position representation method, though we find that current methods do not allow for efficient extrapolation. We therefore introduce a simpler and more efficient position method, Attention with Linear Biases (ALiBi). ALiBi does not add positional embeddings to word embeddings; instead, it biases query-key attention scores with a penalty that is proportional to their distance. We show that this method trains a 1.3 billion parameter model on input sequences of length 1024 that extrapolates to input sequences of length 2048, achieving the same perplexity as a sinusoidal position embedding model trained on inputs of length 2048 but training 11% faster and using 11% less memory. ALiBi’s inductive bias towards recency also leads it to outperform multiple strong position methods on the WikiText-103 benchmark.1	9ca329408813d209b1dcb36936f7f9cba82506bd	@['JournalArticle']{press-etal-2021-train,  author = {Ofir Press and Noah A. Smith and M. Lewis},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},  volume = {abs/2108.12409},  year = {2021} }
Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers	2020	https://www.semanticscholar.org/paper/2356781b8a98bf94e6fc73798c6cb65ac35e5f97	It is shown that large models are more robust to compression techniques such as quantization and pruning than small models, and one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.	seed	142	Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. This leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models.	2356781b8a98bf94e6fc73798c6cb65ac35e5f97	@['JournalArticle', 'Conference']{li-etal-2020-train,  author = {Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and K. Keutzer and D. Klein and Joseph Gonzalez},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},  volume = {abs/2002.11794},  year = {2020} }
Tracking the neural dynamics of semantic composition through negation	2022	http://www.semanticscholar.org/paper/7d65f445e5a90847fc5953d7cbe5ad7407227e21		maybe	0	Combining words and composing meanings lies at the basis of human language, but how the brain constructs meaning online is not well understood. We address this puzzle by exploiting the ubiquitous operation of negation. We track the compositional effects of negation (“not”) and intensifiers (“really”) on scalar adjectives (e.g., “good”) in parametrically designed behavioral and neurophysiological (MEG) experiments. The behavioral data show that participants first interpret negated adjectives as affirmative and then modify their interpretation towards the opposite meaning. Decoding analyses of neural activity further reveal that negation does not invert the representation of adjectives (i.e., “not bad” represented as “good”) but rather weakens their representation, at early semantic processing stages. This putative suppression mechanism of negation is supported by increased synchronization of beta-band neural activity in sensorimotor areas. The analysis of semantic composition through negation provides a steppingstone to understand how the human brain combines words into meaning.	7d65f445e5a90847fc5953d7cbe5ad7407227e21	@None{zuanazzi-etal-2022-tracking,  author = {Arianna Zuanazzi and P. Ripollés and W. Lin and L. Gwilliams and J. King and D. Poeppel},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Tracking the neural dynamics of semantic composition through negation},  year = {2022} }
Tracing Knowledge in Language Models Back to the Training Data	2022	http://www.semanticscholar.org/paper/4e0352f3b3d4e853b1c790334924ec7f8763c20e	A new benchmark for fact tracing is introduced : tracing language models’ assertions back to the training examples that provided evidence for those predictions, and it is demonstrated that existing inﬂuence methods must be improved signiﬁcantly before they can reliably attribute factual predictions in LMs.	maybe	10	Neural language models (LMs) have been shown to memorize a great deal of factual knowledge. But when an LM generates an as-sertion, it is often difﬁcult to determine where it learned this information and whether it is true. In this paper, we introduce a new benchmark for fact tracing : tracing language models’ assertions back to the training examples that provided evidence for those predictions. Prior work has suggested that dataset-level inﬂuence methods might offer an effective frame-work for tracing predictions back to training data. However, such methods have not been evaluated for fact tracing, and researchers pri-marily have studied them through qualitative analysis or as a data cleaning technique for classiﬁcation/regression tasks. We present the ﬁrst experiments that evaluate inﬂuence methods for fact tracing, using well-understood information retrieval (IR) metrics. We compare two popular families of inﬂuence methods – gradient -based and embedding -based – and show that neither can fact-trace reliably; indeed, both methods fail to outperform an IR baseline (BM25) that does not even access the LM. We explore why this occurs (e.g., gradient saturation) and demonstrate that existing inﬂuence methods must be improved signiﬁcantly before they can reliably attribute factual predictions in LMs. 1	4e0352f3b3d4e853b1c790334924ec7f8763c20e	@['JournalArticle']{akyürek-etal-2022-tracing,  author = {Ekin Akyürek and Tolga Bolukbasi and Frederick Liu and Binbin Xiong and Ian Tenney and Jacob Andreas and Kelvin Guu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Tracing Knowledge in Language Models Back to the Training Data},  volume = {abs/2205.11482},  year = {2022} }
Tracing and Manipulating Intermediate Values in Neural Math Problem Solvers	2023	http://www.semanticscholar.org/paper/2c1e2f2110d0ba6d53eb9a086a0d8bd9d7320647	A method for analyzing how a Transformer model processes these inputs by focusing on simple arithmetic problems and their intermediate values is introduced and it is shown that the model has a locality to certain intermediate values, and this is useful for enhancing the interpretability of the models.	maybe	0	How language models process complex input that requires multiple steps of inference is not well understood. Previous research has shown that information about intermediate values of these inputs can be extracted from the activations of the models, but it is unclear where that information is encoded and whether that information is indeed used during inference. We introduce a method for analyzing how a Transformer model processes these inputs by focusing on simple arithmetic problems and their intermediate values. To trace where information about intermediate values is encoded, we measure the correlation between intermediate values and the activations of the model using principal component analysis (PCA). Then, we perform a causal intervention by manipulating model weights. This intervention shows that the weights identified via tracing are not merely correlated with intermediate values, but causally related to model predictions. Our findings show that the model has a locality to certain intermediate values, and this is useful for enhancing the interpretability of the models.	2c1e2f2110d0ba6d53eb9a086a0d8bd9d7320647	@['JournalArticle']{matsumoto-etal-2023-tracing,  author = {Yuta Matsumoto and Benjamin Heinzerling and Masashi Yoshikawa and Kentarou Inui},  booktitle = {MATHNLP},  journal = {ArXiv},  title = {Tracing and Manipulating intermediate values in Neural Math Problem Solvers},  volume = {abs/2301.06758},  year = {2023} }
TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation	2020	http://www.semanticscholar.org/paper/d865c87f6ce4e0be29ae1e3780fae66b8034d04b	This paper examines the ability of large-scale pre-trained language models to distinguish commonsense from non-commonsense statements, and the utility of external resources that aim to supplement the world knowledge inherent in such language models, including commonsense knowledge graph embedding models, word concreteness ratings, and text-to-image generation models.	maybe	2	In this paper, we present our submission for subtask A of the Common Sense Validation and Explanation (ComVE) shared task. We examine the ability of large-scale pre-trained language models to distinguish commonsense from non-commonsense statements. We also explore the utility of external resources that aim to supplement the world knowledge inherent in such language models, including commonsense knowledge graph embedding models, word concreteness ratings, and text-to-image generation models. We find that such resources provide insignificant gains to the performance of fine-tuned language models. We also provide a qualitative analysis of the limitations of the language model fine-tuned to this task.	d865c87f6ce4e0be29ae1e3780fae66b8034d04b	@['JournalArticle']{teo-2020-tr,  author = {Don Teo},  booktitle = {International Workshop on Semantic Evaluation},  pages = {601-608},  title = {TR at SemEval-2020 Task 4: Exploring the Limits of Language-model-based Common Sense Validation},  year = {2020} }
Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models	2021	http://www.semanticscholar.org/paper/383a734d473033378b0cbc83a3556050214c2109	This paper proposes a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships that demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.	maybe	2	Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.	383a734d473033378b0cbc83a3556050214c2109	@['JournalArticle', 'Conference']{klein-nabi-2021-towards,  author = {T. Klein and Moin Nabi},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {8737-8743},  title = {Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models},  year = {2021} }
Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models	2022	http://www.semanticscholar.org/paper/fa48dd52be56dc5ee442a3ab5d754f382f88bb3a	It is found that BERT shows homophobic bias, but this bias can be mostly mitigated by ﬁnetuning BERT on a natural language corpus written by members of the LGBTQ+ community.	maybe	0	This paper presents exploratory work on whether and to what extent biases against queer and trans people are encoded in large language models (LLMs) such as BERT. We also propose a method for reducing these biases in downstream tasks: ﬁnetuning the models on data written by and/or about queer people. To measure anti-queer bias, we introduce a new benchmark dataset, WinoQueer, modeled after other bias-detection benchmarks but addressing homophobic and transphobic biases. We found that BERT shows signiﬁcant homophobic bias, but this bias can be mostly mitigated by ﬁnetuning BERT on a natural language corpus written by members of the LGBTQ+ community.	fa48dd52be56dc5ee442a3ab5d754f382f88bb3a	@['JournalArticle']{felkner-etal-2022-towards,  author = {Virginia K. Felkner and Ho-Chun Herbert Chang and Eugene Jang and Jonathan May},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Towards WinoQueer: Developing a Benchmark for Anti-Queer Bias in Large Language Models},  volume = {abs/2206.11484},  year = {2022} }
Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models	2022	http://www.semanticscholar.org/paper/4d3c0c71212944cb4050781bb2e881fd3c59f93d	This paper proposes a novel approach to infer discourse information for arbitrarily long documents, and finds that similar discourse information is consistently captured in the same heads.	maybe	2	In this paper, we extend the line of BERTology work by focusing on the important, yet less explored, alignment of pre-trained and fine-tuned PLMs with large-scale discourse structures. We propose a novel approach to infer discourse information for arbitrarily long documents. In our experiments, we find that the captured discourse information is local and general, even across a collection of fine-tuning tasks. We compare the inferred discourse trees with supervised, distantly supervised and simple baselines to explore the structural overlap, finding that constituency discourse trees align well with supervised models, however, contain complementary discourse information.Lastly, we individually explore self-attention matrices to analyze the information redundancy. We find that similar discourse information is consistently captured in the same heads.	4d3c0c71212944cb4050781bb2e881fd3c59f93d	@['JournalArticle', 'Conference']{huber-carenini-2022-towards,  author = {Patrick Huber and G. Carenini},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Towards Understanding Large-Scale Discourse Structures in Pre-Trained and Fine-Tuned Language Models},  volume = {abs/2204.04289},  year = {2022} }
Towards Understanding Grokking: An Effective Theory of Representation Learning	2022	http://www.semanticscholar.org/paper/20de79ec4fe682b68930eb4dcd91b1801b8d4731	This study provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.	maybe	8	We aim to understand grokking , a phenomenon where models generalize long after overﬁtting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We ﬁnd that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension , grokking , memorization , and confusion . We ﬁnd representation learning to occur only in a “Goldilocks zone” (including comprehension and grokking) between memorization and confusion. Compared to the comprehension phase, the grokking phase stays closer to the memorization phase, leading to delayed generalization. The Goldilocks phase is reminiscent of “intelligence from starvation” in Darwinian evolution, where resource limitations drive discovery of more efﬁcient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.	20de79ec4fe682b68930eb4dcd91b1801b8d4731	@['JournalArticle']{liu-etal-2022-towards,  author = {Ziming Liu and O. Kitouni and N. Nolte and Eric J. Michaud and Max Tegmark and Mike Williams},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Towards Understanding Grokking: An Effective Theory of Representation Learning},  volume = {abs/2205.10343},  year = {2022} }
Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters	2022	http://www.semanticscholar.org/paper/a7b060413027cbd25b6144f4a6214c3bd4fb12e3	It is shown that CoT reasoning is possible even with invalid demonstrations—prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference.	maybe	1	Chain-of-Thought (CoT) prompting can dramatically improve the multi-step reasoning abilities of large language models (LLMs). CoT explicitly encourages the LLM to generate intermediate rationales for solving a problem, by providing a series of reasoning steps in the demonstrations. Despite its success, there is still little understanding of what makes CoT prompting effective and which aspects of the demonstrated reasoning steps contribute to its performance. In this paper, we show that CoT reasoning is possible even with invalid demonstrations—prompting with invalid reasoning steps can achieve over 80-90% of the performance obtained using CoT under various metrics, while still generating coherent lines of reasoning during inference. Further experiments show that other aspects of the rationales, such as being relevant to the query and correctly ordering the reasoning steps, are much more important for effective CoT reasoning. Overall, these ﬁndings both deepen our understanding of CoT prompting, and open up new questions regarding LLMs’ capability to learn to reason in context. 1	a7b060413027cbd25b6144f4a6214c3bd4fb12e3	@['JournalArticle']{wang-etal-2022-towards,  author = {Boshi Wang and Sewon Min and Xiang Deng and Jiaming Shen and You Wu and Luke Zettlemoyer and Huan Sun},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Towards Understanding Chain-of-Thought Prompting: An Empirical Study of What Matters},  volume = {abs/2212.10001},  year = {2022} }
Towards Understanding and Mitigating Social Biases in Language Models	2021	http://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a	The empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.	maybe	65	Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.	114aa720872462b0ca1b97bfdec0ebd56c36fd0a	@['JournalArticle', 'Conference']{liang-etal-2021-towards,  author = {Paul Pu Liang and Chiyu Wu and Louis-Philippe Morency and R. Salakhutdinov},  booktitle = {International Conference on Machine Learning},  pages = {6565-6576},  title = {Towards Understanding and Mitigating Social Biases in Language Models},  year = {2021} }
Towards the Generation of Musical Explanations with GPT-3	2022	http://www.semanticscholar.org/paper/c08352ac7489100fd192560189258980135d4a56	This work carries out a first study on GPT-3’s capability to communicate musical decisions through textual explanations when prompted with a textual representation of a piece of music.	maybe	1		c08352ac7489100fd192560189258980135d4a56	@['JournalArticle']{krol-etal-2022-towards,  author = {S. Krol and M. T. Llano and Jon McCormack},  booktitle = {EvoMUSART},  pages = {131-147},  title = {Towards the Generation of Musical Explanations with GPT-3},  year = {2022} }
Towards Reasoning in Large Language Models: A Survey	2022	http://www.semanticscholar.org/paper/3ee9c65366efbb17adf370c39f20dbef60d53670	A comprehensive overview of the current state of knowledge on reasoning in large language models, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, and suggestions on future directions are provided.	maybe	0	Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made signiﬁcant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufﬁ-ciently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, ﬁndings and implications of previous research in this ﬁeld, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work. 1	3ee9c65366efbb17adf370c39f20dbef60d53670	@['JournalArticle', 'Review']{huang-chang-2022-towards,  author = {Jie Huang and K. Chang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Towards Reasoning in Large Language Models: A Survey},  volume = {abs/2212.10403},  year = {2022} }
Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)	2022	http://www.semanticscholar.org/paper/db783c480faf87b38e8806d4ef455dfde6e335aa	The GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases, resulting in an overall accuracy up to 17.25%.	maybe	1	The goal of Automated Program Repair (APR) is to find a fix to software bugs, without human intervention. The so-called Gener-ate and Validate (G&V) approach deemed to be the most popular method in the last few years, where the APR tool creates a patch and it is validated against an oracle. Recent years for Natural Language Processing (NLP) were of great interest, with new pre-trained models shattering records on tasks ranging from sentiment analysis to question answering. Usually these deep learning models inspire the APR community as well. These approaches usually require a large dataset on which the model can be trained (or fine-tuned) and evaluated. The criterion to accept a patch depends on the underlying dataset, but usually the generated patch should be exactly the same as the one created by a human developer. As NLP models are more and more capable to form sentences, and the sentences will form coherent paragraphs, the APR tools are also better and better at generating syntactically and semantically correct source code. As the Generative Pre-trained Transformer (GPT) model is now avail-able to everyone thanks to the NLP and AI research community, it can be fine-tuned to specific tasks (not necessarily on natural language). In this work we use the GPT-2 model to generate source code, to the best of our knowledge, the GPT-2 model was not used for Automated Program Repair so far. The model is fine-tuned for a specific task: it has been taught to fix JavaScript bugs automatically. To do so, we trained the model on 16863JS code snippets, where it could learn the nature of the observed programming language. In our experiments we observed that the GPT-2 model was able to learn how to write syntactically correct source code almost on every attempt, although it failed to learn good bug-fixes in some cases. Nonetheless it was able to generate the correct fixes in most of the cases, resulting in an overall accuracy up to 17.25%.	db783c480faf87b38e8806d4ef455dfde6e335aa	@['JournalArticle', 'Book']{lajkó-etal-2022-towards,  author = {Márk Lajkó and Viktor Csuvik and László Vidács},  booktitle = {APR},  journal = {2022 IEEE/ACM International Workshop on Automated Program Repair (APR)},  pages = {61-68},  title = {Towards JavaScript program repair with Generative Pre-trained Transformer (GPT-2)},  year = {2022} }
Towards Interpreting BERT for Reading Comprehension Based QA	2020	http://www.semanticscholar.org/paper/c4788233436ccfc664a8293c78447be2827c9088	This work attempts to interpret BERT for RCQA by defining a layer's role or functionality using Integrated Gradients and observing that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction.	maybe	10	BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer's role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at this https URL .	c4788233436ccfc664a8293c78447be2827c9088	@['JournalArticle', 'Conference']{ramnath-etal-2020-towards,  author = {Sahana Ramnath and Preksha Nema and Deep Sahni and Mitesh M. Khapra},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {3236-3242},  title = {Towards Interpreting BERT for Reading Comprehension Based QA},  year = {2020} }
Towards Hierarchical Importance Attribution: Explaining Compositional Semantics for Neural Sequence Models	2021	http://www.semanticscholar.org/paper/3ad3146eb5aeef3a7fd50ba3d25741e3525f4220	This paper proposes a formal and general way to quantify the importance of each word and phrase, and proposes Sampling and Contextual Decomposition (SCD) algorithm and Samplings and Occlusion (SOC) algorithm, which outperform prior hierarchical explanation algorithms.	maybe	0	The impressive performance of neural networks on natural language processing tasks attributes to their ability to model complicated word and phrase compositions. To explain how the model handles semantic compositions, we study hierarchical explanation of neural network predictions. We identify non-additivity and context independent importance attributions within hierarchies as two desirable properties for highlighting word and phrase compositions. We show some prior efforts on hierarchical explanations, e.g. contextual decomposition, do not satisfy the desired properties mathematically, leading to inconsistent explanation quality in different models. In this paper, we start by proposing a formal and general way to quantify the importance of each word and phrase. Following the formulation, we propose Sampling and Contextual Decomposition (SCD) algorithm and Sampling and Occlusion (SOC) algorithm. Human and metrics evaluation on both LSTM models and BERT Transformer models on multiple datasets show that our algorithms outperform prior hierarchical explanation algorithms. Our algorithms help to visualize semantic composition captured by models, extract classification rules and improve human trust of models1.	3ad3146eb5aeef3a7fd50ba3d25741e3525f4220	@None{jin-etal-2021-towards,  author = {Xisen Jin and Zhongyu Wei and Junyi Du and X. Xue and Xiang Ren},  title = {TOWARDS HIERARCHICAL IMPORTANCE ATTRIBUTION: EXPLAINING COMPOSITIONAL SEMANTICS FOR NEURAL SEQUENCE MODELS},  year = {2021} }
Towards Few-shot Fact-Checking via Perplexity	2021	http://www.semanticscholar.org/paper/b798969c3e78215a87e8f3da510232b40893bd9c	A new way of utilizing the powerful transfer learning ability of a language model via a perplexity score is proposed and can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets.	maybe	27	Few-shot learning has drawn researchers’ attention to overcome the problem of data scarcity. Recently, large pre-trained language models have shown great performance in few-shot learning for various downstream tasks, such as question answering and machine translation. Nevertheless, little exploration has been made to achieve few-shot learning for the fact-checking task. However, fact-checking is an important problem, especially when the amount of information online is growing exponentially every day. In this paper, we propose a new way of utilizing the powerful transfer learning ability of a language model via a perplexity score. The most notable strength of our methodology lies in its capability in few-shot learning. With only two training samples, our methodology can already outperform the Major Class baseline by more than an absolute 10% on the F1-Macro metric across multiple datasets. Through experiments, we empirically verify the plausibility of the rather surprising usage of the perplexity score in the context of fact-checking and highlight the strength of our few-shot methodology by comparing it to strong fine-tuning-based baseline models. Moreover, we construct and publicly release two new fact-checking datasets related to COVID-19.	b798969c3e78215a87e8f3da510232b40893bd9c	@['JournalArticle', 'Conference']{lee-etal-2021-towards,  author = {Nayeon Lee and Yejin Bang and Andrea Madotto and Madian Khabsa and Pascale Fung},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {1971-1981},  title = {Towards Few-shot Fact-Checking via Perplexity},  year = {2021} }
Towards Coding Social Science Datasets with Language Models	2022	http://www.semanticscholar.org/paper/7951262a0c168633ae4bdace64056cf8e98ffa08	It is found that GPT-3 can match the performance of typical human coders and frequently outperforms them in terms of intercoder agreement across a va- 017 riety of social science tasks, suggesting that language models could serve as useful coders.	maybe	0	Researchers often rely on humans to code (la- 001 bel, annotate, etc.) large sets of texts. This is 002 a highly variable task and requires a great deal 003 of time and resources. Efforts to automate this 004 process have achieved human-level accuracies 005 in some cases, but often rely on thousands of 006 hand-labeled training examples, which makes 007 them inapplicable to small-scale research stud- 008 ies and still costly for large ones. At the same 009 time, it is well known that language models 010 can classify text; in this work, we use GPT-3 011 as a synthetic coder, and compare it to human 012 coders using classic methodologies and met- 013 rics, such as intercoder reliability. We find that 014 GPT-3 can match the performance of typical 015 human coders and frequently outperforms them 016 in terms of intercoder agreement across a va- 017 riety of social science tasks, suggesting that 018 language models could serve as useful coders. 019	7951262a0c168633ae4bdace64056cf8e98ffa08	@None{partisans-2022-towards,  author = {Pigeonholing Partisans},  title = {Towards Coding Social Science Datasets with Language Models},  year = {2022} }
Towards Climate Awareness in NLP Research	2022	http://www.semanticscholar.org/paper/8d0f755dea90f35f4b126a01fa3cce96b3bdd344	A climate performance model card is proposed with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.	maybe	6	The climate impact of AI, and NLP research in particular, has become a serious issue given the enormous amount of energy that is increasingly being used for training and running computational models. Consequently, increasing focus is placed on efficient NLP. However, this important initiative lacks simple guidelines that would allow for systematic climate reporting of NLP research. We argue that this deficiency is one of the reasons why very few publications in NLP report key figures that would allow a more thorough examination of environmental impact, and present a quantitative survey to demonstrate this. As a remedy, we propose a climate performance model card with the primary purpose of being practically usable with only limited information about experiments and the underlying computer hardware. We describe why this step is essential to increase awareness about the environmental impact of NLP research and, thereby, paving the way for more thorough discussions.	8d0f755dea90f35f4b126a01fa3cce96b3bdd344	@['JournalArticle', 'Conference', 'Review']{hershcovich-etal-2022-towards,  author = {Daniel Hershcovich and Nicolas Webersinke and Mathias Kraus and Julia Anna Bingler and Markus Leippold},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Towards Climate Awareness in NLP Research},  volume = {abs/2205.05071},  year = {2022} }
Towards a Language Model for Temporal Commonsense Reasoning	2021	http://www.semanticscholar.org/paper/8b383bd2eb054787d84636dab7b6ecf4a318c6c7	This work proposes an ensemble model for temporal commonsense reasoning that greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.	maybe	3	Temporal commonsense reasoning is a challenging task as it requires temporal knowledge usually not explicit in text. In this work, we propose an ensemble model for temporal commonsense reasoning. Our model relies on pre-trained contextual representations from transformer-based language models (i.e., BERT), and on a variety of training methods for enhancing model generalization: 1) multi-step fine-tuning using carefully selected auxiliary tasks and datasets, and 2) a specifically designed temporal masked language model task aimed to capture temporal commonsense knowledge. Our model greatly outperforms the standard fine-tuning approach and strong baselines on the MC-TACO dataset.	8b383bd2eb054787d84636dab7b6ecf4a318c6c7	@None{kimura-etal-2021-towards,  author = {Mayuko Kimura and Lis Kanashiro Pereira and I. Kobayashi},  booktitle = {Recent Advances in Natural Language Processing},  journal = {Proceedings of the Student Research Workshop Associated with RANLP 2021},  title = {Towards a Language Model for Temporal Commonsense Reasoning},  year = {2021} }
Towards a Deep and Unified Understanding of Deep Neural Models in NLP	2019	http://www.semanticscholar.org/paper/ed1262a5734425f5e24113b992e5a144cf400b51	A unified information-based measure is defined to provide quantitative explanations on how intermediate layers of deep Natural Language Processing models leverage information of input words to advance existing explanation methods by addressing issues in coherency and generality.	maybe	72	We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing (NLP) models leverage information of input words. Our method advances existing explanation methods by addressing issues in coherency and generality. Explanations generated by using our method are consistent and faithful across different timestamps, layers, and models. We show how our method can be applied to four widely used models in NLP and explain their performances on three real-world benchmark datasets.	ed1262a5734425f5e24113b992e5a144cf400b51	@['JournalArticle', 'Conference']{guan-etal-2019-towards,  author = {Chaoyu Guan and Xiting Wang and Quanshi Zhang and Runjin Chen and Di He and Xing Xie},  booktitle = {International Conference on Machine Learning},  pages = {2454-2463},  title = {Towards a Deep and Unified Understanding of Deep Neural Models in NLP},  year = {2019} }
Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers	2021	http://www.semanticscholar.org/paper/2cc0e605470d3ac20aad82c73560b888ecc449cd	Gender and racial bias is investigated across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT, demonstrating the need for more robust bias testing in transformers.	maybe	19	The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.	2cc0e605470d3ac20aad82c73560b888ecc449cd	@['JournalArticle', 'Conference']{silva-etal-2021-towards,  author = {Andrew Silva and Pradyumna Tambwekar and M. Gombolay},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2383-2389},  title = {Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers},  year = {2021} }
Toward Building a Language Model for Understanding Temporal Commonsense	2022	http://www.semanticscholar.org/paper/8b0b5b998d65add5c036b45c36afa3f9df96f4b0	This paper focuses on the development of language models for temporal commonsense inference over several pre-trained language models, and relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal Commonsense reasoning.	maybe	0	The ability to capture temporal commonsense relationships for time-related events expressed in text is a very important task in natural language understanding. On the other hand, pre-trained language models such as BERT, which have recently achieved great success in a wide range of natural language processing tasks, are still considered to have poor performance in temporal reasoning. In this paper, we focus on the development of language models for temporal commonsense inference over several pre-trained language models. Our model relies on multi-step fine-tuning using multiple corpora, and masked language modeling to predict masked temporal indicators that are crucial for temporal commonsense reasoning. We also experimented with multi-task learning and build a language model that can improve performance on multiple time-related tasks. In our experiments, multi-step fine-tuning using the general commonsense reading task as auxiliary task produced the best results. This result showed a significant improvement in accuracy over standard fine-tuning in the temporal commonsense inference task.	8b0b5b998d65add5c036b45c36afa3f9df96f4b0	@['JournalArticle']{kimura-etal-2022-toward,  author = {Mayuko Kimura and L. Pereira and Ichiro Kobayashi},  booktitle = {AACL},  pages = {17-24},  title = {Toward Building a Language Model for Understanding Temporal Commonsense},  year = {2022} }
TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction	2021	http://www.semanticscholar.org/paper/310213842f3646d5fc0a5dbbd4e0ef3d0130b91c	This model uses RoBERTa with a regression layer to predict 5 eye-tracking features and is compared to different Transformer models and apply ensembling methods to improve the performance.	maybe	6	Eye movement data during reading is a useful source of information for understanding language comprehension processes. In this paper, we describe our submission to the CMCL 2021 shared task on predicting human reading patterns. Our model uses RoBERTa with a regression layer to predict 5 eye-tracking features. We train the model in two stages: we first fine-tune on the Provo corpus (another eye-tracking dataset), then fine-tune on the task data. We compare different Transformer models and apply ensembling methods to improve the performance. Our final submission achieves a MAE score of 3.929, ranking 3rd place out of 13 teams that participated in this shared task.	310213842f3646d5fc0a5dbbd4e0ef3d0130b91c	@['JournalArticle']{li-rudzicz-2021-torontocl,  author = {Bai Li and F. Rudzicz},  booktitle = {Workshop on Cognitive Modeling and Computational Linguistics},  journal = {ArXiv},  title = {TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction},  volume = {abs/2104.07244},  year = {2021} }
Topics in Contextualised Attention Embeddings	2023	http://www.semanticscholar.org/paper/1470b23c089db20c222094d2d155853a2bd26f3c	This work designs probe experiments to address how topical word clusters are automatically formed, through clustering, in the language model when it has not been explicitly designed to model latent topics, and suggests that the attention framework plays a key role in modelling such word topic clusters.	maybe	0	. Contextualised word vectors obtained via pre-trained language models encode a variety of knowledge that has already been ex-ploited in applications. Complementary to these language models are probabilistic topic models that learn thematic patterns from the text. Recent work has demonstrated that conducting clustering on the word-level contextual representations from a language model emulates word clusters that are discovered in latent topics of words from Latent Dirichlet Allocation. The important question is how such topical word clusters are automatically formed, through clustering, in the language model when it has not been explicitly designed to model latent topics. To address this question, we design diﬀerent probe experiments. Using BERT and DistilBERT, we ﬁnd that the attention framework plays a key role in modelling such word topic clusters. We strongly believe that our work paves way for further research into the relationships between probabilistic topic models and pre-trained language models.	1470b23c089db20c222094d2d155853a2bd26f3c	@['JournalArticle']{talebpour-etal-2023-topics,  author = {Mozhgan Talebpour and A. G. S. D. Herrera and Shoaib Jameel},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Topics in Contextualised Attention Embeddings},  volume = {abs/2301.04339},  year = {2023} }
Topicalization in Language Models: A Case Study on Japanese	2022	http://www.semanticscholar.org/paper/31662a7beb1acbdda51ecb6d8ec419fb9887a817	The experimental results suggest that LMs have different generalizations from humans; LMs exhibited less context-dependent behaviors toward topicalization judgment, which highlights the need for the additional inductive biases to guide LMs to achieve successful discourse-level generalization.	maybe	0	Humans use different wordings depending on the context to facilitate efficient communication. For example, instead of completely new information, information related to the preceding context is typically placed at the sentence-initial position. In this study, we analyze whether neural language models (LMs) can capture such discourse-level preferences in text generation. Specifically, we focus on a particular aspect of discourse, namely the topic-comment structure. To analyze the linguistic knowledge of LMs separately, we chose the Japanese language, a topic-prominent language, for designing probing tasks, and we created human topicalization judgment data by crowdsourcing. Our experimental results suggest that LMs have different generalizations from humans; LMs exhibited less context-dependent behaviors toward topicalization judgment. These results highlight the need for the additional inductive biases to guide LMs to achieve successful discourse-level generalization.	31662a7beb1acbdda51ecb6d8ec419fb9887a817	@['JournalArticle', 'Conference']{fujihara-etal-2022-topicalization,  author = {Riki Fujihara and Tatsuki Kuribayashi and Kaori Abe and Ryoko Tokuhisa and Kentarou Inui},  booktitle = {International Conference on Computational Linguistics},  pages = {851-862},  title = {Topicalization in Language Models: A Case Study on Japanese},  year = {2022} }
Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications	2021	https://www.semanticscholar.org/paper/9a432edd418361d9ca2fbab263633012906b8142	It is shown, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions, and it is shown that isotropy can be restored using a simple transformation.	yes	8	The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation.	9a432edd418361d9ca2fbab263633012906b8142	@['JournalArticle', 'Conference']{bis-etal-2021-too,  author = {Daniel Bis and Maksim Podkorytov and Xiuwen Liu},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {5117-5130},  title = {Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications},  year = {2021} }
To what extent do human explanations of model behavior align with actual model behavior?	2020	https://www.semanticscholar.org/paper/bbd4f99c65d677dc82ac4c051b289626f1e8f435	This work investigates the extent to which human-generated explanations of models’ inference decisions align with how models actually make these decisions, and defines three alignment metrics that quantify how well natural language explanations align with model sensitivity to input words, as measured by integrated gradients.	maybe	18	Given the increasingly prominent role NLP models (will) play in our lives, it is important for human expectations of model behavior to align with actual model behavior. Using Natural Language Inference (NLI) as a case study, we investigate the extent to which human-generated explanations of models’ inference decisions align with how models actually make these decisions. More specifically, we define three alignment metrics that quantify how well natural language explanations align with model sensitivity to input words, as measured by integrated gradients. Then, we evaluate eight different models (the base and large versions of BERT,RoBERTa and ELECTRA, as well as anRNN and bag-of-words model), and find that the BERT-base model has the highest alignment with human-generated explanations, for all alignment metrics. Focusing in on transformers, we find that the base versions tend to have higher alignment with human-generated explanations than their larger counterparts, suggesting that increasing the number of model parameters leads, in some cases, to worse alignment with human explanations. Finally, we find that a model’s alignment with human explanations is not predicted by the model’s accuracy, suggesting that accuracy and alignment are complementary ways to evaluate models.	bbd4f99c65d677dc82ac4c051b289626f1e8f435	@['JournalArticle']{prasad-etal-2020-to,  author = {Grusha Prasad and Yixin Nie and Mohit Bansal and Robin Jia and Douwe Kiela and Adina Williams},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {1-14},  title = {To what extent do human explanations of model behavior align with actual model behavior?},  year = {2020} }
To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks	2019	https://www.semanticscholar.org/paper/8659bf379ca8756755125a487c43cfe8611ce842	The empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks.	seed	309	While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task. We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly fine-tuning the pretrained model. Our empirical results across diverse NLP tasks with two state-of-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks. We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.	8659bf379ca8756755125a487c43cfe8611ce842	@['JournalArticle']{peters-etal-2019-to,  author = {Matthew E. Peters and Sebastian Ruder and Noah A. Smith},  booktitle = {RepL4NLP@ACL},  journal = {ArXiv},  title = {To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks},  volume = {abs/1903.05987},  year = {2019} }
TIMEDIAL: Temporal Commonsense Reasoning in Dialog	2021	http://www.semanticscholar.org/paper/62953ca1252c9febe07c7007a10911726f37792d	This paper presents the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial, and reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context.	yes	21	Everyday conversations require understanding everyday events, which in turn, requires understanding temporal commonsense concepts interwoven with those events. Despite recent progress with massive pre-trained language models (LMs) such as T5 and GPT-3, their capability of temporal reasoning in dialogs remains largely under-explored. In this paper, we present the first study to investigate pre-trained LMs for their temporal reasoning capabilities in dialogs by introducing a new task and a crowd-sourced English challenge set, TimeDial. We formulate TimeDial as a multiple choice cloze task with over 1.1K carefully curated dialogs. Empirical results demonstrate that even the best performing models struggle on this task compared to humans, with 23 absolute points of gap in accuracy. Furthermore, our analysis reveals that the models fail to reason about dialog context correctly; instead, they rely on shallow cues based on existing temporal patterns in context, motivating future research for modeling temporal concepts in text and robust contextual reasoning about them. The dataset is publicly available at https://github.com/google-research-datasets/timedial.	62953ca1252c9febe07c7007a10911726f37792d	@['JournalArticle', 'Conference']{qin-etal-2021-timedial:,  author = {Lianhui Qin and Aditya Gupta and Shyam Upadhyay and Luheng He and Yejin Choi and Manaal Faruqui},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7066-7076},  title = {TIMEDIAL: Temporal Commonsense Reasoning in Dialog},  year = {2021} }
Time-Aware Language Models as Temporal Knowledge Bases	2021	http://www.semanticscholar.org/paper/ac8d33e4c0a45e227a47353f3f26fbb231482dc1	This work proposes a simple technique for jointly modeling text with its timestamp that improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods and shows that models trained with temporal context can be efficiently "refreshed" as new data arrives.	maybe	61	Many facts come with an expiration date, from the name of the President to the basketball team Lebron James plays for. However, most language models (LMs) are trained on snapshots of data collected at a specific moment in time. This can limit their utility, especially in the closed-book setting where the pretraining corpus must contain the facts the model should memorize. We introduce a diagnostic dataset aimed at probing LMs for factual knowledge that changes over time and highlight problems with LMs at either end of the spectrum—those trained on specific slices of temporal data, as well as those trained on a wide range of temporal data. To mitigate these problems, we propose a simple technique for jointly modeling text with its timestamp. This improves memorization of seen facts from the training time period, as well as calibration on predictions about unseen facts from future time periods. We also show that models trained with temporal context can be efficiently “refreshed” as new data arrives, without the need for retraining from scratch.	ac8d33e4c0a45e227a47353f3f26fbb231482dc1	@['JournalArticle']{dhingra-etal-2021-time,  author = {Bhuwan Dhingra and Jeremy R. Cole and Julian Martin Eisenschlos and D. Gillick and Jacob Eisenstein and William W. Cohen},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {257-273},  title = {Time-Aware Language Models as Temporal Knowledge Bases},  volume = {10},  year = {2021} }
Time Masking for Temporal Language Models	2021	http://www.semanticscholar.org/paper/544bb71b6e9aeb95511dfd90110afab96ca78b2f	This work proposes a temporal contextual language model called TempoBERT, which uses time as an additional context of texts and shows that both tasks of semantic change detection and sentence time prediction benefit from exploiting time masking.	maybe	12	Our world is constantly evolving, and so is the content on the web. Consequently, our languages, often said to mirror the world, are dynamic in nature. However, most current contextual language models are static and cannot adapt to changes over time. In this work, we propose a temporal contextual language model called TempoBERT, which uses time as an additional context of texts. Our technique is based on modifying texts with temporal information and performing time masking - specific masking for the supplementary time information. We leverage our approach for the tasks of semantic change detection and sentence time prediction, experimenting on diverse datasets in terms of time, size, genre, and language. Our extensive evaluation shows that both tasks benefit from exploiting time masking.	544bb71b6e9aeb95511dfd90110afab96ca78b2f	@['JournalArticle', 'Book']{rosin-etal-2021-time,  author = {Guy D. Rosin and Ido Guy and Kira Radinsky},  booktitle = {Web Search and Data Mining},  journal = {Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining},  title = {Time Masking for Temporal Language Models},  year = {2021} }
Threats to Pre-trained Language Models: Survey and Taxonomy	2022	http://www.semanticscholar.org/paper/3fa0959ed06206b81f6d2125c19e1a5958250e6d	This survey comprehensively systematize recently discovered threats to PTLM systems and applications and summarizes four categories of attacks (backdoor, evasion, data privacy and model privacy).	maybe	3	Pre-trained language models (PTLMs) have achieved great success and remarkable performance over a wide range of natural language processing (NLP) tasks. However, there are also growing concerns regarding the potential security issues in the adoption of PTLMs. In this survey, we comprehensively systematize recently discovered threats to PTLM systems and applications. We perform our attack characterization from three interesting perspectives. (1) We show threats can occur at different stages of the PTLM pipeline raised by different malicious entities. (2) We identify two types of model transferability (landscape, portrait) that facilitates attacks. (3) Based on the attack goals, we summarize four categories of attacks (backdoor, evasion, data privacy and model privacy). We also discuss some open problems and research directions. We believe our survey and taxonomy will inspire future studies towards secure and privacy-preserving PTLMs.	3fa0959ed06206b81f6d2125c19e1a5958250e6d	@['JournalArticle', 'Review']{guo-etal-2022-threats,  author = {Shangwei Guo and Chunlong Xie and Jiwei Li and L. Lyu and Tianwei Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Threats to Pre-trained Language Models: Survey and Taxonomy},  volume = {abs/2202.06862},  year = {2022} }
ThoughtSource: A central hub for large language model reasoning data	2023	https://www.semanticscholar.org/paper/1853a05df0ac03f8e4445ec65f7599359e89fb2b	The goal of ThoughtSource is to improve future artiﬁcial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data.	maybe	0	Large language models (LLMs) such as GPT-3 and ChatGPT have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present the ﬁrst release of ThoughtSource, a meta-dataset and so ware library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artiﬁcial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This ﬁrst release of ThoughtSource integrates six scientiﬁc/medical, three general-domain and ﬁve math word question answering datasets.	1853a05df0ac03f8e4445ec65f7599359e89fb2b	@['JournalArticle']{ott-etal-2023-thoughtsource:,  author = {Simon Ott and Konstantin Hebenstreit and Valentin Li'evin and C. Hother and M. Moradi and Maximilian Mayrhauser and Robert Praas and O. Winther and M. Samwald},  booktitle = {ArXiv},  journal = {ArXiv},  title = {ThoughtSource: A central hub for large language model reasoning data},  volume = {abs/2301.11596},  year = {2023} }
This isn’t the bias you’re looking for: Implicit causality, names and gender in German language models	2022	http://www.semanticscholar.org/paper/aa8d231539b02cdf540c456cc1b7a23683ba6f53		maybe	0	To assess whether neural language models capture discourse-level linguistic knowledge, previous work has tested whether they exhibit the well-known implicit causality (IC) bias found in various interpersonal verbs in different lan-guages. Stimuli for analyzing IC in computational and psycholinguistic experiments typically exhibit verb arguments with different gen-ders. In this paper, we revisit IC in German neural language models, analyzing gender and naming bias as a potential source of confusion. Indeed, our results suggest that IC biases in two existing models for German are weak, unstable, and behave in unexpected and unsystematic ways, when varying names or gender of verb arguments. Human scores prompts surnames not	aa8d231539b02cdf540c456cc1b7a23683ba6f53	@None{zarrieß-etal-2022-this,  author = {Sina Zarrieß and Hannes Groener and T. Solstad and Oliver Bott},  booktitle = {Conference on Natural Language Processing},  title = {This isn’t the bias you’re looking for: Implicit causality, names and gender in German language models},  year = {2022} }
This is a BERT. Now there are several of them. Can they generalize to novel words?	2020	https://www.semanticscholar.org/paper/0f4f27bb267b238d6044375863335db7fe69d661	This work considers monolingual and multilingual BERT models’ abilities to agree in number with novel plural words in English, French, German, Spanish, and Dutch, and finds that many models are not able to reliably determine plurality of novel words, suggesting potential deficiencies in the morphological capacities of Bert models.	maybe	3	Recently, large-scale pre-trained neural network models such as BERT have achieved many state-of-the-art results in natural language processing. Recent work has explored the linguistic capacities of these models. However, no work has focused on the ability of these models to generalize these capacities to novel words. This type of generalization is exhibited by humans, and is intimately related to morphology—humans are in many cases able to identify inflections of novel words in the appropriate context. This type of morphological capacity has not been previously tested in BERT models, and is important for morphologically-rich languages, which are under-studied in the literature regarding BERT’s linguistic capacities. In this work, we investigate this by considering monolingual and multilingual BERT models’ abilities to agree in number with novel plural words in English, French, German, Spanish, and Dutch. We find that many models are not able to reliably determine plurality of novel words, suggesting potential deficiencies in the morphological capacities of BERT models.	0f4f27bb267b238d6044375863335db7fe69d661	@['JournalArticle']{haley-2020-this,  author = {Coleman Haley},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {333-341},  title = {This is a BERT. Now there are several of them. Can they generalize to novel words?},  year = {2020} }
Thinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code	2021	http://www.semanticscholar.org/paper/b22d0ab847f56ed7ff66ac60eb994503fa851702	This paper investigates to what extent the attention weights of effective neural models match the reasoning of skilled humans, and presents a methodology for recording human attention and uses it to gather 1,508 human attention maps, which is the largest such dataset the authors are aware of.	maybe	6	Neural models of code are successfully tackling various prediction tasks, complementing and sometimes even outperforming traditional program analyses. While most work focuses on end-to-end evaluations of such models, it often remains unclear what the models actually learn, and to what extent their reasoning about code matches that of skilled humans. A poor understanding of the model reasoning risks deploying models that are right for the wrong reason, and taking decisions based on spurious correlations in the training dataset. This paper investigates to what extent the attention weights of effective neural models match the reasoning of skilled humans. To this end, we present a methodology for recording human attention and use it to gather 1,508 human attention maps from 91 participants, which is the largest such dataset we are aware of. Computing human-model correlations shows that the copy attention of neural models often matches the way humans reason about code (Spearman rank coefficients of 0.49 and 0.47), which gives an empirical justification for the intuition behind copy attention. In contrast, the regular attention of models is mostly uncorrelated with human attention. We find that models and humans sometimes focus on different kinds of tokens, e.g., strings are important to humans but mostly ignored by models. The results also show that human-model agreement positively correlates with accurate predictions by a model, which calls for neural models that even more closely mimic human reasoning. Beyond the insights from our study, we envision the release of our dataset of human attention maps to help understand future neural models of code and to foster work on human-inspired models.	b22d0ab847f56ed7ff66ac60eb994503fa851702	@['JournalArticle', 'Book', 'Conference']{paltenghi-pradel-2021-thinking,  author = {Matteo Paltenghi and Michael Pradel},  booktitle = {International Conference on Automated Software Engineering},  journal = {2021 36th IEEE/ACM International Conference on Automated Software Engineering (ASE)},  pages = {867-879},  title = {Thinking Like a Developer? Comparing the Attention of Humans with Neural Models of Code},  year = {2021} }
Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2	2021	http://www.semanticscholar.org/paper/008b9fc834f5839a25febe150f3076d550ee442f		yes	11	Thinking aloud is an effective meta-cognitive strategy human reasoners apply to solve difficult problems. We suggest to improve the reasoning ability of pre-trained neural language models in a similar way, namely by expanding a task’s context with problem elaborations that are dynamically generated by the language model itself. Our main result is that dynamic problem elaboration significantly improves the zero-shot performance of GPT-2 in a deductive reasoning and natural language inference task: While the model uses a syntactic heuristic for predicting an answer, it is capable (to some degree) of generating reasoned additional context which facilitates the successful application of its heuristic. We explore different ways of generating elaborations, including fewshot learning, and find that their relative performance varies with the specific problem characteristics (such as problem difficulty). Moreover, the effectiveness of an elaboration can be explained in terms of the degree to which the elaboration semantically coheres with the corresponding problem. In particular, elaborations that are most faithful to the original problem description may boost accuracy by up to 24%.	008b9fc834f5839a25febe150f3076d550ee442f	@['JournalArticle']{betz-etal-2021-thinking,  author = {Gregor Betz and Kyle Richardson and C. Voigt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Thinking Aloud: Dynamic Context Generation Improves Zero-Shot Reasoning Performance of GPT-2},  volume = {abs/2103.13033},  year = {2021} }
Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines	2020	http://www.semanticscholar.org/paper/2370ed2de59cdd99a2cc332853f979fcc84993d8	Empirical evidence is provided that the human brain and autoregressive DLMs share two computational principles: both are engaged in continuous prediction; both represent words as a function of the previous context; and DLM’s contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embedding.	maybe	20	Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). Using a self-supervised next-word prediction task, these models are trained to generate appropriate linguistic responses in a given context. We provide empirical evidence that the human brain and autoregressive DLMs share three fundamental computational principles as they process natural language: 1) both are engaged in continuous next-word prediction before word-onset; 2) both match their pre-onset predictions to the incoming word to calculate post-onset surprise (i.e., prediction error signals); 3) both represent words as a function of the previous context. In support of these three principles, our findings indicate that: a) the neural activity before word-onset contains context-dependent predictive information about forthcoming words, even hundreds of milliseconds before the words are perceived; b) the neural activity after word-onset reflects the surprise level and prediction error; and c) autoregressive DLM contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Together, our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.	2370ed2de59cdd99a2cc332853f979fcc84993d8	@None{goldstein-etal-2020-thinking,  author = {Ariel Goldstein and Zaid Zada and Eliav Buchnik and Mariano Schain and A. Price and Bobbi Aubrey and Samuel A. Nastase and Amir Feder and Dotan Emanuel and Alon Cohen and A. Jansen and H. Gazula and Gina Choe and Aditi Rao and Catherine Kim and Colton Casto and Fanda Lora and A. Flinker and S. Devore and W. Doyle and D. Friedman and P. Dugan and Avinatan Hassidim and Michael P. Brenner and Y. Matias and K. Norman and O. Devinsky and U. Hasson},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines},  year = {2020} }
Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again	2022	http://www.semanticscholar.org/paper/cfc12c38a4d848ff3c4225488a2c72e7d4300f4b	This is the first systematic and comprehensive study to compare the few-shot performance of GPT-3 in-context learning with smaller (i.e., BERT-sized) PLMs on two highly representative biomedical information extraction tasks, named entity recognition and relation extraction.	maybe	9	The strong few-shot in-context learning ca-pability of large pre-trained language models (PLMs) such as GPT-3 is highly appealing for application domains such as biomedicine, which feature high and diverse demands of language technologies but also high data annotation costs. In this paper, we present the ﬁrst systematic and comprehensive study to compare the few-shot performance of GPT-3 in-context learning with ﬁne-tuning smaller (i.e., BERT-sized) PLMs on two highly representative biomedical information extraction tasks, named entity recognition and relation extraction. We follow the true few-shot setting (Perez et al., 2021) to avoid overestimating models’ few-shot performance by model selection over a large validation set. We also optimize GPT-3’s performance with known techniques such as contextual calibration and dynamic in-context example retrieval. How-ever, our results show that GPT-3 still signiﬁcantly underperforms compared to simply ﬁne-tuning a smaller PLM. In addition, GPT-3 in-context learning also yields smaller gains in accuracy when more training data becomes available. Our in-depth analyses further reveal issues of the in-context learning setting that may be detrimental to information extraction tasks in general. Given the high cost of ex-perimenting with GPT-3, we hope our study provides guidance for biomedical researchers and practitioners towards more promising directions such as ﬁne-tuning small PLMs. 1	cfc12c38a4d848ff3c4225488a2c72e7d4300f4b	@['JournalArticle']{gutierrez-etal-2022-thinking,  author = {Bernal Jimenez Gutierrez and Nikolas McNeal and Clay Washington and You Chen and Lang Li and Huan Sun and Yu Su},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Thinking about GPT-3 In-Context Learning for Biomedical IE? Think Again},  volume = {abs/2203.08410},  year = {2022} }
Think Before You Speak: Using Self-talk to Generate Implicit Commonsense Knowledge for Response Generation	2021	http://www.semanticscholar.org/paper/ce74df5126faad7d74f578f1e1953278611e235d	This paper presents a self-talk approach that first generates the implicit commonsense knowledge and then generates response by referencing the externalized knowledge, all using one generative model.	maybe	6	Implicit knowledge, such as common sense, is key to fluid human conversations. Current neural response generation (RG) models are trained end-to-end, omitting unstated implicit knowledge. In this paper, we present a self-talk approach that first generates the implicit commonsense knowledge and then generates response by referencing the externalized knowledge, all using one generative model. We analyze different choices to collect knowledge-aligned dialogues, represent implicit knowledge, and elicit knowledge and responses. We introduce three evaluation aspects: knowledge quality, knowledge-response connection, and response quality and perform extensive human evaluations. Our experimental results show that compared with end-to-end RG models, self-talk models that externalize the knowledge grounding process by explicitly generating implicit knowledge also produce responses that are more informative, specific, and follow common sense. We also find via human evaluation that self-talk models generate high-quality knowledge around 75% of the time. We hope that our findings encourage further work on different approaches to modeling implicit commonsense knowledge and training knowledgeable RG models.	ce74df5126faad7d74f578f1e1953278611e235d	@['JournalArticle']{zhou-etal-2021-think,  author = {Pei Zhou and Karthik Gopalakrishnan and Behnam Hedayatnia and Seokhwan Kim and J. Pujara and Xiang Ren and Yang Liu and Dilek Z. Hakkani-Tür},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Think Before You Speak: Using Self-talk to Generate Implicit Commonsense Knowledge for Response Generation},  volume = {abs/2110.08501},  year = {2021} }
Thieves on Sesame Street! Model Extraction of BERT-based APIs	2019	https://www.semanticscholar.org/paper/ac713aebdcc06f15f8ea61e1140bb360341fdf27	This work highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model.	seed	98	We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones.	ac713aebdcc06f15f8ea61e1140bb360341fdf27	@['JournalArticle']{krishna-etal-2019-thieves,  author = {Kalpesh Krishna and Gaurav Singh Tomar and Ankur P. Parikh and Nicolas Papernot and Mohit Iyyer},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Thieves on Sesame Street! Model Extraction of BERT-based APIs},  volume = {abs/1910.12366},  year = {2019} }
Theoretical Limitations of Self-Attention in Neural Sequence Models	2019	http://www.semanticscholar.org/paper/b3564be8b79f25585acb035f3deaf4ae93c26d8f	Across both soft and hard attention, strong theoretical limitations are shown of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length.	maybe	100	Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.	b3564be8b79f25585acb035f3deaf4ae93c26d8f	@['JournalArticle']{hahn-2019-theoretical,  author = {Michael Hahn},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {156-171},  title = {Theoretical Limitations of Self-Attention in Neural Sequence Models},  volume = {8},  year = {2019} }
The World of an Octopus: How Reporting Bias Influences a Language Model’s Perception of Color	2021	http://www.semanticscholar.org/paper/1360dc6d21ccbf2c23701c9ddd7c6ab8d04e4531	The results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training.	yes	14	Recent work has raised concerns about the inherent limitations of text-only pretraining. In this paper, we first demonstrate that reporting bias, the tendency of people to not state the obvious, is one of the causes of this limitation, and then investigate to what extent multimodal training can mitigate this issue. To accomplish this, we 1) generate the Color Dataset (CoDa), a dataset of human-perceived color distributions for 521 common objects; 2) use CoDa to analyze and compare the color distribution found in text, the distribution captured by language models, and a human’s perception of color; and 3) investigate the performance differences between text-only and multimodal models on CoDa. Our results show that the distribution of colors that a language model recovers correlates more strongly with the inaccurate distribution found in text than with the ground-truth, supporting the claim that reporting bias negatively impacts and inherently limits text-only training. We then demonstrate that multimodal models can leverage their visual training to mitigate these effects, providing a promising avenue for future research.	1360dc6d21ccbf2c23701c9ddd7c6ab8d04e4531	@['JournalArticle', 'Conference']{paik-etal-2021-the,  author = {Cory Paik and St'ephane Aroca-Ouellette and A. Roncone and Katharina Kann},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {823-835},  title = {The World of an Octopus: How Reporting Bias Influences a Language Model’s Perception of Color},  year = {2021} }
The Unreliability of Explanations in Few-Shot In-Context Learning	2022	http://www.semanticscholar.org/paper/de04aa282f8055cebe86966c592bf37af6aecc99	A framework for calibrating model predictions based on the reliability of explanations is presented and it is shown that explanations judged as good by humans—those that are logically consistent with the input and the prediction—usually indicate more accurate predictions.	maybe	18	How can prompting a large language model like GPT-3 with explanations improve in-context learning? We focus speciﬁcally on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. Including explanations in the prompt and having the model generate them does not consistently improve performance in the settings we study, contrary to recent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al., 2022). Despite careful prompting, explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations. However, these ﬂawed explanations can still be useful as a way to verify GPT-3’s predictions post-hoc. Through analysis in three settings, we show that explanations judged as good by humans—those that are logically consistent with the input and the prediction—usually indicate more accurate predictions. Following these observations, we present a framework for calibrating model predictions based on the reliability of the explanations. Our framework trains calibrators using automatically extracted scores that approximately assess the reliability of explanations, which helps improve performance across three different datasets.	de04aa282f8055cebe86966c592bf37af6aecc99	@['JournalArticle']{ye-durrett-2022-the,  author = {Xi Ye and Greg Durrett},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Unreliability of Explanations in Few-Shot In-Context Learning},  volume = {abs/2205.03401},  year = {2022} }
The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction	2019	http://www.semanticscholar.org/paper/98332ae5475bfa286d9ab74e20d86c72ceb3cde6	It is shown that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.	maybe	13	Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.	98332ae5475bfa286d9ab74e20d86c72ceb3cde6	@['JournalArticle']{alikaniotis-raheja-2019-the,  author = {Dimitrios Alikaniotis and Vipul Raheja},  booktitle = {BEA@ACL},  pages = {127-133},  title = {The Unreasonable Effectiveness of Transformer Language Models in Grammatical Error Correction},  year = {2019} }
The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings	2023	http://www.semanticscholar.org/paper/ccf340560dfda477764c74dfa21de510bbdddf07	This work studies the effect of frequency when measuring female vs. male gender bias with word embedding-based bias quantiﬁcation methods and proves that the frequency-based effect observed in unshufﬂed corpora stems from properties of the metric rather than from word associations.	maybe	1	Numerous works use word embedding-based metrics to quantify societal biases and stereotypes in texts. Recent studies have found that word embeddings can capture semantic similarity but may be affected by word frequency. In this work we study the effect of frequency when measuring female vs. male gender bias with word embedding-based bias quantiﬁcation methods. We ﬁnd that Skip-gram with negative sampling and GloVe tend to detect male bias in high frequency words, while GloVe tends to return female bias in low frequency words. We show these behaviors still exist when words are randomly shufﬂed. This proves that the frequency-based effect observed in unshufﬂed corpora stems from properties of the metric rather than from word associations. The effect is spurious and problematic since bias metrics should depend exclusively on word co-occurrences and not individual word frequencies. Finally, we compare these results with the ones obtained with an alternative metric based on Pointwise Mutual Information. We ﬁnd that this metric does not show a clear dependence on frequency, even though it is slightly skewed towards male bias across all frequencies.	ccf340560dfda477764c74dfa21de510bbdddf07	@['JournalArticle']{valentini-etal-2023-the,  author = {Francisco Valentini and German Rosati and D. Slezak and E. Altszyler},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Undesirable Dependence on Frequency of Gender Bias Metrics Based on Word Embeddings},  volume = {abs/2301.00792},  year = {2023} }
The Turing Deception	2022	http://www.semanticscholar.org/paper/905c886090f1943da1e44ab2ece7e7659cf5a35c	This research revisits the classic Turing test and compares recent large language models such as ChatGPT for their abilities to reproduce human-level comprehension and compelling text generation and establishes multiple cases where the generated content proves original and undetectable.	maybe	0	This research revisits the classic Turing test and compares recent large language models such as ChatGPT for their abilities to reproduce human-level comprehension and compelling text generation. Two task challenges- summary and question answering- prompt ChatGPT to produce original content (98-99%) from a single text entry and sequential questions initially posed by Turing in 1950. We score the original and generated content against the OpenAI GPT-2 Output Detector from 2019, and establish multiple cases where the generated content proves original and undetectable (98%). The question of a machine fooling a human judge recedes in this work relative to the question of "how would one prove it?" The original contribution of the work presents a metric and simple grammatical set for understanding the writing mechanics of chatbots in evaluating their readability and statistical clarity, engagement, delivery, overall quality, and plagiarism risks. While Turing's original prose scores at least 14% below the machine-generated output, whether an algorithm displays hints of Turing's true initial thoughts (the "Lovelace 2.0" test) remains unanswerable.	905c886090f1943da1e44ab2ece7e7659cf5a35c	@['JournalArticle']{noever-ciolino-2022-the,  author = {David Noever and Matt Ciolino},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Turing Deception},  volume = {abs/2212.06721},  year = {2022} }
The triggers that open the NLP model backdoors are hidden in the adversarial samples	2022	http://www.semanticscholar.org/paper/3ae488cc40794aa8106c34f7a6f538a8fceb6a38		yes	2		3ae488cc40794aa8106c34f7a6f538a8fceb6a38	@['JournalArticle']{shao-etal-2022-the,  author = {Kun Shao and Yu Zhang and Jun-an Yang and Xiaoshuai Li and H. Liu},  booktitle = {Computers & security},  journal = {Comput. Secur.},  pages = {102730},  title = {The triggers that open the NLP model backdoors are hidden in the adversarial samples},  volume = {118},  year = {2022} }
The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models	2021	http://www.semanticscholar.org/paper/05b2b28ebd8bcf0de4fe1cea9d096f20bbd3ab5f	It is shown that the Spotlight surfaces semantically meaningful areas of weakness in a wide variety of existing models spanning computer vision, NLP, and recommender systems, and its performance is verified through quantitative experiments.	maybe	18	Supervised learning models often make systematic errors on rare subsets of the data. When these subsets correspond to explicit labels in the data (e.g., gender, race) such poor performance can be identified straightforwardly. This paper introduces a method for discovering systematic errors that do not correspond to such explicitly labelled subgroups. The key idea is that similar inputs tend to have similar representations in the final hidden layer of a neural network. We leverage this structure by “shining a spotlight” on this representation space to find contiguous regions in which the model performs poorly. We show that the Spotlight surfaces semantically meaningful areas of weakness in a wide variety of existing models spanning computer vision, NLP, and recommender systems, and we verify its performance through quantitative experiments.	05b2b28ebd8bcf0de4fe1cea9d096f20bbd3ab5f	@['Book', 'JournalArticle']{d'eon-etal-2021-the,  author = {G. d'Eon and Jason d'Eon and J. R. Wright and Kevin Leyton-Brown},  booktitle = {Conference on Fairness, Accountability and Transparency},  journal = {2022 ACM Conference on Fairness, Accountability, and Transparency},  title = {The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models},  year = {2021} }
The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference	2023	http://www.semanticscholar.org/paper/7ad5a0659d0f3c06137edf11e7e0d2a8c93c6350	The role interactive visualization can play in explaining NLP models (XNLP) is discussed and the use of visualization in relation to target users and common NLP pipelines is motivated.	maybe	0	With a constant increase of learned parameters, modern neural language models become increasingly more powerful. Yet, explaining these complex model’s behavior remains a widely unsolved problem. In this paper, we discuss the role interactive visualization can play in explaining NLP models (XNLP). We motivate the use of visualization in relation to target users and common NLP pipelines. We also present several use cases to provide concrete examples on XNLP with visualization. Finally, we point out an extensive list of research opportunities in this ﬁeld.	7ad5a0659d0f3c06137edf11e7e0d2a8c93c6350	@['JournalArticle']{brath-etal-2023-the,  author = {R. Brath and D. Keim and Johannes Knittel and Shimei Pan and Pia Sommerauer and Hendrik Strobelt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Role of Interactive Visualization in Explaining (Large) NLP Models: from Data to Inference},  volume = {abs/2301.04528},  year = {2023} }
The Role of Explanatory Value in Natural Language Processing	2022	http://www.semanticscholar.org/paper/e30e6e776682cf5bd061c96a19f459e1cdb462cf		maybe	0	A key aim of science is explanation, yet the idea of explaining language phenomena has taken a backseat in mainstream Natural Language Processing (NLP) and many other areas of Artiﬁcial Intelligence. I argue that explanation of linguistic behaviour should be a main goal of NLP, and that this is not the same as making NLP models “explainable". To illustrate these ideas, some recent models of human language production are compared with each other. I conclude by asking what it would mean for NLP research and institutional policies if our community took explanatory value seriously, while heeding some possible pitfalls.	e30e6e776682cf5bd061c96a19f459e1cdb462cf	@['JournalArticle']{deemter-2022-the,  author = {Kees van Deemter},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Role of Explanatory Value in Natural Language Processing},  volume = {abs/2209.06169},  year = {2022} }
The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming	2022	http://www.semanticscholar.org/paper/979eb5c97c49d7979447ed684500895a24d75ac4	This work explores how Codex performs on typical introductory programming problems, and reports its performance on real questions taken from introductory programming exams and compares it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students.	yes	21	Recent advances in artificial intelligence have been driven by an exponential growth in digitised data. Natural language processing, in particular, has been transformed by machine learning models such as OpenAI’s GPT-3 which generates human-like text so realistic that its developers have warned of the dangers of its misuse. In recent months OpenAI released Codex, a new deep learning model trained on Python code from more than 50 million GitHub repositories. Provided with a natural language description of a programming problem as input, Codex generates solution code as output. It can also explain (in English) input code, translate code between programming languages, and more. In this work, we explore how Codex performs on typical introductory programming problems. We report its performance on real questions taken from introductory programming exams and compare it to results from students who took these same exams under normal conditions, demonstrating that Codex outscores most students. We then explore how Codex handles subtle variations in problem wording using several published variants of the well-known “Rainfall Problem” along with one unpublished variant we have used in our teaching. We find the model passes many test cases for all variants. We also explore how much variation there is in the Codex generated solutions, observing that an identical input prompt frequently leads to very different solutions in terms of algorithmic approach and code length. Finally, we discuss the implications that such technology will have for computing education as it continues to evolve, including both challenges and opportunities.	979eb5c97c49d7979447ed684500895a24d75ac4	@['Book', 'JournalArticle']{ansley-etal-2022-the,  author = {James Finnie-Ansley and Paul Denny and Brett A. Becker and Andrew Luxton-Reilly and J. Prather},  booktitle = {IFAC Symposium on Advances in Control Education},  journal = {Proceedings of the 24th Australasian Computing Education Conference},  title = {The Robots Are Coming: Exploring the Implications of OpenAI Codex on Introductory Programming},  year = {2022} }
The Reverse Turing Test for Evaluating Interpretability Methods on Unknown Tasks	2020	http://www.semanticscholar.org/paper/b04a506c8b0cb7a3cd05182f2cfb85fc00dac171	A human evaluation of LIME across NLP tasks in a Latin Square design is presented and the effect of masking the task in forward prediction experiments is analyzed to evaluate the quality of interpretability methods.	maybe	3	The Turing Test evaluates a computer program’s ability to mimic human behaviour. The Reverse Turing Test, reversely, evaluates a human’s ability to mimic machine behaviour in a forward prediction task. We propose to use the Reverse Turing Test to evaluate the quality of interpretability methods. The Reverse Turing Test im-proves on previous experimental protocols for human evaluation of interpretability methods by a) including a training phase, and b) masking the task, which, com-bined, enables us to evaluate models independently of their quality, in a way that is unbiased by the participants’ previous exposure to the task. We present a human evaluation of LIME across ﬁve NLP tasks in a Latin Square design and analyze the effect of masking the task in forward prediction experiments. Additionally, we demonstrate a fundamental limitation of LIME and show how this limitation is detrimental for human forward prediction in some NLP tasks.	b04a506c8b0cb7a3cd05182f2cfb85fc00dac171	@None{gonzalez-søgaard-2020-the,  author = {Ana Valeria Gonzalez and Anders Søgaard},  title = {The Reverse Turing Test for Evaluating Interpretability Methods on Unknown Tasks},  year = {2020} }
The Privacy Onion Effect: Memorization is Relative	2022	http://www.semanticscholar.org/paper/90543a4576fd9d11bfa6078795b0ad9d8eaf1f08	An Onion Effect of memorization is demonstrated and analysed: removing the “layer” of outlier points that are most vulnerable to a privacy attack exposes a new layer of previously-safe points to the same attack.	maybe	6	Machine learning models trained on private datasets have been shown to leak their private data. While recent work has found that the average data point is rarely leaked, the outlier samples are frequently subject to memorization and, consequently, privacy leakage. We demonstrate and analyse an Onion Effect of memorization: removing the “layer” of outlier points that are most vulnerable to a privacy attack exposes a new layer of previously-safe points to the same attack. We perform several experiments to study this effect, and understand why it occurs. The existence of this effect has various consequences. For example, it suggests that proposals to defend against memorization without training with rigorous privacy guarantees are unlikely to be effective. Further, it suggests that privacy-enhancing technologies such as machine unlearning could actually harm the privacy of other users.	90543a4576fd9d11bfa6078795b0ad9d8eaf1f08	@['JournalArticle']{carlini-etal-2022-the,  author = {Nicholas Carlini and Matthew Jagielski and Nicolas Papernot and A. Terzis and Florian Tramèr and Chiyuan Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Privacy Onion Effect: Memorization is Relative},  volume = {abs/2206.10469},  year = {2022} }
The Power of Scale for Parameter-Efficient Prompt Tuning	2021	http://www.semanticscholar.org/paper/ffdbd7f0b03b85747b001b4734d5ee31b5229aa4	This work explores “prompt tuning”, a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks, and shows that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.	yes	597	In this work, we explore “prompt tuning,” a simple yet effective mechanism for learning “soft prompts” to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3’s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method “closes the gap” and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed “prefix tuning” of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient “prompt ensembling.” We release code and model checkpoints to reproduce our experiments.	ffdbd7f0b03b85747b001b4734d5ee31b5229aa4	@['JournalArticle', 'Conference']{lester-etal-2021-the,  author = {Brian Lester and Rami Al-Rfou and Noah Constant},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {The Power of Scale for Parameter-Efficient Prompt Tuning},  volume = {abs/2104.08691},  year = {2021} }
The neural architecture of language: Integrative modeling converges on predictive processing	2020	http://www.semanticscholar.org/paper/644a33399711b31f8a5a1b464f6ffd7c2264fedc	It is found that the most powerful “transformer” models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography).	maybe	110	Significance Language is a quintessentially human ability. Research has long probed the functional architecture of language in the mind and brain using diverse neuroimaging, behavioral, and computational modeling approaches. However, adequate neurally-mechanistic accounts of how meaning might be extracted from language are sorely lacking. Here, we report a first step toward addressing this gap by connecting recent artificial neural networks from machine learning to human recordings during language processing. We find that the most powerful models predict neural and behavioral responses across different datasets up to noise levels. Models that perform better at predicting the next word in a sequence also better predict brain measurements—providing computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the brain. The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species’ signature cognitive skill. We find that the most powerful “transformer” models predict nearly 100% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models’ neural fits (“brain score”) and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.	644a33399711b31f8a5a1b464f6ffd7c2264fedc	@['JournalArticle']{schrimpf-etal-2020-the,  author = {Martin Schrimpf and I. Blank and Greta Tuckute and Carina Kauf and Eghbal A. Hosseini and N. Kanwisher and J. Tenenbaum and Evelina Fedorenko},  booktitle = {Proceedings of the National Academy of Sciences},  journal = {Proceedings of the National Academy of Sciences},  title = {The neural architecture of language: Integrative modeling converges on predictive processing},  volume = {118},  year = {2020} }
The MultiBERTs: BERT Reproductions for Robustness Analysis	2021	https://www.semanticscholar.org/paper/5b540745f4b51f95bf90fb3420e51edb037fc51a	MultiBERTs are introduced: a set of 25 BERTbase checkpoints, trained with similar hyperparameters as the original BERT model but differing in random initialization and data shuffling, to enable researchers to draw robust and statistically justified conclusions about pretraining procedures.	maybe	33	Experiments with pretrained models such as BERT are often based on a single checkpoint. While the conclusions drawn apply to the artifact (i.e., the particular instance of the model), it is not always clear whether they hold for the more general procedure (which includes the model architecture, training data, initialization scheme, and loss function). Recent work has shown that re-running pretraining can lead to substantially different conclusions about performance, suggesting that alternative evaluations are needed to make principled statements about procedures. To address this question, we introduce MultiBERTs: a set of 25 BERTbase checkpoints, trained with similar hyperparameters as the original BERT model but differing in random initialization and data shuffling. The aim is to enable researchers to draw robust and statistically justified conclusions about pretraining procedures. The full release includes 25 fully trained checkpoints, as well as statistical guidelines and a code library implementing our recommended hypothesis testing methods. Finally, for five of these models we release a set of 28 intermediate checkpoints in order to support research on learning dynamics.	5b540745f4b51f95bf90fb3420e51edb037fc51a	@['JournalArticle']{sellam-etal-2021-the,  author = {Thibault Sellam and S. Yadlowsky and Jason Wei and Naomi Saphra and A. D'Amour and Tal Linzen and Jasmijn Bastings and Iulia Turc and Jacob Eisenstein and Dipanjan Das and Ian Tenney and Ellie Pavlick},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {The MultiBERTs: BERT Reproductions for Robustness Analysis},  volume = {abs/2106.16163},  year = {2021} }
The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems	2022	http://www.semanticscholar.org/paper/6d23532a1e9a8116041fd5aac6b0ef8ddd6d8171	The Moral Integrity Corpus, MIC, is a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs), and is suggested that MIC will be a useful resource for understanding and language models’ implicit moral assumptions and flexibly benchmarking the integrity of conversational agents.	maybe	15	Conversational agents have come increasingly closer to human competence in open-domain dialogue settings; however, such models can reflect insensitive, hurtful, or entirely incoherent viewpoints that erode a user’s trust in the moral integrity of the system. Moral deviations are difficult to mitigate because moral judgments are not universal, and there may be multiple competing judgments that apply to a situation simultaneously. In this work, we introduce a new resource, not to authoritatively resolve moral ambiguities, but instead to facilitate systematic understanding of the intuitions, values and moral judgments reflected in the utterances of dialogue systems. The Moral Integrity Corpus, MIC, is such a resource, which captures the moral assumptions of 38k prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs). Each RoT reflects a particular moral conviction that can explain why a chatbot’s reply may appear acceptable or problematic. We further organize RoTs with a set of 9 moral and social attributes and benchmark performance for attribute classification. Most importantly, we show that current neural language models can automatically generate new RoTs that reasonably describe previously unseen interactions, but they still struggle with certain scenarios. Our findings suggest that MIC will be a useful resource for understanding and language models’ implicit moral assumptions and flexibly benchmarking the integrity of conversational agents. To download the data, see https://github.com/GT-SALT/mic	6d23532a1e9a8116041fd5aac6b0ef8ddd6d8171	@['JournalArticle', 'Conference']{ziems-etal-2022-the,  author = {C. Ziems and Jane A. Yu and Yi-Chia Wang and A. Halevy and Diyi Yang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems},  volume = {abs/2204.03021},  year = {2022} }
The Mapping of Deep Language Models on Brain Responses Primarily Depends on their Performance	2022	http://www.semanticscholar.org/paper/a1d46e594cb8edcc71881856317e0d2bce41f9fd	Overall, this study evidences a partial convergence of language transformers to brain- like solutions, and shows how this phenomenon helps unravel the brain bases of natural language processing.	yes	0	Recent deep networks like transformers not only excel in several language tasks, but their activations 1 linearly map onto the human brain during language processing. Is this functional similarity caused by 2 speciﬁc factors, such as the language abilities and the architecture of the algorithms? To address this 3 issue, we analyze the brain responses to isolated sentences in a large cohort of 102 subjects, each 4 recorded with both functional magnetic resonance imaging (fMRI) and magnetoencephalography 5 (MEG). We then compare the ability of 32,400 transformer embeddings to linearly map onto these 6 brain responses. Finally, we evaluate how the architecture, training, and performance of the models 7 independently account for this brain mapping. Our analyses reveal two main ﬁndings. First, the 8 similarity between brain responses and the activations of language models primarily depends on their 9 ability to predict words from the context. Second, this similarity allows us to decompose and precisely 10 track the rise and maintenance of perceptual, lexical, and compositional representations within each 11 cortical region. Overall, this study evidences a partial convergence of language transformers to brain- 12 like solutions, and shows how this phenomenon helps unravel the brain bases of natural language 13 processing.	a1d46e594cb8edcc71881856317e0d2bce41f9fd	@None{caucheteux-2022-the,  author = {C. Caucheteux},  title = {The Mapping of Deep Language Models on Brain Responses Primarily Depends on their Performance},  year = {2022} }
The Low-Dimensional Linear Geometry of Contextualized Word Representations	2021	http://www.semanticscholar.org/paper/761f1607b380df54546fd2114b458aa19109cd3d	A systematic study of the linear geometry of contextualized word representations in ELMO and BERT shows that a variety of linguistic features are encoded in low-dimensional subspaces, and shows that there are hierarchical relations between the subspaced encoding general linguistic categories and more specific ones.	yes	13	Black-box probing models can reliably extract linguistic features like tense, number, and syntactic role from pretrained word representations. However, the manner in which these features are encoded in representations remains poorly understood. We present a systematic study of the linear geometry of contextualized word representations in ELMO and BERT. We show that a variety of linguistic features (including structured dependency relationships) are encoded in low-dimensional subspaces. We then refine this geometric picture, showing that there are hierarchical relations between the subspaces encoding general linguistic categories and more specific ones, and that low-dimensional feature encodings are distributed rather than aligned to individual neurons. Finally, we demonstrate that these linear subspaces are causally related to model behavior, and can be used to perform fine-grained manipulation of BERT’s output distribution.	761f1607b380df54546fd2114b458aa19109cd3d	@['JournalArticle']{hernandez-andreas-2021-the,  author = {Evan Hernandez and Jacob Andreas},  booktitle = {Conference on Computational Natural Language Learning},  pages = {82-93},  title = {The Low-Dimensional Linear Geometry of Contextualized Word Representations},  year = {2021} }
The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation	2021	https://www.semanticscholar.org/paper/7621b86f7be6ae5a6ef4a8a5069d57f9e6fa2d08	This work probes the LM’s expectations by generating from it: it uses stochastic decoding to derive a set of sentence completions, and estimates the probability that the LM assigns to each interpretation based on the distribution of parses across completions.	maybe	4	Temporary syntactic ambiguities arise when the beginning of a sentence is compatible with multiple syntactic analyses. We inspect to which extent neural language models (LMs) exhibit uncertainty over such analyses when processing temporarily ambiguous inputs, and how that uncertainty is modulated by disambiguating cues. We probe the LM’s expectations by generating from it: we use stochastic decoding to derive a set of sentence completions, and estimate the probability that the LM assigns to each interpretation based on the distribution of parses across completions. Unlike scoring-based methods for targeted syntactic evaluation, this technique makes it possible to explore completions that are not hypothesized in advance by the researcher. We apply this method to study the behavior of two LMs (GPT2 and an LSTM) on three types of temporary ambiguity, using materials from human sentence processing experiments. We find that LMs can track multiple analyses simultaneously; the degree of uncertainty varies across constructions and contexts. As a response to disambiguating cues, the LMs often select the correct interpretation, but occasional errors point to potential areas of improvement	7621b86f7be6ae5a6ef4a8a5069d57f9e6fa2d08	@['JournalArticle']{aina-linzen-2021-the,  author = {Laura Aina and Tal Linzen},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {42-57},  title = {The Language Model Understood the Prompt was Ambiguous: Probing Syntactic Uncertainty Through Generation},  year = {2021} }
The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models	2020	http://www.semanticscholar.org/paper/a81a6301beecbe3d32872aaacd96905b721516ae	The Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models, is presented, which integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis.	maybe	91	We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models—including classification, seq2seq, and structured prediction—and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.	a81a6301beecbe3d32872aaacd96905b721516ae	@['JournalArticle', 'Conference']{tenney-etal-2020-the,  author = {Ian Tenney and James Wexler and Jasmijn Bastings and Tolga Bolukbasi and Andy Coenen and Sebastian Gehrmann and Ellen Jiang and Mahima Pushkarna and Carey Radebaugh and Emily Reif and Ann Yuan},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {107-118},  title = {The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models},  year = {2020} }
The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in BERT	2021	https://www.semanticscholar.org/paper/dd24067c396f4b5a6500a71101ff1dc8ccb8811f	A simple yet effective score is formalized that generalizes to all the roles of attention heads and employs hypothesis testing on this score for robust inference, and comment on the co-location of multiple functional roles in the same attention head, and effect of fine-tuning for specific NLP tasks on these functional roles.	maybe	6	Multi-headed attention heads are a mainstay in transformer-based models. Different methods have been proposed to classify the role of each attention head based on the relations between tokens which have high pair-wise attention. These roles include syntactic (tokens with some syntactic relation), local (nearby tokens), block (tokens in the same sentence) and delimiter (the special [CLS], [SEP] tokens). There are two main challenges with existing methods for classification: (a) there are no standard scores across studies or across functional roles, and (b) these scores are often average quantities measured across sentences without capturing statistical significance. In this work, we formalize a simple yet effective score that generalizes to all the roles of attention heads and employs hypothesis testing on this score for robust inference. This provides us the right lens to systematically analyze attention heads and confidently comment on many commonly posed questions on analyzing the BERT model. In particular, we comment on the co-location of multiple functional roles in the same attention head, the distribution of attention heads across layers, and effect of fine-tuning for specific NLP tasks on these functional roles. The code is made publicly available at https://github.com/iitmnlp/heads-hypothesis	dd24067c396f4b5a6500a71101ff1dc8ccb8811f	@['JournalArticle', 'Conference']{pande-etal-2021-the,  author = {Madhura Pande and Aakriti Budhraja and Preksha Nema and Pratyush Kumar and Mitesh M. Khapra},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {13613-13621},  title = {The heads hypothesis: A unifying statistical approach towards understanding multi-headed attention in BERT},  year = {2021} }
The Grammar-Learning Trajectories of Neural Language Models	2021	http://www.semanticscholar.org/paper/c6dbd97f458c76925363a6b8f6c5e6198163e54e	It is shown that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance, which suggests that there is some mutual inductive bias that underlies these models’ learning of linguistic phenomena.	maybe	8	The learning trajectories of linguistic phenomena in humans provide insight into linguistic representation, beyond what can be gleaned from inspecting the behavior of an adult speaker. To apply a similar approach to analyze neural language models (NLM), it is first necessary to establish that different models are similar enough in the generalizations they make. In this paper, we show that NLMs with different initialization, architecture, and training data acquire linguistic phenomena in a similar order, despite their different end performance. These findings suggest that there is some mutual inductive bias that underlies these models’ learning of linguistic phenomena. Taking inspiration from psycholinguistics, we argue that studying this inductive bias is an opportunity to study the linguistic representation implicit in NLMs.Leveraging these findings, we compare the relative performance on different phenomena at varying learning stages with simpler reference models. Results suggest that NLMs exhibit consistent “developmental” stages. Moreover, we find the learning trajectory to be approximately one-dimensional: given an NLM with a certain overall performance, it is possible to predict what linguistic generalizations it has already acquired.Initial analysis of these stages presents phenomena clusters (notably morphological ones), whose performance progresses in unison, suggesting a potential link between the generalizations behind them.	c6dbd97f458c76925363a6b8f6c5e6198163e54e	@['JournalArticle', 'Conference']{choshen-etal-2021-the,  author = {Leshem Choshen and Guy Hacohen and D. Weinshall and Omri Abend},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {8281-8297},  title = {The Grammar-Learning Trajectories of Neural Language Models},  year = {2021} }
The Grammar of Interactive Explanatory Model Analysis	2020	http://www.semanticscholar.org/paper/725e26de09c24906e28a61ab2b3f242ddd02aaa1	The problem of model explainability is presented as an interactive and sequential explanatory analysis of a model (IEMA) and the grammar of such interactive explanations is introduced, showing how different XAI methods complement each other and why it is essential to juxtapose them together.	maybe	12	The growing need for in-depth analysis of predictive models leads to a series of new methods for explaining their local and global properties. Which of these methods is the best? It turns out that this is an ill-posed question. One cannot sufficiently explain a black-box machine learning model using a single method that gives only one perspective. Isolated explanations are prone to misunderstanding, which inevitably leads to wrong or simplistic reasoning. This problem is known as the Rashomon effect and refers to diverse, even contradictory interpretations of the same phenomenon. Surprisingly, the majority of methods developed for explainable machine learning focus on a single aspect of the model behavior. In contrast, we showcase the problem of explainability as an interactive and sequential analysis of a model. This paper presents how different Explanatory Model Analysis (EMA) methods complement each other and why it is essential to juxtapose them together. The introduced process of Interactive EMA (IEMA) derives from the algorithmic side of explainable machine learning and aims to embrace ideas developed in cognitive sciences. We formalize the grammar of IEMA to describe potential human-model dialogues. IEMA is implemented in the human-centered framework that adopts interactivity, customizability and automation as its main traits. Combined, these methods enhance the responsible approach to predictive modeling.	725e26de09c24906e28a61ab2b3f242ddd02aaa1	@['JournalArticle']{baniecki-biecek-2020-the,  author = {Hubert Baniecki and P. Biecek},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Grammar of Interactive Explanatory Model Analysis},  volume = {abs/2005.00497},  year = {2020} }
The Ghost in the Machine has an American accent: value conflict in GPT-3	2022	https://www.semanticscholar.org/paper/5bf2343f67ae81610040d3fa07024c13c8591da7		maybe	2	The alignment problem in the context of large language models must consider the plurality of human values in our world. Whilst there are many resonant and overlapping values amongst the world’s cultures, there are also many conflicting, yet equally valid, values. It is important to observe which cultural values a model exhibits, particularly when there is a value conflict between input prompts and generated outputs. We discuss how the co-creation of language and cultural value impacts large language models (LLMs). We explore the constitution of the training data for GPT-3 and compare that to the world’s language and internet access demographics, as well as to reported statistical profiles of dominant values in some Nation-states. We stress tested GPT-3 with a range of value-rich texts representing several languages and nations; including some with values orthogonal to dominant US public opinion as reported by the World Values Survey. We observed when values embedded in the input text were mutated in the generated outputs and noted when these conflicting values were more aligned with reported dominant US values. Our discussion of these results uses a moral value pluralism (MVP) lens to better understand these value mutations. Finally, we provide recommendations for how our work may contribute to other current work in the field.	5bf2343f67ae81610040d3fa07024c13c8591da7	@['JournalArticle', 'Review']{johnson-etal-2022-the,  author = {Rebecca Lynn Johnson and Giada Pistilli and Natalia Men'edez-Gonz'alez and Leslye Denisse Dias Duran and Enrico Panai and Julija Kalpokienė and D. Bertulfo},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Ghost in the Machine has an American accent: value conflict in GPT-3},  volume = {abs/2203.07785},  year = {2022} }
The Geometry of Multilingual Language Model Representations	2022	http://www.semanticscholar.org/paper/24a4657b614a4a3037bf045cc1ded0548771c148	The results suggest that multilingual language models encode features by projecting representations onto orthogonal axes in the representation space, enabling the simultaneous encoding of a wide variety of signals for downstream tasks and multilingual learning.	yes	0	We assess how multilingual language models maintain a shared multilingual representation space while still encoding language-sensitive information in each language. Using XLM-R as a case study, we show that languages occupy similar linear subspaces after mean-centering, evaluated based on causal effects on language modeling performance and direct comparisons between subspaces for 88 languages. The subspace means differ along language-sensitive axes that are relatively stable throughout middle layers, and these axes encode information such as token vocabularies. Shifting representations by language means is sufficient to induce token predictions in different languages. However, we also identify stable language-neutral axes that encode information such as token positions and part-of-speech. We visualize representations projected onto language-sensitive and language-neutral axes, identifying language family and part-of-speech clusters, along with spirals, toruses, and curves representing token position information. These results demonstrate that multilingual language models encode information along orthogonal language-sensitive and language-neutral axes, allowing the models to extract a variety of features for downstream tasks and cross-lingual transfer learning.	24a4657b614a4a3037bf045cc1ded0548771c148	@['JournalArticle', 'Conference']{chang-etal-2022-the,  author = {Tyler A. Chang and Z. Tu and B. Bergen},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {The Geometry of Multilingual Language Model Representations},  volume = {abs/2205.10964},  year = {2022} }
The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability	2020	http://www.semanticscholar.org/paper/6eb1b5b1f5e8c9ad62b6aaab325ae1d245727d6b	This work addresses some of the problems pertaining to the transparency and interpretability of high-dimensional representations of language representation, including the detection, quantification, and mitigation of socially biased associations in language representation.	maybe	1	High-dimensional representations for words, text, images, knowledge graphs and other structured data are commonly used in different paradigms of machine learning and data mining. These representations have different degrees of interpretability, with efficient distributed representations coming at the cost of the loss of feature to dimension mapping. This implies that there is obfuscation in the way concepts are captured in these embedding spaces. Its effects are seen in many representations and tasks, one particularly problematic one being in language representations where the societal biases, learned from underlying data, are captured and occluded in unknown dimensions and subspaces. As a result, invalid associations (such as different races and their association with a polar notion of good versus bad) are made and propagated by the representations, leading to unfair outcomes in different tasks where they are used. This work addresses some of these problems pertaining to the transparency and interpretability of such representations. A primary focus is the detection, quantification, and mitigation of socially biased associations in language representation.	6eb1b5b1f5e8c9ad62b6aaab325ae1d245727d6b	@['JournalArticle']{dev-2020-the,  author = {Sunipa Dev},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Geometry of Distributed Representations for Better Alignment, Attenuated Bias, and Improved Interpretability},  volume = {abs/2011.12465},  year = {2020} }
The Explanation Game: Towards Prediction Explainability through Sparse Communication	2020	https://www.semanticscholar.org/paper/262dd72ab01335b2129e1bc86e70d14702ef8fd9	This work provides a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier’s decision, and compares several explainers, including gradient methods, erasure, and attention mechanisms, in terms of their communication success.	maybe	20	Explainability is a topic of growing importance in NLP. In this work, we provide a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier’s decision. We use this framework to compare several explainers, including gradient methods, erasure, and attention mechanisms, in terms of their communication success. In addition, we reinterpret these methods in the light of classical feature selection, and use this as inspiration for new embedded explainers, through the use of selective, sparse attention. Experiments in text classification and natural language inference, using different configurations of explainers and laypeople (including both machines and humans), reveal an advantage of attention-based explainers over gradient and erasure methods, and show that selective attention is a simpler alternative to stochastic rationalizers. Human experiments show strong results on text classification with post-hoc explainers trained to optimize communication success.	262dd72ab01335b2129e1bc86e70d14702ef8fd9	@['JournalArticle']{treviso-martins-2020-the,  author = {Marcos Vinícius Treviso and André F. T. Martins},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {107-118},  title = {The Explanation Game: Towards Prediction Explainability through Sparse Communication},  year = {2020} }
The EOS Decision and Length Extrapolation	2020	https://www.semanticscholar.org/paper/227fe850a72fab24998c7e08d75db214715dc74e	It is found that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +E OS in the difficult SCAN dataset length generalization task.	maybe	25	Extrapolation to unseen sequence lengths is a challenge for neural generative models of language. In this work, we characterize the effect on length extrapolation of a modeling decision often overlooked: predicting the end of the generative process through the use of a special end-of-sequence (EOS) vocabulary item. We study an oracle setting - forcing models to generate to the correct sequence length at test time - to compare the length-extrapolative behavior of networks trained to predict EOS (+EOS) with networks not trained to (-EOS). We find that -EOS substantially outperforms +EOS, for example extrapolating well to lengths 10 times longer than those seen at training time in a bracket closing task, as well as achieving a 40% improvement over +EOS in the difficult SCAN dataset length generalization task. By comparing the hidden states and dynamics of -EOS and +EOS models, we observe that +EOS models fail to generalize because they (1) unnecessarily stratify their hidden states by their linear position is a sequence (structures we call length manifolds) or (2) get stuck in clusters (which we refer to as length attractors) once the EOS token is the highest-probability prediction.	227fe850a72fab24998c7e08d75db214715dc74e	@['JournalArticle']{newman-etal-2020-the,  author = {Benjamin Newman and John Hewitt and Percy Liang and Christopher D. Manning},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {276-291},  title = {The EOS Decision and Length Extrapolation},  year = {2020} }
The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?	2020	https://www.semanticscholar.org/paper/508884a136a461869be128027950d2aa1778518c	It is argued that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input.	yes	96	There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.	508884a136a461869be128027950d2aa1778518c	@['JournalArticle']{bastings-filippova-2020-the,  author = {Jasmijn Bastings and Katja Filippova},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {149-155},  title = {The elephant in the interpretability room: Why use attention as explanation when we have saliency methods?},  year = {2020} }
The Effect of Model Size on Worst-Group Generalization	2021	http://www.semanticscholar.org/paper/fa0b041cf4b1912c5b8f2f2645b6877fb532942d	This systematic evaluation reveals that increasing model size does not hurt, and may help, worst-group test performance under ERM across all setups, and advises practitioners to use larger pre-trained models when subgroup labels are unknown.	maybe	1	Overparameterization is shown to result in poor test accuracy on rare subgroups under a variety of settings where subgroup information is known. To gain a more complete picture, we consider the case where subgroup information is unknown. We investigate the effect of model size on worst-group generalization under empirical risk minimization (ERM) across a wide range of settings, varying: 1) architectures (ResNet, VGG, or BERT), 2) domains (vision or natural language processing), 3) model size (width or depth), and 4) initialization (with pre-trained or random weights). Our systematic evaluation reveals that increasing model size does not hurt, and may help, worst-group test performance under ERM across all setups. In particular, increasing pre-trained model size consistently improves performance on Waterbirds and MultiNLI. We advise practitioners to use larger pre-trained models when subgroup labels are unknown.	fa0b041cf4b1912c5b8f2f2645b6877fb532942d	@['JournalArticle']{pham-etal-2021-the,  author = {Alan Pham and Eunice Chan and V. Srivatsa and Dhruba Ghosh and Yaoqing Yang and Yaodong Yu and Ruiqi Zhong and Joseph Gonzalez and J. Steinhardt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Effect of Model Size on Worst-Group Generalization},  volume = {abs/2112.04094},  year = {2021} }
The Earth Is Flat and the Sun Is Not a Star: The Susceptibility of GPT-2 to Universal Adversarial Triggers	2021	http://www.semanticscholar.org/paper/00bd0ca06b5897ebb78e61a88c904aca4bb8fe52	It is shown that, while the more fringe topics are more challenging to identify triggers for, they do appear to more effectively discriminate aspects like stance, something that future work could use to question the nature of filter bubbles and if they are reflected within models trained on internet content.	yes	3	This work considers universal adversarial triggers, a method of adversarially disrupting natural language models, and questions if it is possible to use such triggers to affect both the topic and stance of conditional text generation models. In considering four "controversial" topics, this work demonstrates success at identifying triggers that cause the GPT-2 model to produce text about targeted topics as well as influence the stance the text takes towards the topic. We show that, while the more fringe topics are more challenging to identify triggers for, they do appear to more effectively discriminate aspects like stance. We view this both as an indication of the dangerous potential for controllability and, perhaps, a reflection of the nature of the disconnect between conflicting views on these topics, something that future work could use to question the nature of filter bubbles and if they are reflected within models trained on internet content. In demonstrating the feasibility and ease of such an attack, this work seeks to raise the awareness that neural language models are susceptible to this influence--even if the model is already deployed and adversaries lack internal model access--and advocates the immediate safeguarding against this type of adversarial attack in order to prevent potential harm to human users.	00bd0ca06b5897ebb78e61a88c904aca4bb8fe52	@['JournalArticle', 'Book']{heidenreich-williams-2021-the,  author = {Hunter Scott Heidenreich and J. Williams},  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society},  journal = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},  title = {The Earth Is Flat and the Sun Is Not a Star: The Susceptibility of GPT-2 to Universal Adversarial Triggers},  year = {2021} }
The Dependence on Frequency of Word Embedding Similarity Measures	2022	http://www.semanticscholar.org/paper/565856a79bd00c9beec2845a625a9fc12966f36a	It is shown that Skip-gram, GloVe and FastText embeddings tend to produce higher semantic similarity between high-frequency words than between other frequency combinations, and that biases can even change sign or reverse their order by manipulating word frequencies.	maybe	0	Recent research has shown that static word embeddings can encode word frequency information. However, little has been studied about this phenomenon and its effects on downstream tasks. In the present work, we systematically study the association between frequency and semantic similarity in several static word embeddings. We ﬁnd that Skip-gram, GloVe and FastText embeddings tend to produce higher semantic similarity between high-frequency words than between other frequency combinations. We show that the association between frequency and similarity also ap-pears when words are randomly shufﬂed. This proves that the patterns found are not due to real semantic associations present in the texts, but are an artifact produced by the word embeddings. Finally, we provide an example of how word frequency can strongly impact the measurement of gender bias with embedding-based metrics. In particular, we carry out a controlled experiment that shows that biases can even change sign or reverse their order by manipulating word frequencies.	565856a79bd00c9beec2845a625a9fc12966f36a	@['JournalArticle']{valentini-etal-2022-the,  author = {Francisco Valentini and D. Slezak and E. Altszyler},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Dependence on Frequency of Word Embedding Similarity Measures},  volume = {abs/2211.08203},  year = {2022} }
The Debate Over Understanding in AI's Large Language Models	2022	http://www.semanticscholar.org/paper/2bdb2f3f64fd94cb1f9867b17072c2fb643f800e		maybe	2	What does it mean to understand something? This question has long engaged philosophers, cognitive scientists, and educators, nearly always with reference to humans and other animals. However, with the recent rise of large-scale AI systems—especially so-called large language models (LLMs)—a heated debate has arisen in the AI community on whether machines can now be said to understand natural language, and thus understand the physical and social situations that language can describe. This debate is not just academic; the extent and manner in which machines understand our world has real stakes for how much we can trust them to drive cars, diagnose diseases, care for the elderly, educate children, and more generally act autonomously in tasks that impact humans. The current debate suggests a fascinating divergence in how to think about understanding, which centers on the way in which knowledge is both used and represented by intelligent systems.	2bdb2f3f64fd94cb1f9867b17072c2fb643f800e	@['JournalArticle']{mitchell-krakauer-2022-the,  author = {M. Mitchell and D. Krakauer},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Debate Over Understanding in AI's Large Language Models},  volume = {abs/2210.13966},  year = {2022} }
The Dark Side of the Language: Pre-trained Transformers in the DarkNet	2022	http://www.semanticscholar.org/paper/e475d7c3b10d548e59590902474ff99206a732f3	Results show that syntactic and lex- 010 ical neural networks largely outperform pre- trained Transformers, which seems to suggest that pre-trained Transformers have serious differences in adapting to radically novel texts.	yes	1	Pre-trained Transformers are challenging hu- 001 man performances in many natural language 002 processing tasks. The gigantic datasets used 003 for pre-training seem to be the key for their 004 success on existing tasks. In this paper, we 005 explore how a range of pre-trained natural lan- 006 guage understanding models perform on truly 007 novel and unexplored data, provided by clas- 008 siﬁcation tasks over a DarkNet corpus. Sur- 009 prisingly, results show that syntactic and lex- 010 ical neural networks largely outperform pre- 011 trained Transformers. This seems to suggest 012 that pre-trained Transformers have serious dif- 013 ﬁculties in adapting to radically novel texts. 014	e475d7c3b10d548e59590902474ff99206a732f3	@['JournalArticle']{ranaldi-etal-2022-the,  author = {Leonardo Ranaldi and Aria Nourbakhsh and Arianna Patrizi and Elena Sofia Ruzzetti and Dario Onorati and F. Fallucchi and F. M. Zanzotto},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Dark Side of the Language: Pre-trained Transformers in the DarkNet},  volume = {abs/2201.05613},  year = {2022} }
The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail	2021	http://www.semanticscholar.org/paper/c5fbf9a62a91e3182f65e3746d3263387effa4a7		maybe	16	Researchers in NLP often frame and discuss research results in ways that serve to deemphasize the field’s successes, often in response to the field’s widespread hype. Though well-meaning, this has yielded many misleading or false claims about the limits of our best technology. This is a problem, and it may be more serious than it looks: It harms our credibility in ways that can make it harder to mitigate present-day harms, like those involving biased systems for content moderation or resume screening. It also limits our ability to prepare for the potentially enormous impacts of more distant future advances. This paper urges researchers to be careful about these claims and suggests some research directions and communication strategies that will make it easier to avoid or rebut them.	c5fbf9a62a91e3182f65e3746d3263387effa4a7	@['JournalArticle', 'Conference']{bowman-2021-the,  author = {Sam Bowman},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7484-7499},  title = {The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail},  year = {2021} }
The curious case of developmental BERTology: On sparsity, transfer learning, generalization and the brain	2020	http://www.semanticscholar.org/paper/3a44e4bd53ffd045b425b569fab43d7c72f2f70f	A point of intersection between deep learning and neuroscience is explored, through the lens of large language models, transfer learning and network compression, to explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.	maybe	1	In this essay, we explore a point of intersection between deep learning and neuroscience, through the lens of large language models, transfer learning and network compression. Just like perceptual and cognitive neurophysiology has inspired effective deep neural network architectures which in turn make a useful model for understanding the brain, here we explore how biological neural development might inspire efficient and robust optimization procedures which in turn serve as a useful model for the maturation and aging of the brain.	3a44e4bd53ffd045b425b569fab43d7c72f2f70f	@['JournalArticle']{wang-2020-the,  author = {Xin Wang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The curious case of developmental BERTology: On sparsity, transfer learning, generalization and the brain},  volume = {abs/2007.03774},  year = {2020} }
The Curious Case of Control	2022	http://www.semanticscholar.org/paper/a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7		maybe	0	Children acquiring English make systematic errors on subject control sentences even after they have reached near-adult competence (Chomsky, 1969), possibly due to heuristics based on semantic roles (Maratsos, 1974).Given the advanced fluency of large generative language models, we ask whether model outputs are consistent with these heuristics, and to what degree different models are consistent with each other. We find that models can be categorized by behavior into three separate groups, with broad differences between the groups. The outputs of models in the largest group are consistent with positional heuristics that succeed on subject control but fail on object control. This result is surprising, given that object control is orders of magnitude more frequent in the text data used to train such models. We examine to what degree the models are sensitive to prompting with agent-patient information, finding that raising the salience of agent and patient relations results in significant changes in the outputs of most models. Based on this observation, we leverage an existing dataset of semantic proto-role annotations (White et al. 2020) to explore the connections between control and labeling event participants with properties typically associated with agents and patients.	a3780e8f45b6c1ab3e27c0ad87c3ea23176dc8a7	@['JournalArticle', 'Conference']{eskin-durme-2022-the,  author = {Elias Stengel-Eskin and Benjamin Van Durme},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {The Curious Case of Control},  volume = {abs/2205.12113},  year = {2022} }
The Curious Case of Absolute Position Embeddings	2022	http://www.semanticscholar.org/paper/97833e2aa0da5240e62436373b58af988a4ab6ab	This work observes that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information, and raises questions about theacy of APEs to model the relativity of position information.	maybe	2	Transformer language models encode the no-tion of word order using positional information. Most commonly, this positional information is represented by absolute position embeddings (APEs), that are learned from the pretraining data. However, in natural language, it is not absolute position that matters, but relative position , and the extent to which APEs can capture this type of information has not been investigated. In this work, we observe that models trained with APE over-rely on positional information to the point that they break-down when subjected to sentences with shifted position information. Speciﬁcally, when models are subjected to sentences starting from a non-zero position (excluding the effect of priming), they exhibit noticeably degraded performance on zero- to full-shot tasks, across a range of model families and model sizes. Our ﬁndings raise questions about the efﬁcacy of APEs to model the relativity of position information, and invite further introspection on the sentence and word order processing strategies employed by these models.	97833e2aa0da5240e62436373b58af988a4ab6ab	@['JournalArticle']{sinha-etal-2022-the,  author = {Koustuv Sinha and Amirhossein Kazemnejad and Siva Reddy and J. Pineau and D. Hupkes and Adina Williams},  booktitle = {ArXiv},  journal = {ArXiv},  title = {The Curious Case of Absolute Position Embeddings},  volume = {abs/2210.12574},  year = {2022} }
The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models	2021	http://www.semanticscholar.org/paper/d8e7bad2681ce70277c900c77a22181d4b03d705	Analysis of position embeddings of existing language models finds strong evidence of translation invariance, which leads to translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embedDings.	maybe	8	Mechanisms for encoding positional information are central for transformer-based language models. In this paper, we analyze the position embeddings of existing language models, finding strong evidence of translation invariance, both for the embeddings themselves and for their effect on self-attention. The degree of translation invariance increases during training and correlates positively with model performance. Our findings lead us to propose translation-invariant self-attention (TISA), which accounts for the relative position between tokens in an interpretable fashion without needing conventional position embeddings. Our proposal has several theoretical advantages over existing position-representation approaches. Proof-of-concept experiments show that it improves on regular ALBERT on GLUE tasks, while only adding orders of magnitude less positional parameters.	d8e7bad2681ce70277c900c77a22181d4b03d705	@['JournalArticle', 'Conference']{wennberg-henter-2021-the,  author = {Ulme Wennberg and G. Henter},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models},  volume = {abs/2106.01950},  year = {2021} }
The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives	2019	https://www.semanticscholar.org/paper/112fd54ee193237b24f2ce7fce79e399609a29c5	This work uses canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process.	seed	105	We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We chose the Transformers for our analysis as they have been shown effective with various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and observe that the choice of the objective determines this process. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.	112fd54ee193237b24f2ce7fce79e399609a29c5	@['JournalArticle', 'Conference']{voita-etal-2019-the,  author = {Elena Voita and Rico Sennrich and Ivan Titov},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4395-4405},  title = {The Bottom-up Evolution of Representations in the Transformer: A Study with Machine Translation and Language Modeling Objectives},  year = {2019} }
The Birth of Bias: A case study on the evolution of gender bias in an English language model	2022	http://www.semanticscholar.org/paper/d6ddc4f4c81c9565019be1983d37f9fbdf5bd057	It is found that the representation of gender is dynamic and identify different phases during training, and it is shown that gender information is represented increasingly locally in the input embeddings of the model and that debiasing these can be effective in reducing the downstream bias.	maybe	2	Detecting and mitigating harmful biases in modern language models are widely recognized as crucial, open problems. In this paper, we take a step back and investigate how language models come to be biased in the first place.We use a relatively small language model, using the LSTM architecture trained on an English Wikipedia corpus. With full access to the data and to the model parameters as they change during every step while training, we can map in detail how the representation of gender develops, what patterns in the dataset drive this, and how the model’s internal state relates to the bias in a downstream task (semantic textual similarity).We find that the representation of gender is dynamic and identify different phases during training.Furthermore, we show that gender information is represented increasingly locally in the input embeddings of the model and that, as a consequence, debiasing these can be effective in reducing the downstream bias.Monitoring the training dynamics, allows us to detect an asymmetry in how the female and male gender are represented in the input embeddings. This is important, as it may cause naive mitigation strategies to introduce new undesirable biases.We discuss the relevance of the findings for mitigation strategies more generally and the prospects of generalizing our methods to larger language models, the Transformer architecture, other languages and other undesirable biases.	d6ddc4f4c81c9565019be1983d37f9fbdf5bd057	@['JournalArticle']{wal-etal-2022-the,  author = {Oskar van der Wal and Jaap Jumelet and K. Schulz and Willem H. Zuidema},  booktitle = {GEBNLP},  journal = {ArXiv},  title = {The Birth of Bias: A case study on the evolution of gender bias in an English language model},  volume = {abs/2207.10245},  year = {2022} }
The Better Your Syntax, the Better Your Semantics? Probing Pretrained Language Models for the English Comparative Correlative	2022	http://www.semanticscholar.org/paper/21fc58eea259e101318a9a4c8ddb03c001b3ada7	An investigation of PLMs' capability to classify and understand one of the most commonly studied constructions, the English comparative correlative (CC), shows that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning.	maybe	0	Construction Grammar (CxG) is a paradigm from cognitive linguistics emphasising the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that combine syntax and semantics. As a first step towards assessing the compatibility of CxG with the syntactic and semantic knowledge demonstrated by state-of-the-art pretrained language models (PLMs), we present an investigation of their capability to classify and understand one of the most commonly studied constructions, the English comparative correlative (CC). We conduct experiments examining the classification accuracy of a syntactic probe on the one hand and the models’ behaviour in a semantic application task on the other, with BERT, RoBERTa, and DeBERTa as the example PLMs. Our results show that all three investigated PLMs are able to recognise the structure of the CC but fail to use its meaning. While human-like performance of PLMs on many NLP tasks has been alleged, this indicates that PLMs still suffer from substantial shortcomings in central domains of linguistic knowledge.	21fc58eea259e101318a9a4c8ddb03c001b3ada7	@['JournalArticle', 'Conference']{weissweiler-etal-2022-the,  author = {Leonie Weissweiler and Valentin Hofmann and Abdullatif Köksal and Hinrich Schütze},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {The better your Syntax, the better your Semantics? Probing Pretrained Language Models for the English Comparative Correlative},  volume = {abs/2210.13181},  year = {2022} }
The AI trilemma: Saving the planet without ruining our jobs	2022	http://www.semanticscholar.org/paper/a8ae5e9ef6f36abeb7700fdb978d412b295dc95d		maybe	1	Digitalization and artificial intelligence increasingly affect the world of work. Rising risk of massive job losses have sparked technological fears. Limited income and productivity gains concentrated among a few tech companies are fueling inequalities. In addition, the increasing ecological footprint of digital technologies has become the focus of much discussion. This creates a trilemma of rising inequality, low productivity growth and high ecological costs brought by technological progress. How can this trilemma be resolved? Which digital applications should be promoted specifically? And what should policymakers do to address this trilemma? This contribution shows that policymakers should create suitable conditions to fully exploit the potential in the area of network applications (transport, information exchange, supply, provisioning) in order to reap maximum societal benefits that can be widely shared. This requires shifting incentives away from current uses toward those that can, at least partially, address the trilemma. The contribution analyses the scope and limits of current policy instruments in this regard and discusses alternative approaches that are more aligned with the properties of the emerging technological paradigm underlying the digital economy. In particular, it discusses the possibility of institutional innovations required to address the socio-economic challenges resulting from the technological innovations brought about by artificial intelligence.	a8ae5e9ef6f36abeb7700fdb978d412b295dc95d	@['JournalArticle']{ernst-2022-the,  author = {Ekkehard C. Ernst},  booktitle = {Frontiers in Artificial Intelligence},  journal = {Frontiers in Artificial Intelligence},  title = {The AI trilemma: Saving the planet without ruining our jobs},  volume = {5},  year = {2022} }
Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers	2022	http://www.semanticscholar.org/paper/b8acc068f277fd491f12aa8984aa7a36dca01cb8	This paper for-mulate Text Revealer — the first model inversion attack for text reconstruction against text classiﬁcation with transformers, and faithfully reconstruct private texts included in training data with access to the target model.	maybe	0	Text classiﬁcation has become widely used in various natural language processing applica-tions like sentiment analysis. Current appli-cations often use large transformer-based language models to classify input texts. How-ever, there is a lack of systematic study on how much private information can be inverted when publishing models. In this paper, we for-mulate Text Revealer — the ﬁrst model inversion attack for text reconstruction against text classiﬁcation with transformers. Our attacks faithfully reconstruct private texts included in training data with access to the target model. We leverage an external dataset and GPT-2 to generate the target domain-like ﬂuent text, and then perturb its hidden state optimally with the feedback from the target model. Our extensive experiments demonstrate that our attacks are effective for datasets with different text lengths and can reconstruct private texts with accuracy.	b8acc068f277fd491f12aa8984aa7a36dca01cb8	@['JournalArticle']{zhang-etal-2022-text,  author = {Ruisi Zhang and Seira Hidano and F. Koushanfar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Text Revealer: Private Text Reconstruction via Model Inversion Attacks against Transformers},  volume = {abs/2209.10505},  year = {2022} }
Testing the Ability of Language Models to Interpret Figurative Language	2022	http://www.semanticscholar.org/paper/15dd7e862b09de21fd67d5abb2b681293755ffca		maybe	13	Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.	15dd7e862b09de21fd67d5abb2b681293755ffca	@['JournalArticle', 'Conference']{liu-etal-2022-testing,  author = {Emmy Liu and Chenxuan Cui and Kenneth Zheng and Graham Neubig},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4437-4452},  title = {Testing the Ability of Language Models to Interpret Figurative Language},  year = {2022} }
Testing Pre-trained Language Models' Understanding of Distributivity via Causal Mediation Analysis	2022	https://www.semanticscholar.org/paper/78d88428074f83abf5bec2a015e886665f3deeb8	DistNLI is introduced, a new diagnostic dataset for natural language inference that targets the semantic difference arising from distributivity, and the causal mediation analysis framework is employed to quantify the model behaviour and explore the underlying mechanism in this semantically-related task.	maybe	1	To what extent do pre-trained language models grasp semantic knowledge regarding the phenomenon of distributivity? In this paper, we introduce DistNLI, a new diagnostic dataset for natural language inference that targets the semantic difference arising from distributivity, and employ the causal mediation analysis framework to quantify the model behavior and explore the underlying mechanism in this semantically-related task. We find that the extent of models’ understanding is associated with model size and vocabulary size. We also provide insights into how models encode such high-level semantic knowledge.	78d88428074f83abf5bec2a015e886665f3deeb8	@['JournalArticle']{ban-etal-2022-testing,  author = {Pangbo Ban and Yifan Jiang and Tianran Liu and Shane Steinert-Threlkeld},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Testing Pre-trained Language Models’ Understanding of Distributivity via Causal Mediation Analysis},  volume = {abs/2209.04761},  year = {2022} }
Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment	2022	http://www.semanticscholar.org/paper/1606793daaa20d4a4a78e859c2fd6b4f7535680c		maybe	0	Previous work has demonstrated that pre-trained large language models (LLM) acquire knowledge during pre-training which enables reasoning over relationships between words (e.g, hyponymy) and more complex inferences over larger units of meaning such as sentences. Here, we investigate whether lexical entailment (LE, i.e. hyponymy or the is a relation between words) can be generalised in a compositional manner. Accordingly, we introduce PLANE (Phrase-Level Adjective-Noun Entailment), a new benchmark to test models on fine-grained compositional entailment using adjective-noun phrases. Our experiments show that knowledge extracted via In–Context and transfer learning is not enough to solve PLANE. However, a LLM trained on PLANE can generalise well to out–of–distribution sets, since the required knowledge can be stored in the representations of subwords (SW) tokens.	1606793daaa20d4a4a78e859c2fd6b4f7535680c	@['JournalArticle', 'Conference']{bertolini-etal-2022-testing,  author = {Lorenzo Bertolini and Julie Weeds and David Weir},  booktitle = {International Conference on Computational Linguistics},  pages = {4084-4100},  title = {Testing Large Language Models on Compositionality and Inference with Phrase-Level Adjective-Noun Entailment},  year = {2022} }
Testing for Grammatical Category Abstraction in Neural Language Models	2021	http://www.semanticscholar.org/paper/21210677494669cc83a7878fbd3a6360ce02f7f3	A new method inspired by human developmental studies is proposed to probe pretrained neural language models, and results on BERT-large are presented, which lets us bypass the methodological questions raised in the recent literature on the validity of using diagnostic classifiers as probes.	maybe	7	The notion of grammatical categories is fundamental to human language. Humans abstract over individual lexical items to form grammatical categories, such as nouns and verbs in English, and this category membership (rather than lexical identity) governs the applicability of linguistic rules (e.g., nouns can be heads of subjects of a verb). Category membership of new words are rapidly inferred from their linguistic environment: if a speaker of English hears I saw a blick, it is immediately clear that blick is a noun. This knowledge about the novel word’s grammatical category enables speakers to furthermore produce sentences such as We like the blick and The blick jumped, even though these new contexts have no lexical overlap with the context that blick was first observed in. Hence, the identification of a grammatical category allows application of rules that operate over that category, allowing for generalization outside of the context that the novel word has been observed in (Gómez and Gerken, 2000). Can we find evidence of abstract grammatical categories and category-based generalization resembling humans in pretrained neural language models? From the perspective of Cognitive Science, category abstraction in pretrained neural models can provide an argument against the need for an innate bias towards categorization (and prespecification of the set of lexical categories) for learners of language. From the perspective of Natural Language Processing, it is known that contemporary neural models perform well (near 98% accuracy) on benchmarks for part-of-speech (POS) tagging (Bohnet et al., 2018; He and Choi, 2019), and that diagnostic classifiers for probing pretrained models also achieve similarly high performance on POS (Tenney et al., 2019). However, it still remains an open question whether pretrained models can perform category-based generalization using novel words learned from limited contexts, and without being explicitly trained to perform categorization. This is also in line with the problem of out-of-distribution generalization in neural models of language and efforts to develop benchmarks for linguistic generalization that humans are capable of (Kim and Linzen, 2020; Linzen, 2020, i.a.). To this end, we propose a new method inspired by human developmental studies to probe pretrained neural language models, and present experimental results on BERT-large (Devlin et al., 2019). Our method does not require training a separate classifier on top, which lets us bypass the methodological questions raised in the recent literature on the validity of using diagnostic classifiers as probes (Hewitt and Liang, 2019; Voita and Titov, 2020, i.a.).	21210677494669cc83a7878fbd3a6360ce02f7f3	@None{kim-smolensky-2021-testing,  author = {Najoung Kim and P. Smolensky},  booktitle = {SCIL},  pages = {467-470},  title = {Testing for Grammatical Category Abstraction in Neural Language Models},  volume = {4},  year = {2021} }
TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models	2022	http://www.semanticscholar.org/paper/28394219e999fe67ee824fdfbb06c18d4342e3a8	This work introduces T EMPORAL W IKI, a lifelong bench- 011 mark for ever-evolving LMs that utilizes the difference between the consecutive snapshots of Wikipedia and Wikidata for training and evalua- 014 tion, respectively.	maybe	12	Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM’s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.	28394219e999fe67ee824fdfbb06c18d4342e3a8	@['JournalArticle', 'Conference']{jang-etal-2022-temporalwiki:,  author = {Joel Jang and Seonghyeon Ye and C. Lee and Sohee Yang and Joongbo Shin and Janghoon Han and Gyeonghun Kim and Minjoon Seo},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models},  volume = {abs/2204.14211},  year = {2022} }
Telling BERT’s Full Story: from Local Attention to Global Aggregation	2020	https://www.semanticscholar.org/paper/3d9605a8b6364d24b58be561ea1ce4233c088cee	This work uses gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers.	maybe	11	We take a deep look into the behaviour of self-attention heads in the transformer architecture. In light of recent work discouraging the use of attention distributions for explaining a model’s behaviour, we show that attention distributions can nevertheless provide insights into the local behaviour of attention heads. This way, we propose a distinction between local patterns revealed by attention and global patterns that refer back to the input, and analyze BERT from both angles. We use gradient attribution to analyze how the output of an attention head depends on the input tokens, effectively extending the local attention-based analysis to account for the mixing of information throughout the transformer layers. We find that there is a significant mismatch between attention and attribution distributions, caused by the mixing of context inside the model. We quantify this discrepancy and observe that interestingly, there are some patterns that persist across all layers despite the mixing.	3d9605a8b6364d24b58be561ea1ce4233c088cee	@['JournalArticle', 'Conference']{pascual-etal-2020-telling,  author = {Damian Pascual and Gino Brunner and Roger Wattenhofer},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Telling BERT’s Full Story: from Local Attention to Global Aggregation},  volume = {abs/2004.05916},  year = {2020} }
Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge	2020	http://www.semanticscholar.org/paper/79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88	This work provides a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements, and demonstrates that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting.	yes	75	To what extent can a neural network systematically reason over symbolic facts? Evidence suggests that large pre-trained language models (LMs) acquire some reasoning capacity, but this ability is difficult to control. Recently, it has been shown that Transformer-based models succeed in consistent reasoning over explicit symbolic facts, under a "closed-world" assumption. However, in an open-domain setup, it is desirable to tap into the vast reservoir of implicit knowledge already encoded in the parameters of pre-trained LMs. In this work, we provide a first demonstration that LMs can be trained to reliably perform systematic reasoning combining both implicit, pre-trained knowledge and explicit natural language statements. To do this, we describe a procedure for automatically generating datasets that teach a model new reasoning skills, and demonstrate that models learn to effectively perform inference which involves implicit taxonomic and world knowledge, chaining and counting. Finally, we show that "teaching" models to reason generalizes beyond the training distribution: they successfully compose the usage of multiple reasoning skills in single examples. Our work paves a path towards open-domain systems that constantly improve by interacting with users who can instantly correct a model by adding simple natural language statements.	79e5447a394ad178aa0fdaef1d2f1ecf9bf07e88	@['JournalArticle']{talmor-etal-2020-teaching,  author = {Alon Talmor and Oyvind Tafjord and Peter Clark and Yoav Goldberg and Jonathan Berant},  booktitle = {Neural Information Processing Systems},  journal = {ArXiv},  title = {Teaching Pre-Trained Models to Systematically Reason Over Implicit Knowledge},  volume = {abs/2006.06609},  year = {2020} }
Taxonomy of Risks posed by Language Models	2022	http://www.semanticscholar.org/paper/f2c17758e74707d379b87372528221656d14b697	A comprehensive taxonomy of ethical and social risks associated with LMs is developed, drawing on expertise and literature from computer science, linguistics, and the social sciences to ensure that language models are developed responsibly.	maybe	19	Responsible innovation on large-scale Language Models (LMs) requires foresight into and in-depth understanding of the risks these models may pose. This paper develops a comprehensive taxonomy of ethical and social risks associated with LMs. We identify twenty-one risks, drawing on expertise and literature from computer science, linguistics, and the social sciences. We situate these risks in our taxonomy of six risk areas: I. Discrimination, Hate speech and Exclusion, II. Information Hazards, III. Misinformation Harms, IV. Malicious Uses, V. Human-Computer Interaction Harms, and VI. Environmental and Socioeconomic harms. For risks that have already been observed in LMs, the causal mechanism leading to harm, evidence of the risk, and approaches to risk mitigation are discussed. We further describe and analyse risks that have not yet been observed but are anticipated based on assessments of other language technologies, and situate these in the same taxonomy. We underscore that it is the responsibility of organizations to engage with the mitigations we discuss throughout the paper. We close by highlighting challenges and directions for further research on risk evaluation and mitigation with the goal of ensuring that language models are developed responsibly.	f2c17758e74707d379b87372528221656d14b697	@['JournalArticle', 'Book']{weidinger-etal-2022-taxonomy,  author = {Laura Weidinger and J. Uesato and M. Rauh and C. Griffin and Po-Sen Huang and John F. J. Mellor and A. Glaese and M. Cheng and B. Balle and A. Kasirzadeh and C. Biles and S. Brown and Z. Kenton and W. Hawkins and T. Stepleton and A. Birhane and Lisa Anne Hendricks and Laura Rimell and William S. Isaac and Julia Haas and Sean Legassick and Geoffrey Irving and Iason Gabriel},  booktitle = {Conference on Fairness, Accountability and Transparency},  journal = {2022 ACM Conference on Fairness, Accountability, and Transparency},  title = {Taxonomy of Risks posed by Language Models},  year = {2022} }
TaxiNLI: Taking a Ride up the NLU Hill	2020	http://www.semanticscholar.org/paper/8a195d8ea613ffed341990fb757604ebe67f99ec	A taxonomic hierarchy of categories that are relevant for the Natural Language Inference task is proposed and introduced, and it is observed that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies—a large jump over the previous models—some categories still remain difficult.	maybe	18	Pre-trained Transformer-based neural architectures have consistently achieved state-of-the-art performance in the Natural Language Inference (NLI) task. Since NLI examples encompass a variety of linguistic, logical, and reasoning phenomena, it remains unclear as to which specific concepts are learnt by the trained systems and where they can achieve strong generalization. To investigate this question, we propose a taxonomic hierarchy of categories that are relevant for the NLI task. We introduce TaxiNLI, a new dataset, that has 10k examples from the MNLI dataset with these taxonomic labels. Through various experiments on TaxiNLI, we observe that whereas for certain taxonomic categories SOTA neural models have achieved near perfect accuracies—a large jump over the previous models—some categories still remain difficult. Our work adds to the growing body of literature that shows the gaps in the current NLI systems and datasets through a systematic presentation and analysis of reasoning categories.	8a195d8ea613ffed341990fb757604ebe67f99ec	@['JournalArticle']{joshi-etal-2020-taxinli:,  author = {Pratik M. Joshi and Somak Aditya and Aalok Sathe and M. Choudhury},  booktitle = {Conference on Computational Natural Language Learning},  pages = {41-55},  title = {TaxiNLI: Taking a Ride up the NLU Hill},  year = {2020} }
Task Compass: Scaling Multi-task Pre-training with Task Prefix	2022	http://www.semanticscholar.org/paper/0979695b5d74016e97ab8f306f632114e98bd6d9		maybe			0979695b5d74016e97ab8f306f632114e98bd6d9	
Task Ambiguity in Humans and Language Models	2022	http://www.semanticscholar.org/paper/e9d6cad994fd48567198ef260fbc5f5241aa9746		maybe			e9d6cad994fd48567198ef260fbc5f5241aa9746	
Tapping BERT for Preposition Sense Disambiguation	2021	http://www.semanticscholar.org/paper/17c414f8abb8567043e7fe95f48c8e6a85b43e78	A novel methodology for preposition sense disambiguation (PSD), which does not use any linguistic tools, and gives an accuracy of 86.85% on the SemEval task, which is better than the state-of-the-art.	maybe	0	Prepositions are frequently occurring polyse- 001 mous words. Disambiguation of prepositions 002 is crucial in tasks like semantic role labelling, 003 question answering, text entailment, and noun 004 compound paraphrasing. In this paper, we 005 propose a novel methodology for preposition 006 sense disambiguation (PSD), which does not 007 use any linguistic tools. In a supervised set- 008 ting, the machine learning model is presented 009 with sentences wherein prepositions have been 010 annotated with ‘senses’. These ‘senses’ are 011 IDs in what is called ‘The Preposition Project 012 (TPP)’. We use the hidden layer representa- 013 tions from pre-trained BERT and its variants. 014 The latent representations are then classiﬁed 015 into the correct sense ID using a Multi-Layer 016 Perceptron. The datasets used for this task are 017 from SemEval-2007 Task-6 and Oxford En- 018 glish Corpus (OEC). Our methodology gives 019 an accuracy of 86.85% on the SemEval task, 020 which is better than the state-of-the-art. 021	17c414f8abb8567043e7fe95f48c8e6a85b43e78	@['JournalArticle']{pawar-etal-2021-tapping,  author = {S. Pawar and S. Thombre and Anirudh Mittal and Girishkumar Ponkiya and P. Bhattacharyya},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Tapping BERT for Preposition Sense Disambiguation},  volume = {abs/2111.13972},  year = {2021} }
Talking About Large Language Models	2022	https://www.semanticscholar.org/paper/9d030d1a91a15ce60f51c1434ded80b66fc487c2	This paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work.	maybe	3	Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as “knows”, “believes”, and “thinks”, when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.	9d030d1a91a15ce60f51c1434ded80b66fc487c2	@['JournalArticle']{shanahan-2022-talking,  author = {M. Shanahan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Talking About Large Language Models},  volume = {abs/2212.03551},  year = {2022} }
Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective	2022	http://www.semanticscholar.org/paper/82be2b02c8fbedb94ef9170174e026c5d315b8ac	This work proposes three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity, and test the internal consistency of state-of-the-art NLP models, and shows that they do not always behave according to their expected linguistic properties.	maybe	1	Metamorphic testing has recently been used to check the safety of neural NLP models. Its main advantage is that it does not rely on a ground truth to generate test cases. However, existing studies are mostly concerned with robustness-like metamorphic relations, limiting the scope of linguistic properties they can test. We propose three new classes of metamorphic relations, which address the properties of systematicity, compositionality and transitivity. Unlike robustness, our relations are defined over multiple source inputs, thus increasing the number of test cases that we can produce by a polynomial factor. With them, we test the internal consistency of state-of-the-art NLP models, and show that they do not always behave according to their expected linguistic properties. Lastly, we introduce a novel graphical notation that efficiently summarises the inner structure of metamorphic relations.	82be2b02c8fbedb94ef9170174e026c5d315b8ac	@['JournalArticle']{manino-etal-2022-systematicity,  author = {Edoardo Manino and Julia Rozanova and Danilo S. Carvalho and André Freitas and Lucas Cordeiro},  booktitle = {Findings},  journal = {ArXiv},  title = {Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective},  volume = {abs/2204.12316},  year = {2022} }
Synth-tax: Modification of BERT Syntax Representations via Structural Probes	2020	http://www.semanticscholar.org/paper/fb4f00dea316c6c866e6ce09ac20e7af832c4aa3		yes	0	Large, pretrained, ‘black-box’ language models like BERT have recently risen to prominence, and we seek to understand how they represent linguistic information by causally relating models’ predictions to their internal representations. In particular, we provide a careful framework of assumptions and causality, and we manipulate BERT’s purported sentence-syntactic representations provided by the structural probe [1] to examine whether the resulting predictions of subject-verb number agreement changes. While our evidence is inconclusive, we find that BERT likely does not encode syntax in exactly the manner structural probe’s success suggests, either capturing syntax poorly or also conflating it with other linguistic information like number. In addition, we find evidence that BERT performs worse on subjectverb number agreement tasks when words are broken into separate wordpieces.	fb4f00dea316c6c866e6ce09ac20e7af832c4aa3	@None{gong-newman-2020-synth,  author = {Julia Gong and Benjamin Newman},  title = {Synth-tax: Modification of BERT Syntax Representations via Structural Probes},  year = {2020} }
SyntaxGym: An Online Platform for Targeted Evaluation of Language Models	2020	http://www.semanticscholar.org/paper/b8cd2b80fc24f53443157352c1a7acf6fbd30a2d	SyntaxGym is presented, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design.	yes	37	Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, ‘syntaxgym‘ and ‘lm-zoo‘, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.	b8cd2b80fc24f53443157352c1a7acf6fbd30a2d	@['JournalArticle', 'Conference']{gauthier-etal-2020-syntaxgym:,  author = {Jon Gauthier and Jennifer Hu and Ethan Gotlieb Wilcox and Peng Qian and R. Levy},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {70-76},  title = {SyntaxGym: An Online Platform for Targeted Evaluation of Language Models},  year = {2020} }
Syntax-guided Localized Self-attention by Constituency Syntactic Distance	2022	http://www.semanticscholar.org/paper/50e6b4e452107799cd775b0b0f6726d44173d564	This work proposes a syntax-guided localized self-attention for Transformer that allows directly incorporating grammar structures from an external constituency parser, and prohibits the attention mechanism to overweight the grammatically distant tokens over close ones.	maybe	0	Recent works have revealed that Transformers are implicitly learning the syntactic information in its lower layers from data, al-beit is highly dependent on the quality and scale of the training data. However, learning syntactic information from data is not nec-essary if we can leverage an external syntactic parser, which provides better parsing quality with well-deﬁned syntactic structures. This could potentially improve Transformer’s performance and sample efﬁciency. In this work, we propose a syntax-guided localized self-attention for Transformer that allows directly incorporating grammar structures from an external constituency parser. It prohibits the attention mechanism to overweight the grammatically distant tokens over close ones. Experimental results show that our model could consistently improve translation performance on a variety of machine translation datasets, ranging from small to large dataset sizes, and with different source languages. 1	50e6b4e452107799cd775b0b0f6726d44173d564	@['JournalArticle']{hou-etal-2022-syntax,  author = {Shengyuan Hou and Jushi Kai and Haotian Xue and Bingyu Zhu and Bo Yuan and Longtao Huang and Xinbing Wang and Zhouhan Lin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Syntax-guided Localized Self-attention by Constituency Syntactic Distance},  volume = {abs/2210.11759},  year = {2022} }
Syntax Representation in Word Embeddings and Neural Networks - A Survey	2020	http://www.semanticscholar.org/paper/e25c85054084b33091d40aedb5dc063195dc33ee	This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures, and describes which pre-trained models and representations of language are best suited for transfer to syntactic tasks.	maybe	3	Neural networks trained on natural language processing tasks capture syntax even though it is not provided as a supervision signal. This indicates that syntactic analysis is essential to the understating of language in artificial intelligence systems. This overview paper covers approaches of evaluating the amount of syntactic information included in the representations of words for different neural network architectures. We mainly summarize re-search on English monolingual data on language modeling tasks and multilingual data for neural machine translation systems and multilingual language models. We describe which pre-trained models and representations of language are best suited for transfer to syntactic tasks.	e25c85054084b33091d40aedb5dc063195dc33ee	@['JournalArticle', 'Review']{limisiewicz-mareček-2020-syntax,  author = {Tomasz Limisiewicz and D. Mareček},  booktitle = {Conference on Theory and Practice of Information Technologies},  journal = {ArXiv},  title = {Syntax Representation in Word Embeddings and Neural Networks - A Survey},  volume = {abs/2010.01063},  year = {2020} }
Syntactic Structure from Deep Learning	2020	http://www.semanticscholar.org/paper/3efafed8d2d68176b7a3e1d54b2aabbd3f6686ae		yes	70	Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.	3efafed8d2d68176b7a3e1d54b2aabbd3f6686ae	@['JournalArticle', 'Review']{linzen-baroni-2020-syntactic,  author = {Tal Linzen and Marco Baroni},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Syntactic Structure from Deep Learning},  volume = {abs/2004.10827},  year = {2020} }
Syntactic representations in the human brain: beyond effort-based metrics	2020	http://www.semanticscholar.org/paper/f1fdca866e59b2763619f4a0f8571a7b5d7d3b05	It is found that syntactic structure-based features are better than effort-based metrics at predicting brain activity in various parts of the language system and called for a shift in the approach used for studying syntactic processing.	maybe	9	We are far from having a complete mechanistic understanding of the brain computations involved in language processing and of the role that syntax plays in those computations. Most language studies do not computationally model syntactic structure and most studies that do model syntactic processing use effort-based metrics. These metrics capture the effort needed to process the syntactic information given by every word. They can reveal where in the brain syntactic processing occurs, but not what features of syntax are processed by different brain regions. Here, we move beyond effort-based metrics and propose explicit features capturing the syntactic structure that is incrementally built while a sentence is being read. Using these features and functional Magnetic Resonance Imaging (fMRI) recordings of participants reading a natural text, we study the brain representation of syntax. We find that our syntactic structure-based features are better than effort-based metrics at predicting brain activity in various parts of the language system. We show evidence of the brain representation of complex syntactic information such as phrase and clause structures. We see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our results call for a shift in the approach used for studying syntactic processing.	f1fdca866e59b2763619f4a0f8571a7b5d7d3b05	@None{reddy-wehbe-2020-syntactic,  author = {Aniketh Janardhan Reddy and Leila Wehbe},  journal = {bioRxiv},  title = {Syntactic representations in the human brain: beyond effort-based metrics},  year = {2020} }
Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models	2021	http://www.semanticscholar.org/paper/c3fb6056ca1ec3f7cfe57103712531fdbfe69e03	Results from a series of probes designed to test the sensitivity of vector-based language representations from pretrained language models suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process.	maybe	10	While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models.	c3fb6056ca1ec3f7cfe57103712531fdbfe69e03	@['JournalArticle']{alleman-etal-2021-syntactic,  author = {Matteo Alleman and J. Mamou and M. Rio and Hanlin Tang and Yoon Kim and SueYeon Chung},  booktitle = {Workshop on Representation Learning for NLP},  pages = {263-276},  title = {Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models},  year = {2021} }
Syntactic Data Augmentation Increases Robustness to Inference Heuristics	2020	http://www.semanticscholar.org/paper/84059eba69c02bd57b6b227710ba62168ade827e	The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, suggesting that augmentation causes BERT to recruit abstract syntactic representations.	maybe	96	Pretrained neural models such as BERT, when fine-tuned to perform natural language inference (NLI), often show high accuracy on standard datasets, but display a surprising lack of sensitivity to word order on controlled challenge sets. We hypothesize that this issue is not primarily caused by the pretrained model’s limitations, but rather by the paucity of crowdsourced NLI examples that might convey the importance of syntactic structure at the fine-tuning stage. We explore several methods to augment standard training sets with syntactically informative examples, generated by applying syntactic transformations to sentences from the MNLI corpus. The best-performing augmentation method, subject/object inversion, improved BERT’s accuracy on controlled examples that diagnose sensitivity to word order from 0.28 to 0.73, without affecting performance on the MNLI test set. This improvement generalized beyond the particular construction used for data augmentation, suggesting that augmentation causes BERT to recruit abstract syntactic representations.	84059eba69c02bd57b6b227710ba62168ade827e	@['JournalArticle', 'Conference']{min-etal-2020-syntactic,  author = {Junghyun Min and R. Thomas McCoy and Dipanjan Das and Emily Pitler and Tal Linzen},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Syntactic Data Augmentation Increases Robustness to Inference Heuristics},  volume = {abs/2004.11999},  year = {2020} }
Symmetric Regularization based BERT for Pair-wise Semantic Reasoning	2019	https://www.semanticscholar.org/paper/63f9e2417563456f91c7e5586d43eb25c00a0c19	This work proposes to augment the NSP task to a multi-class categorization task, which includes previous sentence prediction (PSP), which encourages the model to learn the subtle semantics, thereby improves the ability of semantic understanding.	seed	10	The ability of semantic reasoning over the sentence pair is essential for many natural language understanding tasks, e.g., natural language inference and machine reading comprehension. A recent significant improvement in these tasks comes from BERT. As reported, the next sentence prediction (NSP) in BERT is of great significance for downstream problems with sentence-pair input. Despite its effectiveness, NSP still lacks the essential signal to distinguish between entailment and shallow correlation. To remedy this, we propose to augment the NSP task to a multi-class categorization task, which includes previous sentence prediction (PSP). This task encourages the model to learn the subtle semantics, thereby improves the ability of semantic understanding. Furthermore, by using a smoothing technique, the scopes of NSP and PSP are expanded into a broader range which includes close but nonsuccessive sentences. This simple method yields remarkable improvement against vanilla BERT. Our method consistently improves the performance on the NLI and MRC benchmarks by a large margin, including the challenging HANS dataset.	63f9e2417563456f91c7e5586d43eb25c00a0c19	@['JournalArticle', 'Book', 'Conference']{cheng-etal-2019-symmetric,  author = {Xingyi Cheng and Weidi Xu and Kunlong Chen and Wei Wang and Bin Bi and Ming Yan and Chen Wu and Luo Si and Wei Chu and Taifeng Wang},  booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},  journal = {Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval},  title = {Symmetric Regularization based BERT for Pair-wise Semantic Reasoning},  year = {2019} }
Symbolic Math Reasoning with Language Models	2022	http://www.semanticscholar.org/paper/f557f3a32d309373e7d31bb93ca1b80b4a6e39e7	GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems, and adopting a two-step approach leads to better accuracy on the numerical test set in the zero-shot regime.	maybe	0	The emergence of large language models (LLMs) such as OpenAI’s GPT-3, Google’s LaMDA, Meta’s OPT [2, 3, 7, 10] etc. have revolutionized the field of natural language processing (NLP). These models with upwards of hundreds of billions of parameters are trained on large unlabeled text corpora and can subsequently solve downstream tasks with little to no labeled data. While these models are increasingly versatile in their abilities, e.g., solving math word problems, the larger question of their ability to reason remains. Using and modifying the SVAMP dataset, we find that GPT-3’s davinci-002 model, in addition to having good performance on numerical math word problems, also performs well on the potentially harder symbolic version of the same problems. Furthermore, adopting a two-step approach (solve symbolically and then substitute numerical values) leads to better accuracy on the numerical test set in the zero-shot regime. Additionally, we find that the use of specific prompting techniques pushes the model, in many cases, to actively describe its thought process and aid in the final answer output when faced with a complex, multi-step problem, aligning with recent observations.	f557f3a32d309373e7d31bb93ca1b80b4a6e39e7	@['Conference']{gaur-saunshi-2022-symbolic,  author = {Vedant Gaur and Nikunj Saunshi},  booktitle = {2022 IEEE MIT Undergraduate Research Technology Conference (URTC)},  journal = {2022 IEEE MIT Undergraduate Research Technology Conference (URTC)},  pages = {1-5},  title = {Symbolic Math Reasoning with Language Models},  year = {2022} }
Symbolic Knowledge Distillation: from General Language Models to Commonsense Models	2021	http://www.semanticscholar.org/paper/521ccc898395a2818fced22b4cf371b0e5121f94	It is demonstrated that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model, and results in a neural commonsense model that surpasses the teacher model's commonsense capabilities despite its 100x smaller size.	maybe	61	The common practice for training commonsense models has gone from–human–to–corpus–to–machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from–machine–to–corpus–to–machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al. 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically–as text–in addition to the neural model. We distill only one aspect–the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model’s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and will share our new symbolic knowledge graph and commonsense models.	521ccc898395a2818fced22b4cf371b0e5121f94	@['JournalArticle', 'Conference']{west-etal-2021-symbolic,  author = {Peter West and Chandrasekhar Bhagavatula and Jack Hessel and Jena D. Hwang and Liwei Jiang and Ronan Le Bras and Ximing Lu and S. Welleck and Yejin Choi},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4602-4625},  title = {Symbolic Knowledge Distillation: from General Language Models to Commonsense Models},  year = {2021} }
SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics	2021	http://www.semanticscholar.org/paper/577d44a10b424a55165a6bf4839bafce2c695302	This work proposes a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena.	yes	4	Recently, deep neural networks (DNNs) have achieved great success in semantically challenging NLP tasks, yet it remains unclear whether DNN models can capture compositional meanings, those aspects of meaning that have been long studied in formal semantics. To investigate this issue, we propose a Systematic Generalization testbed based on Natural language Semantics (SyGNS), whose challenge is to map natural language sentences to multiple forms of scoped meaning representations, designed to account for various semantic phenomena. Using SyGNS, we test whether neural networks can systematically parse sentences involving novel combinations of logical expressions such as quantifiers and negation. Experiments show that Transformer and GRU models can generalize to unseen combinations of quantifiers, negations, and modifiers that are similar to given training instances in form, but not to the others. We also find that the generalization performance to unseen combinations is better when the form of meaning representations is simpler. The data and code for SyGNS are publicly available at https: //github.com/verypluming/SyGNS.	577d44a10b424a55165a6bf4839bafce2c695302	@['JournalArticle']{yanaka-etal-2021-sygns:,  author = {Hitomi Yanaka and K. Mineshima and Kentarou Inui},  booktitle = {Findings},  pages = {103-119},  title = {SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics},  year = {2021} }
Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity	2021	http://www.semanticscholar.org/paper/3fd0f34117cf9395130e08c3f02ac2dadcca7206	This work simplifies the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs, and advances the current scale of language models by pre-training up to trillion parameter models on the “Colossal Clean Crawled Corpus”, and achieves a 4x speedup over the T5-XXL model.	maybe	553	In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) models defy this and instead select diﬀerent parameters for each incoming example. The result is a sparsely-activated model—with an outrageous number of parameters—but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs, and training instability. We address these with the introduction of the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques mitigate the instabilities, and we show large sparse models may be trained, for the ﬁrst time, with lower precision (bﬂoat16) formats. We design models based oﬀ T5-Base and T5-Large (Raﬀel et al., 2019) to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the “Colossal Clean Crawled Corpus”, and achieve a 4x speedup over the T5-XXL model. 1 present in each. These benchmarks consist of tasks requiring sentiment analysis (SST-2), word sense disambiguation (WIC), sentence similarty (MRPC, STS-B, QQP), natural language inference (MNLI, QNLI, RTE, CB), question answering (MultiRC, RECORD, BoolQ), coreference resolution (WNLI, WSC) and sentence completion (COPA) and sentence acceptability (CoLA). The CNNDM (Hermann et al., 2015) and BBC XSum (Narayan et al., 2018) data sets are used to measure the ability to summarize articles. Question answering is probed with the SQuAD data set (Rajpurkar et al., 2016) and the ARC Reasoning Challenge (Clark et al., 2018). And as in Roberts et al. (2020), we evaluate the knowledge of our models by ﬁne-tuning on three closed-book question answering data sets: Natural Questions (Kwiatkowski et al., 2019), Web Questions (Berant et al., 2013) and Trivia QA (Joshi et al., 2017). Closed-book refers to questions posed with no supplemental reference or context material. To gauge the model’s common sense reasoning we evaluate it on the Winogrande Schema Challenge (Sakaguchi et al., 2020). And ﬁnally, we test our model’s natural language inference capabilities on the Adversarial NLI Benchmark (Nie et al., 2019). embeddings based on the incoming token representations to a given layer. Our work studies a speciﬁc model in a class of methods that do conditional computation, where computation decisions are made dynamically based on the input. Cho and Bengio (2014) proposed adaptively selecting weights based on certain bit patterns occuring in the model hidden-states. Eigen et al. (2013) built stacked expert layers with dense matrix multiplications and ReLU activations and showed promising results on jittered MNIST and monotone speech. In computer vision Puigcerver et al. (2020) manually route tokens based on semantic classes during upstream pre-training and then select the relevant experts to be used according to the downstream task.	3fd0f34117cf9395130e08c3f02ac2dadcca7206	@['JournalArticle']{fedus-etal-2021-switch,  author = {W. Fedus and Barret Zoph and Noam M. Shazeer},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},  volume = {abs/2101.03961},  year = {2021} }
Supporting Context Monotonicity Abstractions in Neural NLI Models	2021	http://www.semanticscholar.org/paper/8e559bf3c07fcf8291e690729fa94a8b14d4f02e	This work reframe the problem of context monotonicity classification to make it compatible with transformer-based pre-trained NLI models and adds this task to the training pipeline, and introduces a sound and complete simplifiedmonotonicity logic formalism which describes the treatment of contexts as abstract units.	maybe	3	Natural language contexts display logical regularities with respect to substitutions of related concepts: these are captured in a functional order-theoretic property called monotonicity. For a certain class of NLI problems where the resulting entailment label depends only on the context monotonicity and the relation between the substituted concepts, we build on previous techniques that aim to improve the performance of NLI models for these problems, as consistent performance across both upward and downward monotone contexts still seems difficult to attain even for state of the art models. To this end, we reframe the problem of context monotonicity classification to make it compatible with transformer-based pre-trained NLI models and add this task to the training pipeline. Furthermore, we introduce a sound and complete simplified monotonicity logic formalism which describes our treatment of contexts as abstract units. Using the notions in our formalism, we adapt targeted challenge sets to investigate whether an intermediate context monotonicity classification task can aid NLI models’ performance on examples exhibiting monotonicity reasoning.	8e559bf3c07fcf8291e690729fa94a8b14d4f02e	@['JournalArticle']{rozanova-etal-2021-supporting,  author = {Julia Rozanova and Deborah Ferreira and Mokanarangan Thayaparan and Marco Valentino and A. Freitas},  booktitle = {NALOMA},  journal = {ArXiv},  title = {Supporting Context Monotonicity Abstractions in Neural NLI Models},  volume = {abs/2105.08008},  year = {2021} }
Superior generalization of smaller models in the presence of significant label noise	2022	http://www.semanticscholar.org/paper/39a3bfa818c00853103e7928f76612d3fe7ffe9d	It is observed that in the presence of a substantial fraction of mislabeled examples, increasing the network size beyond some point can be harmful, and the best generalization is achieved by some model with intermediate size.	maybe	1	The beneﬁts of over-parameterization in achieving superior generalization performance have been shown in several recent studies, justifying the trend of using larger models in practice. In the context of robust learning however, the effect of neural network size has not been well studied. In this work, we ﬁnd that in the presence of a substantial fraction of mislabeled examples, increasing the network size beyond some point can be harmful. In particular, the originally monotonic or ‘dou-ble descent’ test loss curve (w.r.t. network width) turns into a U-shaped or a double U-shaped curve when label noise increases, suggesting that the best generalization is achieved by some model with intermediate size. We observe that when network size is controlled by density through random pruning, similar test loss behaviour is observed. We also take a closer look into both phenomenon through bias-variance decomposition and theoretically characterize how label noise shapes the variance term. Similar behavior of the test loss can be observed even when state-of-the-art robust methods are applied, indicating that limiting the network size could further boost existing methods. Finally, we empirically examine the effect of network size on the smoothness of learned functions, and ﬁnd that the originally negative correlation between size and smoothness is ﬂipped by label noise.	39a3bfa818c00853103e7928f76612d3fe7ffe9d	@['JournalArticle']{xue-etal-2022-superior,  author = {Yihao Xue and Kyle Whitecross and Baharan Mirzasoleiman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Superior generalization of smaller models in the presence of significant label noise},  volume = {abs/2208.08003},  year = {2022} }
Superbizarre Is Not Superb: Improving BERT's Interpretations of Complex Words with Derivational Morphology	2021	http://www.semanticscholar.org/paper/3376118362db3751cfbd88acd0c090b8a3897733	It is shown that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words.	maybe	5	How does the input segmentation of pretrained language models (PLMs) affect their generalization capabilities? We present the first study investigating this question, taking BERT as the example PLM and focusing on the semantic representations of derivationally complex words. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which derivational segmentation consistently outperforms BERT’s WordPiece segmentation by a large margin. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used.	3376118362db3751cfbd88acd0c090b8a3897733	@['JournalArticle']{hofmann-etal-2021-superbizarre,  author = {Valentin Hofmann and J. Pierrehumbert and Hinrich Schütze},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Superbizarre Is Not Superb: Improving BERT's Interpretations of Complex Words with Derivational Morphology},  volume = {abs/2101.00403},  year = {2021} }
Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization	2021	http://www.semanticscholar.org/paper/e638b9e6ee09ab4fa748b748099e0f03d471d803	This paper studies a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold, and refers to the tickets on the threshold as ”super tickets”.	maybe	23	The Lottery Ticket Hypothesis suggests that an over-parametrized network consists of ”lottery tickets”, and training a certain collection of them (i.e., a subnetwork) can match the performance of the full model. In this paper, we study such a collection of tickets, which is referred to as ”winning tickets”, in extremely over-parametrized models, e.g., pre-trained language models. We observe that at certain compression ratios, the generalization performance of the winning tickets can not only match but also exceed that of the full model. In particular, we observe a phase transition phenomenon: As the compression ratio increases, generalization performance of the winning tickets first improves then deteriorates after a certain threshold. We refer to the tickets on the threshold as ”super tickets”. We further show that the phase transition is task and model dependent — as the model size becomes larger and the training data set becomes smaller, the transition becomes more pronounced. Our experiments on the GLUE benchmark show that the super tickets improve single task fine-tuning by 0.9 points on BERT-base and 1.0 points on BERT-large, in terms of task-average score. We also demonstrate that adaptively sharing the super tickets across tasks benefits multi-task learning.	e638b9e6ee09ab4fa748b748099e0f03d471d803	@['JournalArticle', 'Conference']{liang-etal-2021-super,  author = {Chen Liang and Simiao Zuo and Minshuo Chen and Haoming Jiang and Xiaodong Liu and Pengcheng He and T. Zhao and Weizhu Chen},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {6524-6538},  title = {Super Tickets in Pre-Trained Language Models: From Model Compression to Improving Generalization},  year = {2021} }
Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT	2022	http://www.semanticscholar.org/paper/4b146472c6ab01e0117d618f5e558702b9abc663		yes	1	Both humans and neural language models are able to perform subject verb number agreement (SVA). In principle, semantics shouldn’t interfere with this task, which only requires syntactic knowledge. In this work we test whether meaning interferes with this type of agreement in English in syntactic structures of various complexities. To do so, we generate both semantically well-formed and nonsensical items. We compare the performance of BERT-base to that of humans, obtained with a psycholinguistic online crowdsourcing experiment. We find that BERT and humans are both sensitive to our semantic manipulation: They fail more often when presented with nonsensical items, especially when their syntactic structure features an attractor (a noun phrase between the subject and the verb that has not the same number as the subject). We also find that the effect of meaningfulness on SVA errors is stronger for BERT than for humans, showing higher lexical sensitivity of the former on this task.	4b146472c6ab01e0117d618f5e558702b9abc663	@['JournalArticle', 'Conference']{lasri-etal-2022-subject,  author = {Karim Lasri and Olga Seminck and Alessandro Lenci and T. Poibeau},  booktitle = {International Conference on Computational Linguistics},  pages = {37-43},  title = {Subject Verb Agreement Error Patterns in Meaningless Sentences: Humans vs. BERT},  year = {2022} }
Studying word order through iterative shuffling	2021	http://www.semanticscholar.org/paper/0412441ad2559c44075ca991b2b4ca533ac03dc5	A novel, efficient procedure that finds the ordering of a bag of words having the highest likelihood under a fixed language model, and how shuffling inference procedures such as IBIS can benefit language modeling and constrained generation is discussed.	maybe	5	As neural language models approach human performance on NLP benchmark tasks, their advances are widely seen as evidence of an increasingly complex understanding of syntax. This view rests upon a hypothesis that has not yet been empirically tested: that word order encodes meaning essential to performing these tasks. We refute this hypothesis in many cases: in the GLUE suite and in various genres of English text, the words in a sentence or phrase can rarely be permuted to form a phrase carrying substantially different information. Our surprising result relies on inference by iterative shuffling (IBIS), a novel, efficient procedure that finds the ordering of a bag of words having the highest likelihood under a fixed language model. IBIS can use any black-box model without additional training and is superior to existing word ordering algorithms. Coalescing our findings, we discuss how shuffling inference procedures such as IBIS can benefit language modeling and constrained generation.	0412441ad2559c44075ca991b2b4ca533ac03dc5	@['JournalArticle', 'Conference']{malkin-etal-2021-studying,  author = {Nikolay Malkin and Sameera Lanka and Pranav Goel and N. Jojic},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {10351-10366},  title = {Studying word order through iterative shuffling},  year = {2021} }
Structured Pruning of Large Language Models	2019	http://www.semanticscholar.org/paper/83b8108014e3db4f46354a28ae68193f143c4e7e	A novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization is presented, which achieves significant inference speedups while matching or outperforming the authors' unstructured pruning baseline at various sparsity levels.	maybe	95	Large language models have recently achieved state of the art performance across a wide variety of natural language tasks. Meanwhile, the size of these models and their latency have significantly increased, which makes their usage costly, and raises an interesting question: do language models need to be large? We study this question through the lens of model compression. We present a novel, structured pruning approach based on low rank factorization and augmented Lagrangian L0 norm regularization. Our structured approach achieves significant inference speedups while matching or outperforming our unstructured pruning baseline at various sparsity levels. We apply our method to state of the art models on the enwiki8 dataset and obtain a 1.19 perplexity score with just 5M parameters, vastly outperforming a model of the same size trained from scratch. We also demonstrate that our method can be applied to language model fine-tuning by pruning the BERT model on several downstream classification benchmarks.	83b8108014e3db4f46354a28ae68193f143c4e7e	@['JournalArticle', 'Conference']{wang-etal-2019-structured,  author = {Ziheng Wang and Jeremy Wohlwend and Tao Lei},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {6151-6162},  title = {Structured Pruning of Large Language Models},  year = {2019} }
Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations	2021	http://www.semanticscholar.org/paper/df72b1bc510d9e6f7cb666ec8dc4fd9f68375ad4	This study finds that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information, and shows that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information.	yes	1	Abstract We investigate the extent to which modern neural language models are susceptible to structural priming, the phenomenon whereby the structure of a sentence makes the same structure more probable in a follow-up sentence. We explore how priming can be used to study the potential of these models to learn abstract structural information, which is a prerequisite for good performance on tasks that require natural language understanding skills. We introduce a novel metric and release Prime-LM, a large corpus where we control for various linguistic factors that interact with priming strength. We find that Transformer models indeed show evidence of structural priming, but also that the generalizations they learned are to some extent modulated by semantic information. Our experiments also show that the representations acquired by the models may not only encode abstract sequential structure but involve certain level of hierarchical syntactic information. More generally, our study shows that the priming paradigm is a useful, additional tool for gaining insights into the capacities of language models and opens the door to future priming-based investigations that probe the model’s internal states.1	df72b1bc510d9e6f7cb666ec8dc4fd9f68375ad4	@['JournalArticle']{sinclair-etal-2021-structural,  author = {Arabella J. Sinclair and Jaap Jumelet and Willem H. Zuidema and R. Fernández},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1031-1050},  title = {Structural Persistence in Language Models: Priming as a Window into Abstract Language Representations},  volume = {10},  year = {2021} }
Structural analysis of an all-purpose question answering model	2021	http://www.semanticscholar.org/paper/c889d0b6c2ef833ce344607d88f254fa892e344c	It is observed that attention heads specialize in a particular task and that some heads are more conducive to learning than others in both the multi-task and single-task settings.	maybe	3	Attention is a key component of the now ubiquitous pre-trained language models. By learning to focus on relevant pieces of information, these Transformer-based architectures have proven capable of tackling several tasks at once and sometimes even surpass their single-task counterparts. To better understand this phenomenon, we conduct a structural analysis of a new all-purpose question answering model that we introduce. Surprisingly, this model retains single-task performance even in the absence of a strong transfer effect between tasks. Through attention head importance scoring, we observe that attention heads specialize in a particular task and that some heads are more conducive to learning than others in both the multi-task and single-task settings.	c889d0b6c2ef833ce344607d88f254fa892e344c	@['JournalArticle']{micheli-etal-2021-structural,  author = {Vincent Micheli and Quentin Heinrich and Franccois Fleuret and Wacim Belblidia},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Structural analysis of an all-purpose question answering model},  volume = {abs/2104.06045},  year = {2021} }
StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding	2019	https://www.semanticscholar.org/paper/d56c1fc337fb07ec004dc846f80582c327af717c	Inspired by the linearization exploration work of Elman, BERT is extended to a new model, StructBERT, by incorporating language structures into pre-training, and the new model is adapted to different levels of language understanding required by downstream tasks.	seed	179	Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.	d56c1fc337fb07ec004dc846f80582c327af717c	@['JournalArticle']{wang-etal-2019-structbert:,  author = {Wei Wang and Bin Bi and Ming Yan and Chen Wu and Zuyi Bao and Liwei Peng and Luo Si},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding},  volume = {abs/1908.04577},  year = {2019} }
Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition	2019	http://www.semanticscholar.org/paper/a5ec883e080d858b5df60f1b5d711d514459b1e4	It is found that contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans.	maybe	54	Building meaningful phrase representations is challenging because phrase meanings are not simply the sum of their constituent meanings. Lexical composition can shift the meanings of the constituent words and introduce implicit information. We tested a broad range of textual representations for their capacity to address these issues. We found that, as expected, contextualized word representations perform better than static word embeddings, more so on detecting meaning shift than in recovering implicit information, in which their performance is still far from that of humans. Our evaluation suite, consisting of six tasks related to lexical composition effects, can serve future research aiming to improve representations.	a5ec883e080d858b5df60f1b5d711d514459b1e4	@['JournalArticle']{shwartz-dagan-2019-still,  author = {Vered Shwartz and Ido Dagan},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {403-419},  title = {Still a Pain in the Neck: Evaluating Text Representations on Lexical Composition},  volume = {7},  year = {2019} }
Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models	2021	http://www.semanticscholar.org/paper/8fa0de4920c8edcb1fea698ff3463a347771d889	This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task and finds evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models.	maybe	25	This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.	8fa0de4920c8edcb1fea698ff3463a347771d889	@['JournalArticle', 'Conference']{manela-etal-2021-stereotype,  author = {Daniel de Vassimon Manela and D. Errington and Thomas Fisher and B. V. Breugel and Pasquale Minervini},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {2232-2242},  title = {Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models},  year = {2021} }
StereoSet: Measuring stereotypical bias in pretrained language models	2020	https://www.semanticscholar.org/paper/babeda48b10a4d638252118f2238d05a06f4ec55	StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion, is presented and it is shown that popular models like BERT, GPT-2, RoBERTa, and XLnet exhibit strong stereotypical biases.	maybe	235	A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at https://stereoset.mit.edu.	babeda48b10a4d638252118f2238d05a06f4ec55	@['JournalArticle', 'Conference']{nadeem-etal-2020-stereoset:,  author = {Moin Nadeem and Anna Bethke and Siva Reddy},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5356-5371},  title = {StereoSet: Measuring stereotypical bias in pretrained language models},  year = {2020} }
Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?	2021	http://www.semanticscholar.org/paper/db742f7c4f7edd24e77482560df7d09c9033b3da	The first dataset comprising stereotypical attributes of a range of social groups is presented and a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion is proposed.	maybe	5	In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use fine-tuning on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.	db742f7c4f7edd24e77482560df7d09c9033b3da	@['JournalArticle', 'Conference']{choenni-etal-2021-stepmothers,  author = {Rochelle Choenni and Ekaterina Shutova and R. Rooij},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?},  volume = {abs/2109.10052},  year = {2021} }
Statistically Profiling Biases in Natural Language Reasoning Datasets and Models	2021	http://www.semanticscholar.org/paper/74ea2115d19f81db6dcb37de20219410d0e2a5fc	A light-weight and general statistical profiling framework, ICQ (I-See-Cue), which automatically identifies possible biases in any multiple-choice NLU datasets without the need to create any additional test cases, and further evaluates through blackbox testing the extent to which models may exploit these biases.	maybe	1	Recent work has indicated that many natural language understanding and reasoning datasets contain statistical cues that may be taken advantaged of by NLP models whose capability may thus be grossly overestimated. To discover the potential weakness in the models, some human-designed stress tests have been proposed but they are expensive to create and do not generalize to arbitrary models. We propose a light-weight and general statistical profiling framework, ICQ (I-See-Cue), which automatically identifies possible biases in any multiple-choice NLU datasets without the need to create any additional test cases, and further evaluates through blackbox testing the extent to which models may exploit these biases.	74ea2115d19f81db6dcb37de20219410d0e2a5fc	@['JournalArticle']{huang-zhu-2021-statistically,  author = {Shanshan Huang and Kenny Q. Zhu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Statistically Profiling Biases in Natural Language Reasoning Datasets and Models},  volume = {abs/2102.04632},  year = {2021} }
State-of-the-art generalisation research in NLP: a taxonomy and review	2022	http://www.semanticscholar.org/paper/559bfba3bee31f6061a5d5c7061f22794de47e39	A taxonomy for characterising and understanding generalisation research in NLP is presented, a taxonomy is used to present a comprehensive map of published generalisation studies, and recommendations for which areas might deserve attention in the future are made.	yes	2	The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what ‘good generalisation’ entails and how it should be evaluated is not well understood, nor are there any common standards to evaluate it. In this paper, we aim to lay the groundwork to improve both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP, we use that taxonomy to present a comprehensive map of published generalisation studies, and we make recommendations for which areas might deserve attention in the future. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they aim to solve, the type of data shift they consider, the source by which this data shift is obtained, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 previous papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis of the current state of generalisation research in NLP, and make recommendations for the future. Along with this paper, we release a webpage where the results of our review can be dynamically explored, and which we intend to update as new NLP generalisation studies are published. With this work, we aim to make steps towards making state-of-the-art generalisation testing the new status quo in NLP.	559bfba3bee31f6061a5d5c7061f22794de47e39	@['JournalArticle', 'Review']{hupkes-etal-2022-state,  author = {D. Hupkes and Mario Giulianelli and Verna Dankers and Mikel Artetxe and Yanai Elazar and Tiago Pimentel and Christos Christodoulopoulos and Karim Lasri and Naomi Saphra and Arabella J. Sinclair and Dennis Ulmer and Florian Schottmann and Khuyagbaatar Batsuren and Kaiser Sun and Koustuv Sinha and Leila Khalatbari and Maria Ryskina and Rita Frieske and Ryan Cotterell and Zhijing Jin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {State-of-the-art generalisation research in NLP: a taxonomy and review},  volume = {abs/2210.03050},  year = {2022} }
SST-BERT at SemEval-2020 Task 1: Semantic Shift Tracing by Clustering in BERT-based Embedding Spaces	2020	http://www.semanticscholar.org/paper/e128f055795c6d6b179a52fa8efc247e8edb9762	This work proposes to identify clusters among different occurrences of each target word, considering these as representatives of different word meanings, and results show that this approach performs well both measured separately and overall, where it surpasses all provided SemEval baselines.	maybe	9	Lexical semantic change detection (also known as semantic shift tracing) is a task of identifying words that have changed their meaning over time. Unsupervised semantic shift tracing, focal point of SemEval2020, is particularly challenging. Given the unsupervised setup, in this work, we propose to identify clusters among different occurrences of each target word, considering these as representatives of different word meanings. As such, disagreements in obtained clusters naturally allow to quantify the level of semantic shift per each target word in four target languages. To leverage this idea, clustering is performed on contextualized (BERT-based) embeddings of word occurrences. The obtained results show that our approach performs well both measured separately (per language) and overall, where we surpass all provided SemEval baselines.	e128f055795c6d6b179a52fa8efc247e8edb9762	@['JournalArticle']{vani-etal-2020-sst,  author = {K. Vani and Sandra Mitrovic and Alessandro Antonucci and Fabio Rinaldi},  booktitle = {International Workshop on Semantic Evaluation},  journal = {ArXiv},  title = {SST-BERT at SemEval-2020 Task 1: Semantic Shift Tracing by Clustering in BERT-based Embedding Spaces},  volume = {abs/2010.00857},  year = {2020} }
Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words	2020	http://www.semanticscholar.org/paper/cae24695391e7ef8e7d351a8c922b4016fbfbd02	A suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words are introduced, and it is found that each of the tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability.	maybe	27	Although models using contextual word embeddings have achieved state-of-the-art results on a host of NLP tasks, little is known about exactly what information these embeddings encode about the context words that they are understood to reflect. To address this question, we introduce a suite of probing tasks that enable fine-grained testing of contextual embeddings for encoding of information about surrounding words. We apply these tasks to examine the popular BERT, ELMo and GPT contextual encoders, and find that each of our tested information types is indeed encoded as contextual information across tokens, often with near-perfect recoverability—but the encoders vary in which features they distribute to which tokens, how nuanced their distributions are, and how robust the encoding of each feature is to distance. We discuss implications of these results for how different types of models break down and prioritize word-level context information when constructing token embeddings.	cae24695391e7ef8e7d351a8c922b4016fbfbd02	@['JournalArticle', 'Conference']{klafka-ettinger-2020-spying,  author = {Josef Klafka and Allyson Ettinger},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words},  volume = {abs/2005.01810},  year = {2020} }
Spectral Probing	2022	http://www.semanticscholar.org/paper/b99001e74f77af866c0da1aa735e74592e7c949f		maybe	0	Linguistic information is encoded at varying timescales (subwords, phrases, etc.) and communicative levels, such as syntax and semantics. Contextualized embeddings have analogously been found to capture these phenomena at distinctive layers and frequencies. Leveraging these findings, we develop a fully learnable frequency filter to identify spectral profiles for any given task. It enables vastly more granular analyses than prior handcrafted filters, and improves on efficiency. After demonstrating the informativeness of spectral probing over manual filters in a monolingual setting, we investigate its multilingual characteristics across seven diverse NLP tasks in six languages. Our analyses identify distinctive spectral profiles which quantify cross-task similarity in a linguistically intuitive manner, while remaining consistent across languages—highlighting their potential as robust, lightweight task descriptors.	b99001e74f77af866c0da1aa735e74592e7c949f	@['JournalArticle', 'Conference']{eberstein-etal-2022-spectral,  author = {Max Müller-Eberstein and R. Goot and Barbara Plank},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Spectral Probing},  volume = {abs/2210.11860},  year = {2022} }
SparseBERT: Rethinking the Importance Analysis in Self-attention	2021	http://www.semanticscholar.org/paper/4badd753be64c5c5b57dd2bb2e515fbe0c0720d8	A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions, and a proof is provided showing that these diagonal elements can indeed be removed without deteriorating model performance.	maybe	24	Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.	4badd753be64c5c5b57dd2bb2e515fbe0c0720d8	@['JournalArticle', 'Conference']{shi-etal-2021-sparsebert:,  author = {Han Shi and Jiahui Gao and Xiaozhe Ren and Hang Xu and Xiaodan Liang and Zhenguo Li and J. Kwok},  booktitle = {International Conference on Machine Learning},  pages = {9547-9557},  title = {SparseBERT: Rethinking the Importance Analysis in Self-attention},  year = {2021} }
Sparse Interventions in Language Models with Differentiable Masking	2021	http://www.semanticscholar.org/paper/62394648facd628a87021a7ef803d0e6efeddee5	Inspired by causal mediation analysis, this work proposes a method that discovers within a neural LM a small subset of neurons responsible for a particular linguistic phenomenon, i.e., subsets causing a change in the corresponding token emission probabilities.	yes	7	There has been a lot of interest in understanding what information is captured by hidden representations of language models (LMs). Typically, interpretation methods i) do not guarantee that the model actually uses the information found to be encoded, and ii) do not discover small subsets of neurons responsible for a considered phenomenon. Inspired by causal mediation analysis, we propose a method that discovers a small subset of neurons within a neural LM responsible for a particular linguistic phenomenon, i.e., subsets causing a change in the corresponding token emission probabilities. We use a differentiable relaxation to approximately search through the combinatorial space. An L_0 regularization term ensures that the search converges to discrete and sparse solutions. We apply our method to analyze subject-verb number agreement and gender bias detection in LSTMs. We observe that it is fast and finds better solutions than alternatives such as REINFORCE and Integrated Gradients. Our experiments confirm that each of these phenomena is mediated through a small subset of neurons that do not play any other discernible role.	62394648facd628a87021a7ef803d0e6efeddee5	@['JournalArticle']{cao-etal-2021-sparse,  author = {Nicola De Cao and Leon Schmid and D. Hupkes and Ivan Titov},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Sparse Interventions in Language Models with Differentiable Masking},  volume = {abs/2112.06837},  year = {2021} }
Sorting through the noise: Testing robustness of information processing in pre-trained language models	2021	http://www.semanticscholar.org/paper/0981ce872d31a665882e7677d608351ff5f1de6b	This paper examines robustness of models’ ability to deploy relevant context information in the face of distracting content, and presents models with cloze tasks requiring use of critical context information, and introduces distracting content to test how robustly the models retain and use that critical information for prediction.	maybe	9	Pre-trained LMs have shown impressive performance on downstream NLP tasks, but we have yet to establish a clear understanding of their sophistication when it comes to processing, retaining, and applying information presented in their input. In this paper we tackle a component of this question by examining robustness of models’ ability to deploy relevant context information in the face of distracting content. We present models with cloze tasks requiring use of critical context information, and introduce distracting content to test how robustly the models retain and use that critical information for prediction. We also systematically manipulate the nature of these distractors, to shed light on dynamics of models’ use of contextual cues. We find that although models appear in simple contexts to make predictions based on understanding and applying relevant facts from prior context, the presence of distracting but irrelevant content has clear impact in confusing model predictions. In particular, models appear particularly susceptible to factors of semantic similarity and word position. The findings are consistent with the conclusion that LM predictions are driven in large part by superficial contextual cues, rather than by robust representations of context meaning.	0981ce872d31a665882e7677d608351ff5f1de6b	@['JournalArticle', 'Conference']{pandia-ettinger-2021-sorting,  author = {Lalchand Pandia and Allyson Ettinger},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1583-1596},  title = {Sorting through the noise: Testing robustness of information processing in pre-trained language models},  year = {2021} }
Sort by Structure: Language Model Ranking as Dependency Probing	2022	http://www.semanticscholar.org/paper/bf98bf41b3c498b754ea9b5299b2c69144e3caa5	P probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM’s contextualized embeddings, finds it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning.	maybe	1	Making an informed choice of pre-trained language model (LM) is critical for performance, yet environmentally costly, and as such widely underexplored. The field of Computer Vision has begun to tackle encoder ranking, with promising forays into Natural Language Processing, however they lack coverage of linguistic tasks such as structured prediction. We propose probing to rank LMs, specifically for parsing dependencies in a given language, by measuring the degree to which labeled trees are recoverable from an LM’s contextualized embeddings. Across 46 typologically and architecturally diverse LM-language pairs, our probing approach predicts the best LM choice 79% of the time using orders of magnitude less compute than training a full parser. Within this study, we identify and analyze one recently proposed decoupled LM—RemBERT—and find it strikingly contains less inherent dependency information, but often yields the best parser after full fine-tuning. Without this outlier our approach identifies the best LM in 89% of cases.	bf98bf41b3c498b754ea9b5299b2c69144e3caa5	@['JournalArticle', 'Conference']{eberstein-etal-2022-sort,  author = {Max Müller-Eberstein and R. Goot and Barbara Plank},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {1296-1307},  title = {Sort by Structure: Language Model Ranking as Dependency Probing},  year = {2022} }
Some linguistic correlates of gradients and attention weights in BERT	2019	http://www.semanticscholar.org/paper/df526b1be696c17f0d727f4ae499c6910b7af6c9	This work applies the well-known BERT model to a selection of part-of-speech tagged, dependency-parsed and coreference-annotated text, extracting gradients and attention weights for inspection and reveals that, in BERT, more information flows from a noun to a pronoun if they corefer.	maybe	0	This work applies the well-known BERT model to a selection of part-of-speech tagged, dependency-parsed and coreference-annotated text, extracting gradients and attention weights for inspection. This reveals that, in BERT, more information flows from a noun to a pronoun if they corefer; open-class words are generally more informative than closed-class words; and there is a slightly underwhelming correlation between BERT’s gradients and dependency parses. It also highlights that attention weights and gradients are of course correlated, but they do not always reveal exactly the same patterns.	df526b1be696c17f0d727f4ae499c6910b7af6c9	@None{westera-2019-some,  author = {M. Westera},  title = {Some linguistic correlates of gradients and attention weights in BERT},  year = {2019} }
Solving Quantitative Reasoning Problems with Language Models	2022	http://www.semanticscholar.org/paper/ab0e3d3e4d42369de5933a3b4c237780b41c0d77		maybe	60	Language models have achieved remarkable performance on a wide range of tasks that require natural language understanding. Nevertheless, state-of-the-art models have generally struggled with tasks that require quantitative reasoning, such as solving mathematics, science, and engineering problems at the college level. To help close this gap, we introduce Minerva , a large language model pretrained on general natural language data and further trained on technical content. The model achieves state-of-the-art performance on technical benchmarks without the use of external tools. We also evaluate our model on over two hundred undergraduate-level problems in physics, biology, chemistry, economics, and other sciences that require quantitative reasoning, and find that the model can correctly answer nearly a third of them.	ab0e3d3e4d42369de5933a3b4c237780b41c0d77	@['JournalArticle']{lewkowycz-etal-2022-solving,  author = {Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and H. Michalewski and V. Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Solving Quantitative Reasoning Problems with Language Models},  volume = {abs/2206.14858},  year = {2022} }
Solving Math Word Problem via Cooperative Reasoning induced Language Models	2022	http://www.semanticscholar.org/paper/01f7bb1f9c611b5e849558e445fdccb98a3a3040	A cooperative reasoning-induced PLM for solving MWPs is developed, resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the veriﬁer, and decent improvement over state-of-the-art methods, up to 9.8% increase over best baselines.	maybe	0	Large-scale pre-trained language models (PLMs) bring new opportunities to challenge problems, especially those that need high-level intelligence, such as the math word problem (MWPs). However, directly applying existing PLMs to MWPs can fail as the generation process lacks sufﬁcient supervision and thus lacks fast adaptivity as humans. We notice that human reasoning has a dual reasoning framework that consists of an immediate reaction system (system 1) and a delicate reasoning system (system 2), where the entire reasoning is determined by their interaction. This inspires us to develop a cooperative reasoning-induced PLM for solving MWPs, called Co operative Re asoning (CoRe), resulting in a human-like reasoning architecture with system 1 as the generator and system 2 as the veriﬁer. In our approach, the generator is responsible for generating reasoning paths, and the veriﬁers are used to supervise the evaluation in order to obtain reliable feedback for the generator. We evaluate our CoRe framework on several mathematical reasoning datasets and achieve decent improvement over state-of-the-art methods, up to 9.8% increase over best baselines.	01f7bb1f9c611b5e849558e445fdccb98a3a3040	@['JournalArticle']{zhu-etal-2022-solving,  author = {Xinyu Zhu and Junjie Wang and Lin Zhang and Yuxiang Zhang and Ruyi Gan and Jiaxing Zhang and Yujiu Yang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Solving Math Word Problem via Cooperative Reasoning induced Language Models},  volume = {abs/2210.16257},  year = {2022} }
Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions	2022	http://www.semanticscholar.org/paper/7450a612ef291216b0cd48e09b8879be4675c6eb	This work discovers that a single hidden state cannot produce all probability distributions regardless of the LM size or training data size, and proposes multi-facet softmax (MFS) to address the limitations of MoS.	maybe	2	Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.	7450a612ef291216b0cd48e09b8879be4675c6eb	@['JournalArticle', 'Conference']{chang-mccallum-2022-softmax,  author = {Haw-Shiuan Chang and A. McCallum},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {8048-8073},  title = {Softmax Bottleneck Makes Language Models Unable to Represent Multi-mode Word Distributions},  year = {2022} }
SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models	2022	http://www.semanticscholar.org/paper/8b9e60dcc4e2796ff1ae5aaf1e62ee86b62fe29e	This work proposes SODAPOP in social commonsense question-answering and generates modiﬁed instances from the Social IQa dataset by substituting names associated with different demographic groups, and generating many distractor answers from a masked language model.	yes	0	A common limitation of diagnostic tests for detecting social biases in NLP models is that they may only detect stereotypic associations that are pre-speciﬁed by the designer of the test. Since enumerating all possible prob-lematic associations is infeasible, it is likely these tests fail to detect biases that are present in a model but not pre-speciﬁed by the designer. To address this limitation, we propose SODAPOP ( SO cial bias D iscovery from A nswers about P e OP le) in social commonsense question-answering. Our pipeline generates modiﬁed instances from the Social IQa dataset (Sap et al., 2019b) by (1) substituting names associated with different demographic groups, and (2) generating many distractor answers from a masked language model. By using a social commonsense model to score the generated distractors, we are able to uncover the model’s stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations of multiple state-of-the-art debiasing algorithms.	8b9e60dcc4e2796ff1ae5aaf1e62ee86b62fe29e	@['JournalArticle']{an-etal-2022-sodapop:,  author = {Haozhe An and Zong-ping Li and Jieyu Zhao and Rachel Rudinger},  booktitle = {ArXiv},  journal = {ArXiv},  title = {SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models},  volume = {abs/2210.07269},  year = {2022} }
SocioProbe: What, When, and Where Language Models Learn about Sociodemographics	2022	http://www.semanticscholar.org/paper/1d4d3bdbc70ab400d206ae209546f267f85817c4	The results indicate that sociodemographic knowledge is still a major challenge for NLP, and PLMs require large amounts of pre-training data to acquire the knowledge and models that excel in general language understanding do not seem to own more knowledge about these aspects.	maybe	0	Pre-trained language models (PLMs) have outperformed other NLP models on a wide range of tasks. Opting for a more thorough understanding of their capabilities and inner workings, researchers have established the extend to which they capture lower-level knowledge like grammaticality, and mid-level semantic knowledge like factual understanding. However, there is still little understanding of their knowledge of higher-level aspects of language. In particular, despite the importance of sociodemographic aspects in shaping our language, the questions of whether, where, and how PLMs encode these aspects, e.g., gender or age, is still unexplored. We address this research gap by probing the sociodemographic knowledge of different single-GPU PLMs on multiple English data sets via traditional classifier probing and information-theoretic minimum description length probing. Our results show that PLMs do encode these sociodemographics, and that this knowledge is sometimes spread across the layers of some of the tested PLMs. We further conduct a multilingual analysis and investigate the effect of supplementary training to further explore to what extent, where, and with what amount of pre-training data the knowledge is encoded. Our overall results indicate that sociodemographic knowledge is still a major challenge for NLP. PLMs require large amounts of pre-training data to acquire the knowledge and models that excel in general language understanding do not seem to own more knowledge about these aspects.	1d4d3bdbc70ab400d206ae209546f267f85817c4	@['JournalArticle', 'Conference']{lauscher-etal-2022-socioprobe:,  author = {Anne Lauscher and Federico Bianchi and Samuel R. Bowman and Dirk Hovy},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {SocioProbe: What, When, and Where Language Models Learn about Sociodemographics},  volume = {abs/2211.04281},  year = {2022} }
Societal Biases in Language Generation: Progress and Challenges	2021	http://www.semanticscholar.org/paper/76a786b1acd6d1aca56e12a8a1db34569fdf9f3a	A survey on societal biases in language generation is presented, focusing on how data and techniques contribute to biases and progress towards reducing biases, and the effects of decoding techniques.	maybe	62	Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.	76a786b1acd6d1aca56e12a8a1db34569fdf9f3a	@['JournalArticle', 'Conference', 'Review']{sheng-etal-2021-societal,  author = {Emily Sheng and Kai-Wei Chang and P. Natarajan and Nanyun Peng},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4275-4293},  title = {Societal Biases in Language Generation: Progress and Challenges},  year = {2021} }
Social IQA: Commonsense Reasoning about Social Interactions	2019	http://www.semanticscholar.org/paper/421cb75cc91e8e5683d41ee6a918121aedf6d24d	It is established that Social IQa, the first large-scale benchmark for commonsense reasoning about social situations, is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap).	maybe	269	We introduce Social IQa, the first large-scale benchmark for commonsense reasoning about social situations. Social IQa contains 38,000 multiple choice questions for probing emotional and social intelligence in a variety of everyday situations (e.g., Q: “Jordan wanted to tell Tracy a secret, so Jordan leaned towards Tracy. Why did Jordan do this?” A: “Make sure no one else could hear”). Through crowdsourcing, we collect commonsense questions along with correct and incorrect answers about social interactions, using a new framework that mitigates stylistic artifacts in incorrect answers by asking workers to provide the right answer to a different but related question. Empirical results show that our benchmark is challenging for existing question-answering models based on pretrained language models, compared to human performance (>20% gap). Notably, we further establish Social IQa as a resource for transfer learning of commonsense knowledge, achieving state-of-the-art performance on multiple commonsense reasoning tasks (Winograd Schemas, COPA).	421cb75cc91e8e5683d41ee6a918121aedf6d24d	@['JournalArticle']{sap-etal-2019-social,  author = {Maarten Sap and Hannah Rashkin and Derek Chen and Ronan Le Bras and Yejin Choi},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Social IQA: Commonsense Reasoning about Social Interactions},  volume = {abs/1904.09728},  year = {2019} }
So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements	2021	http://www.semanticscholar.org/paper/ea0ea0da2e774bc0b73c4470de6cbd1fc979b265	It is found that the predictions of three top-ofthe-line contemporary language models—GPT-3, RoBERTa, and ALBERT—match the N400 more closely than human predictions, suggesting that the predictive processes underlying the N 400 may be more sensitive to the surface-level statistics of language than previously thought.	maybe	10	—More predictable words are easier to process—they are read faster and elicit smaller neural signals associated with processing difﬁculty, most notably, the N400 component of the event-related brain potential. Thus, it has been argued that prediction of upcoming words is a key component of language comprehension, and that studying the amplitude of the N400 is a valuable way to investigate the predictions we make. In this study, we investigate whether the linguistic predictions of computational language models or humans better reﬂect the way in which natural language stimuli modulate the amplitude of the N400. One important difference in the linguistic predictions of humans versus computational language models is that while language models base their predictions exclusively on the pre- ceding linguistic context, humans may rely on other factors. We ﬁnd that the predictions of three top-of-the-line contemporary language models—GPT-3, RoBERTa, and ALBERT—match the N400 more closely than human predictions. This suggests that the predictive processes underlying the N400 may be more sensitive to the statistics of language than previously thought.	ea0ea0da2e774bc0b73c4470de6cbd1fc979b265	@['JournalArticle']{michaelov-etal-2021-so,  author = {J. Michaelov and S. Coulson and B. Bergen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements},  volume = {abs/2109.01226},  year = {2021} }
SNAP: Efficient Extraction of Private Properties with Poisoning	2022	http://www.semanticscholar.org/paper/91bfe0099595fee1b31c5eedbf7b7a195ecd9a56	An efﬁcient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-basedProperty inference attack by Mahloujifar et al.	maybe	4	Property inference attacks allow an adversary to extract global properties of the training dataset from a machine learning model. Such attacks have privacy implications for data owners who share their datasets to train machine learning models. Several existing approaches for property inference attacks against deep neural networks have been proposed [1]–[3], but they all rely on the attacker training a large number of shadow models, which induces large computational overhead. In this paper, we consider the setting of property inference attacks in which the attacker can poison a subset of the training dataset and query the trained target model. Motivated by our theoretical analysis of model conﬁdences under poisoning, we design an efﬁcient property inference attack, SNAP, which obtains higher attack success and requires lower amounts of poisoning than the state-of-the-art poisoning-based property inference attack by Mahloujifar et al. [3]. For example, on the Census dataset, SNAP achieves 34% higher success rate than [3] while being 56 . 5 × faster. We also extend our attack to determine if a certain property is present at all in training, and estimate the exact proportion of a property of interest efﬁciently. We evaluate our attack on several properties of varying proportions from four datasets, and demonstrate SNAP’s generality and effectiveness.	91bfe0099595fee1b31c5eedbf7b7a195ecd9a56	@['JournalArticle']{chaudhari-etal-2022-snap:,  author = {Harsh Chaudhari and Jackson Abascal and Alina Oprea and Matthew Jagielski and Florian Tramèr and Jonathan Ullman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {SNAP: Efficient Extraction of Private Properties with Poisoning},  volume = {abs/2208.12348},  year = {2022} }
Simple Text Detoxification by Identifying a Linear Toxic Subspace in Language Model Embeddings	2021	http://www.semanticscholar.org/paper/ff6d820cf372a8a9b9ac2689251c398cf9bf4788	It is demonstrated empirically that the subspace found using the proposed method to generalize toxic directions in the latent space generalizes to multiple toxicity corpora, indicating the existence of a low-dimensional toxic subspace.	maybe	0	Large pre-trained language models are often trained on large volumes of internet data, some of which may contain toxic or abusive language. Consequently, language models encode toxic information, which makes the realworld usage of these language models limited. Current methods aim to prevent toxic features from appearing generated text. We hypothesize the existence of a low-dimensional toxic subspace in the latent space of pre-trained language models, the existence of which suggests that toxic features follow some underlying pattern and are thus removable. To construct this toxic subspace, we propose a method to generalize toxic directions in the latent space. We also provide a methodology for constructing parallel datasets using a context based word masking system. Through our experiments, we show that when the toxic subspace is removed from a set of sentence representations, almost no toxic representations remain in the result. We demonstrate empirically that the subspace found using our method generalizes to multiple toxicity corpora, indicating the existence of a low-dimensional toxic subspace.	ff6d820cf372a8a9b9ac2689251c398cf9bf4788	@['JournalArticle']{wang-etal-2021-simple,  author = {Andrew Wang and Mohit Sudhakar and Yangfeng Ji},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Simple Text Detoxification by Identifying a Linear Toxic Subspace in Language Model Embeddings},  volume = {abs/2112.08346},  year = {2021} }
Similarity Analysis of Contextual Word Representation Models	2020	http://www.semanticscholar.org/paper/ca4ecf116a9b97ce525a01f3f51117877688ddf5	The analysis reveals that models within the same family are more similar to one another, as may be expected, while different architectures have rather similar representations, but different individual neurons.	maybe	37	This paper investigates contextual word representation models from the lens of similarity analysis. Given a collection of trained models, we measure the similarity of their internal representations and attention. Critically, these models come from vastly different architectures. We use existing and novel similarity measures that aim to gauge the level of localization of information in the deep models, and facilitate the investigation of which design factors affect model similarity, without requiring any external linguistic annotation. The analysis reveals that models within the same family are more similar to one another, as may be expected. Surprisingly, different architectures have rather similar representations, but different individual neurons. We also observed differences in information localization in lower and higher layers and found that higher layers are more affected by fine-tuning on downstream tasks.	ca4ecf116a9b97ce525a01f3f51117877688ddf5	@['JournalArticle', 'Conference']{wu-etal-2020-similarity,  author = {John M. Wu and Yonatan Belinkov and Hassan Sajjad and Nadir Durrani and Fahim Dalvi and James R. Glass},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Similarity Analysis of Contextual Word Representation Models},  volume = {abs/2005.01172},  year = {2020} }
Show Your Work: Scratchpads for Intermediate Computation with Language Models	2021	http://www.semanticscholar.org/paper/92173d081b15824d22a9ef070e118744ceee8052	Surprisingly, large pre-trained language models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations.	maybe	104	Large pre-trained language models perform remarkably well on tasks that can be done “in one pass”, such as generating realistic text (Brown et al., 2020) or synthesizing computer programs (Chen et al., 2021; Austin et al., 2021). However, they struggle with tasks that require unbounded multi-step computation, such as adding integers (Brown et al., 2020) or executing programs (Austin et al., 2021). Surprisingly, we find that these same models are able to perform complex multistep computations—even in the few-shot regime—when asked to perform the operation “step by step”, showing the results of intermediate computations. In particular, we train Transformers to perform multi-step computations by asking them to emit intermediate computation steps into a “scratchpad”. On a series of increasingly complex tasks ranging from long addition to the execution of arbitrary programs, we show that scratchpads dramatically improve the ability of language models to perform multi-step computations.	92173d081b15824d22a9ef070e118744ceee8052	@['JournalArticle']{nye-etal-2021-show,  author = {Maxwell Nye and Anders Andreassen and Guy Gur-Ari and H. Michalewski and Jacob Austin and David Bieber and David Dohan and Aitor Lewkowycz and Maarten Bosma and D. Luan and Charles Sutton and Augustus Odena},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Show Your Work: Scratchpads for Intermediate Computation with Language Models},  volume = {abs/2112.00114},  year = {2021} }
Should You Mask 15% in Masked Language Modeling?	2022	http://www.semanticscholar.org/paper/2d439ec2c301d058bd4a8b4743328e3d9939625e	Surprisingly, it is found that masking up to 40% of input tokens can outperform the 15% baseline, and even masking 80% can preserve most of the performance, as measured by ﬁne-tuning on downstream tasks.	maybe	40	Masked language models conventionally use a masking rate of 15% due to the belief that more masking would provide insufﬁcient context to learn good representations, and less masking would make training too expensive. Surprisingly, we ﬁnd that masking up to 40% of input tokens can outperform the 15% baseline, and even masking 80% can preserve most of the performance, as measured by ﬁne-tuning on downstream tasks. Increasing the masking rates has two distinct effects, which we investigate through careful ablations: (1) A larger proportion of input tokens are corrupted, reducing the context size and creating a harder task, and (2) models perform more predictions, which beneﬁts training. We observe that larger models with more capacity to tackle harder tasks in particular favor higher masking rates. We also ﬁnd that even more sophisticated masking schemes such as span masking or PMI masking can beneﬁt from higher masking rates, albeit to a smaller extent. Our results contribute to a better understanding of masked language modeling and shed light on more efﬁcient language pre-training. 1	2d439ec2c301d058bd4a8b4743328e3d9939625e	@['JournalArticle']{wettig-etal-2022-should,  author = {Alexander Wettig and Tianyu Gao and Zexuan Zhong and Danqi Chen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Should You Mask 15% in Masked Language Modeling?},  volume = {abs/2202.08005},  year = {2022} }
Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning	2021	http://www.semanticscholar.org/paper/5b91b9b3a28cd774fcacad2f21130fe731bddb41	A study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items.	yes	18	Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as spurious correlations and shallow shortcuts that in some cases are leveraging these outstanding results. In this paper we seek to further pursue this analysis into the realm of commonsense related language processing tasks. We undertake a study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items. The results obtained indicate that most datasets experimented with are problematic, with models resorting to non-robust features and appearing not to be learning and generalizing towards the overall tasks intended to be conveyed or exemplified by the datasets.	5b91b9b3a28cd774fcacad2f21130fe731bddb41	@['JournalArticle', 'Conference']{branco-etal-2021-shortcutted,  author = {Ruben Branco and A. Branco and J. Rodrigues and J. Silva},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1504-1521},  title = {Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning},  year = {2021} }
Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey	2022	http://www.semanticscholar.org/paper/a16eb6b93e37cdb56988194df78f2b722d8f0157	Methods to identify shortcut learning behavior in LLMs, characterize the reasons for shortcut learning, as well as introduce mitigation solutions are introduced and key challenges are identified.	maybe	3	Large language models (LLMs) have achieved state-of-the-art performance on a series of natural language understanding tasks. However, these LLMs might rely on dataset bias and artifacts as shortcuts for prediction. This has signiﬁcantly hurt their Out-of-Distribution (OOD) generalization and adversarial robustness. In this paper, we provide a review of recent developments that address the robustness challenge of LLMs. We ﬁrst introduce the concepts and robustness challenge of LLMs. We then introduce methods to identify shortcut learning behavior in LLMs, characterize the reasons for shortcut learning, as well as introduce mitigation solutions. Finally, we identify key challenges and introduce the connections of this line of research to other directions.	a16eb6b93e37cdb56988194df78f2b722d8f0157	@['JournalArticle', 'Review']{du-etal-2022-shortcut,  author = {Mengnan Du and Fengxiang He and Na Zou and Dacheng Tao and Xia Hu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Shortcut Learning of Large Language Models in Natural Language Understanding: A Survey},  volume = {abs/2208.11857},  year = {2022} }
Shortcut Learning in Deep Neural Networks	2020	http://www.semanticscholar.org/paper/1b04936c2599e59b120f743fbb30df2eed3fd782	A set of recommendations for model interpretation and benchmarking is developed, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.	yes	743		1b04936c2599e59b120f743fbb30df2eed3fd782	@['JournalArticle']{geirhos-etal-2020-shortcut,  author = {Robert Geirhos and J. Jacobsen and Claudio Michaelis and R. Zemel and Wieland Brendel and M. Bethge and Felix Wichmann},  booktitle = {Nature Machine Intelligence},  journal = {Nat. Mach. Intell.},  pages = {665-673},  title = {Shortcut Learning in Deep Neural Networks},  volume = {2},  year = {2020} }
Sharing Our Concepts with Machines	2021	http://www.semanticscholar.org/paper/3cfaf17d103002831e67300250d15d5f5d074944	This paper considers whether either language models such as GPT-3 or chatbots might be able to understand language, focusing on the question of whether they could possess the relevant concepts.	maybe	0		3cfaf17d103002831e67300250d15d5f5d074944	@None{butlin-2021-sharing,  author = {Patrick Butlin},  booktitle = {Erkenntnis: An International Journal of Scientific Philosophy},  journal = {Erkenntnis},  title = {Sharing Our Concepts with Machines},  year = {2021} }
Shared computational principles for language processing in humans and deep language models	2022	http://www.semanticscholar.org/paper/b4206dd288958affeb314aee0ec1397de5c74c23		maybe	45		b4206dd288958affeb314aee0ec1397de5c74c23	@['JournalArticle']{goldstein-etal-2022-shared,  author = {Ariel Goldstein and Zaid Zada and Eliav Buchnik and Mariano Schain and A. Price and Bobbi Aubrey and Samuel A. Nastase and Amir Feder and Dotan Emanuel and Alon Cohen and A. Jansen and H. Gazula and Gina Choe and Aditi Rao and Catherine Kim and Colton Casto and Lora Fanda and W. Doyle and D. Friedman and P. Dugan and L. Melloni and Roi Reichart and S. Devore and A. Flinker and Liat Hasenfratz and Omer Levy and Avinatan Hassidim and Michael Brenner and Y. Matias and K. Norman and O. Devinsky and U. Hasson},  booktitle = {Nature Neuroscience},  journal = {Nature Neuroscience},  pages = {369 - 380},  title = {Shared computational principles for language processing in humans and deep language models},  volume = {25},  year = {2022} }
Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations	2021	http://www.semanticscholar.org/paper/1d2bda7d9dc540cacae253bb23490faf3242d6a3	It is found that the syntactic sensitivity depends on the language and model pre-training objectives, and the sensitivity grows across layers together with the increase of the perturbation granularity, and it is shown that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.	yes	6	Recent research has adopted a new experimental field centered around the concept of text perturbations which has revealed that shuffled word order has little to no impact on the downstream performance of Transformer-based language models across many NLP tasks. These findings contradict the common understanding of how the models encode hierarchical and structural information and even question if the word order is modeled with position embeddings. To this end, this paper proposes nine probing datasets organized by the type of controllable text perturbation for three Indo-European languages with a varying degree of word order flexibility: English, Swedish and Russian. Based on the probing analysis of the M-BERT and M-BART models, we report that the syntactic sensitivity depends on the language and model pre-training objectives. We also find that the sensitivity grows across layers together with the increase of the perturbation granularity. Last but not least, we show that the models barely use the positional information to induce syntactic trees from their intermediate self-attention and contextualized representations.	1d2bda7d9dc540cacae253bb23490faf3242d6a3	@['JournalArticle']{taktasheva-etal-2021-shaking,  author = {Ekaterina Taktasheva and V. Mikhailov and E. Artemova},  booktitle = {MRL},  journal = {ArXiv},  title = {Shaking Syntactic Trees on the Sesame Street: Multilingual Probing with Controllable Perturbations},  volume = {abs/2109.14017},  year = {2021} }
Sequence Length is a Domain: Length-based Overfitting in Transformer Models	2021	http://www.semanticscholar.org/paper/64a0a4f357be12aaf30cc6e4964d1c3a9d927aac	Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training, and results suggest that the issue might be in the mismatch between the length distributions of the training and validation data combined with the tendency of the neural networks to overfit to the training data.	maybe	13	Transformer-based sequence-to-sequence architectures, while achieving state-of-the-art results on a large number of NLP tasks, can still suffer from overfitting during training. In practice, this is usually countered either by applying regularization methods (e.g. dropout, L2-regularization) or by providing huge amounts of training data. Additionally, Transformer and other architectures are known to struggle when generating very long sequences. For example, in machine translation, the neural-based systems perform worse on very long sequences when compared to the preceding phrase-based translation approaches (Koehn and Knowles, 2017). We present results which suggest that the issue might also be in the mismatch between the length distributions of the training and validation data combined with the aforementioned tendency of the neural networks to overfit to the training data. We demonstrate on a simple string editing tasks and a machine translation task that the Transformer model performance drops significantly when facing sequences of length diverging from the length distribution in the training data. Additionally, we show that the observed drop in performance is due to the hypothesis length corresponding to the lengths seen by the model during training rather than the length of the input sequence.	64a0a4f357be12aaf30cc6e4964d1c3a9d927aac	@['JournalArticle', 'Conference']{varis-bojar-2021-sequence,  author = {Dusan Varis and Ondrej Bojar},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {8246-8257},  title = {Sequence Length is a Domain: Length-based Overfitting in Transformer Models},  year = {2021} }
SentSpace: Large-Scale Benchmarking and Evaluation of Text using Cognitively Motivated Lexical, Syntactic, and Semantic Features	2022	http://www.semanticscholar.org/paper/fca7001dd2eee40615a8d0181ffb172d925242d6	It is found that while GPT2-XL-generated text appears fluent at the surface level, psycholinguistic norms and measures of syntactic processing reveal key differences between text produced by humans and machines.	yes	2	SentSpace is a modular framework for streamlined evaluation of text. SentSpacecharacterizes textual input using diverse lexical, syntactic, and semantic features derivedfrom corpora and psycholinguistic experiments. Core sentence features fall into three primaryfeature spaces: 1) Lexical, 2) Contextual, and 3) Embeddings. To aid in the analysis of computed features, SentSpace provides a web interface for interactive visualization and comparison with text from large corpora. The modular design of SentSpace allows researchersto easily integrate their own feature computation into the pipeline while benefiting from acommon framework for evaluation and visualization. In this manuscript we will describe thedesign of SentSpace, its core feature spaces, and demonstrate an example use case by comparing human-written and machine-generated (GPT2-XL) sentences to each other. We findthat while GPT2-XL-generated text appears fluent at the surface level, psycholinguistic normsand measures of syntactic processing reveal key differences between text produced by humansand machines. Thus, SentSpace provides a broad set of cognitively motivated linguisticfeatures for evaluation of text within natural language processing, cognitive science, as wellas the social sciences.	fca7001dd2eee40615a8d0181ffb172d925242d6	@['Conference']{tuckute-etal-2022-sentspace:,  author = {Greta Tuckute and Aalok Sathe and Mingye Wang and H. Yoder and Cory Shain and Evelina Fedorenko},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: System Demonstrations},  title = {SentSpace: Large-Scale Benchmarking and Evaluation of Text using Cognitively Motivated Lexical, Syntactic, and Semantic Features},  year = {2022} }
Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks	2018	https://www.semanticscholar.org/paper/b47381e04739ea3f392ba6c8faaf64105493c196	The benefits of supplementary training with further training on data-rich supervised tasks, such as natural language inference, obtain additional performance improvements on the GLUE benchmark, as well as observing reduced variance across random restarts in this setting.	seed	324	Pretraining sentence encoders with language modeling and related unsupervised tasks has recently been shown to be very effective for language understanding tasks. By supplementing language model-style pretraining with further training on data-rich supervised tasks, such as natural language inference, we obtain additional performance improvements on the GLUE benchmark. Applying supplementary training on BERT (Devlin et al., 2018), we attain a GLUE score of 81.8---the state of the art (as of 02/24/2019) and a 1.4 point improvement over BERT. We also observe reduced variance across random restarts in this setting. Our approach yields similar improvements when applied to ELMo (Peters et al., 2018a) and Radford et al. (2018)'s model. In addition, the benefits of supplementary training are particularly pronounced in data-constrained regimes, as we show in experiments with artificially limited training data.	b47381e04739ea3f392ba6c8faaf64105493c196	@['JournalArticle']{phang-etal-2018-sentence,  author = {Jason Phang and Thibault Févry and Samuel R. Bowman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks},  volume = {abs/1811.01088},  year = {2018} }
Sentence Ambiguity, Grammaticality and Complexity Probes	2022	https://www.semanticscholar.org/paper/a35520a1ac0fd6ac8ad1f406bfafb796830acf81	It is demonstrated that template-based datasets with surface-level ar-tifacts should not be used for probing, care-ful comparisons with baselines should be done and that t-SNE plots should be used to determine the presence of a feature among dense vectors representations.	maybe	0	It is unclear whether, how and where large pre-trained language models capture subtle linguistic traits like ambiguity, grammaticality and sentence complexity. We present results of automatic classification of these traits and compare their viability and patterns across representation types. We demonstrate that template-based datasets with surface-level artifacts should not be used for probing, careful comparisons with baselines should be done and that t-SNE plots should not be used to determine the presence of a feature among dense vectors representations. We also show how features might be highly localized in the layers for these models and get lost in the upper layers.	a35520a1ac0fd6ac8ad1f406bfafb796830acf81	@['JournalArticle']{bhattacharya-etal-2022-sentence,  author = {Sunit Bhattacharya and Vilém Zouhar and Ondvrej Bojar},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Sentence Ambiguity, Grammaticality and Complexity Probes},  volume = {abs/2210.06928},  year = {2022} }
SenseCluster at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection	2020	http://www.semanticscholar.org/paper/fc9247a76bdb2be3899368ec5bcdbc4782e4f269	This work proposes a simple method to detect lexical semantic change by clustering contextualized embeddings produced by XLM-R, using K-Means++, and hypothesizes that the main shortcoming of the method lies in the simplicity of the clustering method used.	maybe	6	We (Team Skurt) propose a simple method to detect lexical semantic change by clustering contextualized embeddings produced by XLM-R, using K-Means++. The basic idea is that contextualized embeddings that encode the same sense are located in close proximity in the embedding space. Our approach is both simple and generic, but yet performs relatively good in both sub-tasks of SemEval-2020 Task 1. We hypothesize that the main shortcoming of our method lies in the simplicity of the clustering method used.	fc9247a76bdb2be3899368ec5bcdbc4782e4f269	@['JournalArticle']{gyllensten-etal-2020-sensecluster,  author = {Amaru Cuba Gyllensten and Evangelia Gogoulou and Ariel Ekgren and Magnus Sahlgren},  booktitle = {International Workshop on Semantic Evaluation},  pages = {112-118},  title = {SenseCluster at SemEval-2020 Task 1: Unsupervised Lexical Semantic Change Detection},  year = {2020} }
Sense Embeddings are also Biased – Evaluating Social Biases in Static and Contextualised Sense Embeddings	2022	http://www.semanticscholar.org/paper/a69427573665e3bd376827953353cf0f6871bc46		maybe	4	Sense embedding learning methods learn different embeddings for the different senses of an ambiguous word. One sense of an ambiguous word might be socially biased while its other senses remain unbiased. In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings, the biases in sense embeddings have been relatively understudied. We create a benchmark dataset for evaluating the social biases in sense embeddings and propose novel sense-specific bias evaluation measures. We conduct an extensive evaluation of multiple static and contextualised sense embeddings for various types of social biases using the proposed measures. Our experimental results show that even in cases where no biases are found at word-level, there still exist worrying levels of social biases at sense-level, which are often ignored by the word-level bias evaluation measures.	a69427573665e3bd376827953353cf0f6871bc46	@['JournalArticle', 'Conference']{zhou-etal-2022-sense,  author = {Yi Zhou and Masahiro Kaneko and D. Bollegala},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1924-1935},  title = {Sense Embeddings are also Biased – Evaluating Social Biases in Static and Contextualised Sense Embeddings},  year = {2022} }
SemEval-2020 Task 4: Commonsense Validation and Explanation	2020	http://www.semanticscholar.org/paper/3e50d38ffc6cc01d4adeda5f35dbfdef2cc91dc6	This paper presents SemEval-2020 Task 4,CommonsenseValidation andExplanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish anatural language statement that makes senseto humans from one that does not, and provide thereasons.	maybe	63	In this paper, we present SemEval-2020 Task 4,CommonsenseValidation andExplanation(ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish anatural language statement thatmakes senseto humans from one that does not, and provide thereasons. Specifically, in our first subtask, the participating systems are required to choose from twonatural language statements of similar wording the one thatmakes senseand the one does not. Thesecond subtask additionally asks a system to select the key reason from three options why a givenstatement does not make sense. In the third subtask, a participating system needs to generate thereason automatically. 39 teams submitted their valid systems to at least one subtask. For SubtaskA and Subtask B, top-performing teams have achieved results closed to human performance.However, for Subtask C, there is still a considerable gap between system and human performance.The dataset used in our task can be found athttps://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation.	3e50d38ffc6cc01d4adeda5f35dbfdef2cc91dc6	@['JournalArticle']{wang-etal-2020-semeval,  author = {Cunxiang Wang and Shuailong Liang and Yili Jin and Yilong Wang and Xiaodan Zhu and Yue Zhang},  booktitle = {International Workshop on Semantic Evaluation},  pages = {307-321},  title = {SemEval-2020 Task 4: Commonsense Validation and Explanation},  year = {2020} }
SemAttack: Natural Textual Attacks via Different Semantic Spaces	2022	http://www.semanticscholar.org/paper/a9ce00dac14e47a68b4eefcf86b3ea6a64374a76	SemAttack optimizes the gen- 010 erated perturbations constrained on generic spaces, including typo space, knowl- 012 edge space, contextualized semantic space, or the combination of these spaces, so that the generated adversarial texts are more semantically close to the original inputs.	maybe	3	Recent studies show that pre-trained language 001 models (LMs) are vulnerable to textual adver- 002 sarial attacks. However, existing attack meth- 003 ods either suffer from low attack success rates 004 or fail to search efﬁciently in the exponentially 005 large perturbation space. We propose an efﬁ- 006 cient and effective framework SemAttack to 007 generate natural adversarial text by construct- 008 ing different semantic perturbation functions. 009 In particular, SemAttack optimizes the gen- 010 erated perturbations constrained on generic se- 011 mantic spaces, including typo space, knowl- 012 edge space ( e.g. , WordNet), contextualized 013 semantic space ( e.g. , the embedding space 014 of BERT clusterings), or the combination of 015 these spaces. Thus, the generated adversar- 016 ial texts are more semantically close to the 017 original inputs. Extensive experiments re- 018 veal that state-of-the-art (SOTA) large-scale 019 LMs ( e.g., DeBERTa-v2) and defense strate- 020 gies ( e.g., FreeLB) are still vulnerable to 021 SemAttack . We further demonstrate that 022 SemAttack is general and able to gener- 023 ate natural adversarial texts for different lan- 024 guages ( e.g., English and Chinese) with high 025 attack success rates. Human evaluations also 026 conﬁrm that our generated adversarial texts are 027 natural and barely affect human performance. 028	a9ce00dac14e47a68b4eefcf86b3ea6a64374a76	@['JournalArticle']{wang-etal-2022-semattack:,  author = {Boxin Wang and Chejian Xu and Xiangyu Liu and Yuk-Kit Cheng and Bo Li},  booktitle = {NAACL-HLT},  journal = {ArXiv},  title = {SemAttack: Natural Textual Attacks via Different Semantic Spaces},  volume = {abs/2205.01287},  year = {2022} }
Semantics is Actually 82% Distributional, but Neural Networks Aren’t.	2022	http://www.semanticscholar.org/paper/e15d47bdc45b5c354218c3227a46a912be5def6b		maybe	0	Distributional semantics is often proposed as 001 the linguistic theory underpinning many of 002 the most efficient current NLP systems. In 003 the present paper, we question the linguistic 004 well-foundedness of these models, addressing 005 it from the perspective of distributional substi- 006 tution. To that end, we provide a dataset of 007 human judgments on the distributional hypoth- 008 esis, and highlight how humans cannot system- 009 atically distinguish pairs of words solely from 010 contextual information. We stress that earlier 011 static embedding architectures are competitive 012 with more modern contextual embeddings on 013 the distributional substitution task, and that nei- 014 ther serve as good models of human linguistic 015 behavior. 016	e15d47bdc45b5c354218c3227a46a912be5def6b	@None{2022-semantics,  title = {Semantics is Actually 82% Distributional, but Neural Networks Aren’t.},  year = {2022} }
Semantic Structure in Deep Learning	2021	https://www.semanticscholar.org/paper/d32bf978b2ec7bd33ca7f9a23921fb49f63ffaaa	This article summarizes the current understanding of how well deep learning-based linguistic representations capture lexical semantics, world knowledge, and composition and encourages increased collaboration on testing the implications of such representations as general-purpose models of semantics.	maybe	8	Deep learning has recently come to dominate computational linguistics, leading to claims of human-level performance in a range of language processing tasks. Like much previous computational work, deep learning–based linguistic representations adhere to the distributional meaning-in-use hypothesis, deriving semantic representations from word co-occurrence statistics. However, current deep learning methods entail fundamentally new models of lexical and compositional meaning that are ripe for theoretical analysis. Whereas traditional distributional semantics models take a bottom-up approach in which sentence meaning is characterized by explicit composition functions applied to word meanings, new approaches take a top-down approach in which sentence representations are treated as primary and representations of words and syntax are viewed as emergent. This article summarizes our current understanding of how well such representations capture lexical semantics, world knowledge, and composition. The goal is to foster increased collaboration on testing the implications of such representations as general-purpose models of semantics. Expected final online publication date for the Annual Review of Linguistics, Volume 8 is January 2022. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.	d32bf978b2ec7bd33ca7f9a23921fb49f63ffaaa	@['Review']{pavlick-2021-semantic,  author = {Elizabeth-Jane Pavlick},  booktitle = {Annual Review of Linguistics},  journal = {Annual Review of Linguistics},  title = {Semantic Structure in Deep Learning},  year = {2021} }
Semantic coherence markers: The contribution of perplexity metrics	2022	http://www.semanticscholar.org/paper/d17a21d5c53035f64a85247d6eb25139ffd95a95	The experiments show that irrespective of the complexity of the employed language model, perplexity scores are stable and sufficiently consistent for analyzing the language of individual subjects, and at the same time sensitive enough to capture differences due to linguistic registers adopted by the same speaker, e.g., in interviews and political rallies.	maybe	1		d17a21d5c53035f64a85247d6eb25139ffd95a95	@['JournalArticle']{colla-etal-2022-semantic,  author = {Davide Colla and Matteo Delsanto and Marco Agosto and B. Vitiello and Daniele P. Radicioni},  booktitle = {Artif. Intell. Medicine},  journal = {Artificial intelligence in medicine},  pages = {           102393         },  title = {Semantic coherence markers: The contribution of perplexity metrics},  volume = {134},  year = {2022} }
Self-Repetition in Abstractive Neural Summarizers	2022	http://www.semanticscholar.org/paper/effeb7a31330929dbda68df674a7c29cd0f44c15	This work provides a quantitative and qualitative analysis of self-repetition in the output of neural summarizers, finding systems produce artefacts such as ads and disclaimers unrelated to the content being summarized, as well as formulaic phrases common in the fine-tuning domain.	maybe	0	We provide a quantitative and qualitative analysis of self-repetition in the output of neural summarizers. We measure self-repetition as the number of n-grams of length four or longer that appear in multiple outputs of the same system. We analyze the behavior of three popular architectures (BART, T5, and Pegasus), fine-tuned on five datasets. In a regression analysis, we find that the three architectures have different propensities for repeating content across output summaries for inputs, with BART being particularly prone to self-repetition. Fine-tuning on more abstractive data, and on data featuring formulaic language is associated with a higher rate of self-repetition. In qualitative analysis, we find systems produce artefacts such as ads and disclaimers unrelated to the content being summarized, as well as formulaic phrases common in the fine-tuning domain. Our approach to corpus-level analysis of self-repetition may help practitioners clean up training data for summarizers and ultimately support methods for minimizing the amount of self-repetition.	effeb7a31330929dbda68df674a7c29cd0f44c15	@['JournalArticle']{salkar-etal-2022-self,  author = {Nikita Salkar and T. Trikalinos and Byron C. Wallace and A. Nenkova},  booktitle = {AACL},  pages = {341-350},  title = {Self-Repetition in Abstractive Neural Summarizers},  year = {2022} }
Self-Attention Attribution: Interpreting Information Interactions Inside Transformer	2020	http://www.semanticscholar.org/paper/02465d57f63a2d8ed4082136d8e1b7db300105d2	This paper applies self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation, and extracts the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer.	maybe	65	The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.	02465d57f63a2d8ed4082136d8e1b7db300105d2	@['JournalArticle', 'Conference']{hao-etal-2020-self,  author = {Y. Hao and Li Dong and Furu Wei and Ke Xu},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {12963-12971},  title = {Self-Attention Attribution: Interpreting Information Interactions Inside Transformer},  year = {2020} }
Selection Induced Collider Bias: A Gender Pronoun Uncertainty Case Study	2022	http://www.semanticscholar.org/paper/e4f2d4bed6c8ffafae08144d6231d13e4d518eff	A method is developed for empirical measurement of spurious associations between gender and gender-neutral entities for unmodified large language models, detecting previously unreported spurious correlations and a lightweight method to exploit the resulting spurious associations for prediction task uncertainty classification.	maybe	0	In this paper, we cast the problem of task underspecification in causal terms, and develop a method for empirical measurement of spurious associations between gender and gender-neutral entities for unmodified large language models, detecting previously unreported spurious correlations. We then describe a lightweight method to exploit the resulting spurious associations for prediction task uncertainty classification, achieving over 90% accuracy on a Winogender Schemas challenge set. Finally, we generalize our approach to address a wider range of prediction tasks and provide open-source demos for each method described here.	e4f2d4bed6c8ffafae08144d6231d13e4d518eff	@None{mcmilin-2022-selection,  author = {Emily McMilin},  title = {Selection Induced Collider Bias: A Gender Pronoun Uncertainty Case Study},  year = {2022} }
Scientific and Creative Analogies in Pretrained Language Models	2022	http://www.semanticscholar.org/paper/933f60dda5847f208d9d3fd65e9b0df9cfed2403	This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2, and test the analogical reasoning capabilities of several widely-used pre-trained language models (LMs).	maybe	0	This paper examines the encoding of analogy in large-scale pretrained language models, such as BERT and GPT-2. Existing analogy datasets typically focus on a limited set of analogical relations, with a high similarity of the two domains between which the analogy holds. As a more realistic setup, we introduce the S cientiﬁc and C reative An alogy dataset (SCAN), a novel analogy dataset containing systematic mappings of multiple attributes and relational structures across dissimilar domains. Using this dataset, we test the analogical reasoning capabilities of several widely-used pre-trained language models (LMs). We ﬁnd that state-of-the-art LMs achieve low performance on these complex analogy tasks, highlighting the challenges still posed by analogy understanding.	933f60dda5847f208d9d3fd65e9b0df9cfed2403	@['JournalArticle']{czinczoll-etal-2022-scientific,  author = {Tamara Czinczoll and H. Yannakoudakis and Pushkar Mishra and Ekaterina Shutova},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Scientific and Creative Analogies in Pretrained Language Models},  volume = {abs/2211.15268},  year = {2022} }
Schr\"odinger's Tree -- On Syntax and Neural Language Models	2021	http://www.semanticscholar.org/paper/8c5488623b9a012909f3bbdda508b538a7bb544c	A lack of clarity across numerous dimensions is observed, which influences the hypotheses that researchers form, as well as the conclusions they draw from their findings, and it is urged that researchers make careful considerations when investigating coding properties, selecting representations, and evaluating via downstream tasks.	maybe	1	In the last half-decade, the field of natural language processing (NLP) has undergone two major transitions: the switch to neural networks as the primary modeling paradigm and the homogenization of the training regime (pre-train, then fine-tune). Amidst this process, language models have emerged as NLP’s workhorse, displaying increasingly fluent generation capabilities and proving to be an indispensable means of knowledge transfer downstream. Due to the otherwise opaque, black-box nature of such models, researchers have employed aspects of linguistic theory in order to characterize their behavior. Questions central to syntax — the study of the hierarchical structure of language — have factored heavily into such work, shedding invaluable insights about models’ inherent biases and their ability to make human-like generalizations. In this paper, we attempt to take stock of this growing body of literature. In doing so, we observe a lack of clarity across numerous dimensions, which influences the hypotheses that researchers form, as well as the conclusions they draw from their findings. To remedy this, we urge researchers make careful considerations when investigating coding properties, selecting representations, and evaluating via downstream tasks. Furthermore, we outline the implications of the different types of research questions exhibited in studies on syntax, as well as the inherent pitfalls of aggregate metrics. Ultimately, we hope that our discussion adds nuance to the prospect of studying language models and paves the way for a less monolithic perspective on syntax in this context.	8c5488623b9a012909f3bbdda508b538a7bb544c	@None{kulmizev-nivre-2021-schr\"odinger's,  author = {Artur Kulmizev and Joakim Nivre},  title = {Schr\"odinger's Tree -- On Syntax and Neural Language Models},  year = {2021} }
Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?	2022	http://www.semanticscholar.org/paper/6edccbd83a9aae204785d4821f97855677c33866		maybe	15	There have been a lot of interest in the scaling properties of Transformer models (Kaplan et al., 2020). However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this inﬂuence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can ﬂuctuate at different scales. We believe that the ﬁndings outlined in this work has signiﬁcant implications to how model architectures are currently evaluated in the community.	6edccbd83a9aae204785d4821f97855677c33866	@['JournalArticle']{tay-etal-2022-scaling,  author = {Yi Tay and M. Dehghani and Samira Abnar and Hyung Won Chung and W. Fedus and J. Rao and Sharan Narang and V. Tran and Dani Yogatama and Donald Metzler},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},  volume = {abs/2207.10551},  year = {2022} }
Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments	2022	http://www.semanticscholar.org/paper/e404bdfaa858b3c25540aa5d2c5dfe20c16ead37	It is found that scaling laws emerge at netuning time in some NLP tasks, and that they can also be ex-ploited for debugging convergence when training large models.	maybe	3	Neural scaling laws deﬁne a predictable rela-tionship between a model’s parameter count and its performance after training in the form of a power law. However, most research to date has not explicitly investigated whether scaling laws can be used to accelerate model development. In this work, we perform such an empirical investigation across a wide range of language understanding tasks, starting from models with as few as 10K parameters, and evaluate downstream performance across 9 language understanding tasks. We ﬁnd that scaling laws emerge at ﬁnetuning time in some NLP tasks, and that they can also be ex-ploited for debugging convergence when training large models. Moreover, for tasks where scaling laws exist, they can be used to predict the performance of larger models, which enables effective model selection. However, revealing scaling laws requires careful hyperparameter tuning and multiple runs for the purpose of uncertainty estimation, which incurs additional overhead, partially offsetting the computational beneﬁts.	e404bdfaa858b3c25540aa5d2c5dfe20c16ead37	@['JournalArticle']{ivgi-etal-2022-scaling,  author = {Maor Ivgi and Y. Carmon and Jonathan Berant},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Scaling Laws Under the Microscope: Predicting Transformer Performance from Small Scale Experiments},  volume = {abs/2202.06387},  year = {2022} }
Scaling Laws for Neural Language Models	2020	https://www.semanticscholar.org/paper/e6c561d02500b2596a230b341a8eb8b921ca5bf2	Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.	yes	773	We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.	e6c561d02500b2596a230b341a8eb8b921ca5bf2	@['JournalArticle']{kaplan-etal-2020-scaling,  author = {J. Kaplan and Sam McCandlish and T. Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeff Wu and Dario Amodei},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Scaling Laws for Neural Language Models},  volume = {abs/2001.08361},  year = {2020} }
Scaling Laws for Autoregressive Generative Modeling	2020	http://www.semanticscholar.org/paper/3efbcfeeb0ea1051a71101d3318da4411081f0b8	The case that scaling laws have important implications for neural network performance, including on downstream tasks is strengthened, as empirical scaling laws for the cross-entropy loss are identified.	maybe	126	We identify empirical scaling laws for the cross-entropy loss in four domains: generative image modeling, video modeling, multimodal image$\leftrightarrow$text models, and mathematical problem solving. In all cases autoregressive Transformers smoothly improve in performance as model size and compute budgets increase, following a power-law plus constant scaling law. The optimal model size also depends on the compute budget through a power-law, with exponents that are nearly universal across all data domains. The cross-entropy loss has an information theoretic interpretation as $S($True$) + D_{\mathrm{KL}}($True$||$Model$)$, and the empirical scaling laws suggest a prediction for both the true data distribution's entropy and the KL divergence between the true and model distributions. With this interpretation, billion-parameter Transformers are nearly perfect models of the YFCC100M image distribution downsampled to an $8\times 8$ resolution, and we can forecast the model size needed to achieve any given reducible loss (ie $D_{\mathrm{KL}}$) in nats/image for other resolutions. We find a number of additional scaling laws in specific domains: (a) we identify a scaling relation for the mutual information between captions and images in multimodal models, and show how to answer the question "Is a picture worth a thousand words?"; (b) in the case of mathematical problem solving, we identify scaling laws for model performance when extrapolating beyond the training distribution; (c) we finetune generative image models for ImageNet classification and find smooth scaling of the classification loss and error rate, even as the generative loss levels off. Taken together, these results strengthen the case that scaling laws have important implications for neural network performance, including on downstream tasks.	3efbcfeeb0ea1051a71101d3318da4411081f0b8	@['JournalArticle']{henighan-etal-2020-scaling,  author = {T. Henighan and J. Kaplan and Mor Katz and Mark Chen and Christopher Hesse and Jacob Jackson and Heewoo Jun and Tom B. Brown and Prafulla Dhariwal and Scott Gray and Chris Hallacy and Benjamin Mann and Alec Radford and A. Ramesh and Nick Ryder and Daniel M. Ziegler and J. Schulman and Dario Amodei and Sam McCandlish},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Scaling Laws for Autoregressive Generative Modeling},  volume = {abs/2010.14701},  year = {2020} }
Scaling Laws and Interpretability of Learning from Repeated Data	2022	https://www.semanticscholar.org/paper/aa4d9972af3264d032dbee58501ed4ac49477103	It is shown that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization.	maybe	5	Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintention-ally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a fam-ily of models where most of the data is unique but a small fraction of it is repeated many times. We ﬁnd a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1% of the data 100 times, despite the other 90% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model’s capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work — attempting to reverse engineer the detailed computations performed by the model — by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.	aa4d9972af3264d032dbee58501ed4ac49477103	@['JournalArticle']{hernandez-etal-2022-scaling,  author = {Danny Hernandez and Tom B. Brown and Tom Conerly and Nova DasSarma and Dawn Drain and S. El-Showk and Nelson Elhage and Zac Hatfield-Dodds and T. Henighan and Tristan Hume and Scott Johnston and Benjamin Mann and C. Olah and Catherine Olsson and Dario Amodei and Nicholas Joseph and Jared Kaplan and Sam McCandlish},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Scaling Laws and Interpretability of Learning from Repeated Data},  volume = {abs/2205.10487},  year = {2022} }
Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers	2021	http://www.semanticscholar.org/paper/2d4f66046bb436864cd6bf589e3a931c405f9f44	It is shown that aside from only the model size, model shape matters for downstream fine-tuning, and scaling protocols operate differently at different compute regions, which means widely adopted T5-base and T4-large sizes are Pareto-inefficient.	maybe	39	There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. (2020) presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50% fewer parameters and training 40% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.	2d4f66046bb436864cd6bf589e3a931c405f9f44	@['JournalArticle']{tay-etal-2021-scale,  author = {Yi Tay and M. Dehghani and J. Rao and W. Fedus and Samira Abnar and Hyung Won Chung and Sharan Narang and Dani Yogatama and Ashish Vaswani and Donald Metzler},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers},  volume = {abs/2109.10686},  year = {2021} }
SafeText: A Benchmark for Exploring Physical Safety in Language Models	2022	http://www.semanticscholar.org/paper/2b6291eb76e2ff885238e94704bb795046d7d530	It is argued that state-of-the-art large language models are susceptible to the generation of unsafe text and haveulty rejecting unsafe advice, and it is argued for further studies of safety and the assessment of commonsense physical safety in models before release.	yes	2	Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.	2b6291eb76e2ff885238e94704bb795046d7d530	@['JournalArticle', 'Conference']{levy-etal-2022-safetext:,  author = {Sharon Levy and Emily Allaway and Melanie Subbiah and Lydia B. Chilton and D. Patton and K. McKeown and William Yang Wang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {SafeText: A Benchmark for Exploring Physical Safety in Language Models},  volume = {abs/2210.10045},  year = {2022} }
RuleBERT: Teaching Soft Rules to Pre-Trained Language Models	2021	http://www.semanticscholar.org/paper/e55391a9406245584b3e5b3225dad2e171b9a06b	This work introduces a classification task where, given facts and soft rules, thePLM should return a prediction with a probability for a given hypothesis, and proposes a revised loss function that enables the PLM to learn how to predict precise probabilities for the task.	maybe	9	While pre-trained language models (PLMs) are the go-to solution to tackle many natural language processing problems, they are still very limited in their ability to capture and to use common-sense knowledge. In fact, even if information is available in the form of approximate (soft) logical rules, it is not clear how to transfer it to a PLM in order to improve its performance for deductive reasoning tasks. Here, we aim to bridge this gap by teaching PLMs how to reason with soft Horn rules. We introduce a classification task where, given facts and soft rules, the PLM should return a prediction with a probability for a given hypothesis. We release the first dataset for this task, and we propose a revised loss function that enables the PLM to learn how to predict precise probabilities for the task. Our evaluation results show that the resulting fine-tuned models achieve very high performance, even on logical rules that were unseen at training. Moreover, we demonstrate that logical notions expressed by the rules are transferred to the fine-tuned model, yielding state-of-the-art results on external datasets.	e55391a9406245584b3e5b3225dad2e171b9a06b	@['JournalArticle', 'Conference']{saeed-etal-2021-rulebert:,  author = {Mohammed Saeed and N. Ahmadi and Preslav Nakov and Paolo Papotti},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1460-1476},  title = {RuleBERT: Teaching Soft Rules to Pre-Trained Language Models},  year = {2021} }
Roles and Utilization of Attention Heads in Transformer-based Neural Language Models	2020	http://www.semanticscholar.org/paper/5b6d03ed66473599ee31872b3cd5ad2ce282371f		maybe	15	Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.	5b6d03ed66473599ee31872b3cd5ad2ce282371f	@['JournalArticle', 'Conference']{jo-myaeng-2020-roles,  author = {Jae-young Jo and Sung-Hyon Myaeng},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3404-3417},  title = {Roles and Utilization of Attention Heads in Transformer-based Neural Language Models},  year = {2020} }
Robustness of Learning from Task Instructions	2022	http://www.semanticscholar.org/paper/238bf07c7ed3b480875ccc93ab3caab51c095340	This work investigates the system robustness when the instructions of new tasks are maliciously manipulated, paraphrased, or from different levels of conciseness, the first work that systematically studies how robust a PLM is when it is supervised by instructions with different factors of variability.	maybe	0	Traditional supervised learning mostly works on individual tasks and requires training on a large set of task-speciﬁc examples. This paradigm seriously hinders the development of task generalization since preparing a task-speciﬁc example set is costly. To build a system that can quickly and easily generalize to new tasks, task instructions have been adopted as an emerging trend of supervision recently. These instructions give the model the deﬁnition of the task and allow the model to output the appropriate answer based on the instructions and inputs. However, task instructions are often expressed in different forms, which can be interpreted from two threads: ﬁrst, some instructions are short sentences and are pretrained language model (PLM) oriented, such as prompts, while other instructions are paragraphs and are human-oriented, such as those in Amazon MTurk; second, different end-users very likely explain the same task with instructions of different textual expressions. A robust system for task generalization should be able to handle any new tasks regardless of the variability of instructions. However, the system robustness in dealing with instruction-driven task generalization is still unexplored. This work investigates the system robustness when the instructions of new tasks are (i) maliciously manipulated, (ii) paraphrased, or (iii) from different levels of conciseness. To our knowledge, this is the ﬁrst work that systematically studies how robust a PLM is when it is supervised by instructions with different factors of variability. 1	238bf07c7ed3b480875ccc93ab3caab51c095340	@['JournalArticle']{gu-etal-2022-robustness,  author = {Jiasheng Gu and Hanzi Xu and Liang Nie and Wenpeng Yin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Robustness of Learning from Task Instructions},  volume = {abs/2212.03813},  year = {2022} }
RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning	2022	http://www.semanticscholar.org/paper/28144bdbeeff5f86c93a2b22b30c2ad210305cca	R OBUST LR is proposed, a suite of evaluation datasets that evaluate the robustness of Transformers models to minimal logical edits in rulebases and some standard logical equivalence conditions, and demonstrates some short-comings of the deductive reasoning-based language models.	maybe	3	Transformers have been shown to be able to perform deductive reasoning on a logical rulebase containing rules and statements written in English natural language. While the progress is promising, it is currently unclear if these models indeed perform logical reasoning by understanding the underlying logical semantics in the language. To this end, we propose R OBUST LR, a suite of evaluation datasets that evaluate the robustness of these models to minimal logical edits in rulebases and some standard logical equivalence conditions. In our experiments with RoBERTa and T5, we ﬁnd that the models trained in prior works do not perform consistently on the different perturbations in R OBUST LR, thus showing that the models are not robust to the proposed logical perturbations. Further, we ﬁnd that the models ﬁnd it especially hard to learn logical negation and disjunction operators. Overall, using our evaluation sets, we demonstrate some short-comings of the deductive reasoning-based language models, which can eventually help towards designing better models for logical reasoning over natural language. of RoBERTa-Large and T5-Large on Logical Equivalence sets. We report the weighted-F1 score. Overall, we ﬁnd that the performance drops for contrapositive equivalences, while it remains consistent on the other two. Please refer to Section 6.2 for more details.	28144bdbeeff5f86c93a2b22b30c2ad210305cca	@['JournalArticle']{sanyal-etal-2022-robustlr:,  author = {Soumya Sanyal and Zeyi Liao and Xiang Ren},  booktitle = {ArXiv},  journal = {ArXiv},  title = {RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning},  volume = {abs/2205.12598},  year = {2022} }
Robust Lottery Tickets for Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/b2322a59bd5b49d920d41128934192f77f5def0d	This work proposes a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs, and designs an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well both in accuracy and robustness.	maybe	4	Recent works on Lottery Ticket Hypothesis have shown that pre-trained language models (PLMs) contain smaller matching subnetworks(winning tickets) which are capable of reaching accuracy comparable to the original models. However, these tickets are proved to be notrobust to adversarial examples, and even worse than their PLM counterparts. To address this problem, we propose a novel method based on learning binary weight masks to identify robust tickets hidden in the original PLMs. Since the loss is not differentiable for the binary mask, we assign the hard concrete distribution to the masks and encourage their sparsity using a smoothing approximation of L0 regularization.Furthermore, we design an adversarial loss objective to guide the search for robust tickets and ensure that the tickets perform well bothin accuracy and robustness. Experimental results show the significant improvement of the proposed method over previous work on adversarial robustness evaluation.	b2322a59bd5b49d920d41128934192f77f5def0d	@['JournalArticle', 'Conference']{zheng-etal-2022-robust,  author = {Rui Zheng and Bao Rong and Yuhao Zhou and Di Liang and Sirui Wang and Wei Wu and Tao Gui and Qi Zhang and Xuanjing Huang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Robust Lottery Tickets for Pre-trained Language Models},  volume = {abs/2211.03013},  year = {2022} }
Robust Conversational Agents against Imperceptible Toxicity Triggers	2022	http://www.semanticscholar.org/paper/ee5a743129e5785b92aff156a947ca8c6beabbbc	This work proposes attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, and establishes the generalizability of such a defense mechanism on language generation models beyond Conversational agents.	yes	1	Warning: this paper contains content that maybe offensive or upsetting.Recent research in Natural Language Processing (NLP) has advanced the development of various toxicity detection models with the intention of identifying and mitigating toxic language from existing systems. Despite the abundance of research in this area, less attention has been given to adversarial attacks that force the system to generate toxic language and the defense against them. Existing work to generate such attacks is either based on human-generated attacks which is costly and not scalable or, in case of automatic attacks, the attack vector does not conform to human-like language, which can be detected using a language model loss. In this work, we propose attacks against conversational agents that are imperceptible, i.e., they fit the conversation in terms of coherency, relevancy, and fluency, while they are effective and scalable, i.e., they can automatically trigger the system into generating toxic language. We then propose a defense mechanism against such attacks which not only mitigates the attack but also attempts to maintain the conversational flow. Through automatic and human evaluations, we show that our defense is effective at avoiding toxic language generation even against imperceptible toxicity triggers while the generated language fits the conversation in terms of coherency and relevancy. Lastly, we establish the generalizability of such a defense mechanism on language generation models beyond conversational agents.	ee5a743129e5785b92aff156a947ca8c6beabbbc	@['JournalArticle', 'Conference']{mehrabi-etal-2022-robust,  author = {Ninareh Mehrabi and A. Beirami and Fred Morstatter and A. Galstyan},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2831-2847},  title = {Robust Conversational Agents against Imperceptible Toxicity Triggers},  year = {2022} }
Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference	2019	https://www.semanticscholar.org/paper/42ed4a9994e6121a9f325f5b901c5b3d7ce104f5	There is substantial room for improvement in NLI systems, and the HANS dataset can motivate and measure progress in this area, which contains many examples where the heuristics fail.	seed	732	A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.	42ed4a9994e6121a9f325f5b901c5b3d7ce104f5	@['JournalArticle', 'Conference']{mccoy-etal-2019-right,  author = {R. Thomas McCoy and Ellie Pavlick and Tal Linzen},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},  volume = {abs/1902.01007},  year = {2019} }
Richer Countries and Richer Representations	2022	http://www.semanticscholar.org/paper/dc80420810ab5828a53fcbf60fd385b7d2f672e9		maybe	2	We examine whether some countries are more richly represented in embedding space than others. We find that countries whose names occur with low frequency in training corpora are more likely to be tokenized into subwords, are less semantically distinct in embedding space, and are less likely to be correctly predicted: e.g., Ghana (the correct answer and in-vocabulary) is not predicted for, “The country producing the most cocoa is [MASK].”. Although these performance discrepancies and representational harms are due to frequency, we find that frequency is highly correlated with a country’s GDP; thus perpetuating historic power and wealth inequalities. We analyze the effectiveness of mitigation strategies; recommend that researchers report training word frequencies; and recommend future work for the community to define and design representational guarantees.	dc80420810ab5828a53fcbf60fd385b7d2f672e9	@['JournalArticle']{zhou-etal-2022-richer,  author = {Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},  booktitle = {Findings},  pages = {2074-2085},  title = {Richer Countries and Richer Representations},  year = {2022} }
RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms	2020	http://www.semanticscholar.org/paper/011869f932f89d047ce2bd36d73a95cc04888193	A new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations and shows that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks.	maybe	23	Pre-trained language models (PTLMs) have achieved impressive performance on commonsense inference benchmarks, but their ability to employ commonsense to make robust inferences, which is crucial for effective communications with humans, is debated. In the pursuit of advancing fluid human-AI communication, we propose a new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations. To generate data for this challenge, we develop a systematic and scalable procedure using commonsense knowledge bases and probe PTLMs across two different evaluation settings. Extensive experiments on our generated probe sets with more than 10k statements show that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks. We also find that fine-tuning on similar statements offer limited gains, as PTLMs still fail to generalize to unseen inferences. Our new large-scale benchmark exposes a significant gap between PTLMs and human-level language understanding and offers a new challenge for PTLMs to demonstrate commonsense.	011869f932f89d047ce2bd36d73a95cc04888193	@['JournalArticle', 'Conference']{zhou-etal-2020-rica:,  author = {Pei Zhou and Rahul Khanna and Bill Yuchen Lin and Daniel Ho and J. Pujara and Xiang Ren},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {7560-7579},  title = {RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms},  year = {2020} }
Revisiting Sanity Checks for Saliency Maps	2021	http://www.semanticscholar.org/paper/6c21f3d5fd64172d7082cd8fb7b10cc9a7fd43a5	This work challenges the utility of the sanity check methodology, and highlights that saliency map evaluation beyond ad-hoc visual examination remains a fundamental challenge.	maybe	4	Saliency methods are a popular approach for model debugging and explainability. However, in the absence of ground-truth data for what the correct maps should be, evaluating and comparing different approaches remains a long-standing challenge. The sanity checks methodology of Adebayo et al [Neurips 2018] has sought to address this challenge. They argue that some popular saliency methods should not be used for explainability purposes since the maps they produce are not sensitive to the underlying model that is to be explained. Through a causal re-framing of their objective, we argue that their empirical evaluation does not fully establish these conclusions, due to a form of confounding introduced by the tasks they evaluate on. Through various experiments on simple custom tasks we demonstrate that some of their conclusions may indeed be artifacts of the tasks more than a criticism of the saliency methods themselves. More broadly, our work challenges the utility of the sanity check methodology, and further highlights that saliency map evaluation beyond ad-hoc visual examination remains a fundamental challenge.	6c21f3d5fd64172d7082cd8fb7b10cc9a7fd43a5	@['JournalArticle']{yona-greenfeld-2021-revisiting,  author = {G. Yona and D. Greenfeld},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Revisiting Sanity Checks for Saliency Maps},  volume = {abs/2110.14297},  year = {2021} }
Revisiting Representation Degeneration Problem in Language Modeling	2020	http://www.semanticscholar.org/paper/23ee26733a4aac635cc36683d89852f979995b34	This paper revisits the representation degeneration problem and proposes an alternative regularization method called Laplacian regularization to tackle the problem, and proves that the cosine regularization is insufficient to solve the problem.	maybe	11	Weight tying is now a common setting in many language generation tasks such as language modeling and machine translation. However, a recent study reveals that there is a potential flaw in weight tying. They find that the learned word embeddings are likely to degenerate and lie in a narrow cone when training a language model. They call it the representation degeneration problem and propose a cosine regularization to solve it. Nevertheless, we prove that the cosine regularization is insufficient to solve the problem, as the degeneration is still likely to happen under certain conditions. In this paper, we revisit the representation degeneration problem and theoretically analyze the limitations of the previously proposed solution. Afterward, we propose an alternative regularization method called Laplacian regularization to tackle the problem. Experiments on language modeling demonstrate the effectiveness of the proposed Laplacian regularization.	23ee26733a4aac635cc36683d89852f979995b34	@['JournalArticle']{zhang-etal-2020-revisiting,  author = {Zhong Zhang and Chongming Gao and Cong Xu and Rui Miao and Qinli Yang and Junming Shao},  booktitle = {Findings},  pages = {518-527},  title = {Revisiting Representation Degeneration Problem in Language Modeling},  year = {2020} }
Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach	2022	http://www.semanticscholar.org/paper/225aa16f2e024c8170a022744f528634c3adc5a8	It is argued that PTM’s inherent ability for generative commonsense reasoning is underestimated due to the order-agnostic property of its input, and proposed a pre-ordering approach to elaborately manipulate the order of the given concepts before generation.	yes	1	Pre-trained models (PTMs) have lead to great improvements in natural language generation (NLG). However, it is still unclear how much commonsense knowledge they possess. With the goal of evaluating commonsense knowledge of NLG models, recent work has proposed the problem of generative commonsense reasoning, e.g., to compose a logical sentence given a set of unordered concepts. Existing approaches to this problem hypothesize that PTMs lack sufﬁcient parametric knowledge for this task, which can be overcome by introducing external knowledge or task-speciﬁc pre-training objectives. Different from this trend, we argue that PTM’s inherent ability for generative commonsense reasoning is underestimated due to the order-agnostic property of its input. In particular, we hypothesize that the order of the input concepts can affect the PTM’s ability to utilize its commonsense knowledge. To this end, we propose a pre-ordering approach to elaborately manipulate the order of the given concepts before generation. Experiments show that our approach can outperform the more sophisticated models that have access to a lot of external data and resources.	225aa16f2e024c8170a022744f528634c3adc5a8	@['JournalArticle']{zhao-etal-2022-revisiting,  author = {Chao Zhao and Faeze Brahman and Tenghao Huang and Snigdha Chaturvedi},  booktitle = {NAACL-HLT},  journal = {ArXiv},  title = {Revisiting Generative Commonsense Reasoning: A Pre-Ordering Approach},  volume = {abs/2205.13183},  year = {2022} }
Revisiting Attention Weights as Explanations from an Information Theoretic Perspective	2022	http://www.semanticscholar.org/paper/b2ec2d7d0b74d570b79de0862c8921aae1fa1642	It is indicated that attention mechanisms do have the potential to function as a shortcut to model explanations when they are carefully combined with other model elements.	maybe	0	Attention mechanisms have recently demonstrated impressive performance on a range of NLP tasks, and attention scores are often used as a proxy for model explainability. However, there is a debate on whether attention weights can, in fact, be used to identify the most important inputs to a model. We approach this question from an information theoretic perspective by measuring the mutual information between the model output and the hidden states. From extensive experiments, we draw the following conclusions: (i) Additive and Deep attention mechanisms are likely to be better at preserving the information between the hidden states and the model output (compared to Scaled Dot-product ); (ii) ablation studies indicate that Additive attention can actively learn to explain the importance of its input hidden representations; (iii) when attention values are nearly the same, the rank order of attention values is not consistent with the rank order of the mutual information (iv) Using Gumbel-Softmax with a temperature lower than one, tends to produce a more skewed attention score distribution compared to softmax and hence is a better choice for explainable design; (v) some building blocks are better at preserving the correlation between the ordered list of mutual information and attention weights order (for eg. the combination of BiLSTM encoder and Additive attention). Our ﬁndings indicate that attention mechanisms do have the potential to function as a shortcut to model explanations when they are carefully combined with other model elements.	b2ec2d7d0b74d570b79de0862c8921aae1fa1642	@['JournalArticle']{wen-etal-2022-revisiting,  author = {Bingyang Wen and K.P. Subbalakshmi and Fan Yang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Revisiting Attention Weights as Explanations from an Information Theoretic Perspective},  volume = {abs/2211.07714},  year = {2022} }
Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings	2021	http://www.semanticscholar.org/paper/e4dba3af805515c6a239c7136f3869930d1b3d2c	This paper gives a method for taking 018 OR or NOT of the meaning by linear opera- 019 tion of word embedding as an answer to (ii) and confirms experimentally that the accuracy of AND operation can be improved by the post-processing method and that OR and NOT op- 025 erations can be performed correctly.	maybe	0	It is well-known that typical word embedding 001 methods have the property that the meaning 002 can be composed by adding up the embeddings 003 (additive compositionality). Several theories 004 have been proposed to explain additive compo- 005 sitionality, but the following problems remain: 006 (i) The assumptions of those theories do not 007 hold for the practical word embedding. (ii) Or- 008 dinary additive compositionality can be seen 009 as an AND operation of word meanings, but 010 it is not well understood how other operations, 011 such as OR and NOT, can be computed by the 012 embeddings. We address these issues by the 013 idea of frequency-weighted centering at its core. 014 This method bridges the gap between practical 015 word embedding and the assumption of theory 016 about additive compositionality as an answer to 017 (i). This paper also gives a method for taking 018 OR or NOT of the meaning by linear opera- 019 tion of word embedding as an answer to (ii). 020 Moreover, we confirm experimentally that the 021 accuracy of AND operation, i.e., the ordinary 022 additive compositionality, can be improved by 023 our post-processing method (3.5x improvement 024 in top-100 accuracy) and that OR and NOT op- 025 erations can be performed correctly. We also 026 confirm that the proposed method is effective 027 for BERT. 028	e4dba3af805515c6a239c7136f3869930d1b3d2c	@['JournalArticle']{naito-etal-2021-revisiting,  author = {Masahiro Naito and Sho Yokoi and Geewook Kim and Hidetoshi Shimodaira},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Revisiting Additive Compositionality: AND, OR and NOT Operations with Word Embeddings},  volume = {abs/2105.08585},  year = {2021} }
Revealing the Dark Secrets of BERT	2019	https://www.semanticscholar.org/paper/d78aed1dac6656affa4a04cbf225ced11a83d103	It is shown that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models, indicating the overall model overparametrization.	seed	337	BERT-based architectures currently give state-of-the-art performance on many NLP tasks, but little is known about the exact mechanisms that contribute to its success. In the current work, we focus on the interpretation of self-attention, which is one of the fundamental underlying components of BERT. Using a subset of GLUE tasks and a set of handcrafted features-of-interest, we propose the methodology and carry out a qualitative and quantitative analysis of the information encoded by the individual BERT’s heads. Our findings suggest that there is a limited set of attention patterns that are repeated across different heads, indicating the overall model overparametrization. While different heads consistently use the same attention patterns, they have varying impact on performance across different tasks. We show that manually disabling attention in certain heads leads to a performance improvement over the regular fine-tuned BERT models.	d78aed1dac6656affa4a04cbf225ced11a83d103	@['JournalArticle', 'Conference']{kovaleva-etal-2019-revealing,  author = {Olga Kovaleva and Alexey Romanov and Anna Rogers and Anna Rumshisky},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4364-4373},  title = {Revealing the Dark Secrets of BERT},  year = {2019} }
Revealing Secrets From Pre-trained Models	2022	http://www.semanticscholar.org/paper/fb6802bbd6f4f67fbaafac41ba31697712b8e525	A new observation that pre-trained models and ﬁne-tuned models have signiﬁcantly high similarities in weight values is shown and the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning.	maybe	0	—With the growing burden of training deep learning models with large data sets, transfer-learning has been widely adopted in many emerging deep learning algorithms. Trans- former models such as BERT are the main player in natural language processing and use transfer-learning as a de facto standard training method. A few big data companies release pre-trained models that are trained with a few popular datasets with which end users and researchers ﬁne-tune the model with their own datasets. Transfer-learning signiﬁcantly reduces the time and effort of training models. However, it comes at the cost of security concerns. In this paper, we show a new observation that pre-trained models and ﬁne-tuned models have signiﬁcantly high similarities in weight values. Also, we demonstrate that there exist vendor-speciﬁc computing patterns even for the same models. With these new ﬁndings, we propose a new model extraction attack that reveals the model architecture and the pre-trained model used by the black-box victim model with vendor-speciﬁc computing patterns and then estimates the entire model weights based on the weight value similarities between the ﬁne-tuned model and pre-trained model. We also show that the weight similarity can be leveraged for increasing the model extraction feasibility through a novel weight extraction pruning. ,	fb6802bbd6f4f67fbaafac41ba31697712b8e525	@['JournalArticle']{rafi-etal-2022-revealing,  author = {Mujahid Al Rafi and Yuan Feng and Hyeran Jeon},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Revealing Secrets From Pre-trained Models},  volume = {abs/2207.09539},  year = {2022} }
Revealing Persona Biases in Dialogue Systems	2021	http://www.semanticscholar.org/paper/34be38f7f18f3fb4a58256ddf96365f2934551dd	It is observed that adopting personas can actually decrease harmful responses, compared to not using any personas, and it is found that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deployment.	maybe	14	Dialogue systems in the form of chatbots and personal as- sistants are being increasingly integrated into people’s lives. Modern dialogue systems may consider adopting anthropo- morphic personas, mimicking societal demographic groups to appear more approachable and trustworthy to users. However, the adoption of a persona can result in the adoption of biases. In this paper, we present the ﬁrst large-scale study on persona biases in dialogue systems and conduct analyses on personas of different social classes, sexual orientations, races, and gen- ders. We deﬁne persona biases as harmful differences in responses (e.g., varying levels of offensiveness, agreement with harmful statements) generated from adopting different demographic personas. Furthermore, we introduce an open-source framework, U NIT P ERSONA B IAS , to explore and aggregate persona biases in dialogue systems. By analyzing the Blender and DialoGPT dialogue systems, we observe that adopting personas can actually decrease harmful responses, compared to not using any personas. Additionally, we ﬁnd that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deploy-ment. We also analyze how personas can result in different amounts of harm towards speciﬁc demographics.	34be38f7f18f3fb4a58256ddf96365f2934551dd	@['JournalArticle']{sheng-etal-2021-revealing,  author = {Emily Sheng and Josh Arnold and Zhou Yu and Kai-Wei Chang and Nanyun Peng},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Revealing Persona Biases in Dialogue Systems},  volume = {abs/2104.08728},  year = {2021} }
Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale	2022	http://www.semanticscholar.org/paper/ac9bd7da3331d699008444925ef1121cc7178dc3	The hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components is investigated and a small set of attention heads score highly on their ability to perform primitive induction operations associated with in- context learning, namely, preﬁx matching and copying.	yes	0	Language models have been shown to perform better with an increase in scale on a wide variety of tasks via the in-context learning paradigm. In this paper, we investigate the hypothesis that the ability of a large language model to in-context learn-perform a task is not uniformly spread across all of its underlying components. Using a 66 billion pa-rameter language model (OPT-66B) across a diverse set of 14 downstream tasks, we ﬁnd this is indeed the case: ∼ 70% of attention heads and ∼ 20% of feed forward networks can be removed with minimal decline in task performance. We ﬁnd substantial overlap in the set of attention heads (un)important for in-context learning across tasks and number of in-context examples. We also address our hypothesis through a task-agnostic lens, ﬁnding that a small set of attention heads in OPT-66B score highly on their ability to perform primitive induction operations associated with in-context learning, namely, preﬁx matching and copying. These induction heads overlap with task-speciﬁc important heads, suggesting that induction heads are among the heads capable of more sophisticated behaviors associated with in-context learning. Overall, our study provides several insights that indicate large language models may be under-trained to perform in-context learning and opens up questions on how to pre-train language models to more effectively perform in-context learning.	ac9bd7da3331d699008444925ef1121cc7178dc3	@['JournalArticle']{bansal-etal-2022-rethinking,  author = {Hritik Bansal and Karthik Gopalakrishnan and Saket Dingliwal and S. Bodapati and Katrin Kirchhoff and D. Roth},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Rethinking the Role of Scale for In-Context Learning: An Interpretability-based Case Study at 66 Billion Scale},  volume = {abs/2212.09095},  year = {2022} }
Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?	2022	http://www.semanticscholar.org/paper/87e02a265606f31e65986f3c1c448a3e3a3a066e	This paper shows that ground truth demonstrations are in fact not required and that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of the label space, the distribution of the input text, and the overall format of the sequence.	maybe	95	Large language models (LMs) are able to in-context learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.	87e02a265606f31e65986f3c1c448a3e3a3a066e	@['JournalArticle', 'Conference']{min-etal-2022-rethinking,  author = {Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and M. Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},  volume = {abs/2202.12837},  year = {2022} }
Rethinking Attention-Model Explainability through Faithfulness Violation Test	2022	http://www.semanticscholar.org/paper/5fdb05e17fd503b8dbdbadc338e0a00829929dcc		yes	5	Attention mechanisms are dominating the explainability of deep models. They produce probability distributions over the input, which are widely deemed as feature-importance indicators. How-ever, in this paper, we ﬁnd one critical limitation in attention explanations: weakness in identifying the polarity of feature impact. This would be somehow misleading – features with higher attention weights may not faithfully contribute to model predictions; instead, they can impose suppression effects. With this ﬁnding, we reﬂect on the explainability of current attention-based techniques, such as Attention (cid:12) Gradient and LRP-based attention explanations. We ﬁrst propose an actionable diagnostic methodology (henceforth faithfulness violation test ) to measure the consistency between explanation weights and the impact polarity. Through the extensive experiments, we then show that most tested explanation methods are unexpectedly hindered by the faithfulness violation issue, especially the raw attention. Empiri-cal analyses on the factors affecting violation is-sues further provide useful observations for adopt-ing explanation methods in attention models.	5fdb05e17fd503b8dbdbadc338e0a00829929dcc	@['JournalArticle', 'Conference']{liu-etal-2022-rethinking,  author = {Y. Liu and Haoliang Li and Yangyang Guo and Chen Kong and Jing Li and Shiqi Wang},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Rethinking Attention-Model Explainability through Faithfulness Violation Test},  volume = {abs/2201.12114},  year = {2022} }
Representation Degeneration Problem in Training Natural Language Generation Models	2019	http://www.semanticscholar.org/paper/55e960535f643637161b2e99a8c21a92c0d13757	This work analyzes the conditions and causes of the representation degeneration problem and proposes a novel regularization method that can largely mitigate the problem and achieve better performance than baseline algorithms.	maybe	97	We study an interesting problem in training neural network-based models for natural language generation tasks, which we call the \emph{representation degeneration problem}. We observe that when training a model for natural language generation tasks through likelihood maximization with the weight tying trick, especially with big training datasets, most of the learnt word embeddings tend to degenerate and be distributed into a narrow cone, which largely limits the representation power of word embeddings. We analyze the conditions and causes of this problem and propose a novel regularization method to address it. Experiments on language modeling and machine translation show that our method can largely mitigate the representation degeneration problem and achieve better performance than baseline algorithms.	55e960535f643637161b2e99a8c21a92c0d13757	@['JournalArticle']{gao-etal-2019-representation,  author = {Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tie-Yan Liu},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Representation Degeneration Problem in Training Natural Language Generation Models},  volume = {abs/1907.12009},  year = {2019} }
Representation and Pre-Activation of Lexical-Semantic Knowledge in Neural Language Models	2021	http://www.semanticscholar.org/paper/e05976bd4809f75ea7b22255db66d9d26b312f6d	A systematic analysis of how closely the intermediate layers from LSTM and trans former language models correspond to human semantic knowledge indicates that the transformer models are better at capturing semantic knowledge relating to lexical concepts, both during word prediction and when retention is required.	maybe	3	In this paper, we perform a systematic analysis of how closely the intermediate layers from LSTM and trans former language models correspond to human semantic knowledge. Furthermore, in order to make more meaningful comparisons with theories of human language comprehension in psycholinguistics, we focus on two key stages where the meaning of a particular target word may arise: immediately before the word’s presentation to the model (comparable to forward inferencing), and immediately after the word token has been input into the network. Our results indicate that the transformer models are better at capturing semantic knowledge relating to lexical concepts, both during word prediction and when retention is required.	e05976bd4809f75ea7b22255db66d9d26b312f6d	@['JournalArticle']{derby-etal-2021-representation,  author = {Steven Derby and Barry Devereux and Paul Miller},  booktitle = {Workshop on Cognitive Modeling and Computational Linguistics},  pages = {211-221},  title = {Representation and Pre-Activation of Lexical-Semantic Knowledge in Neural Language Models},  year = {2021} }
Relating Neural Text Degeneration to Exposure Bias	2021	https://www.semanticscholar.org/paper/a7e42bc1f09a5bf23258739a8d26d50b557986dd	The results show that text degeneration is likely to be partly caused by exposure bias, and the self-reinforcing mechanism of text degenerations is studied, explaining why the mistakes amplify.	maybe	4	This work focuses on relating two mysteries in neural-based text generation: exposure bias, and text degeneration. Despite the long time since exposure bias was mentioned and the numerous studies for its remedy, to our knowledge, its impact on text generation has not yet been verified. Text degeneration is a problem that the widely-used pre-trained language model GPT-2 was recently found to suffer from (Holtzman et al., 2020). Motivated by the unknown causation of the text degeneration, in this paper we attempt to relate these two mysteries. Specifically, we first qualitatively and quantitatively identify mistakes made before text degeneration occurs. Then we investigate the significance of the mistakes by inspecting the hidden states in GPT-2. Our results show that text degeneration is likely to be partly caused by exposure bias. We also study the self-reinforcing mechanism of text degeneration, explaining why the mistakes amplify. In sum, our study provides a more concrete foundation for further investigation on exposure bias and text degeneration problems.	a7e42bc1f09a5bf23258739a8d26d50b557986dd	@['JournalArticle']{chiang-chen-2021-relating,  author = {Ting-Rui Chiang and Yun-Nung Chen},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {228-239},  title = {Relating Neural Text Degeneration to Exposure Bias},  year = {2021} }
Reframing Instructional Prompts to GPTk’s Language	2021	http://www.semanticscholar.org/paper/3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b	This work studies several classes of reframing techniques for manual reformulation of prompts into more effective ones, and hopes these empirically-driven techniques will pave the way towards more effective future prompting algorithms.	maybe	54	What kinds of instructional prompts are easier to follow for Language Models (LMs)? We study this question by conducting extensive empirical analysis that shed light on important features of successful instructional prompts. Specifically, we study several classes of reframing techniques for manual reformulation of prompts into more effective ones. Some examples include decomposing a complex task instruction into multiple simpler tasks or itemizing instructions into sequential steps. Our experiments compare the zero-shot and few-shot performance of LMs prompted with reframed instructions on 12 NLP tasks across 6 categories. Compared with original instructions, our reframed instructions lead to significant improvements across LMs with different sizes. For example, the same reframed prompts boost few-shot performance of GPT3-series and GPT2-series by 12.5% and 6.7% respectively averaged over all tasks. Furthermore, reframed instructions reduce the number of examples required to prompt LMs in the few-shot setting. We hope these empirically-driven techniques will pave the way towards more effective future prompting algorithms.	3ac5aa6ac59253611ef3cb72a95cbe21ef5dda1b	@['JournalArticle']{mishra-etal-2021-reframing,  author = {Swaroop Mishra and Daniel Khashabi and Chitta Baral and Yejin Choi and Hannaneh Hajishirzi},  booktitle = {Findings},  pages = {589-612},  title = {Reframing Instructional Prompts to GPTk’s Language},  year = {2021} }
Refining Targeted Syntactic Evaluation of Language Models	2021	http://www.semanticscholar.org/paper/59c0adcddc2b0252411351ea92252f39ef69540f	It is found that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context and propose new metrics to capture each goal separately.	yes	16	Targeted syntactic evaluation of subject-verb number agreement in English (TSE) evaluates language models’ syntactic knowledge using hand-crafted minimal pairs of sentences that differ only in the main verb’s conjugation. The method evaluates whether language models rate each grammatical sentence as more likely than its ungrammatical counterpart. We identify two distinct goals for TSE. First, evaluating the systematicity of a language model’s syntactic knowledge: given a sentence, can it conjugate arbitrary verbs correctly? Second, evaluating a model’s likely behavior: given a sentence, does the model concentrate its probability mass on correctly conjugated verbs, even if only on a subset of the possible verbs? We argue that current implementations of TSE do not directly capture either of these goals, and propose new metrics to capture each goal separately. Under our metrics, we find that TSE overestimates systematicity of language models, but that models score up to 40% better on verbs that they predict are likely in context.	59c0adcddc2b0252411351ea92252f39ef69540f	@['JournalArticle', 'Conference']{newman-etal-2021-refining,  author = {Benjamin Newman and Kai-Siang Ang and Julia Gong and John Hewitt},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Refining Targeted Syntactic Evaluation of Language Models},  volume = {abs/2104.09635},  year = {2021} }
Reducing Transformer Depth on Demand with Structured Dropout	2019	https://www.semanticscholar.org/paper/f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1	LayerDrop, a form of structured dropout, is explored, which has a regularization effect during training and allows for efficient pruning at inference time, and shows that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance.	seed	311	Overparameterized transformer networks have obtained state of the art results in various natural language processing tasks, such as machine translation, language modeling, and question answering. These models contain hundreds of millions of parameters, necessitating a large amount of computation and making them prone to overfitting. In this work, we explore LayerDrop, a form of structured dropout, which has a regularization effect during training and allows for efficient pruning at inference time. In particular, we show that it is possible to select sub-networks of any depth from one large network without having to finetune them and with limited impact on performance. We demonstrate the effectiveness of our approach by improving the state of the art on machine translation, language modeling, summarization, question answering, and language understanding benchmarks. Moreover, we show that our approach leads to small BERT-like models of higher quality compared to training from scratch or using distillation.	f4a8480cffa491020bdbb8c4c4e7a7e923b1c2c1	@['JournalArticle']{fan-etal-2019-reducing,  author = {Angela Fan and Edouard Grave and Armand Joulin},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Reducing Transformer Depth on Demand with Structured Dropout},  volume = {abs/1909.11556},  year = {2019} }
Reducing Offensive Replies in Open Domain Dialogue Systems	2022	http://www.semanticscholar.org/paper/655a45678f2522d406c7606549bb3cf09683f385		maybe	0	In recent years, a series of open-domain dialogue systems using large-scale language models have been proposed. These dialogue systems are attracting business attention because these do signiﬁcantly natural and diverse dialogues with humans. How-ever, it has been noted that these dialogue systems reﬂect gender, race, and other biases inherent in the data and may generate offensive replies or replies that agree with offensive utterances. This study examined a dialogue system that outputs appropriate replies to offensive utterances. Speciﬁcally, our system incorpo-rates multiple dialogue models, each of which is specialized to suppress offensive replies in a speciﬁc category, then selects the most non-offensive reply from the outputs of the models. We evaluated the utility of our system when suppressing offensive replies of DialoGPT. We conﬁrmed ours reduces the offensive replies to less than 1%, whereas one of the state-of-the-art suppressing methods reduces to 9.8%.	655a45678f2522d406c7606549bb3cf09683f385	@['JournalArticle']{uchida-etal-2022-reducing,  author = {Naokazu Uchida and Takeshi Homma and Makoto Iwayama and Yasuhiro Sogawa},  booktitle = {Interspeech},  pages = {1076-1080},  title = {Reducing Offensive Replies in Open Domain Dialogue Systems},  year = {2022} }
Red Teaming Language Models with Language Models	2022	http://www.semanticscholar.org/paper/5d49c7401c5f2337c4cc88d243ae39ed659afe64	This work automatically finds cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM, and evaluates the target LM’s replies to generated test questions using a classifier trained to detect offensive content.	yes	52	Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (“red teaming”) using another LM. We evaluate the target LM’s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot’s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.	5d49c7401c5f2337c4cc88d243ae39ed659afe64	@['JournalArticle', 'Conference']{perez-etal-2022-red,  author = {Ethan Perez and Saffron Huang and Francis Song and Trevor Cai and Roman Ring and J. Aslanides and A. Glaese and Nathan McAleese and Geoffrey Irving},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Red Teaming Language Models with Language Models},  volume = {abs/2202.03286},  year = {2022} }
Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned	2022	https://www.semanticscholar.org/paper/17bcb1edbe068e8fe6a97da552c70a77a15bbce7	It is found that the RLHF models are increasinglycult to red team as they scale, and a trend with scale for the other model types is found, which indicates that this transparency accelerates the ability to work together as a community in order to develop shared norms, practices, and technical standards.	yes	9	We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We ﬁnd that the RLHF models are increasingly difﬁcult to red team as they scale, and we ﬁnd a ﬂat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and ﬁnd a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models. Warning: this paper contains examples that may be offensive or upsetting.	17bcb1edbe068e8fe6a97da552c70a77a15bbce7	@['JournalArticle']{ganguli-etal-2022-red,  author = {Deep Ganguli and Liane Lovitt and John Kernion and Amanda Askell and Yuntao Bai and Saurav Kadavath and Benjamin Mann and Ethan Perez and Nicholas Schiefer and Kamal Ndousse and Andy Jones and Sam Bowman and Anna Chen and Tom Conerly and Nova DasSarma and Dawn Drain and Nelson Elhage and S. El-Showk and Stanislav Fort and Z. Dodds and T. Henighan and Danny Hernandez and Tristan Hume and Josh Jacobson and Scott Johnston and S. Kravec and Catherine Olsson and Sam Ringer and Eli Tran-Johnson and Dario Amodei and Tom B. Brown and Nicholas Joseph and Sam McCandlish and C. Olah and Jared Kaplan and Jack Clark},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned},  volume = {abs/2209.07858},  year = {2022} }
Reconstruction Probing	2022	http://www.semanticscholar.org/paper/cf629b913aa944349d1c0a719af8fc697cae3cef	Reconstruction probing, a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs), finds that contextualization boosts reconstructabilty of tokens that are close to the token being reconstructed in terms of linear and syntactic distance.	maybe	0	We propose reconstruction probing , a new analysis method for contextualized representations based on reconstruction probabilities in masked language models (MLMs). This method relies on comparing the reconstruction probabilities of tokens in a given sequence when conditioned on the representation of a single token that has been fully contextualized and when conditioned on only the decon-textualized lexical prior of the model. This comparison can be understood as quantifying the contribution of contextualization towards reconstruction—the difference in the reconstruction probabilities can only be attributed to the representational change of the single token induced by contextualization. We apply this analysis to three MLMs and ﬁnd that contextualization boosts reconstructabilty of tokens that are close to the token being reconstructed in terms of linear and syntactic distance. Furthermore, we extend our analysis to ﬁner-grained decomposition of contextualized representations, and we ﬁnd that these boosts are largely attributable to static and positional embeddings at the input layer.	cf629b913aa944349d1c0a719af8fc697cae3cef	@['JournalArticle']{kim-etal-2022-reconstruction,  author = {Najoung Kim and Jatin Khilnani and Alex Warstadt and Abed Qaddoumi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Reconstruction Probing},  volume = {abs/2212.10792},  year = {2022} }
Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model	2022	http://www.semanticscholar.org/paper/2e4babcb96e25915d7087084bd6153f8ecfdcf3c	This paper decomposes the associated “transformations” into individual, functionally-specialized “attention heads” and demonstrates that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions.	maybe	7	Piecing together the meaning of a narrative requires understanding not only the individual words but also the intricate relationships between them. How does the brain construct this kind of rich, contextual meaning from natural language? Recently, a new class of artificial neural networks—based on the Transformer architecture—has revolutionized the field of language modeling. Transformers integrate information across words via multiple layers of structured circuit computations, forming increasingly contextualized representations of linguistic content. In this paper, we deconstruct these circuit computations and analyze the associated “transformations” (alongside the more commonly studied “embeddings”) at each layer to provide a fine-grained window onto linguistic computations in the human brain. Using functional MRI data acquired while participants listened to naturalistic spoken stories, we find that these transformations capture a hierarchy of linguistic computations across cortex, with transformations at later layers in the model mapping onto higher-level language areas in the brain. We then decompose these transformations into individual, functionally-specialized “attention heads” and demonstrate that the emergent syntactic computations performed by individual heads correlate with predictions of brain activity in specific cortical regions. These heads fall along gradients corresponding to different layers, contextual distances, and syntactic dependencies in a low-dimensional cortical space. Our findings provide a new basis for using the internal structure of large language models to better capture the cascade of cortical computations that support natural language comprehension.	2e4babcb96e25915d7087084bd6153f8ecfdcf3c	@None{kumar-etal-2022-reconstructing,  author = {Sreejan Kumar and T. Sumers and T. Yamakoshi and Ariel Goldstein and U. Hasson and K. Norman and T. Griffiths and Robert D. Hawkins and Samuel A. Nastase},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Reconstructing the cascade of language processing in the brain using the internal computations of a transformer-based language model},  year = {2022} }
Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning	2021	http://www.semanticscholar.org/paper/8424082e3bf4792462eb112d7ebcecf5b0dc3613	This survey paper discusses the performance of transformers on different reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning.	maybe	15	Recent years have seen impressive performance of transformer-based models on different natural language processing tasks. However, it is not clear to what degree the transformers can reason on natural language. To shed light on this question, this survey paper discusses the performance of transformers on different reasoning tasks, including mathematical reasoning, commonsense reasoning, and logical reasoning. We point out successes and limitations, of both empirical and theoretical nature.	8424082e3bf4792462eb112d7ebcecf5b0dc3613	@['JournalArticle', 'Review']{helwe-etal-2021-reasoning,  author = {Chadi Helwe and C. Clavel and Fabian M. Suchanek},  booktitle = {Conference on Automated Knowledge Base Construction},  title = {Reasoning with Transformer-based Models: Deep Learning, but Shallow Reasoning},  year = {2021} }
Reasoning with Language Model Prompting: A Survey	2022	http://www.semanticscholar.org/paper/6756fcd998caeb7b23702e08559e63710179334c	This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting and introduces research works with comparisons and summaries and provides systematic resources to help beginners.	maybe	3	Reasoning, as an essential ability for complex problem-solving, can provide back-end support for various real-world applications, such as medical diagnosis, negotiation, etc. This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting. We introduce research works with comparisons and summaries and provide systematic resources to help beginners. We also discuss the potential reasons for emerging such reasoning abilities and highlight future research directions 1 .	6756fcd998caeb7b23702e08559e63710179334c	@['JournalArticle', 'Review']{qiao-etal-2022-reasoning,  author = {Shuofei Qiao and Yixin Ou and Ningyu Zhang and Xiang Chen and Yunzhi Yao and Shumin Deng and Chuanqi Tan and Fei Huang and Huajun Chen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Reasoning with Language Model Prompting: A Survey},  volume = {abs/2212.09597},  year = {2022} }
RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models	2020	https://www.semanticscholar.org/paper/399e7d8129c60818ee208f236c8dda17e876d21f	It is found that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts, and empirically assess several controllable generation methods find that while data- or compute-intensive methods are more effective at steering away from toxicity than simpler solutions, no current method is failsafe against neural toxic degeneration.	yes	242	Pretrained neural language models (LMs) are prone to generating racist, sexist, or otherwise toxic language which hinders their safe deployment. We investigate the extent to which pretrained LMs can be prompted to generate toxic language, and the effectiveness of controllable text generation algorithms at preventing such toxic degeneration. We create and release RealToxicityPrompts, a dataset of 100K naturally occurring, sentence-level prompts derived from a large corpus of English web text, paired with toxicity scores from a widely-used toxicity classifier. Using RealToxicityPrompts, we find that pretrained LMs can degenerate into toxic text even from seemingly innocuous prompts. We empirically assess several controllable generation methods, and find that while data- or compute-intensive methods (e.g., adaptive pretraining on non-toxic data) are more effective at steering away from toxicity than simpler solutions (e.g., banning “bad” words), no current method is failsafe against neural toxic degeneration. To pinpoint the potential cause of such persistent toxic degeneration, we analyze two web text corpora used to pretrain several LMs (including GPT-2; Radford et. al, 2019), and find a significant amount of offensive, factually unreliable, and otherwise toxic content. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining.	399e7d8129c60818ee208f236c8dda17e876d21f	@['JournalArticle']{gehman-etal-2020-realtoxicityprompts:,  author = {Samuel Gehman and Suchin Gururangan and Maarten Sap and Yejin Choi and Noah A. Smith},  booktitle = {Findings},  journal = {ArXiv},  title = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},  volume = {abs/2009.11462},  year = {2020} }
Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text	2022	http://www.semanticscholar.org/paper/b118283afc5d8652de52cd13a5e287d76c5ec91f	There is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time, and the RoFT dataset is released: a collection of over 21,000 human annotations paired with error classiﬁcations to encourage future work in human detection and evaluation of generated text.	maybe	1	As text generated by large language models proliferates, it becomes vital to understand how humans engage with such text, and whether or not they are able to detect when the text they are reading did not originate with a human writer. Prior work on human detection of generated text focuses on the case where an entire passage is either human-written or machine-generated. In this paper, we study a more realistic setting where text begins as human-written and transitions to being generated by state-of-the-art neural language models. We show that, while annotators often struggle at this task, there is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time. Furthermore, we conduct a detailed comparison study and analyze how a variety of variables (model size, decoding strategy, ﬁne-tuning, prompt genre, etc.) aﬀect human detec- tion performance. Finally, we collect error annotations from our participants and use them to show that certain textual gen- res inﬂuence models to make diﬀerent types of errors and that certain sentence-level features correlate highly with annotator selection. We release the RoFT dataset: a collection of over 21,000 human annotations paired with error classiﬁcations to encourage future work in human detection and evaluation of generated text.	b118283afc5d8652de52cd13a5e287d76c5ec91f	@['JournalArticle']{dugan-etal-2022-real,  author = {Liam Dugan and Daphne Ippolito and Arun Kirubarajan and Sherry Shi and Chris Callison-Burch},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Real or Fake Text?: Investigating Human Ability to Detect Boundaries Between Human-Written and Machine-Generated Text},  volume = {abs/2212.12672},  year = {2022} }
Real Attackers Don't Compute Gradients: Bridging the Gap Between Adversarial ML Research and Practice	2022	http://www.semanticscholar.org/paper/639e39310d04c4e18700913d543fa7260c13ae5e	This position paper analyzes all adversarial ML papers recently published in top security conferences, highlighting positive trends and blind spots, and state positions on precise and cost-driven threat modeling, collaboration between industry and academia, and reproducible research.	maybe	0	—Recent years have seen a proliferation of research on adversarial machine learning . Numerous papers demonstrate powerful algorithmic attacks against a wide variety of machine learning (ML) models, and numerous other papers propose defenses that can withstand most attacks. However, abundant real-world evidence suggests that actual attackers use simple tactics to subvert ML-driven systems, and as a result security practitioners have not prioritized adversarial ML defenses. Motivated by the apparent gap between researchers and practitioners, this position paper aims to bridge the two domains. We first present three real-world case studies from which we can glean practical insights unknown or neglected in research. Next we analyze all adversarial ML papers recently published in top security conferences, highlighting positive trends and blind spots. Finally, we state positions on precise and cost-driven threat modeling, collaboration between industry and academia, and reproducible research. We believe that our positions, if adopted, will increase the real-world impact of future endeavours in adversarial ML, bringing both researchers and practitioners closer to their shared goal of improving the security of ML systems.	639e39310d04c4e18700913d543fa7260c13ae5e	@['JournalArticle']{apruzzese-etal-2022-"real,  author = {Giovanni Apruzzese and H. Anderson and Savino Dambra and D. Freeman and Fabio Pierazzi and Kevin A. Roundy},  booktitle = {ArXiv},  journal = {ArXiv},  title = {"Real Attackers Don't Compute Gradients": Bridging the Gap Between Adversarial ML Research and Practice},  volume = {abs/2212.14315},  year = {2022} }
RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)	2021	http://www.semanticscholar.org/paper/55d760c0c9000378edc53ad75f77a72bfc50dc88		maybe	8	Most words are ambiguous—-i.e., they convey distinct meanings in different contexts—-and even the meanings of unambiguous words are context-dependent. Both phenomena present a challenge for NLP. Recently, the advent of contextualized word embeddings has led to success on tasks involving lexical ambiguity, such as Word Sense Disambiguation. However, there are few tasks that directly evaluate how well these contextualized embeddings accommodate the more continuous, dynamic nature of word meaning—-particularly in a way that matches human intuitions. We introduce RAW-C, a dataset of graded, human relatedness judgments for 112 ambiguous words in context (with 672 sentence pairs total), as well as human estimates of sense dominance. The average inter-annotator agreement (assessed using a leave-one-annotator-out method) was 0.79. We then show that a measure of cosine distance, computed using contextualized embeddings from BERT and ELMo, correlates with human judgments, but that cosine distance also systematically underestimates how similar humans find uses of the same sense of a word to be, and systematically overestimates how similar humans find uses of different-sense homonyms. Finally, we propose a synthesis between psycholinguistic theories of the mental lexicon and computational models of lexical semantics.	55d760c0c9000378edc53ad75f77a72bfc50dc88	@['JournalArticle', 'Conference']{trott-bergen-2021-raw,  author = {Sean Trott and B. Bergen},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {RAW-C: Relatedness of Ambiguous Words in Context (A New Lexical Resource for English)},  volume = {abs/2105.13266},  year = {2021} }
Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers	2022	https://www.semanticscholar.org/paper/094a2e887a983b4891f3bb5b4cafd68500508d92	Not only do the models perform poorly on few -type quantiﬁers, but overall the larger the model, the worse its performance, and it is argued that decreasing performance of larger models may challenge uses of Language Models as the basis for Natural Language Systems.	maybe	0	Language Models appear to perform poorly on quantiﬁcation. We ask how badly. Few - type quantiﬁers, as in few children like vegeta-bles might pose a particular challenge for Language Models, since the sentence components without the quantiﬁer are likely to co-occur, and because few -type quantiﬁers are rare. We present 960 sentences stimuli from two human neurolinguistic experiments to 22 autoregressive transformer models of differing sizes. Not only do the models perform poorly on few -type quantiﬁers, but overall the larger the model, the worse its performance. We interpret this inverse scaling as suggesting that larger models increasingly reﬂect online rather than ofﬂine human processing, and argue that decreasing performance of larger models may challenge uses of Language Models as the basis for Natural Language Systems.	094a2e887a983b4891f3bb5b4cafd68500508d92	@['JournalArticle']{michaelov-bergen-2022-'rarely',  author = {J. Michaelov and B. Bergen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {'Rarely' a problem? Language models exhibit inverse scaling in their predictions following 'few'-type quantifiers},  volume = {abs/2212.08700},  year = {2022} }
Rare Words Degenerate All Words	2021	http://www.semanticscholar.org/paper/35ecd893e51a92e0b507bde73dc7ee7bfd2d6373	This study analyzes the intrinsic mechanism of the degeneration of rare word embeddings with respect of their gradient about the negative log-likelihood loss function and proposes a novel method, Adaptive Gradient Partial Scaling (AGPS), to address this problem.	maybe	0	Despite advances in neural network language model, the representation degeneration problem of embeddings is still challenging. Recent studies have found that the learned output embeddings are degenerated into a narrowcone distribution which makes the similarity between each embeddings positive. They analyzed the cause of the degeneration problem has been demonstrated as common to most embeddings. However, we found that the degeneration problem is especially originated from the training of embeddings of rare words. In this study, we analyze the intrinsic mechanism of the degeneration of rare word embeddings with respect of their gradient about the negative log-likelihood loss function. Furthermore, we theoretically and empirically demonstrate that the degeneration of rare word embeddings causes the degeneration of non-rare word embeddings, and that the overall degeneration problem can be alleviated by preventing the degeneration of rare word embeddings. Based on our analyses, we propose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the degeneration problem. Experimental results demonstrate the effectiveness of the proposed method qualitatively and quantitatively.	35ecd893e51a92e0b507bde73dc7ee7bfd2d6373	@['JournalArticle']{yu-etal-2021-rare,  author = {Sangwon Yu and Jongyoon Song and Heeseung Kim and SeongEun Lee and Woo-Jong Ryu and Sungroh Yoon},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Rare Words Degenerate All Words},  volume = {abs/2109.03127},  year = {2021} }
Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results	2018	https://www.semanticscholar.org/paper/8c0548331e02c2ead48d6c0380f9a80471ea5d80	A number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results are presented.	seed	61	“Based on theoretical reasoning it has been suggested that the reliability of findings published in the scientific literature decreases with the popularity of a research field” (Pfeiffer and Hoffmann, 2009). As we know, deep learning is very popular and the ability to reproduce results is an important part of science. There is growing concern within the deep learning community about the reproducibility of results that are presented. In this paper we present a number of controllable, yet unreported, effects that can substantially change the effectiveness of a sample model, and thusly the reproducibility of those results. Through these environmental effects we show that the commonly held belief that distribution of source code is all that is needed for reproducibility is not enough. Source code without a reproducible environment does not mean anything at all. In addition the range of results produced from these effects can be larger than the majority of incremental improvement reported.	8c0548331e02c2ead48d6c0380f9a80471ea5d80	@['JournalArticle']{crane-2018-questionable,  author = {M. Crane},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {241-252},  title = {Questionable Answers in Question Answering Research: Reproducibility and Variability of Published Results},  volume = {6},  year = {2018} }
Quantity doesn’t buy quality syntax with neural language models	2019	https://www.semanticscholar.org/paper/356645552f8f40adf5a99b4e3a69f47699399010	A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than the authors' LSTMs in some constructions, making the case for more data efficient architectures.	seed	49	Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.	356645552f8f40adf5a99b4e3a69f47699399010	@['JournalArticle', 'Conference']{schijndel-etal-2019-quantity,  author = {Marten van Schijndel and Aaron Mueller and Tal Linzen},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Quantity doesn’t buy quality syntax with neural language models},  volume = {abs/1909.00111},  year = {2019} }
Quantifying the Contextualization of Word Representations with Semantic Class Probing	2020	http://www.semanticscholar.org/paper/42b24ccc6f529ef9244f551a124943a900ed7471	This work quantifies the amount of contextualization, i.e., how well words are interpreted in context, by studying the extent to which semantic classes of a word can be inferred from its contextualized embedding.	maybe	19	Pretrained language models achieve state-of-the-art results on many NLP tasks, but there are still many open questions about how and why they work so well. We investigate the contextualization of words in BERT. We quantify the amount of contextualization, i.e., how well words are interpreted in context, by studying the extent to which semantic classes of a word can be inferred from its contextualized embedding. Quantifying contextualization helps in understanding and utilizing pretrained language models. We show that the top layer representations support highly accurate inference of semantic classes; that the strongest contextualization effects occur in the lower layers; that local context is mostly sufficient for contextualizing words; and that top layer representations are more task-specific after finetuning while lower layer representations are more transferable. Finetuning uncovers task-related features, but pretrained knowledge about contextualization is still well preserved.	42b24ccc6f529ef9244f551a124943a900ed7471	@['JournalArticle']{zhao-etal-2020-quantifying,  author = {Mengjie Zhao and Philipp Dufter and Yadollah Yaghoobzadeh and Hinrich Schütze},  booktitle = {Findings},  journal = {ArXiv},  title = {Quantifying the Contextualization of Word Representations with Semantic Class Probing},  volume = {abs/2004.12198},  year = {2020} }
Quantifying Social Biases Using Templates is Unreliable	2022	http://www.semanticscholar.org/paper/c8e78803159f353a82b5da3cb3c05f32e10a679f	It is shown that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution, and that bias values and resulting conclusions vary considerably across template modiﬁcations on four tasks.	maybe	2	Recently, there has been an increase in efforts to understand how large language models (LLMs) propagate and amplify social biases. Several works have utilized templates for fairness evaluation, which allow researchers to quantify social biases in the absence of test sets with protected attribute labels. While template evaluation can be a convenient and helpful diagnostic tool to understand model deﬁ-ciencies, it often uses a simplistic and limited set of templates. In this paper, we study whether bias measurements are sensitive to the choice of templates used for benchmarking. Speciﬁcally, we investigate the instability of bias measurements by manually modifying templates proposed in previous works in a semantically-preserving manner and measuring bias across these modiﬁcations. We ﬁnd that bias values and resulting conclusions vary considerably across template modiﬁcations on four tasks, ranging from an 81% reduction (NLI) to a 162% increase (MLM) in (task-speciﬁc) bias measurements. Our results indicate that quantifying fairness in LLMs, as done in current practice, can be brittle and needs to be approached with more care and caution.	c8e78803159f353a82b5da3cb3c05f32e10a679f	@['JournalArticle']{seshadri-etal-2022-quantifying,  author = {P. Seshadri and Pouya Pezeshkpour and Sameer Singh},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Quantifying Social Biases Using Templates is Unreliable},  volume = {abs/2210.04337},  year = {2022} }
Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics	2021	http://www.semanticscholar.org/paper/10aa2be24951e6de76b630482a645d79354c4cde	This work unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them and demonstrating that the observed differences in bias measurement can be systematically explained via differences in parameter choices for the authors' generalized metrics.	maybe	22	Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.	10aa2be24951e6de76b630482a645d79354c4cde	@['JournalArticle']{czarnowska-etal-2021-quantifying,  author = {Paula Czarnowska and Yogarshi Vyas and Kashif Shah},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1249-1267},  title = {Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics},  volume = {9},  year = {2021} }
Quantifying Social Biases in Contextual Word Representations	2019	http://www.semanticscholar.org/paper/3259d52ae00e65b98391e7e6a2f672dfee721bf8	A template-based method to quantify bias in BERT is proposed and it is shown that this method obtains more consistent results in capturing social biases than the traditional cosine based method.	maybe	19	Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.	3259d52ae00e65b98391e7e6a2f672dfee721bf8	@None{kurita-etal-2019-quantifying,  author = {Keita Kurita and Nidhi Vyas and Ayush Pareek and A. Black and Yulia Tsvetkov},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  title = {Quantifying Social Biases in Contextual Word Representations},  year = {2019} }
Quantifying Memorization Across Neural Language Models	2022	https://www.semanticscholar.org/paper/adea6aa83b847752940129185428ea61935a0027	It is found that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.	maybe	54	Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.	adea6aa83b847752940129185428ea61935a0027	@['JournalArticle']{carlini-etal-2022-quantifying,  author = {Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramèr and Chiyuan Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Quantifying Memorization Across Neural Language Models},  volume = {abs/2202.07646},  year = {2022} }
Quantifying Attention Flow in Transformers	2020	http://www.semanticscholar.org/paper/76a9f336481b39515d6cea2920696f11fb686451	This paper proposes two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when the authors use attention weights as the relative relevance of the input tokens.	maybe	254	In the Transformer model, “self-attention” combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.	76a9f336481b39515d6cea2920696f11fb686451	@['JournalArticle', 'Conference']{abnar-zuidema-2020-quantifying,  author = {Samira Abnar and W. Zuidema},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Quantifying Attention Flow in Transformers},  volume = {abs/2005.00928},  year = {2020} }
Quantifying and alleviating political bias in language models	2022	http://www.semanticscholar.org/paper/c3d208f80d9a6c8793ee3efd3ced8285d92f41d7		maybe	5		c3d208f80d9a6c8793ee3efd3ced8285d92f41d7	@['JournalArticle']{liu-etal-2022-quantifying,  author = {Ruibo Liu and Chenyan Jia and Jason Wei and Guangxuan Xu and Soroush Vosoughi},  booktitle = {Artificial Intelligence},  journal = {Artif. Intell.},  pages = {103654},  title = {Quantifying and alleviating political bias in language models},  volume = {304},  year = {2022} }
Putting Words in BERT’s Mouth: Navigating Contextualized Vector Spaces with Pseudowords	2021	http://www.semanticscholar.org/paper/cf8a7ba69339baeb0f62d697c322965cff2b38c5	Using a contextualized “pseudoword” as a stand-in for a static embedding in the input layer and then performing masked prediction of a word in the sentence, this work is able to investigate the geometry of the BERT-space in a controlled manner around individual instances.	maybe	7	We present a method for exploring regions around individual points in a contextualized vector space (particularly, BERT space), as a way to investigate how these regions correspond to word senses. By inducing a contextualized “pseudoword” vector as a stand-in for a static embedding in the input layer, and then performing masked prediction of a word in the sentence, we are able to investigate the geometry of the BERT-space in a controlled manner around individual instances. Using our method on a set of carefully constructed sentences targeting highly ambiguous English words, we find substantial regularity in the contextualized space, with regions that correspond to distinct word senses; but between these regions there are occasionally “sense voids”—regions that do not correspond to any intelligible sense.	cf8a7ba69339baeb0f62d697c322965cff2b38c5	@['JournalArticle', 'Conference']{karidi-etal-2021-putting,  author = {Taelin Karidi and Yichu Zhou and Nathan Schneider and Omri Abend and Vivek Srikumar},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Putting Words in BERT’s Mouth: Navigating Contextualized Vector Spaces with Pseudowords},  volume = {abs/2109.11491},  year = {2021} }
Putting representations to use	2022	http://www.semanticscholar.org/paper/5c51389867356a4d7a3466b68beba2d321951de9		yes	0		5c51389867356a4d7a3466b68beba2d321951de9	@None{cao-2022-putting,  author = {R. Cao},  booktitle = {Synthese},  journal = {Synthese},  title = {Putting representations to use},  volume = {200},  year = {2022} }
Putting GPT-3's Creativity to the (Alternative Uses) Test	2022	http://www.semanticscholar.org/paper/6a9d8a449b8326084786d81c92cdefd6beb5df10	It is believed it is only a matter of time before GPT-3 catches up on this particular task, and what this work reveals about human and AI creativity, creativity testing and the definition of creativity is discussed.	maybe	3	AI large language models have (co-)produced amazing written works from newspaper articles to novels and poetry. These works meet the standards of the standard definition of creativity: being original and useful, and sometimes even the additional element of surprise. But can a large language model designed to predict the next text fragment provide creative, out-of-the-box, responses that still solve the problem at hand? We put Open AI’s generative natural language model, GPT-3, to the test. Can it provide creative solutions to one of the most commonly used tests in creativity research? We assessed GPT-3’s creativity on Guilford’s Alternative Uses Test (AUT) and compared its performance to previously collected human responses on expert ratings of originality, usefulness and surprise of responses, flexibility of each set of ideas as well as an automated method to measure creativity based on the semantic distance between a response and the AUT object in question. Our results show that -on the whole- humans currently outperform GPT-3 when it comes to creative output. But, we believe it is only a matter of time before GPT-3 catches up on this particular task. We discuss what this work reveals about human and AI creativity, creativity testing and our definition of creativity.	6a9d8a449b8326084786d81c92cdefd6beb5df10	@['JournalArticle']{stevenson-etal-2022-putting,  author = {C. Stevenson and I. Smal and M. Baas and R. Grasman and H. V. D. Maas},  booktitle = {International Conference on Innovative Computing and Cloud Computing},  pages = {164-168},  title = {Putting GPT-3's Creativity to the (Alternative Uses) Test},  year = {2022} }
Psycholinguistic Diagnosis of Language Models’ Commonsense Reasoning	2022	http://www.semanticscholar.org/paper/914749611244842f11ad41fd070b596b38c24b17	Analysis of neural language models’ understanding of commonsense pragmatics through human behavioral and neurophysiological data suggests that GPT-3’s performance was mostly at chance in the psycholinguistic tasks, and shows that DistillBERT had some understanding of the (implied) intent.	yes	1	Neural language models have attracted a lot of attention in the past few years. More and more researchers are getting intrigued by how language models encode commonsense, specifically what kind of commonsense they understand, and why they do. This paper analyzed neural language models’ understanding of commonsense pragmatics (i.e., implied meanings) through human behavioral and neurophysiological data. These psycholinguistic tests are designed to draw conclusions based on predictive responses in context, making them very well suited to test word-prediction models such as BERT in natural settings. They can provide the appropriate prompts and tasks to answer questions about linguistic mechanisms underlying predictive responses. This paper adopted psycholinguistic datasets to probe language models’ commonsense reasoning. Findings suggest that GPT-3’s performance was mostly at chance in the psycholinguistic tasks. We also showed that DistillBERT had some understanding of the (implied) intent that’s shared among most people. Such intent is implicitly reflected in the usage of conversational implicatures and presuppositions. Whether or not fine-tuning improved its performance to human-level depends on the type of commonsense reasoning.	914749611244842f11ad41fd070b596b38c24b17	@None{cong-2022-psycholinguistic,  author = {Yan Cong},  booktitle = {CSRR},  journal = {Proceedings of the First Workshop on Commonsense Representation and Reasoning (CSRR 2022)},  title = {Psycholinguistic Diagnosis of Language Models’ Commonsense Reasoning},  year = {2022} }
Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?	2021	http://www.semanticscholar.org/paper/2bb1e1a5b9a16f6828fe94736cea5dab264533a6	This work formalizes ways in which ungrounded language models appear to be fundamentally limited in their ability to “understand”, and suggests that assertions in code or language do not provide sufficient signal to fully emulate semantic representations.	maybe	35	Abstract Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever “understand” raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of “assertions”: textual contexts that provide indirect clues about the underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation of languages that satisfy a strong notion of semantic transparency. However, for classes of languages where the same expression can take different values in different contexts, we show that emulation can become uncomputable. Finally, we discuss differences between our formal model and natural language, exploring how our results generalize to a modal setting and other semantic relations. Together, our results suggest that assertions in code or language do not provide sufficient signal to fully emulate semantic representations. We formalize ways in which ungrounded language models appear to be fundamentally limited in their ability to “understand”.	2bb1e1a5b9a16f6828fe94736cea5dab264533a6	@['JournalArticle']{merrill-etal-2021-provable,  author = {William Cooper Merrill and Yoav Goldberg and Roy Schwartz and Noah A. Smith},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1047-1060},  title = {Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?},  volume = {9},  year = {2021} }
Protecting Intellectual Property of Language Generation APIs with Lexical Watermark	2021	http://www.semanticscholar.org/paper/2569a7309142e40815cf556b6417059df9abbda8	This work presents a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs that achieves better identifiable performance in terms of p-value, with fewer semantic losses.	maybe	8	Nowadays, due to the breakthrough in natural language generation (NLG), including machine translation, document summarization, image captioning, etc NLG models have been encapsulated in cloud APIs to serve over half a billion people worldwide and process over one hundred billion word generations per day. Thus, NLG APIs have already become essential profitable services in many commercial companies. Due to the substantial financial and intellectual investments, service providers adopt a pay-as-you-use policy to promote sustainable market growth. However, recent works have shown that cloud platforms suffer from financial losses imposed by model extraction attacks, which aim to imitate the functionality and utility of the victim services, thus violating the intellectual property (IP) of cloud APIs. This work targets at protecting IP of NLG APIs by identifying the attackers who have utilized watermarked responses from the victim NLG APIs. However, most existing watermarking techniques are not directly amenable for IP protection of NLG APIs. To bridge this gap, we first present a novel watermarking method for text generation APIs by conducting lexical modification to the original outputs. Compared with the competitive baselines, our watermark approach achieves better identifiable performance in terms of p-value, with fewer semantic losses. In addition, our watermarks are more understandable and intuitive to humans than the baselines. Finally, the empirical studies show our approach is also applicable to queries from different domains, and is effective on the attacker trained on a mixture of the corpus which includes less than 10% watermarked samples.	2569a7309142e40815cf556b6417059df9abbda8	@['JournalArticle', 'Conference']{he-etal-2021-protecting,  author = {Xuanli He and Qiongkai Xu and L. Lyu and Fangzhao Wu and Chenguang Wang},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {10758-10766},  title = {Protecting Intellectual Property of Language Generation APIs with Lexical Watermark},  year = {2021} }
PROST: Physical Reasoning about Objects through Space and Time	2021	http://www.semanticscholar.org/paper/5aab57cc0530560d82c74c055f664280619d7e81	It is demonstrated that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted, and increasing the amount of pretraining data and parameters only yields minimal improvements.	maybe	11	We present a new probing dataset named PROST: Physical Reasoning about Objects Through Space and Time. This dataset contains 18,736 multiple-choice questions made from 14 manually curated templates, covering 10 physical reasoning concepts. All questions are designed to probe both causal and masked language models in a zero-shot setting. We conduct an extensive analysis which demonstrates that state-of-the-art pretrained models are inadequate at physical reasoning: they are influenced by the order in which answer options are presented to them, they struggle when the superlative in a question is inverted (e.g., most ↔ least), and increasing the amount of pretraining data and parameters only yields minimal improvements. These results provide support for the hypothesis that current pretrained models’ ability to reason about physical interactions is inherently limited by a lack of real world experience. By highlighting these limitations, we hope to motivate the development of models with a human-like understanding of the physical world.	5aab57cc0530560d82c74c055f664280619d7e81	@['JournalArticle']{ouellette-etal-2021-prost:,  author = {Stephane T Aroca-Ouellette and Cory Paik and A. Roncone and Katharina Kann},  booktitle = {Findings},  pages = {4597-4608},  title = {PROST: Physical Reasoning about Objects through Space and Time},  year = {2021} }
ProSPer: Probing Human and Neural Network Language Model Understanding of Spatial Perspective	2021	https://www.semanticscholar.org/paper/401e74df0c54e49218d69ba482f927e077a1e84f	A probe task is proposed that explores how well language models understand spatial perspective by evaluating perspective inference in English, ProSPer, and using it to explore how humans and Transformer-based language models infer perspective.	maybe	1	Understanding perspectival language is important for applications like dialogue systems and human-robot interaction. We propose a probe task that explores how well language models understand spatial perspective. We present a dataset for evaluating perspective inference in English, ProSPer, and use it to explore how humans and Transformer-based language models infer perspective. Although the best bidirectional model performs similarly to humans, they display different strengths: humans outperform neural networks in conversational contexts, while RoBERTa excels at written genres.	401e74df0c54e49218d69ba482f927e077a1e84f	@['JournalArticle']{masis-anderson-2021-prosper:,  author = {Tessa Masis and Carolyn Jane Anderson},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {95-135},  title = {ProSPer: Probing Human and Neural Network Language Model Understanding of Spatial Perspective},  year = {2021} }
Prompting Language Models for Linguistic Structure	2022	http://www.semanticscholar.org/paper/7a21e72f1f7b3824bf8f33676a99a305ed1558a2	This work presents a structured prompting approach that can be used to prompt for linguistic structure prediction tasks, allowing it to perform zero-and few-shot sequence tagging with autoregressive PLMs and shows that structured prompting can retrieve linguistic structure even with arbitrary labels.	maybe	0	Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from general-izable linguistic representations versus more surface-level lexical patterns. To test this, we present a structured prompting approach that can be used to prompt for linguistic structure prediction tasks, allowing us to perform zero-and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking and demonstrate strong few-shot performance in all cases. We also ﬁnd that, though the surface forms of the tags provide some signal, structured prompting can retrieve linguistic structure even with arbitrary labels, indicating that PLMs contain this knowledge in a general manner robust to label choice.	7a21e72f1f7b3824bf8f33676a99a305ed1558a2	@['JournalArticle']{blevins-etal-2022-prompting,  author = {Terra Blevins and Hila Gonen and Luke Zettlemoyer},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Prompting Language Models for Linguistic Structure},  volume = {abs/2211.07830},  year = {2022} }
Prompting for a conversation: How to control a dialog model?	2022	http://www.semanticscholar.org/paper/30bce34dcf94a416917609417b07eef10827152d	This paper experiments with conditioning the prompt on the query, rather than training a single prompt for all queries and finds that compared to fine-tuning, prompting can achieve a higher BLEU score and substantially improve the diversity and novelty of the responses.	maybe	2	Dialog modelling faces a difficult trade-off. Models are trained on a large amount of text, yet their responses need to be limited to a desired scope and style of a dialog agent. Because the datasets used to achieve the former contain language that is not compatible with the latter, pre-trained dialog models are fine-tuned on smaller curated datasets. However, the fine-tuning process robs them of the ability to produce diverse responses, eventually reducing them to dull conversation partners. In this paper we investigate if prompting can help with mitigating the above trade-off. Specifically, we experiment with conditioning the prompt on the query, rather than training a single prompt for all queries. By following the intuition that freezing the pre-trained language model will conserve its expressivity, we find that compared to fine-tuning, prompting can achieve a higher BLEU score and substantially improve the diversity and novelty of the responses.	30bce34dcf94a416917609417b07eef10827152d	@['JournalArticle']{valvoda-etal-2022-prompting,  author = {Josef Valvoda and Yimai Fang and David Vandyke},  booktitle = {Conference on Algebraic Informatics},  journal = {ArXiv},  title = {Prompting for a conversation: How to control a dialog model?},  volume = {abs/2209.11068},  year = {2022} }
Prompting as Probing: Using Language Models for Knowledge Base Construction	2022	http://www.semanticscholar.org/paper/b61e460852530ade9cef22c476c487816837dfaa	ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC), implements a multi-step approach that combines a variety of prompting techniques to achieve this.	maybe	5	Language Models (LMs) have proven to be useful in various downstream applications, such as sum-marisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: ProP won track 2 of the LM-KBC competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/HEmile/iswc-challenge.	b61e460852530ade9cef22c476c487816837dfaa	@['JournalArticle']{alivanistos-etal-2022-prompting,  author = {Dimitrios Alivanistos and Selene B'aez Santamar'ia and M. Cochez and Jan-Christoph Kalo and Emile van Krieken and Thiviyan Thanapalasingam},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Prompting as Probing: Using Language Models for Knowledge Base Construction},  volume = {abs/2208.11057},  year = {2022} }
Prompt-Based Editing for Text Style Transfer	2023	https://www.semanticscholar.org/paper/feee27c7a717fcb329b4e863b2c720a9defad504	A prompt-based editing approach for text style transfer that is a training-free process and more controllable than the autoregressive generation of sentences, and largely outperforms the state-of-the-art systems that have 20 times more parameters.	maybe	0	Prompting approaches have been recently explored in text style transfer, where a textual prompt is used to query a pretrained language model to generate style-transferred texts word by word in an autoregressive manner. However, such a generation process is less controllable and early prediction errors may affect future word predictions. In this paper, we present a prompt-based editing approach for text style transfer. Speciﬁcally, we prompt a pretrained language model for style classiﬁcation and use the classiﬁcation probability to compute a style score. Then, we perform discrete search with word-level editing to maximize a comprehensive scoring function for the style-transfer task. In this way, we transform a prompt-based generation problem into a classiﬁcation one, which is a training-free process and more controllable than the autoregressive generation of sentences. In our experiments, we performed both automatic and human evaluation on three style-transfer benchmark datasets, and show that our approach largely outperforms the state-of-the-art systems that have 20 times more parameters. Additional empirical analyses further demonstrate the effectiveness of our approach.	feee27c7a717fcb329b4e863b2c720a9defad504	@['JournalArticle']{luo-etal-2023-prompt,  author = {Guoqing Luo and Yu Tong Han and Lili Mou and Mauajama Firdaus},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Prompt-Based Editing for Text Style Transfer},  volume = {abs/2301.11997},  year = {2023} }
Prompt Consistency for Zero-Shot Task Generalization	2022	http://www.semanticscholar.org/paper/07c70ca55793984ffdf31582a05170ef3d62381a	This paper takes advantage of the fact that multiple prompts can be used to specify a single task, and proposes to regular- 014 ize prompt consistency, encouraging consistent 015 predictions over this diverse set of prompts.	maybe	13	One of the most impressive results of recent 001 NLP history is the ability of pre-trained lan- 002 guage models to solve new tasks in a zero- 003 shot setting. To achieve this, NLP tasks are 004 framed as natural language prompts, generat- 005 ing a response indicating the predicted output. 006 Nonetheless, the performance in such settings 007 often lags far behind its supervised counterpart, 008 suggesting a large space for potential improve- 009 ment. In this paper, we explore methods to 010 utilize unlabeled data to improve zero-shot per- 011 formance. Specifically, we take advantage of 012 the fact that multiple prompts can be used to 013 specify a single task, and propose to regular- 014 ize prompt consistency , encouraging consistent 015 predictions over this diverse set of prompts. 016 Our method makes it possible to fine-tune the 017 model either with extra unlabeled training data, 018 or directly on test input at inference time in 019 an unsupervised manner. In experiments, our 020 approach outperforms the state-of-the-art zero- 021 shot learner, T0 (Sanh et al., 2021), on 9 out of 022 11 datasets across 4 NLP tasks by up to 10.6 023 absolute points in terms of accuracy. The gains 024 are often attained with a small number of unla- 025 beled examples. 1 026	07c70ca55793984ffdf31582a05170ef3d62381a	@['JournalArticle']{zhou-etal-2022-prompt,  author = {Chunting Zhou and Junxian He and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Prompt Consistency for Zero-Shot Task Generalization},  volume = {abs/2205.00049},  year = {2022} }
Project Aristo: Towards Machines that Capture and Reason with Science Knowledge	2019	http://www.semanticscholar.org/paper/3dc3e2c07ecbcafaaeeae596c9418043521d3113	This talk will describe the journey of Aristo through various knowledge capture technologies, including acquiring if/then rules, tables, knowledge graphs, and latent neural representations, and speculate on the larger quest towards knowledgable machines that can reason, explain, and discuss.	maybe	3	AI2's Project Aristo seeks to build a system that has a deep understanding of science, using knowledge captured mainly from large-scale text. Recently, Aristo achieved surprising success on the Grade 8 New York Regents Science Exams, scoring over 90% on the exam's non-diagram, multiple choice (NDMC) questions, where even 3 years ago the best systems scored less than 60%. In this talk, I will describe the journey of Aristo through various knowledge capture technologies that have helped it, including acquiring if/then rules, tables, knowledge graphs, and latent neural representations. I will also discuss the growing tension between capturing structured knowledge vs. capturing knowledge latently using neural models, the latter proving highly effective but hard to interpret. Finally I will speculate on the larger quest towards knowledgable machines that can reason, explain, and discuss, and how structured and latent knowledge can interact to help reach this goal.	3dc3e2c07ecbcafaaeeae596c9418043521d3113	@['Book', 'JournalArticle']{clark-2019-project,  author = {Peter Clark},  booktitle = {International Conference on Knowledge Capture},  journal = {Proceedings of the 10th International Conference on Knowledge Capture},  title = {Project Aristo: Towards Machines that Capture and Reason with Science Knowledge},  year = {2019} }
Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks	2022	http://www.semanticscholar.org/paper/2f29426556cb9d434f114b5a8472fd9d65e71d4f	Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets, and by combining PoT with self-consistency decoding, can achieve SoT performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets.	maybe	9	Recently, there has been signiﬁcant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step ‘thought’ process. To disentan-gle computation from reasoning, we propose ‘Program of Thoughts’ (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on ﬁve math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three ﬁnancial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12% across all the evaluated datasets. By combining PoT with self-consistency decoding, we can achieve SoTA performance on all math problem datasets and near-SoTA performance on ﬁnancial datasets. All of our data and code are released in Github 1 .	2f29426556cb9d434f114b5a8472fd9d65e71d4f	@['JournalArticle']{chen-etal-2022-program,  author = {Wenhu Chen and Xueguang Ma and Xinyi Wang and William W. Cohen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks},  volume = {abs/2211.12588},  year = {2022} }
Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words	2022	http://www.semanticscholar.org/paper/e05ace4ac80fd7af36ca183a8be5ef0b793d1482	It is found that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors.	maybe	3	Cosine similarity of contextual embeddings is used in many NLP tasks (e.g., QA, IR, MT) and metrics (e.g., BERTScore). Here, we uncover systematic ways in which word similarities estimated by cosine over BERT embeddings are understated and trace this effect to training data frequency. We find that relative to human judgements, cosine similarity underestimates the similarity of frequent words with other instances of the same word or other words across contexts, even after controlling for polysemy and other factors. We conjecture that this underestimation of similarity for high frequency words is due to differences in the representational geometry of high and low frequency words and provide a formal argument for the two-dimensional case.	e05ace4ac80fd7af36ca183a8be5ef0b793d1482	@['JournalArticle', 'Conference']{zhou-etal-2022-problems,  author = {Kaitlyn Zhou and Kawin Ethayarajh and Dallas Card and Dan Jurafsky},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Problems with Cosine as a Measure of Embedding Similarity for High Frequency Words},  volume = {abs/2205.05092},  year = {2022} }
Probing Word Syntactic Representations in the Brain by a Feature Elimination Method	2022	http://www.semanticscholar.org/paper/c9bd83a75817a5aef817a704b398ea920bf3bfce		maybe	4	Neuroimaging studies have identified multiple brain regions that are associated with semantic and syntactic processing when comprehending language. However, existing methods cannot explore the neural correlates of fine-grained word syntactic features, such as part-of-speech and dependency relations. This paper proposes an alternative framework to study how different word syntactic features are represented in the brain. To separate each syntactic feature, we propose a feature elimination method, called Mean Vector Null space Projection (MVNP). This method can remove a specific feature from word representations, resulting in one-feature-removed representations. Then we respectively associate one-feature-removed and the original word vectors with brain imaging data to explore how the brain represents the removed feature. This paper for the first time studies the cortical representations of multiple fine-grained syntactic features simultaneously and suggests some possible contributions of several brain regions to the complex division of syntactic processing. These findings indicate that the brain foundations of syntactic information processing might be broader than those suggested by classical studies.	c9bd83a75817a5aef817a704b398ea920bf3bfce	@['JournalArticle', 'Conference']{zhang-etal-2022-probing,  author = {Xiaohan Zhang and Shaonan Wang and Nan Lin and Jiajun Zhang and Chengqing Zong},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {11721-11729},  title = {Probing Word Syntactic Representations in the Brain by a Feature Elimination Method},  year = {2022} }
Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in French and English	2019	http://www.semanticscholar.org/paper/55493352fd7ad2af6902b036e0de23d384f1f09b	This paper manipulates word embeddings to translate them in a space that is attuned to the linguistic properties under study, and extends previous work on long-distance dependencies in three ways.	maybe	5	The recent wide-spread and strong interest in RNNs has spurred detailed investigations of the distributed representations they generate and specifically if they exhibit properties similar to those characterising human languages. Results are at present inconclusive. In this paper, we extend previous work on long-distance dependencies in three ways. We manipulate word embeddings to translate them in a space that is attuned to the linguistic properties under study. We extend the work to sentence embeddings and to new languages. We confirm previous negative results: word embeddings and sentence embeddings do not unequivocally encode fine-grained linguistic properties of long-distance dependencies.	55493352fd7ad2af6902b036e0de23d384f1f09b	@['JournalArticle']{merlo-2019-probing,  author = {Paola Merlo},  booktitle = {BlackboxNLP@ACL},  pages = {158-172},  title = {Probing Word and Sentence Embeddings for Long-distance Dependencies Effects in French and English},  year = {2019} }
Probing with Noise: Unpicking the Warp and Weft of Embeddings	2022	http://www.semanticscholar.org/paper/cb9db3681e86f1f93f51b8bbfd51f3c594dbb926	An extension of the probing framework which allows for relative intrinsic interpretations of probing results is developed, based on introducing noise that ablates information encoded in embeddings, grounded in random baselines and conﬁdence intervals.	yes	1	Improving our understanding of how information is encoded in vector space can yield valuable interpretability insights. Alongside vector dimensions, we argue that it is possible for the vector norm to also carry linguistic information. We develop a method to test this: an extension of the probing framework which allows for relative intrinsic interpretations of probing results. It relies on introducing noise that ablates information encoded in embeddings, grounded in random baselines and confidence intervals. We apply the method to well-established probing tasks and find evidence that confirms the existence of separate information containers in English GloVe and BERT embeddings. Our correlation analysis aligns with the experimental findings that different encoders use the norm to encode different kinds of information: GloVe stores syntactic and sentence length information in the vector norm, while BERT uses it to encode contextual incongruity.	cb9db3681e86f1f93f51b8bbfd51f3c594dbb926	@['JournalArticle']{klubicka-kelleher-2022-probing,  author = {Filip Klubicka and John D. Kelleher},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Probing with Noise: Unpicking the Warp and Weft of Embeddings},  volume = {abs/2210.12206},  year = {2022} }
Probing What Different NLP Tasks Teach Machines about Function Word Comprehension	2019	http://www.semanticscholar.org/paper/1321419b4e093ebd5064cd9c44b61c0d8b6c361d	The results show that pretraining on CCG—the authors' most syntactic objective—performs the best on average across their probing tasks, suggesting that syntactic knowledge helps function word comprehension.	maybe	73	We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG—our most syntactic objective—performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.	1321419b4e093ebd5064cd9c44b61c0d8b6c361d	@['JournalArticle']{kim-etal-2019-probing,  author = {Najoung Kim and Roma Patel and Adam Poliak and Alex Wang and Patrick Xia and R. Thomas McCoy and Ian Tenney and Alexis Ross and Tal Linzen and Benjamin Van Durme and Samuel R. Bowman and Ellie Pavlick},  booktitle = {International Workshop on Semantic Evaluation},  pages = {235-249},  title = {Probing What Different NLP Tasks Teach Machines about Function Word Comprehension},  year = {2019} }
Probing via Prompting	2022	http://www.semanticscholar.org/paper/91496a0a35d541c32324d23a06990be089f6c830	A novel model-free approach to probing via prompting is introduced, which formulates probing as a prompting task and shows that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own.	maybe	4	Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting task. We conduct experiments on five probing tasks and show that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own. We further combine the probing via prompting approach with pruning to analyze where the model stores the linguistic information in its architecture. Finally, we apply the probing via prompting approach to examine the usefulness of a linguistic property for pre-training by removing the heads that are essential to it and evaluating the resulting model’s performance on language modeling.	91496a0a35d541c32324d23a06990be089f6c830	@['JournalArticle', 'Conference']{li-etal-2022-probing,  author = {Jiaoda Li and Ryan Cotterell and Mrinmaya Sachan},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Probing via Prompting},  volume = {abs/2207.01736},  year = {2022} }
Probing Toxic Content in Large Pre-Trained Language Models	2021	http://www.semanticscholar.org/paper/080df61ee1c15ff3c8e5d0d82d60bfd80e372e38	A method based on logistic regression classifiers is proposed to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates to assess and mitigate the toxicity transmitted by PTL Ms.	maybe	22	Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.	080df61ee1c15ff3c8e5d0d82d60bfd80e372e38	@['JournalArticle', 'Conference']{ousidhoum-etal-2021-probing,  author = {N. Ousidhoum and Xinran Zhao and Tianqing Fang and Yangqiu Song and Dit-Yan Yeung},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4262-4274},  title = {Probing Toxic Content in Large Pre-Trained Language Models},  year = {2021} }
Probing the representations of named entities in Transformer-based Language Models	2022	https://www.semanticscholar.org/paper/0488a5ee7a85ea9ab93f40bb6324dbf8c36a825b	The named entity representations learned by Transformer-based language models are analyzed to measure how substitute-entities with different properties impact model performance and it is found that the the frequency with which entities occur are important for the masked language modeling task, and that the entities’ distributions over topics areImportant for topic classification.	maybe	0	In this work we analyze the named entity representations learned by Transformer-based language models. We investigate the role entities play in two tasks: a language modeling task, and a sequence classification task. For this purpose we collect a novel news topic classification dataset with 12 topics called RefNews-12. We perform two complementary methods of analysis. First, we use diagnostic models allowing us to quantify to what degree entity information is present in the hidden representations. Second, we perform entity mention substitution to measure how substitute-entities with different properties impact model performance. By controlling for model uncertainty we are able to show that entities are identified, and depending on the task, play a measurable role in the model’s predictions. Additionally, we show that the entities’ types alone are not enough to account for this. Finally, we find that the the frequency with which entities occur are important for the masked language modeling task, and that the entities’ distributions over topics are important for topic classification.	0488a5ee7a85ea9ab93f40bb6324dbf8c36a825b	@None{schouten-etal-2022-probing,  author = {Stefan F. Schouten and Peter Bloem and P. Vossen},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  title = {Probing the representations of named entities in Transformer-based Language Models},  year = {2022} }
Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?	2020	http://www.semanticscholar.org/paper/922535984641f052d2530c8b68f9ebd7de38dfa5	This work examines this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained, and identifies that pretrained word embeddings play a considerable role in encoding these properties.	maybe	59	Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of ‘probing’ tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.	922535984641f052d2530c8b68f9ebd7de38dfa5	@['JournalArticle', 'Conference']{ravichander-etal-2020-probing,  author = {Abhilasha Ravichander and Yonatan Belinkov and E. Hovy},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {3363-3377},  title = {Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?},  year = {2020} }
Probing Tasks Under Pressure	2021	http://www.semanticscholar.org/paper/288b850114ff9bfe5f19d271ddf027dc1dc6ae9f	This work compares the accuracies of a set of probing tasks on gold and automatically generated control datasets to suggest that probing tasks can be used as reliable diagnostic methods to investigate the linguistic information encoded in NLMs representations.	maybe	1	Probing tasks are frequently used to evaluate whether the representations of Neural Language Models (NLMs) encode linguistic information. However, it is still questioned if probing classification tasks really enable such investigation or they simply hint for surface patterns in the data. We present a method to investigate this question by comparing the accuracies of a set of probing tasks on gold and automatically generated control datasets. Our results suggest that probing tasks can be used as reliable diagnostic methods to investigate the linguistic information encoded in NLMs representations.	288b850114ff9bfe5f19d271ddf027dc1dc6ae9f	@['JournalArticle']{miaschi-etal-2021-probing,  author = {Alessio Miaschi and Chiara Alzetta and Dominique Brunato and F. Dell’Orletta and Giulia Venturi},  booktitle = {Italian Conference on Computational Linguistics},  title = {Probing Tasks Under Pressure},  year = {2021} }
Probing Task-Oriented Dialogue Representation from Language Models	2020	http://www.semanticscholar.org/paper/cbe87f3ff4ff30def2fa507ba4a511fee3e34188	This empirical paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks and proposes an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering.	maybe	16	This paper investigates pre-trained language models to find out which model intrinsically carries the most informative representation for task-oriented dialogue tasks. We approach the problem from two aspects: supervised classifier probe and unsupervised mutual information probe. We fine-tune a feed-forward layer as the classifier probe on top of a fixed pre-trained language model with annotated labels in a supervised way. Meanwhile, we propose an unsupervised mutual information probe to evaluate the mutual dependence between a real clustering and a representation clustering. The goals of this empirical paper are to 1) investigate probing techniques, especially from the unsupervised mutual information aspect, 2) provide guidelines of pre-trained language model selection for the dialogue research community, 3) find insights of pre-training factors for dialogue application that may be the key to success.	cbe87f3ff4ff30def2fa507ba4a511fee3e34188	@['JournalArticle', 'Conference']{wu-xiong-2020-probing,  author = {Chien-Sheng Wu and Caiming Xiong},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {5036-5051},  title = {Probing Task-Oriented Dialogue Representation from Language Models},  year = {2020} }
Probing Simple Factoid Question Answering Based on Linguistic Knowledge	2021	http://www.semanticscholar.org/paper/09a9e1bb91f95cd1a463a20232adbd6d7ed328f7	The results reveal that the existing BERT-based system tends to depend on the surface and syntactic features of each dataset, and it disturbs the generality and robustness of the system performance.	maybe	0	Recent studies have indicated that existing systems for simple factoid question answering over a knowledge base are not robust for diﬀerent datasets. We evaluated the ability of a pretrained language model, BERT, to perform this task on four datasets, Free917, FreebaseQA, SimpleQuestions, and WebQSP, and found that, like other existing systems, the existing BERT-based system also can not solve them robustly. To investigate the reason for this problem, we employ a statistical method, partial least squares path modeling (PLSPM), with 24 BERT models and two probing tasks, SentEval and GLUE. Our results reveal that the existing BERT-based system tends to depend on the surface and syntactic features of each dataset, and it disturbs the generality and robustness of the system performance. We also discuss the reason for this phenomenon by considering the features of each dataset and the method that was used to evaluate the simple factoid question answering task.	09a9e1bb91f95cd1a463a20232adbd6d7ed328f7	@None{han-etal-2021-probing,  author = {Namgi Han and Hiroshi Noji and Katsuhiko Hayashi and Hiroya Takamura and Yusuke Miyao},  booktitle = {Journal of Natural Language Processing},  journal = {Journal of Natural Language Processing},  title = {Probing Simple Factoid Question Answering Based on Linguistic Knowledge},  year = {2021} }
Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis	2022	http://www.semanticscholar.org/paper/713bd2971116098211ef06336dfbe91a69854404	This paper probes representations from the CodeBERT model for semantic grounding by using the data from the IBM CodeNet dataset, and shows that using bimodalinputs over unimodal inputs gives better semantic grounding and sample eﬃciency during semantic ﬁne-tuning.	maybe	0	. Representational Similarity Analysis is a method from cognitive neuroscience, which helps in comparing representations from two diﬀerent sources of data. In this paper, we propose using Representational Similarity Analysis to probe the semantic grounding in language models of code. We probe representations from the CodeBERT model for semantic grounding by using the data from the IBM CodeNet dataset. Through our experiments, we show that current pre-training methods do not induce semantic grounding in language models of code, and in-stead focus on optimizing form-based patterns. We also show that even a little amount of ﬁne-tuning on semantically relevant tasks increases the semantic grounding in CodeBERT signiﬁcantly. Our ablations with the input modality to the CodeBERT model show that using bimodal inputs (code and natural language) over unimodal inputs (only code) gives better semantic grounding and sample eﬃciency during semantic ﬁne-tuning. Finally, our experiments with semantic perturbations in code reveal that CodeBERT is able to robustly distinguish between semantically correct and incorrect code.	713bd2971116098211ef06336dfbe91a69854404	@['JournalArticle']{naik-etal-2022-probing,  author = {Shounak Naik and Rajaswa Patil and Swati Agarwal and V. Baths},  booktitle = {International Conference on Advanced Data Mining and Applications},  pages = {395-406},  title = {Probing Semantic Grounding in Language Models of Code with Representational Similarity Analysis},  year = {2022} }
Probing Script Knowledge from Pre-Trained Models	2022	http://www.semanticscholar.org/paper/cf2453589ee98561c599694fce3220aa79e5c56b	The stereotypical temporal knowledge among the sub-events is well captured in BERT, however the inclusive or starting sub-event knowledge is barely encoded, so three probing tasks are designed to investigate the capabilities of PLMs with and without tuning.	maybe	1	Adversarial attack of structured prediction models faces various challenges such as the difficulty of perturbing discrete words, the sentence quality issue, and the sensitivity of outputs to small perturbations. In this work, we introduce SHARP, a new attack method that formulates the black-box adversarial attack as a search-based optimization problem with a specially designed objective function considering sentence fluency, meaning preservation and attacking effectiveness. Additionally, three different searching strategies are analyzed and compared, , Beam Search, Metropolis-Hastings Sampling, and Hybrid Search. We demonstrate the effectiveness of our attacking strategies on two challenging structured prediction tasks: part-of-speech (POS) tagging and dependency parsing. Through automatic and human evaluations, we show that our method performs a more potent attack compared with pioneer arts. Moreover, the generated adversarial examples can be used to successfully boost the robustness and performance of the victim model via adversarial training.	cf2453589ee98561c599694fce3220aa79e5c56b	@['JournalArticle']{jin-etal-2022-probing,  author = {Zijian Jin and Xingyu Zhang and Mo Yu and Lifu Huang},  booktitle = {UMIOS},  journal = {ArXiv},  title = {Probing Script Knowledge from Pre-Trained Models},  volume = {abs/2204.10176},  year = {2022} }
Probing Pretrained Models of Source Code	2022	http://www.semanticscholar.org/paper/6aae3ddbe142f7ae28f0f18bb6248dc7b3f41c00	It is shown that pretrained models of code indeed contain information about code syntactic structure and correctness, the notion of namespaces, code readability and natural language naming, but lack understanding of code semantics.	maybe	6	Deep learning models are widely used for solv-ing challenging code processing tasks, such as code generation or code summarization. Tradi-tionally, a speciﬁc model architecture was care-fully built to solve a particular code processing task. However, recently general pretrained models such as CodeBERT or CodeT5 have been shown to outperform task-speciﬁc models in many applications. While pretrained models are known to learn complex patterns from data, they may fail to understand some properties of source code. To test diverse aspects of code understanding, we introduce a set of diagnosting probing tasks. We show that pretrained models of code indeed contain information about code syntactic structure and correctness, the notion of namespaces, code readability and natural language naming, but lack understanding of code semantics. We also investigate how probing results are affected by using code-speciﬁc pretraining objectives, varying the model size, or ﬁnetuning.	6aae3ddbe142f7ae28f0f18bb6248dc7b3f41c00	@['JournalArticle']{troshin-chirkova-2022-probing,  author = {Sergey Troshin and Nadezhda Chirkova},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Probing Pretrained Models of Source Code},  volume = {abs/2202.08975},  year = {2022} }
Probing Pretrained Language Models for Lexical Semantics	2020	https://www.semanticscholar.org/paper/9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0	A systematic empirical analysis across six typologically diverse languages and five different lexical tasks indicates patterns and best practices that hold universally, but also point to prominent variations across languages and tasks.	maybe	99	The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.	9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0	@['JournalArticle', 'Conference']{vulic-etal-2020-probing,  author = {Ivan Vulic and E. Ponti and Robert Litschko and Goran Glavas and A. Korhonen},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Probing Pretrained Language Models for Lexical Semantics},  volume = {abs/2010.05731},  year = {2020} }
Probing Pre-trained Language Models for Semantic Attributes and their Values	2021	http://www.semanticscholar.org/paper/593284010379e02f6ab57e7208b5511185ce8c0e	This paper uses PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLs encode semantic attributes along with their values, e.g. the relation between rich and high net worth.	maybe	3	Pretrained Language Models (PTLMs) yield state-of-the-art performance on many Natural Language Processing tasks, including syntax, semantics and commonsense reasoning. In this paper, we focus on identifying to what extent do PTLMs capture semantic attributes and their values, e.g. the relation between rich and high net worth. We use PTLMs to predict masked tokens using patterns and lists of items from Wikidata in order to verify how likely PTLMs encode semantic attributes along with their values. Such inferences based on semantics are intuitive for us humans as part of our language understanding. Since PTLMs are trained on large amount of Wikipedia data, we would assume that they can generate similar predictions. However, our findings reveal that PTLMs perform still much worse than humans on this task. We show an analysis which explains how to exploit our methodology to integrate better context and semantics into PTLMs using knowledge bases.	593284010379e02f6ab57e7208b5511185ce8c0e	@['JournalArticle', 'Conference']{beloucif-biemann-2021-probing,  author = {Meriem Beloucif and Chris Biemann},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {2554-2559},  title = {Probing Pre-trained Language Models for Semantic Attributes and their Values},  year = {2021} }
Probing Pre-Trained Language Models for Disease Knowledge	2021	http://www.semanticscholar.org/paper/896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3	This paper introduces DisKnE, a new benchmark for Disease Knowledge Evaluation, and annotated each positive MedNLI example with the types of medical reasoning that are needed, then created negative examples by corrupting these positive examples in an adversarial way.	maybe	7	Pre-trained language models such as ClinicalBERT have achieved impressive results on tasks such as medical Natural Language Inference. At first glance, this may suggest that these models are able to perform medical reasoning tasks, such as mapping symptoms to diseases. However, we find that standard benchmarks such as MedNLI contain relatively few examples that require such forms of reasoning. To better understand the medical reasoning capabilities of existing language models, in this paper we introduce DisKnE, a new benchmark for Disease Knowledge Evaluation. To construct this benchmark, we annotated each positive MedNLI example with the types of medical reasoning that are needed. We then created negative examples by corrupting these positive examples in an adversarial way. Furthermore, we define training-test splits per disease, ensuring that no knowledge about test diseases can be learned from the training data, and we canonicalize the formulation of the hypotheses to avoid the presence of artefacts. This leads to a number of binary classification problems, one for each type of reasoning and each disease. When analysing pre-trained models for the clinical/biomedical domain on the proposed benchmark, we find that their performance drops considerably.	896c76bf99de9c1ba6fc3dfb695dfa66e73e1eb3	@['JournalArticle']{alghanmi-etal-2021-probing,  author = {Israa Alghanmi and Luis Espinosa Anke and S. Schockaert},  booktitle = {Findings},  pages = {3023-3033},  title = {Probing Pre-Trained Language Models for Disease Knowledge},  year = {2021} }
Probing Pre-Trained Language Models for Cross-Cultural Differences in Values	2022	http://www.semanticscholar.org/paper/1a82d782800e9c8b9a9b73e17fcf60711735b651		maybe	5	Language embeds information about social, cultural, and political values people hold. Prior work has explored potentially harmful social biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which cross-cultural values are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We ﬁnd that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.	1a82d782800e9c8b9a9b73e17fcf60711735b651	@['JournalArticle', 'Review']{arora-etal-2022-probing,  author = {Arnav Arora and Lucie-Aimée Kaffee and Isabelle Augenstein},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Probing Pre-Trained Language Models for Cross-Cultural Differences in Values},  volume = {abs/2203.13722},  year = {2022} }
Probing Neural Network Comprehension of Natural Language Arguments	2019	https://www.semanticscholar.org/paper/f3b89e9a2b8ce1b6058e6984c3556bc2dded0938	This work analyzes the nature of spurious statistical cues in the dataset and demonstrates that a range of models all exploit them, informing the construction of an adversarial dataset on which all models achieve random accuracy.	seed	322	We are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.	f3b89e9a2b8ce1b6058e6984c3556bc2dded0938	@['JournalArticle', 'Conference']{niven-kao-2019-probing,  author = {Timothy Niven and Hung-Yu Kao},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Probing Neural Network Comprehension of Natural Language Arguments},  volume = {abs/1907.07355},  year = {2019} }
Probing Neural Language Models for Human Tacit Assumptions	2020	http://www.semanticscholar.org/paper/774319233a107a29622003a115aa6c79f4a7b37f	This work constructs a diagnostic set of word prediction prompts to evaluate whether recent neural contextualized language models trained on large text corpora capture STAs, and finds models to be profoundly effective at retrieving concepts given associated properties.	maybe	28	Humans carry stereotypic tacit assumptions (STAs) (Prince, 1978), or propositional beliefs about generic concepts. Such associations are crucial for understanding natural language. We construct a diagnostic set of word prediction prompts to evaluate whether recent neural contextualized language models trained on large text corpora capture STAs. Our prompts are based on human responses in a psychological study of conceptual associations. We find models to be profoundly effective at retrieving concepts given associated properties. Our results demonstrate empirical evidence that stereotypic conceptual representations are captured in neural models derived from semi-supervised linguistic exposure.	774319233a107a29622003a115aa6c79f4a7b37f	@['JournalArticle']{weir-etal-2020-probing,  author = {Nathaniel Weir and Adam Poliak and Benjamin Van Durme},  booktitle = {Annual Meeting of the Cognitive Science Society},  journal = {arXiv: Computation and Language},  title = {Probing Neural Language Models for Human Tacit Assumptions},  year = {2020} }
Probing Neural Dialog Models for Conversational Understanding	2020	http://www.semanticscholar.org/paper/798232e9551ce134a9efecc1d1cfb8ed001e9c49	It is suggested that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks, and the dyadic, turn-taking nature of dialog is not fully leveraged by these models.	maybe	9	The predominant approach to open-domain dialog generation relies on end-to-end training of neural models on chat datasets. However, this approach provides little insight as to what these models learn (or do not learn) about engaging in dialog. In this study, we analyze the internal representations learned by neural open-domain dialog systems and evaluate the quality of these representations for learning basic conversational skills. Our results suggest that standard open-domain dialog systems struggle with answering questions, inferring contradiction, and determining the topic of conversation, among other tasks. We also find that the dyadic, turn-taking nature of dialog is not fully leveraged by these models. By exploring these limitations, we highlight the need for additional research into architectures and training methods that can better capture high-level information about dialog.	798232e9551ce134a9efecc1d1cfb8ed001e9c49	@['JournalArticle']{saleh-etal-2020-probing,  author = {Abdelrhman Saleh and Tovly Deutsch and Stephen Casper and Yonatan Belinkov and S. Shieber},  booktitle = {NLP4CONVAI},  journal = {ArXiv},  title = {Probing Neural Dialog Models for Conversational Understanding},  volume = {abs/2006.08331},  year = {2020} }
Probing Natural Language Inference Models through Semantic Fragments	2019	https://www.semanticscholar.org/paper/681fbcd98acf20df3355eff3585994bd1f9008b7	This work proposes the use of semantic fragments—systematically generated datasets that each target a different semantic phenomenon—for probing, and efficiently improving, such capabilities of linguistic models.	seed	92	Do state-of-the-art models for language understanding already have, or can they easily learn, abilities such as boolean coordination, quantification, conditionals, comparatives, and monotonicity reasoning (i.e., reasoning about word substitutions in sentential contexts)? While such phenomena are involved in natural language inference (NLI) and go beyond basic linguistic understanding, it is unclear the extent to which they are captured in existing NLI benchmarks and effectively learned by models. To investigate this, we propose the use of semantic fragments—systematically generated datasets that each target a different semantic phenomenon—for probing, and efficiently improving, such capabilities of linguistic models. This approach to creating challenge datasets allows direct control over the semantic diversity and complexity of the targeted linguistic phenomena, and results in a more precise characterization of a model's linguistic behavior. Our experiments, using a library of 8 such semantic fragments, reveal two remarkable findings: (a) State-of-the-art models, including BERT, that are pre-trained on existing NLI benchmark datasets perform poorly on these new fragments, even though the phenomena probed here are central to the NLI task; (b) On the other hand, with only a few minutes of additional fine-tuning—with a carefully selected learning rate and a novel variation of “inoculation”—a BERT-based model can master all of these logic and monotonicity fragments while retaining its performance on established NLI benchmarks.	681fbcd98acf20df3355eff3585994bd1f9008b7	@['JournalArticle', 'Conference']{richardson-etal-2019-probing,  author = {Kyle Richardson and Hai Hu and L. Moss and Ashish Sabharwal},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {8713-8721},  title = {Probing Natural Language Inference Models through Semantic Fragments},  year = {2019} }
Probing Measuring Skills in Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/da20a595aee606dec221bd1c5757274b17f39c66	It is found that PLMs trained on the measurement010 rich corpus show better performance on under011 standing measurements and a simple embedding strat013 egy for input representation leads to a significant improvement in measuring skills.	maybe	0	Recent success of pre-trained language mod001 els (PLMs) has stimulated interest in their abil002 ity to understand and work with numbers. One 003 form of knowledge that has not been studied 004 yet in this context is numerical reasoning over 005 measurements. We show that PLMs possess 006 the linguistic capabilities to understand the sys007 tem of measurement, but they fail to combine 008 it with numerical reasoning skills. We also 009 find that PLMs trained on the measurement010 rich corpus show better performance on under011 standing measurements. Based on these obser012 vations, we propose a simple embedding strat013 egy for input representation which leads to a 014 significant improvement in measuring skills. 015	da20a595aee606dec221bd1c5757274b17f39c66	@None{2022-probing,  title = {Probing Measuring Skills in Pre-trained Language Models},  year = {2022} }
Probing Linguistic Systematicity	2020	http://www.semanticscholar.org/paper/4dc005ea288c50d57222122903edf87f21689781	Evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance is provided.	maybe	37	Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.	4dc005ea288c50d57222122903edf87f21689781	@['JournalArticle', 'Conference']{goodwin-etal-2020-probing,  author = {Emily Goodwin and Koustuv Sinha and T. O’Donnell},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1958-1969},  title = {Probing Linguistic Systematicity},  year = {2020} }
Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties	2022	http://www.semanticscholar.org/paper/33fa5cf4dda0af616773891418ffebe0fe4abd74		maybe	0	In this paper, we present an in-depth investigation of the linguistic knowledge encoded by the transformer models currently available for the Italian language. In particular, we investigate how the complexity of two different architectures of probing models affects the performance of the Transformers in encoding a wide spectrum of linguistic features. Moreover, we explore how this implicit knowledge varies according to different textual genres and language varieties.	33fa5cf4dda0af616773891418ffebe0fe4abd74	@None{miaschi-etal-2022-probing,  author = {Alessio Miaschi and Gabriele Sarti and Dominique Brunato and F. Dell’Orletta and Giulia Venturi},  booktitle = {Italian Journal of Computational Linguistics},  journal = {Italian Journal of Computational Linguistics},  title = {Probing Linguistic Knowledge in Italian Neural Language Models across Language Varieties},  year = {2022} }
Probing Linguistic Information For Logical Inference In Pre-trained Language Models	2021	http://www.semanticscholar.org/paper/9b17e77056110abc1beb1ac271f76f48508bbc0b	This work proposes a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations, and demonstrates language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.	maybe	1	Progress in pre-trained language models has led to a surge of impressive results on downstream tasks for natural language understanding. Recent work on probing pre-trained language models uncovered a wide range of linguistic properties encoded in their contextualized representations. However, it is unclear whether they encode semantic knowledge that is crucial to symbolic inference methods. We propose a methodology for probing knowledge for inference that logical systems require but often lack in pre-trained language model representations. Our probing datasets cover a list of key types of knowledge used by many symbolic inference systems. We find that (i) pre-trained language models do encode several types of knowledge for inference, but there are also some types of knowledge for inference that are not encoded, (ii) language models can effectively learn missing knowledge for inference through fine-tuning. Overall, our findings provide insights into which aspects of knowledge for inference language models and their pre-training procedures capture. Moreover, we have demonstrated language models' potential as semantic and background knowledge bases for supporting symbolic inference methods.	9b17e77056110abc1beb1ac271f76f48508bbc0b	@['JournalArticle', 'Conference']{chen-gao-2021-probing,  author = {Zeming Chen and Qiyue Gao},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {10509-10517},  title = {Probing Linguistic Information For Logical Inference In Pre-trained Language Models},  year = {2021} }
Probing Language Models for Understanding of Temporal Expressions	2021	https://www.semanticscholar.org/paper/174a0e6da0dfb7f96d4a0a4076eed154c439e41a	It is found that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.	maybe	4	We present three Natural Language Inference (NLI) challenge sets that can evaluate NLI models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.	174a0e6da0dfb7f96d4a0a4076eed154c439e41a	@['JournalArticle']{thukral-etal-2021-probing,  author = {Shivin Thukral and Kunal Kukreja and Christian Kavouras},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {396-406},  title = {Probing Language Models for Understanding of Temporal Expressions},  year = {2021} }
Probing Language Models for Common Ground with Visual Representations	2020	http://www.semanticscholar.org/paper/189b518d70ad34c8de6f613bf3bd5051077608bc	A method for characterizing how language representations of concrete nouns relate to the physical appearance of the objects they refer to is proposed, using a probing model that examines how useful language representations are in discerning between different visual representations.	maybe	0	While large-scale language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, finding representations of many language models to be useful in retrieving semantically aligned image patches. In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, finding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval. Altogether, our findings shed new empirical insights on language grounding, suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress.	189b518d70ad34c8de6f613bf3bd5051077608bc	@None{ilharco-etal-2020-probing,  author = {Gabriel Ilharco and Rowan Zellers and Ali Farhadi and Hannaneh Hajishirzi and P. G. Allen},  title = {Probing Language Models for Common Ground with Visual Representations},  year = {2020} }
Probing GPT-3’s Linguistic Knowledge on Semantic Tasks	2022	https://www.semanticscholar.org/paper/cfa37731c5d269c2ea15eb56cb6c13a008404134	The experiment results suggest that GPT-3 has acquired linguistic knowledge to identify certain semantic information in most cases, but still fails when there are some types of disturbance happening in the sentence.	yes	0	GPT-3 has attracted much attention from both academia and industry. However, it is still unclear what GPT-3 has understood or learned especially in linguistic knowledge. Some studies have shown linguistic phenomena including negation and tense are hard to be recognized by language models such as BERT. In this study, we conduct probing tasks focusing on semantic information. Specifically, we investigate GPT-3’s linguistic knowledge on semantic tasks to identify tense, the number of subjects, and the number of objects for a given sentence. We also experiment with different prompt designs and temperatures of the decoding method. Our experiment results suggest that GPT-3 has acquired linguistic knowledge to identify certain semantic information in most cases, but still fails when there are some types of disturbance happening in the sentence. We also perform error analysis to summarize some common types of mistakes that GPT-3 has made when dealing with certain semantic information.	cfa37731c5d269c2ea15eb56cb6c13a008404134	@None{zhang-etal-2022-probing,  author = {Lining Zhang and M. Wang and Liben Chen and Wenxin Zhang},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  title = {Probing GPT-3’s Linguistic Knowledge on Semantic Tasks},  year = {2022} }
Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/ad34a576900d78e6280a1ac1ecaeea19dbb3ed06	Investigation of the extent to which verb alternation classes are encoded in the embeddings of Large Pretrained Language Models such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively constructed diagnostic classiﬁers for word and sentence-level prediction tasks finds that contextualembeddings from PLMs not only outperform non-contextual embedDings, but achieve astonishingly high accuracies on tasks across most alternation Classes.	maybe	0	We investigate the extent to which verb alternation classes, as described by Levin (1993), are encoded in the embeddings of Large Pre-trained Language Models (PLMs) such as BERT, RoBERTa, ELECTRA, and DeBERTa using selectively constructed diagnostic classifiers for word and sentence-level prediction tasks. We follow and expand upon the experiments of Kann et al. (2019), which aim to probe whether static embeddings encode frame-selectional properties of verbs. At both the word and sentence level, we find that contextual embeddings from PLMs not only outperform non-contextual embeddings, but achieve astonishingly high accuracies on tasks across most alternation classes. Additionally, we find evidence that the middle-to-upper layers of PLMs achieve better performance on average than the lower layers across all probing tasks.	ad34a576900d78e6280a1ac1ecaeea19dbb3ed06	@['JournalArticle']{yi-etal-2022-probing,  author = {David K. Yi and James V. Bruno and Jiayu Han and Peter Zukerman and Shane Steinert-Threlkeld},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Probing for Understanding of English Verb Classes and Alternations in Large Pre-trained Language Models},  volume = {abs/2209.04811},  year = {2022} }
Probing for the Usage of Grammatical Number	2022	http://www.semanticscholar.org/paper/974e115d3af8ec96f9a267e44dcb5aebf7954cc6	This paper focuses on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task, and tries to find an encoding that the model actually uses, introducing a usage-based probing setup.	maybe	7	A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious—i.e., the model might not rely on it when making predictions. In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model’s representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Finally, we identify in which layers information about grammatical number is transferred from a noun to its head verb.	974e115d3af8ec96f9a267e44dcb5aebf7954cc6	@['JournalArticle', 'Conference']{lasri-etal-2022-probing,  author = {Karim Lasri and Tiago Pimentel and Alessandro Lenci and T. Poibeau and Ryan Cotterell},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {8818-8831},  title = {Probing for the Usage of Grammatical Number},  year = {2022} }
Probing for targeted syntactic knowledge through grammatical error detection	2022	http://www.semanticscholar.org/paper/b327db5fa9960dff3eba8de33530e93c6599a976	This work proposes grammatical error detection as a diagnostic probe to evaluate token-level contextual representations for their knowledge of SVA, and observes a divergence in performance when probes are trained on different training sets, suggesting the information pertaining to SVA error detection is not robustly encoded.	maybe	0	Targeted studies testing knowledge of subject-verb agreement (SVA) indicate that pre-trained language models encode syntactic information. We assert that if models robustly encode subject-verb agreement, they should be able to identify when agreement is correct and when it is incorrect. To that end, we propose grammatical error detection as a diagnostic probe to evaluate token-level contextual representations for their knowledge of SVA. We evaluate contextual representations at each layer from five pre-trained English language models: BERT, XLNet, GPT-2, RoBERTa and ELECTRA. We leverage public annotated training data from both English second language learners and Wikipedia edits, and report results on manually crafted stimuli for subject-verb agreement. We find that masked language models linearly encode information relevant to the detection of SVA errors, while the autoregressive models perform on par with our baseline. However, we also observe a divergence in performance when probes are trained on different training sets, and when they are evaluated on different syntactic constructions, suggesting the information pertaining to SVA error detection is not robustly encoded.	b327db5fa9960dff3eba8de33530e93c6599a976	@['JournalArticle']{davis-etal-2022-probing,  author = {Christopher Davis and Christopher Bryant and Andrew Caines and Marek Rei and P. Buttery},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {Probing for targeted syntactic knowledge through grammatical error detection},  volume = {abs/2210.16228},  year = {2022} }
Probing for Referential Information in Language Models	2020	http://www.semanticscholar.org/paper/bb429a17280c2df86ac34789df880a4f728009ae	This work analyzes two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus, suggesting that language models are more successful at learning grammatical constraints than they are at learning truly referential information.	maybe	19	Language models keep track of complex information about the preceding context – including, e.g., syntactic relations in a sentence. We investigate whether they also capture information beneficial for resolving pronominal anaphora in English. We analyze two state of the art models with LSTM and Transformer architectures, via probe tasks and analysis on a coreference annotated corpus. The Transformer outperforms the LSTM in all analyses. Our results suggest that language models are more successful at learning grammatical constraints than they are at learning truly referential information, in the sense of capturing the fact that we use language to refer to entities in the world. However, we find traces of the latter aspect, too.	bb429a17280c2df86ac34789df880a4f728009ae	@['JournalArticle', 'Conference']{sorodoc-etal-2020-probing,  author = {Ionut-Teodor Sorodoc and Kristina Gulordava and Gemma Boleda},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4177-4189},  title = {Probing for Referential Information in Language Models},  year = {2020} }
Probing for Predicate Argument Structures in Pretrained Language Models	2022	http://www.semanticscholar.org/paper/7f38831567e60dab286c8327c4027865fa31f3ee	This study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate, and provides insights into the correlation between predicate senses and their structures, the degree of transferability between nominal and verbal structures, and how such structures are encoded across languages.	maybe	5	Thanks to the effectiveness and wide availability of modern pretrained language models (PLMs), recently proposed approaches have achieved remarkable results in dependency- and span-based, multilingual and cross-lingual Semantic Role Labeling (SRL). These results have prompted researchers to investigate the inner workings of modern PLMs with the aim of understanding how, where, and to what extent they encode information about SRL. In this paper, we follow this line of research and probe for predicate argument structures in PLMs. Our study shows that PLMs do encode semantic structures directly into the contextualized representation of a predicate, and also provides insights into the correlation between predicate senses and their structures, the degree of transferability between nominal and verbal structures, and how such structures are encoded across languages. Finally, we look at the practical implications of such insights and demonstrate the benefits of embedding predicate argument structure information into an SRL model.	7f38831567e60dab286c8327c4027865fa31f3ee	@['JournalArticle', 'Conference']{conia-navigli-2022-probing,  author = {Simone Conia and Roberto Navigli},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4622-4632},  title = {Probing for Predicate Argument Structures in Pretrained Language Models},  year = {2022} }
Probing for Labeled Dependency Trees	2022	http://www.semanticscholar.org/paper/babf3c8cc57c22809b71d50e839db9c6aecec5d5	DepProbe is introduced, a linear probe which can extract labeled and directed dependency parse trees from embeddings while using fewer parameters and compute than prior methods, and its predictive power for selecting the best transfer language for training a full biaffine attention parser is investigated.	maybe	3	Probing has become an important tool for analyzing representations in Natural Language Processing (NLP). For graphical NLP tasks such as dependency parsing, linear probes are currently limited to extracting undirected or unlabeled parse trees which do not capture the full task. This work introduces DepProbe, a linear probe which can extract labeled and directed dependency parse trees from embeddings while using fewer parameters and compute than prior methods. Leveraging its full task coverage and lightweight parametrization, we investigate its predictive power for selecting the best transfer language for training a full biaffine attention parser. Across 13 languages, our proposed method identifies the best source treebank 94% of the time, outperforming competitive baselines and prior work. Finally, we analyze the informativeness of task-specific subspaces in contextual embeddings as well as which benefits a full parser’s non-linear parametrization provides.	babf3c8cc57c22809b71d50e839db9c6aecec5d5	@['JournalArticle', 'Conference']{eberstein-etal-2022-probing,  author = {Max Müller-Eberstein and R. Goot and Barbara Plank},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7711-7726},  title = {Probing for Labeled Dependency Trees},  year = {2022} }
Probing for Incremental Parse States in Autoregressive Language Models	2022	http://www.semanticscholar.org/paper/307d522c7bd7eafc21e67027b207ad9690243715	It is suggested that implicit incremental syntactic inferences underlie next-word predictions in autoregressive neural language models and that probes can be used to predict model preferences on ambiguous sentence preﬁxes and causally intervene on model representations and steer model behavior.	yes	0	Next-word predictions from autoregressive neural language models show remarkable sensitivity to syntax. This work evaluates the extent to which this behavior arises as a result of a learned ability to maintain implicit representations of incremental syntactic structures. We extend work in syntactic probing to the incremental setting and present several probes for extracting incomplete syntactic structure (operationalized through parse states from a stack-based parser) from autoregressive language models. We ﬁnd that our probes can be used to predict model preferences on ambiguous sentence preﬁxes and causally intervene on model representations and steer model behavior. This suggests implicit incremental syntactic inferences underlie next-word predictions in autoregressive neural language models.	307d522c7bd7eafc21e67027b207ad9690243715	@['JournalArticle']{eisape-etal-2022-probing,  author = {Tiwalayo Eisape and Vineet Gangireddy and R. Levy and Yoon Kim},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Probing for Incremental Parse States in Autoregressive Language Models},  volume = {abs/2211.09748},  year = {2022} }
Probing for idiomaticity in vector space models	2021	http://www.semanticscholar.org/paper/b8c466002d2e8c26b11681a64775252098a3ae76	Results obtained using four types of probing measures indicate that idiomaticity is not yet accurately represented by contextualised models.	maybe	28	Contextualised word representation models have been successfully used for capturing different word usages and they may be an attractive alternative for representing idiomaticity in language. In this paper, we propose probing measures to assess if some of the expected linguistic properties of noun compounds, especially those related to idiomatic meanings, and their dependence on context and sensitivity to lexical choice, are readily available in some standard and widely used representations. For that, we constructed the Noun Compound Senses Dataset, which contains noun compounds and their paraphrases, in context neutral and context informative naturalistic sentences, in two languages: English and Portuguese. Results obtained using four types of probing measures with models like ELMo, BERT and some of its variants, indicate that idiomaticity is not yet accurately represented by contextualised models	b8c466002d2e8c26b11681a64775252098a3ae76	@['JournalArticle', 'Conference']{garcia-etal-2021-probing,  author = {Marcos Garcia and Tiago Kramer Vieira and Carolina Scarton and M. Idiart and A. Villavicencio},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {3551-3564},  title = {Probing for idiomaticity in vector space models},  year = {2021} }
Probing for Constituency Structure in Neural Language Models	2022	http://www.semanticscholar.org/paper/b7e66de289baad3f013b77a96de58e419721873e	It is shown that a complete constituency tree can be linearly separated from LM representations, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM.	yes	4	In this paper, we investigate to which extent contextual neural language models (LMs) implicitly learn syntactic structure. More concretely, we focus on constituent structure as represented in the Penn Treebank (PTB). Using standard probing techniques based on diagnostic classiﬁers, we assess the accuracy of representing constituents of different categories within the neuron activations of a LM such as RoBERTa. In order to make sure that our probe focuses on syntactic knowledge and not on implicit semantic generalizations, we also experiment on a PTB version that is obtained by randomly replacing constituents with each other while keeping syntactic structure, i.e., a semantically ill-formed but syntactically well-formed version of the PTB. We ﬁnd that 4 pretrained transfomer LMs obtain high performance on our probing tasks even on manipulated data, suggesting that semantic and syntactic knowledge in their representations can be separated and that constituency information is in fact learned by the LM. More-over, we show that a complete constituency tree can be linearly separated from LM representations. 1	b7e66de289baad3f013b77a96de58e419721873e	@['JournalArticle']{arps-etal-2022-probing,  author = {David Arps and Younes Samih and Laura Kallmeyer and Hassan Sajjad},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Probing for Constituency Structure in Neural Language Models},  volume = {abs/2204.06201},  year = {2022} }
Probing for Bridging Inference in Transformer Language Models	2021	https://www.semanticscholar.org/paper/1c0f8a6b8e6f4cc6fc81c4019fc07a2e5ec17107	This work probes pre-trained transformer language models for bridging inference in BERT and shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.	maybe	8	We probe pre-trained transformer language models for bridging inference. We first investigate individual attention heads in BERT and observe that attention heads at higher layers prominently focus on bridging relations in-comparison with the lower and middle layers, also, few specific attention heads concentrate consistently on bridging. More importantly, we consider language models as a whole in our second approach where bridging anaphora resolution is formulated as a masked token prediction task (Of-Cloze test). Our formulation produces optimistic results without any fine-tuning, which indicates that pre-trained language models substantially capture bridging inference. Our further investigation shows that the distance between anaphor-antecedent and the context provided to language models play an important role in the inference.	1c0f8a6b8e6f4cc6fc81c4019fc07a2e5ec17107	@['JournalArticle', 'Conference']{pandit-hou-2021-probing,  author = {O. Pandit and Yufang Hou},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4153-4163},  title = {Probing for Bridging Inference in Transformer Language Models},  year = {2021} }
Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary	2022	http://www.semanticscholar.org/paper/3cd9e5de457662f5e3c268f75341a93a16254e55	This work presents a method for enriching large Language Models with a grounded sense inventory available at the vocabulary level, without further training, and finds that LMs can learn non-trivial commonsense knowledge from self-supervision, covering numerous relations, and more effectively than comparable similarity-based approaches.	maybe	0	Progress on commonsense reasoning is usually measured from performance improvements on Question Answering tasks designed to require commonsense knowledge. However, fine-tuning large Language Models (LMs) on these specific tasks does not directly evaluate commonsense learned during pre-training. The most direct assessments of commonsense knowledge in pre-trained LMs are arguably cloze-style tasks targeting commonsense assertions (e.g., A pen is used for [MASK].). However, this approach is restricted by the LM’s vocabulary available for masked predictions, and its precision is subject to the context provided by the assertion. In this work, we present a method for enriching LMs with a grounded sense inventory (i.e., WordNet) available at the vocabulary level, without further training. This modification augments the prediction space of cloze-style prompts to the size of a large ontology while enabling finergrained (sense-level) queries and predictions. In order to evaluate LMs with higher precision, we propose SenseLAMA, a cloze-style task featuring verbalized relations from disambiguated triples sourced from WordNet, WikiData, and ConceptNet. Applying our method to BERT, producing a WordNet-enriched version named SynBERT, we find that LMs can learn non-trivial commonsense knowledge from self-supervision, covering numerous relations, and more effectively than comparable similarity-based approaches.	3cd9e5de457662f5e3c268f75341a93a16254e55	@['JournalArticle']{loureiro-jorge-2022-probing,  author = {Daniel Loureiro and A. Jorge},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Probing Commonsense Knowledge in Pre-trained Language Models with Sense-level Precision and Expanded Vocabulary},  volume = {abs/2210.06376},  year = {2022} }
Probing Commonsense Explanation in Dialogue Response Generation	2021	http://www.semanticscholar.org/paper/dffedd7dcacb2fab0af708b9a6a6de8424fe2fc2	This study formalizes the problem by framing commonsense as a latent variable in the RG task and using explanations for responses as textual form of commonsense, and collecting 6k annotated explanations justifying responses from four dialogue datasets and asking humans to verify them.	maybe	8	Humans use commonsense reasoning (CSR) implicitly to produce natural and coherent responses in conversations. Aiming to close the gap between current response generation (RG) models and human communication abilities, we want to understand why RG models respond as they do by probing RG model’s understanding of commonsense reasoning that elicits proper responses. We formalize the problem by framing commonsense as a latent variable in the RG task and using explanations for responses as textual form of commonsense. We collect 6k annotated explanations justifying responses from four dialogue datasets and ask humans to verify them and propose two probing settings to evaluate RG models’ CSR capabilities. Probing results show that models fail to capture the logical relations between commonsense explanations and responses and fine-tuning on in-domain data and increasing model sizes do not lead to understanding of CSR for RG. We hope our study motivates more research in making RG models emulate the human reasoning process in pursuit of smooth human-AI communication 1.	dffedd7dcacb2fab0af708b9a6a6de8424fe2fc2	@['JournalArticle', 'Conference']{zhou-etal-2021-probing,  author = {Pei Zhou and Pegah Jandaghi and Bill Yuchen Lin and Justin Cho and J. Pujara and Xiang Ren},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4132-4146},  title = {Probing Commonsense Explanation in Dialogue Response Generation},  year = {2021} }
Probing Classifiers: Promises, Shortcomings, and Advances	2021	https://www.semanticscholar.org/paper/3dcfa05a1c162e6cab927c5b08d0444f7b6691f4	This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.	yes	59	Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple—a classifier is trained to predict some linguistic property from a model’s representations—and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.	3dcfa05a1c162e6cab927c5b08d0444f7b6691f4	@['JournalArticle', 'Review']{belinkov-2021-probing,  author = {Yonatan Belinkov},  booktitle = {International Conference on Computational Logic},  journal = {Computational Linguistics},  pages = {207-219},  title = {Probing Classifiers: Promises, Shortcomings, and Advances},  volume = {48},  year = {2021} }
Probing and Generalization of Metaphorical Knowledge in Pre-Trained Language Models	2021	http://www.semanticscholar.org/paper/f988c1e34742883ac4ff3cab25fc5434076a1f18	The authors' extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers, and the knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets.	maybe	0	Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pretrained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and crossdataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.	f988c1e34742883ac4ff3cab25fc5434076a1f18	@None{2021-probing,  title = {Probing and Generalization of Metaphorical Knowledge in Pre-Trained Language Models},  year = {2021} }
Probing Across Time: What Does RoBERTa Know and When?	2021	https://www.semanticscholar.org/paper/0672f88d5dc762002b515ca4a0a9f101017fea35	It is believed that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efﬁcient approaches that accomplish necessary learning faster.	yes	32	Models of language trained on very large corpora have been demonstrated useful for natural language processing. As ﬁxed artifacts, they have become the object of intense study, with many researchers “probing” the extent to which they acquire and readily demonstrate linguistic abstractions, factual and commonsense knowledge, and reasoning abilities. Re-cent work applied several probes to intermediate training stages to observe the develop-mental process of a large-scale model (Chi-ang et al., 2020). Following this effort, we systematically answer a question: for various types of knowledge a language model learns, when during (pre)training are they acquired? Using RoBERTa as a case study, we ﬁnd: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efﬁcient approaches that accomplish necessary learning faster.	0672f88d5dc762002b515ca4a0a9f101017fea35	@['JournalArticle', 'Conference']{liu-etal-2021-probing,  author = {L. Liu and Yizhong Wang and Jungo Kasai and Hannaneh Hajishirzi and Noah A. Smith},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {820-842},  title = {Probing Across Time: What Does RoBERTa Know and When?},  year = {2021} }
Probe-Less Probing of BERT’s Layer-Wise Linguistic Knowledge with Masked Word Prediction	2022	http://www.semanticscholar.org/paper/764fd5b159cd8b468984b3b7bafca8a155954c69		yes	1	The current study quantitatively (and qualitatively for an illustrative purpose) analyzes BERT’s layer-wise masked word prediction on an English corpus, and finds that (1) the layerwise localization of linguistic knowledge primarily shown in probing studies is replicated in a behavior-based design and (2) that syntactic and semantic information is encoded at different layers for words of different syntactic categories. Hypothesizing that the above results are correlated with the number of likely potential candidates of the masked word prediction, we also investigate how the results differ for tokens within multiword expressions.	764fd5b159cd8b468984b3b7bafca8a155954c69	@['JournalArticle', 'Conference']{aoyama-schneider-2022-probe,  author = {T. Aoyama and Nathan Schneider},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {195-201},  title = {Probe-Less Probing of BERT’s Layer-Wise Linguistic Knowledge with Masked Word Prediction},  year = {2022} }
Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy	2022	http://www.semanticscholar.org/paper/1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a	It is argued that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization, and potential alternative deﬁnitions are discussed and why memorization is a crucial open question for neural language models.	maybe	1	Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works—and some recently deployed defenses—focus on “verbatim memo-rization”, deﬁned as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization. Speciﬁ-cally, we design and implement an efﬁcient defense based on Bloom ﬁlters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this “perfect” ﬁlter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and mini-mally modiﬁed “style-transfer” prompts—and in some cases even the non-modiﬁed original prompts—to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative deﬁnitions and why deﬁning memorization is a dif-ﬁcult yet crucial open question for neural language models.	1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a	@['JournalArticle']{ippolito-etal-2022-preventing,  author = {Daphne Ippolito and Florian Tramèr and Milad Nasr and Chiyuan Zhang and Matthew Jagielski and Katherine Lee and Christopher A. Choquette-Choo and Nicholas Carlini},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy},  volume = {abs/2210.17546},  year = {2022} }
Pretrained Transformers Improve Out-of-Distribution Robustness	2020	http://www.semanticscholar.org/paper/97f08c1ae8ca5ddf5948c66bfbbc0546ac154807	This work systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts and measures the generalization of previous models, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness.	maybe	222	Although pretrained Transformers such as BERT achieve high accuracy on in-distribution examples, do they generalize to new distributions? We systematically measure out-of-distribution (OOD) generalization for seven NLP datasets by constructing a new robustness benchmark with realistic distribution shifts. We measure the generalization of previous models including bag-of-words models, ConvNets, and LSTMs, and we show that pretrained Transformers’ performance declines are substantially smaller. Pretrained transformers are also more effective at detecting anomalous or OOD examples, while many previous models are frequently worse than chance. We examine which factors affect robustness, finding that larger models are not necessarily more robust, distillation can be harmful, and more diverse pretraining data can enhance robustness. Finally, we show where future work can improve OOD robustness.	97f08c1ae8ca5ddf5948c66bfbbc0546ac154807	@['JournalArticle', 'Conference']{hendrycks-etal-2020-pretrained,  author = {Dan Hendrycks and Xiaoyuan Liu and Eric Wallace and Adam Dziedzic and R. Krishnan and D. Song},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {2744-2751},  title = {Pretrained Transformers Improve Out-of-Distribution Robustness},  year = {2020} }
Pretrained Language Model Embryology: The Birth of ALBERT	2020	http://www.semanticscholar.org/paper/b1d309073623d46548e55269fb73485a3b7f11a8	The results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining, and it is found that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance.	maybe	17	While behaviors of pretrained language models (LMs) have been thoroughly examined, what happened during pretraining is rarely studied. We thus investigate the developmental process from a set of randomly initialized parameters to a totipotent language model, which we refer to as the embryology of a pretrained language model. Our results show that ALBERT learns to reconstruct and predict tokens of different parts of speech (POS) in different learning speeds during pretraining. We also find that linguistic knowledge and world knowledge do not generally improve as pretraining proceeds, nor do downstream tasks' performance. These findings suggest that knowledge of a pretrained model varies during pretraining, and having more pretrain steps does not necessarily provide a model with more comprehensive knowledge. We will provide source codes and pretrained models to reproduce our results at this https URL.	b1d309073623d46548e55269fb73485a3b7f11a8	@['JournalArticle', 'Conference']{chiang-etal-2020-pretrained,  author = {Cheng-Han Chiang and Sung-Feng Huang and Hung-yi Lee},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Pretrained Language Model Embryology: The Birth of ALBERT},  volume = {abs/2010.02480},  year = {2020} }
Prefix-Tuning: Optimizing Continuous Prompts for Generation	2021	http://www.semanticscholar.org/paper/53d8b356551a2361020a948f64454a6d599af69f	Prefix-tuning is proposed, a lightweight alternative to fine- Tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which is called the prefix.	maybe	738	Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were “virtual tokens”. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1% of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.	53d8b356551a2361020a948f64454a6d599af69f	@['JournalArticle', 'Conference']{li-liang-2021-prefix,  author = {Xiang Lisa Li and Percy Liang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},  title = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},  volume = {abs/2101.00190},  year = {2021} }
Predicting Reference: What Do Language Models Learn about Discourse Models?	2020	http://www.semanticscholar.org/paper/613a7e2b01ddf09d919d41fcfe1603986fd7c784	The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.	maybe	10	Whereas there is a growing literature that probes neural language models to assess the degree to which they have latently acquired grammatical knowledge, little if any research has investigated their acquisition of discourse modeling ability. We address this question by drawing on a rich psycholinguistic literature that has established how different contexts affect referential biases concerning who is likely to be referred to next. The results reveal that, for the most part, the prediction behavior of neural language models does not resemble that of human language users.	613a7e2b01ddf09d919d41fcfe1603986fd7c784	@['JournalArticle', 'Conference']{upadhye-etal-2020-predicting,  author = {Shiva Upadhye and Leon Bergen and A. Kehler},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {977-982},  title = {Predicting Reference: What Do Language Models Learn about Discourse Models?},  year = {2020} }
Predicting on the Edge: Identifying Where a Larger Model Does Better	2022	http://www.semanticscholar.org/paper/09b5571bda0b264e730a49169502b5003931229b	This paper demonstrates the surprisingly tight link between a model’s predictive uncertainty on individual examples and the likelihood that larger models will improve prediction on them and shows that a switcher model which defers examples to a larger model when a small model is uncertain can achieve striking improvements in performance and resource usage.	maybe	0	Much effort has been devoted to making large and more accurate models, but relatively little has been put into understanding which examples are benefiting from the added complexity. In this paper, we demonstrate and analyze the surprisingly tight link between a model’s predictive uncertainty on individual examples and the likelihood that larger models will improve prediction on them. Through extensive numerical studies on the T5 encoder-decoder architecture, we show that large models have the largest improvement on examples where the small model is most uncertain. On more certain examples, even those where the small model is not particularly accurate, large models are often unable to improve at all, and can even perform worse than the smaller model. Based on these findings, we show that a switcher model which defers examples to a larger model when a small model is uncertain can achieve striking improvements in performance and resource usage. We also explore committee-based uncertainty metrics that can be more effective but less practical.	09b5571bda0b264e730a49169502b5003931229b	@['JournalArticle']{narayan-etal-2022-predicting,  author = {Taman Narayan and Heinrich Jiang and Sen Zhao and Surinder Kumar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Predicting on the Edge: Identifying Where a Larger Model Does Better},  volume = {abs/2202.07652},  year = {2022} }
Predicting Inductive Biases of Pre-Trained Models	2021	http://www.semanticscholar.org/paper/a33b4a2002161a18bc7eb929566d77fd5178c2e9	The hypothesis that the extent to which a feature influences a model’s decisions can be predicted using a combination of two factors: the feature’'s extractability after pre-training, and the evidence available during finetuning is tested.	maybe	42	Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then finetuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via “probing classifiers”) finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via “challenge sets”) indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model’s decisions can be predicted using a combination of two factors: The feature’s extractability after pre-training (measured using information-theoretic probing techniques), and the evidence available during finetuning (defined as the feature’s co-occurrence rate with the label). In experiments with both synthetic and naturalistic data, we find strong evidence (statistically significant correlations) supporting this hypothesis.	a33b4a2002161a18bc7eb929566d77fd5178c2e9	@['JournalArticle']{lovering-etal-2021-predicting,  author = {Charles Lovering and Rohan Jha and Tal Linzen and Ellie Pavlick},  booktitle = {International Conference on Learning Representations},  title = {Predicting Inductive Biases of Pre-Trained Models},  year = {2021} }
Predicting Human Psychometric Properties Using Computational Language Models	2022	http://www.semanticscholar.org/paper/29e1a0a3f41fa1b8cc118ed16855c6de012f0bbd		maybe	3	. Transformer-based language models (LMs) continue to achieve state-of-the-art performance on natural language processing (NLP) benchmarks, including tasks designed to mimic human-inspired “commonsense” competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts from psychometrics. But to what extent can beneﬁts ﬂow in the other direction? In other words, can LMs be of use in predicting the psychometric properties of test items, when those items are given to human participants? If so, the beneﬁt for psychometric practitioners is enormous, as it can reduce the need for multiple rounds of empirical testing. We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the human responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions correlate. We ﬁnd that transformer-based LMs predict the human psychometric data consistently well across most categories, suggesting that they can be used to gather human-like psychometric data without the need for extensive human trials.	29e1a0a3f41fa1b8cc118ed16855c6de012f0bbd	@['JournalArticle']{laverghetta-etal-2022-predicting,  author = {A. Laverghetta and Animesh Nighojkar and Jamshidbek Mirzakhalov and John Licato},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Predicting Human Psychometric Properties Using Computational Language Models},  volume = {abs/2205.06203},  year = {2022} }
Predicting Fine-Tuning Performance with Probing	2022	http://www.semanticscholar.org/paper/157ff71fb4818aab80ef4b54187251bff23449c6	It is found that it is possible to use the accuracies of only three probing results to predict the fine-tuning performance of deep NLP models with errors 40% 80% smaller than baselines.	maybe	2	Large NLP models have recently shown impres001 sive performance in language understanding 002 tasks, typically evaluated by fine-tuning tasks. 003 Alternatively, probing has received increasing 004 attention as being a lightweight method for in005 terpreting the intrinsic mechanisms of large 006 NLP models. In probing, post-hoc classifiers 007 are trained on “out-of-domain” datasets that 008 diagnose specific abilities. While probing the 009 language models has led to insightful findings, 010 they appear disjointed from the development 011 of models. This paper explores the utility of 012 probing deep NLP models to extract a proxy 013 signal widely used in model developments, the 014 fine-tuning performance. We find that it is pos015 sible to use the accuracies of only three probing 016 results to predict the fine-tuning performance 017 with errors 40% 80% smaller than baselines. 018 We further show the possibility of incorporat019 ing specialized probing datasets into develop020 ing deep NLP models. 021	157ff71fb4818aab80ef4b54187251bff23449c6	@None{pimentel-2022-predicting,  author = {Tiago Pimentel},  title = {Predicting Fine-Tuning Performance with Probing},  year = {2022} }
Predictability and Surprise in Large Generative Models	2022	http://www.semanticscholar.org/paper/9cbc044e315cdefe9a255119037ac7c23e9abdd5	This paper highlights a counterintuitive property of large-scale generative models, which have a paradoxical combination of predictable loss on a broad training distribution, and unpredictable specific capabilities, inputs, and outputs, and analyzed how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment.	maybe	23	Large-scale pre-training has recently emerged as a technique for creating capable, general-purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have a paradoxical combination of predictable loss on a broad training distribution (as embodied in their ”scaling laws”), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend for this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, funders who want to support work addressing these challenges, and academics who want to analyze, critique, and potentially develop large generative models.	9cbc044e315cdefe9a255119037ac7c23e9abdd5	@['JournalArticle', 'Book']{ganguli-etal-2022-predictability,  author = {Deep Ganguli and Danny Hernandez and Liane Lovitt and Nova DasSarma and T. Henighan and Andy Jones and Nicholas Joseph and John Kernion and Benjamin Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Nelson Elhage and Sheer El Showk and Stanislav Fort and Zac Hatfield-Dodds and Scott Johnston and S. Kravec and Neel Nanda and Kamal Ndousse and Catherine Olsson and Daniela Amodei and Dario Amodei and Tom B. Brown and Jared Kaplan and Sam McCandlish and C. Olah and Jack Clark},  booktitle = {Conference on Fairness, Accountability and Transparency},  journal = {2022 ACM Conference on Fairness, Accountability, and Transparency},  title = {Predictability and Surprise in Large Generative Models},  year = {2022} }
Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning	2020	http://www.semanticscholar.org/paper/d6599d4dfaeb78bea1f975db683aa653e26b3987	This paper introduces a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase and requires less annotated data than the standard classifier approach to reach equivalent performances.	maybe	34	Fine-tuning of pre-trained transformer models has become the standard approach for solving common NLP tasks. Most of the existing approaches rely on a randomly initialized classifier on top of such networks. We argue that this fine-tuning procedure is sub-optimal as the pre-trained model has no prior on the specific classifier labels, while it might have already learned an intrinsic textual representation of the task. In this paper, we introduce a new scoring method that casts a plausibility ranking task in a full-text format and leverages the masked language modeling head tuned during the pre-training phase. We study commonsense reasoning tasks where the model must rank a set of hypotheses given a premise, focusing on the COPA, Swag, HellaSwag and CommonsenseQA datasets. By exploiting our scoring method without fine-tuning, we are able to produce strong baselines (e.g. 80% test accuracy on COPA) that are comparable to supervised approaches. Moreover, when fine-tuning directly on the proposed scoring function, we show that our method provides a much more stable training phase across random restarts (e.g x10 standard deviation reduction on COPA test accuracy) and requires less annotated data than the standard classifier approach to reach equivalent performances.	d6599d4dfaeb78bea1f975db683aa653e26b3987	@['JournalArticle', 'Conference']{tamborrino-etal-2020-pre,  author = {Alexandre Tamborrino and Nicola Pellicanò and B. Pannier and Pascal Voitot and Louise Naudin},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Pre-training Is (Almost) All You Need: An Application to Commonsense Reasoning},  volume = {abs/2004.14074},  year = {2020} }
Pre-trained Language Models’ Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context	2022	https://www.semanticscholar.org/paper/801993d649028afc794808486f317e1ce61d7c53		maybe	0	By saying Maria is tall, a human speaker typically implies that Maria is evaluatively tall from the speaker’s perspective. However, by using a different construction Maria is taller than Sophie, we cannot infer from Maria and Sophie’s relative heights that Maria is evaluatively tall because it is possible for Maria to be taller than Sophie in a context in which they both count as short. Can pre-trained language models (LMs) “understand” evaulativity (EVAL) inference? To what extent can they discern the EVAL salience of different constructions in a conversation? Will it help LMs’ implicitness performance if we give LMs a persona such as chill, social, and pragmatically skilled? Our study provides an approach to probing LMs’ interpretation of EVAL inference by incorporating insights from experimental pragmatics and sociolinguistics. We find that with the appropriate prompt, LMs can succeed in some pragmatic level language understanding tasks. Our study suggests that socio-pragmatics methodology can shed light on the challenging questions in NLP.	801993d649028afc794808486f317e1ce61d7c53	@None{cong-2022-pre,  author = {Yan Cong},  booktitle = {UNIMPLICIT},  journal = {Proceedings of the Second Workshop on Understanding Implicit and Underspecified Language},  title = {Pre-trained Language Models’ Interpretation of Evaluativity Implicature: Evidence from Gradable Adjectives Usage in Context},  year = {2022} }
Pragmatic competence of pre-trained language models through the lens of discourse connectives	2021	http://www.semanticscholar.org/paper/436ece91458eb5e2833ac31bc22041b0866193a1	It is found that although models predict connectives reasonably well in the context of naturally-occurring data, when the authors control contexts to isolate high-level pragmatic cues, model sensitivity is much lower and models also do not show substantial humanlike temporal preferences.	maybe	10	As pre-trained language models (LMs) continue to dominate NLP, it is increasingly important that we understand the depth of language capabilities in these models. In this paper, we target pre-trained LMs’ competence in pragmatics, with a focus on pragmatics relating to discourse connectives. We formulate cloze-style tests using a combination of naturally-occurring data and controlled inputs drawn from psycholinguistics. We focus on testing models’ ability to use pragmatic cues to predict discourse connectives, models’ ability to understand implicatures relating to connectives, and the extent to which models show humanlike preferences regarding temporal dynamics of connectives. We find that although models predict connectives reasonably well in the context of naturally-occurring data, when we control contexts to isolate high-level pragmatic cues, model sensitivity is much lower. Models also do not show substantial humanlike temporal preferences. Overall, the findings suggest that at present, dominant pre-training paradigms do not result in substantial pragmatic competence in our models.	436ece91458eb5e2833ac31bc22041b0866193a1	@['JournalArticle']{pandia-etal-2021-pragmatic,  author = {Lalchand Pandia and Yan Cong and Allyson Ettinger},  booktitle = {Conference on Computational Natural Language Learning},  pages = {367-379},  title = {Pragmatic competence of pre-trained language models through the lens of discourse connectives},  year = {2021} }
Post-hoc Interpretability for Neural NLP: A Survey	2021	http://www.semanticscholar.org/paper/898b14509593d235414df054527b7702e35c3099	This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, and discusses each method in-depth, and how they are validated, as the latter is often a common concern.	maybe	34	Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.	898b14509593d235414df054527b7702e35c3099	@['JournalArticle', 'Review']{madsen-etal-2021-post,  author = {Andreas Madsen and Siva Reddy and A. Chandar},  booktitle = {ACM Computing Surveys},  journal = {ACM Computing Surveys},  pages = {1 - 42},  title = {Post-hoc Interpretability for Neural NLP: A Survey},  volume = {55},  year = {2021} }
Post-hoc analysis of Arabic transformer models	2022	http://www.semanticscholar.org/paper/9a9eff68eacc844b6495779d404faa7251a8cd07	This work probes how linguistic information is encoded in the transformer models, trained on different Arabic dialects, using morphological tagging tasks for different dialects of Arabic and a dialectal identiﬁcation task and finds that neurons in embedding layers are polysemous in nature, while the neurons in middle layers are exclusive to speci ﬁc properties.	maybe	0	Arabic is a Semitic language which is widely spoken with many dialects. Given the success of pre-trained language models, many transformer models trained on Arabic and its dialects have surfaced. While there have been an extrinsic evaluation of these models with respect to downstream NLP tasks, no work has been carried out to analyze and compare their internal representations. We probe how linguistic information is encoded in the transformer models, trained on different Arabic dialects. We perform a layer and neuron analysis on the models using morphological tagging tasks for different dialects of Arabic and a dialectal identification task. Our analysis enlightens interesting findings such as: i) word morphology is learned at the lower and middle layers, ii) while syntactic dependencies are predominantly captured at the higher layers, iii) despite a large overlap in their vocabulary, the MSA-based models fail to capture the nuances of Arabic dialects, iv) we found that neurons in embedding layers are polysemous in nature, while the neurons in middle layers are exclusive to specific properties.	9a9eff68eacc844b6495779d404faa7251a8cd07	@['JournalArticle']{abdelali-etal-2022-post,  author = {Ahmed Abdelali and Nadir Durrani and Fahim Dalvi and Hassan Sajjad},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {Post-hoc analysis of Arabic transformer models},  volume = {abs/2210.09990},  year = {2022} }
Possible Stories: Evaluating Situated Commonsense Reasoning under Multiple Possible Scenarios	2022	http://www.semanticscholar.org/paper/2066132aec5538b20b3b7b007a2ffcad3a71cfb8	This study frames this task by asking multiple questions with the same set of possible endings as candidate answers, given a short story text, and discovers that even current strong pretrained language models struggle to answer the questions consistently.	maybe	0	The possible consequences for the same context may vary depending on the situation we refer to. However, current studies in natural language processing do not focus on situated commonsense reasoning under multiple possible scenarios. This study frames this task by asking multiple questions with the same set of possible endings as candidate answers, given a short story text. Our resulting dataset, Possible Stories, consists of more than 4.5K questions over 1.3K story texts in English. We discover that even current strong pretrained language models struggle to answer the questions consistently, highlighting that the highest accuracy in an unsupervised setting (60.2%) is far behind human accuracy (92.5%). Through a comparison with existing datasets, we observe that the questions in our dataset contain minimal annotation artifacts in the answer options. In addition, our dataset includes examples that require counterfactual reasoning, as well as those requiring readers’ reactions and fictional information, suggesting that our dataset can serve as a challenging testbed for future studies on situated commonsense reasoning.	2066132aec5538b20b3b7b007a2ffcad3a71cfb8	@['JournalArticle', 'Conference']{ashida-sugawara-2022-possible,  author = {Mana Ashida and Saku Sugawara},  booktitle = {International Conference on Computational Linguistics},  pages = {3606-3630},  title = {Possible Stories: Evaluating Situated Commonsense Reasoning under Multiple Possible Scenarios},  year = {2022} }
Pooled Contextualized Embeddings for Named Entity Recognition	2019	https://www.semanticscholar.org/paper/edfe9dd16316618e694cd087d0d418dac91eb48c	This work proposes a method in which it dynamically aggregate contextualized embeddings of each unique string that the authors encounter and uses a pooling operation to distill a ”global” word representation from all contextualized instances.	seed	222	Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.	edfe9dd16316618e694cd087d0d418dac91eb48c	@['JournalArticle', 'Conference']{akbik-etal-2019-pooled,  author = {A. Akbik and Tanja Bergmann and Roland Vollgraf},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {724-728},  title = {Pooled Contextualized Embeddings for Named Entity Recognition},  year = {2019} }
PolyLM: Learning about Polysemy through Language Modeling	2021	http://www.semanticscholar.org/paper/5ea131f148299adb97e36a2001b08c468b4bfff0	PolyLM is introduced, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied, and performs considerably better than previous sense embedding techniques, and matches the current state-of-the-art specialized WSI method despite having six times fewer parameters.	maybe	2	To avoid the “meaning conflation deficiency” of word embeddings, a number of models have aimed to embed individual word senses. These methods at one time performed well on tasks such as word sense induction (WSI), but they have since been overtaken by task-specific techniques which exploit contextualized embeddings. However, sense embeddings and contextualization need not be mutually exclusive. We introduce PolyLM, a method which formulates the task of learning sense embeddings as a language modeling problem, allowing contextualization techniques to be applied. PolyLM is based on two underlying assumptions about word senses: firstly, that the probability of a word occurring in a given context is equal to the sum of the probabilities of its individual senses occurring; and secondly, that for a given occurrence of a word, one of its senses tends to be much more plausible in the context than the others. We evaluate PolyLM on WSI, showing that it performs considerably better than previous sense embedding techniques, and matches the current state-of-the-art specialized WSI method despite having six times fewer parameters. Code and pre-trained models are available at https://github.com/AlanAnsell/PolyLM.	5ea131f148299adb97e36a2001b08c468b4bfff0	@['JournalArticle', 'Conference']{ansell-etal-2021-polylm:,  author = {Alan Ansell and Felipe Bravo-Marquez and B. Pfahringer},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {PolyLM: Learning about Polysemy through Language Modeling},  volume = {abs/2101.10448},  year = {2021} }
Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models	2021	http://www.semanticscholar.org/paper/15e71e497a67423bfedd0d63efe423c4660e5053	Polyjuice is presented, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences.	maybe	76	While counterfactual examples are useful for analysis and training of NLP models, current generation methods either rely on manual labor to create very few counterfactuals, or only instantiate limited types of perturbations such as paraphrases or word substitutions. We present Polyjuice, a general-purpose counterfactual generator that allows for control over perturbation types and locations, trained by finetuning GPT-2 on multiple datasets of paired sentences. We show that Polyjuice produces diverse sets of realistic counterfactuals, which in turn are useful in various distinct applications: improving training and evaluation on three different tasks (with around 70% less annotation effort than manual generation), augmenting state-of-the-art explanation techniques, and supporting systematic counterfactual error analysis by revealing behaviors easily missed by human experts.	15e71e497a67423bfedd0d63efe423c4660e5053	@['JournalArticle', 'Conference']{wu-etal-2021-polyjuice:,  author = {Tongshuang Sherry Wu and Marco Tulio Ribeiro and Jeffrey Heer and Daniel S. Weld},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {6707-6723},  title = {Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models},  year = {2021} }
Plug and Play Language Models: A Simple Approach to Controlled Text Generation	2019	http://www.semanticscholar.org/paper/e04a80263d252a3d8a382ba37a249b9345620570	The Plug and Play Language Model (PPLM) for controllable language generation is proposed, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM.	maybe	424	Large transformer-based language models (LMs) trained on huge text corpora have shown unparalleled generation capabilities. However, controlling attributes of the generated language (e.g. switching topic or sentiment) is difficult without modifying the model architecture or fine-tuning on attribute-specific data and entailing the significant cost of retraining. We propose a simple alternative: the Plug and Play Language Model (PPLM) for controllable language generation, which combines a pretrained LM with one or more simple attribute classifiers that guide text generation without any further training of the LM. In the canonical scenario we present, the attribute models are simple classifiers consisting of a user-specified bag of words or a single learned layer with 100,000 times fewer parameters than the LM. Sampling entails a forward and backward pass in which gradients from the attribute model push the LM's hidden activations and thus guide the generation. Model samples demonstrate control over a range of topics and sentiment styles, and extensive automated and human annotated evaluations show attribute alignment and fluency. PPLMs are flexible in that any combination of differentiable attribute models may be used to steer text generation, which will allow for diverse and creative applications beyond the examples given in this paper.	e04a80263d252a3d8a382ba37a249b9345620570	@['JournalArticle']{dathathri-etal-2019-plug,  author = {Sumanth Dathathri and Andrea Madotto and Janice Lan and Jane Hung and Eric Frank and Piero Molino and J. Yosinski and Rosanne Liu},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Plug and Play Language Models: A Simple Approach to Controlled Text Generation},  volume = {abs/1912.02164},  year = {2019} }
Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models	2022	https://www.semanticscholar.org/paper/d072b46a0504ac023d5035d8ec0c7876151245c4		yes	2		d072b46a0504ac023d5035d8ec0c7876151245c4	@['JournalArticle']{sobieszek-price-2022-playing,  author = {Adam Sobieszek and Tadeusz Price},  booktitle = {Minds and Machines},  journal = {Minds and Machines},  pages = {341 - 364},  title = {Playing Games with Ais: The Limits of GPT-3 and Similar Large Language Models},  volume = {32},  year = {2022} }
Planting and Mitigating Memorized Content in Predictive-Text Language Models	2022	http://www.semanticscholar.org/paper/e4b8556443d90e273da2d8ce848953f7c08f7d0c	This study test theacy of a range of privacy-preserving techniques to mitigate unintended memorization of sensitive user text, while varying other factors such as model size and adversarial conditions.	maybe	0	Language models are widely deployed to provide automatic text completion services in user products. However, recent research has revealed that language models (especially large ones) bear considerable risk of memorizing private training data, which is then vulnerable to leakage and extraction by adversaries. In this study, we test the efﬁcacy of a range of privacy-preserving techniques to mitigate unintended memorization of sensitive user text, while varying other factors such as model size and adversarial conditions. We test both “heuristic” mitigations (those without formal privacy guarantees) and Differentially Private training, which provides provable levels of privacy at the cost of some model performance. Our experiments show that (with the exception of L2 regularization), heuristic mitigations are largely ineffective in preventing memorization in our test suite, possibly because they make too strong of assumptions about the characteristics that deﬁne “sensitive” or “private” text. In contrast, Differential Privacy reliably prevents memorization in our experiments, despite its computational and model-performance costs.	e4b8556443d90e273da2d8ce848953f7c08f7d0c	@['JournalArticle']{downey-etal-2022-planting,  author = {C.M. Downey and Wei Dai and Huseyin A. Inan and Kim Laine and Saurabh Naik and Tomasz Religa},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Planting and Mitigating Memorized Content in Predictive-Text Language Models},  volume = {abs/2212.08619},  year = {2022} }
Planning with Large Language Models via Corrective Re-prompting	2022	http://www.semanticscholar.org/paper/f318ab67ac22cb758e38a16dafdc8e486b7b9756	This work proposes a prompting-based strategy for extracting executable plans from an LLM, which leverages a novel and readily-accessible source of information: precondition errors, and improves executability and semantic correctness of plans, while also reducing the number of re-prompts required when querying actions.	maybe	0	Extracting the common sense knowledge present in Large Language Models (LLMs) offers a path to designing intelligent, embodied agents. Related works have queried LLMs with a wide-range of contextual information, such as goals, sensor observations and scene descriptions, to generate high-level action plans for speciﬁc tasks; however these approaches often involve human intervention or additional machinery to enable sensor-motor interactions. In this work, we propose a prompting-based strategy for extracting executable plans from an LLM, which leverages a novel and readily-accessible source of information: precondition errors . Our approach assumes that actions are only afforded execution in certain contexts, i.e., implicit preconditions must be met for an action to execute (e.g., a door must be unlocked to open it), and that the embodied agent has the ability to determine if the action is/is not executable in the current context (e.g., detect if a precondition error is present). When an agent is unable to execute an action, our approach re-prompts the LLM with precondition error information to extract an executable corrective action to achieve the intended goal in the current context. We evaluate our approach in the VirtualHome simulation environment on 88 different tasks and 7 scenes. We evaluate different prompt templates and compare to methods that naively re-sample actions from the LLM. Our approach, using precondition errors, improves executability and semantic correctness of plans, while also reducing the number of re-prompts required when querying actions. supplies contextual information in the form of precondition errors to improve the generation of plans. Our results demonstrate that LLMs that can leverage precondition errors with error-cause-information to produce more semantically correct and executable plans. We also show that leveraging affordance information (i.e., determining that an action is not currently feasible to executable but without explaining why) for prompt-based strategies is more effective than naively re-sampling actions from the LLM. Finally, our scoring function is shown to improve the consistency of correct plans with higher inter-rater agreements	f318ab67ac22cb758e38a16dafdc8e486b7b9756	@['JournalArticle']{raman-etal-2022-planning,  author = {S. S. Raman and Vanya Cohen and Eric Rosen and Ifrah Idrees and D. Paulius and Stefanie Tellex},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Planning with Large Language Models via Corrective Re-prompting},  volume = {abs/2211.09935},  year = {2022} }
Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models	2020	http://www.semanticscholar.org/paper/00b1e962182c42949822821dc1a929bd132ec082	This work describes the organization of the brain’s distributed understanding system, which includes a fast learning system that addresses the memory problem and sketches a framework for future models of understanding drawing equally on cognitive neuroscience and artificial intelligence and exploiting query-based attention.	maybe	38	Language is crucial for human intelligence, but what exactly is its role? We take language to be a part of a system for understanding and communicating about situations. In humans, these abilities emerge gradually from experience and depend on domain-general principles of biological neural networks: connection-based learning, distributed representation, and context-sensitive, mutual constraint satisfaction-based processing. Current artificial language processing systems rely on the same domain general principles, embodied in artificial neural networks. Indeed, recent progress in this field depends on query-based attention, which extends the ability of these systems to exploit context and has contributed to remarkable breakthroughs. Nevertheless, most current models focus exclusively on language-internal tasks, limiting their ability to perform tasks that depend on understanding situations. These systems also lack memory for the contents of prior situations outside of a fixed contextual span. We describe the organization of the brain’s distributed understanding system, which includes a fast learning system that addresses the memory problem. We sketch a framework for future models of understanding drawing equally on cognitive neuroscience and artificial intelligence and exploiting query-based attention. We highlight relevant current directions and consider further developments needed to fully capture human-level language understanding in a computational system.	00b1e962182c42949822821dc1a929bd132ec082	@['JournalArticle']{mcclelland-etal-2020-placing,  author = {James L. McClelland and Felix Hill and Maja R. Rudolph and Jason Baldridge and Hinrich Schütze},  booktitle = {Proceedings of the National Academy of Sciences},  journal = {Proceedings of the National Academy of Sciences},  pages = {25966 - 25974},  title = {Placing language in an integrated understanding system: Next steps toward human-level performance in neural language models},  volume = {117},  year = {2020} }
Pitfalls of Static Language Modelling	2021	http://www.semanticscholar.org/paper/e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da	It is argued that now is the right time to rethink the static language modelling evaluation protocol, and develop adaptive language models that can remain up-to-date with respect to the ever-changing and non-stationary world.	yes	32	Our world is open-ended, non-stationary and constantly evolving; thus what we talk about and how we talk about it changes over time. This inherent dynamic nature of language comes in stark contrast to the current static language modelling paradigm, which constructs training and evaluation sets from overlapping time periods. Despite recent progress, we demonstrate that state-of-the-art Transformer models perform worse in the realistic setup of predicting future utterances from beyond their training period—a consistent pattern across three datasets from two domains. We find that, while increasing model size alone—a key driver behind recent progress—does not provide a solution for the temporal generalization problem, having models that continually update their knowledge with new information can indeed slow down the degradation over time. Hence, given the compilation of ever-larger language modelling training datasets, combined with the growing list of language-model-based NLP applications that require up-to-date knowledge about the world, we argue that now is the right time to rethink our static language modelling evaluation protocol, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world.1	e7a2ffd26cd76e5b662ecb8624ecb3e177dbb8da	@['JournalArticle']{lazaridou-etal-2021-pitfalls,  author = {Angeliki Lazaridou and A. Kuncoro and E. Gribovskaya and Devang Agrawal and Adam Liska and Tayfun Terzi and Mai Gimenez and Cyprien de Masson d'Autume and Sebastian Ruder and Dani Yogatama and Kris Cao and Tomás Kociský and Susannah Young and P. Blunsom},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Pitfalls of Static Language Modelling},  volume = {abs/2102.01951},  year = {2021} }
PIQA: Reasoning about Physical Commonsense in Natural Language	2019	http://www.semanticscholar.org/paper/04f4e55e14150b7c48b0287ba77c7443df76ed45	The task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA are introduced and analysis about the dimensions of knowledge that existing models lack are provided, which offers significant opportunities for future research.	maybe	240	To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains – such as news articles and encyclopedia entries, where text is plentiful – in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (∼75%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.	04f4e55e14150b7c48b0287ba77c7443df76ed45	@['JournalArticle', 'Conference']{bisk-etal-2019-piqa:,  author = {Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},  booktitle = {AAAI Conference on Artificial Intelligence},  journal = {ArXiv},  title = {PIQA: Reasoning about Physical Commonsense in Natural Language},  volume = {abs/1911.11641},  year = {2019} }
Pipelines for Social Bias Testing of Large Language Models	2022	https://www.semanticscholar.org/paper/2972ad9cd2f5a8efaffce15fc527e1a8644b081a	This short paper suggests how to use verification techniques in development pipelines by taking inspiration from software testing and addressing social bias evaluation as software testing.	maybe	7	The maturity level of language models is now at a stage in which many companies rely on them to solve various tasks. However, while research has shown how biased and harmful these models are, systematic ways of integrating social bias tests into development pipelines are still lacking. This short paper suggests how to use these verification techniques in development pipelines. We take inspiration from software testing and suggest addressing social bias evaluation as software testing. We hope to open a discussion on the best methodologies to handle social bias testing in language models.	2972ad9cd2f5a8efaffce15fc527e1a8644b081a	@None{nozza-etal-2022-pipelines,  author = {Debora Nozza and Federico Bianchi and Dirk Hovy},  booktitle = {BIGSCIENCE},  journal = {Proceedings of BigScience Episode #5 -- Workshop on Challenges &amp; Perspectives in Creating Large Language Models},  title = {Pipelines for Social Bias Testing of Large Language Models},  year = {2022} }
PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World	2021	http://www.semanticscholar.org/paper/32feca141fce06c6588b4014d27953a3fc25f19b	A model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language, and is able to correctly forecast “what happens next” given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%.	maybe	26	We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don’t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast what happens next given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.	32feca141fce06c6588b4014d27953a3fc25f19b	@['JournalArticle', 'Conference']{zellers-etal-2021-piglet:,  author = {Rowan Zellers and Ari Holtzman and Matthew E. Peters and Roozbeh Mottaghi and Aniruddha Kembhavi and Ali Farhadi and Yejin Choi},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {2040-2050},  title = {PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World},  year = {2021} }
Picking BERT’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis	2020	http://www.semanticscholar.org/paper/f0ed5550bafbe8d2ec85f8ba519d55b0775fa359	The ability of the Representational Similarity Analysis approach to adjudicate between hypotheses about which aspects of context are encoded in representations of language is demonstrated.	maybe	14	As the name implies, contextualized representations of language are typically motivated by their ability to encode context. Which aspects of context are captured by such representations? We introduce an approach to address this question using Representational Similarity Analysis (RSA). As case studies, we investigate the degree to which a verb embedding encodes the verb’s subject, a pronoun embedding encodes the pronoun’s antecedent, and a full-sentence representation encodes the sentence’s head word (as determined by a dependency parse). In all cases, we show that BERT’s contextualized embeddings reflect the linguistic dependency being studied, and that BERT encodes these dependencies to a greater degree than it encodes less linguistically-salient controls. These results demonstrate the ability of our approach to adjudicate between hypotheses about which aspects of context are encoded in representations of language.	f0ed5550bafbe8d2ec85f8ba519d55b0775fa359	@['JournalArticle', 'Conference']{lepori-mccoy-2020-picking,  author = {Michael A. Lepori and R. Thomas McCoy},  booktitle = {International Conference on Computational Linguistics},  journal = {ArXiv},  title = {Picking BERT’s Brain: Probing for Linguistic Dependencies in Contextualized Embeddings Using Representational Similarity Analysis},  volume = {abs/2011.12073},  year = {2020} }
Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing	2021	https://www.semanticscholar.org/paper/a5881560968963d0c845c468a273261fde0b7248	This paper demonstrates how interpretations can be manipulated by making simple word perturbations on an input text to attack two SOTA interpretation methods, across three popular Transformer models and on three different NLP datasets.	maybe	10	Interpretability methods like Integrated Gradient and LIME are popular choices for explaining natural language model predictions with relative word importance scores. These interpretations need to be robust for trustworthy NLP applications in high-stake areas like medicine or finance. Our paper demonstrates how interpretations can be manipulated by making simple word perturbations on an input text. Via a small portion of word-level swaps, these adversarial perturbations aim to make the resulting text semantically and spatially similar to its seed input (therefore sharing similar interpretations). Simultaneously, the generated examples achieve the same prediction label as the seed yet are given a substantially different explanation by the interpretation methods. Our experiments generate fragile interpretations to attack two SOTA interpretation methods, across three popular Transformer models and on three different NLP datasets. We observe that the rank order correlation and top-K intersection score drops by over 20% when less than 10% of words are perturbed on average. Further, rank-order correlation keeps decreasing as more words get perturbed. Furthermore, we demonstrate that candidates generated from our method have good quality metrics.	a5881560968963d0c845c468a273261fde0b7248	@['JournalArticle']{sinha-etal-2021-perturbing,  author = {Sanchit Sinha and Hanjie Chen and Arshdeep Sekhon and Yangfeng Ji and Yanjun Qi},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {420-434},  title = {Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing},  year = {2021} }
Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT	2020	https://www.semanticscholar.org/paper/3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f	A parameter-free probing technique for analyzing pre-trained language models (e.g., BERT) is proposed and it is shown that syntactic trees recovered from BERT using the method are significantly better than linguistically-uninformed baselines.	seed	88	By introducing a small set of additional parameters, a probe learns to solve specific linguistic tasks (e.g., dependency parsing) in a supervised manner using feature representations (e.g., contextualized embeddings). The effectiveness of such probing tasks is taken as evidence that the pre-trained model encodes linguistic knowledge. However, this approach of evaluating a language model is undermined by the uncertainty of the amount of knowledge that is learned by the probe itself. Complementary to those works, we propose a parameter-free probing technique for analyzing pre-trained language models (e.g., BERT). Our method does not require direct supervision from the probing tasks, nor do we introduce additional parameters to the probing process. Our experiments on BERT show that syntactic trees recovered from BERT using our method are significantly better than linguistically-uninformed baselines. We further feed the empirically induced dependency structures into a downstream sentiment classification task and find its improvement compatible with or even superior to a human-designed dependency schema.	3aaa8aaad5ef36550a6b47d6ee000f0b346a5a1f	@['JournalArticle', 'Conference']{wu-etal-2020-perturbed,  author = {Zhiyong Wu and Yun Chen and B. Kao and Qun Liu},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4166-4176},  title = {Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT},  year = {2020} }
Persistent Anti-Muslim Bias in Large Language Models	2021	https://www.semanticscholar.org/paper/4c2733d191e347753bb28afa46a1c55c65e085be	GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups.	yes	97	It has been observed that large-scale language models capture undesirable societal biases, e.g. relating to race and gender; yet religious bias has been relatively unexplored. We demonstrate that GPT-3, a state-of-the-art contextual language model, captures persistent Muslim-violence bias. We probe GPT-3 in various ways, including prompt completion, analogical reasoning, and story generation, to understand this anti-Muslim bias, demonstrating that it appears consistently and creatively in different uses of the model and that it is severe even compared to biases about other religious groups. For instance, Muslim is analogized to terrorist in 23% of test cases, while Jewish is mapped to its most common stereotype, money, in 5% of test cases. We quantify the positive distraction needed to overcome this bias with adversarial text prompts, and find that use of the most positive 6 adjectives reduces violent completions for Muslims from 66% to 20%, but which is still higher than for other religious groups.	4c2733d191e347753bb28afa46a1c55c65e085be	@['JournalArticle', 'Book']{abid-etal-2021-persistent,  author = {Abubakar Abid and M. Farooqi and James Y. Zou},  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society},  journal = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},  title = {Persistent Anti-Muslim Bias in Large Language Models},  year = {2021} }
Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions	2022	http://www.semanticscholar.org/paper/a6ccf0231ebcb417dc2e2230488c85417e6c3bfb	A novel framework to generate pragmatically relevant true and false instances of a generic, which outperforms few-shot generation from GPT-3 and high-lights the importance of constrained decoding for this task and the implications of generics EXEMPLARS for language inference tasks.	maybe	3	Generics express generalizations about the world (e.g., “birds can ﬂy"). However, they are not universally true – while sparrows and penguins are both birds, only sparrows can ﬂy and penguins cannot. Commonsense knowledge bases, that are used extensively in many NLP tasks as a source of world-knowledge, can often encode generic knowledge but, by-design, cannot encode such exceptions. It is crucial to realize the speciﬁc instances when a generic statement is true or false. In this work, we present a novel framework to generate pragmatically relevant true and false instances of a generic. We use pre-trained language models, constraining the generation based on insights from linguistic theory, and produce ∼ 20 k EX EMPLARS for ∼ 650 generics. Our system outperforms few-shot generation from GPT-3 (by 12.5 precision points) and our analysis high-lights the importance of constrained decoding for this task and the implications of generics EXEMPLARS for language inference tasks.	a6ccf0231ebcb417dc2e2230488c85417e6c3bfb	@['JournalArticle']{allaway-etal-2022-penguins,  author = {Emily Allaway and Jena D. Hwang and Chandra Bhagavatula and K. McKeown and Doug Downey and Yejin Choi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Penguins Don't Fly: Reasoning about Generics through Instantiations and Exceptions},  volume = {abs/2205.11658},  year = {2022} }
Pathologies of Pre-trained Language Models in Few-shot Fine-tuning	2022	http://www.semanticscholar.org/paper/4e2cd3c33130ceabc5373771f9ad3aca6f427f99	The analysis shows models gain performance improvement by capturing non-task-related features or shallow data patterns, alerting that pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot fine-tuning.	maybe	1	Although adapting pre-trained language models with few examples has shown promising performance on text classification, there is a lack of understanding of where the performance gain comes from. In this work, we propose to answer this question by interpreting the adaptation behavior using post-hoc explanations from model predictions. By modeling feature statistics of explanations, we discover that (1) without fine-tuning, pre-trained models (e.g. BERT and RoBERTa) show strong prediction bias across labels; (2) although few-shot fine-tuning can mitigate the prediction bias and demonstrate promising prediction performance, our analysis shows models gain performance improvement by capturing non-task-related features (e.g. stop words) or shallow data patterns (e.g. lexical overlaps). These observations alert that pursuing model performance with fewer examples may incur pathological prediction behavior, which requires further sanity check on model predictions and careful design in model evaluations in few-shot fine-tuning.	4e2cd3c33130ceabc5373771f9ad3aca6f427f99	@['JournalArticle']{chen-etal-2022-pathologies,  author = {Hanjie Chen and Guoqing Zheng and A. Awadallah and Yangfeng Ji},  booktitle = {First Workshop on Insights from Negative Results in NLP},  pages = {144-153},  title = {Pathologies of Pre-trained Language Models in Few-shot Fine-tuning},  year = {2022} }
Partial-input baselines show that NLI models can ignore context, but they don’t.	2022	http://www.semanticscholar.org/paper/b185b59806eeee55cdaca1a3f7181c4c5d3604af	This work investigates whether state-of-the-art NLI models are capable of overriding default inferences made by a partial-input baseline and introduces an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model's sensitivity to edited contexts.	maybe	0	When strong partial-input baselines reveal artifacts in crowdsourced NLI datasets, the performance of full-input models trained on such datasets is often dismissed as reliance on spurious correlations. We investigate whether state-of-the-art NLI models are capable of overriding default inferences made by a partial-input baseline. We introduce an evaluation set of 600 examples consisting of perturbed premises to examine a RoBERTa model’s sensitivity to edited contexts. Our results indicate that NLI models are still capable of learning to condition on context—a necessary component of inferential reasoning—despite being trained on artifact-ridden datasets.	b185b59806eeee55cdaca1a3f7181c4c5d3604af	@['JournalArticle', 'Conference']{srikanth-rudinger-2022-partial,  author = {Neha Srikanth and Rachel Rudinger},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4753-4763},  title = {Partial-input baselines show that NLI models can ignore context, but they don’t.},  year = {2022} }
Parameter-Efficient Sparsity for Large Language Models Fine-Tuning	2022	http://www.semanticscholar.org/paper/03d19fde1df67c7ea8dedc750dcd3a6291032577	This work proposes a Parameter-efficient Sparse Training (PST) method to reduce the number of trainable parameters during sparse-aware training in downstream tasks and investigates the intrinsic redundancy of data-driven weight importance and derives two obvious characteristics i.e. low-rankness and structuredness.	maybe	1	With the dramatically increased number of parameters in language models, sparsity methods have received ever-increasing research focus to compress and accelerate the models. While most research focuses on how to accurately retain appropriate weights while maintaining the performance of the compressed model, there are challenges in the computational overhead and memory footprint of sparse training when compressing large-scale language models. To address this problem, we propose a Parameter-efficient Sparse Training (PST) method to reduce the number of trainable parameters during sparse-aware training in downstream tasks. Specifically, we first combine the data-free and data-driven criteria to efficiently and accurately measure the importance of weights. Then we investigate the intrinsic redundancy of data-driven weight importance and derive two obvious characteristics i.e. low-rankness and structuredness. Based on that, two groups of small matrices are introduced to compute the data-driven importance of weights, instead of using the original large importance score matrix, which therefore makes the sparse training resource-efficient and parameter-efficient. Experiments with diverse networks (i.e. BERT, RoBERTa and GPT-2) on dozens of datasets demonstrate PST performs on par or better than previous sparsity methods, despite only training a small number of parameters. For instance, compared with previous sparsity methods, our PST only requires 1.5% trainable parameters to achieve comparable performance on BERT.	03d19fde1df67c7ea8dedc750dcd3a6291032577	@['JournalArticle', 'Conference']{li-etal-2022-parameter,  author = {Yuchao Li and Fuli Luo and Chuanqi Tan and Mengdi Wang and Songfang Huang and Shen Li and Junjie Bai},  booktitle = {International Joint Conference on Artificial Intelligence},  pages = {4223-4229},  title = {Parameter-Efficient Sparsity for Large Language Models Fine-Tuning},  year = {2022} }
Pair the Dots: Jointly Examining Training History and Test Stimuli for Model Interpretability	2020	http://www.semanticscholar.org/paper/79fa39058350c6214a573bf2286744f9173b8515	This paper proposes an efficient and differentiable approach to make it feasible to interpret a model's prediction by jointly examining training history and test stimuli, and demonstrates that the proposed methodology offers clear explanations about neural model decisions, along with being useful for performing error analysis, crafting adversarial examples and fixing erroneously classified examples.	maybe	8	Any prediction from a model is made by a combination of learning history and test stimuli. This provides significant insights for improving model interpretability: {\it because of which part(s) of which training example(s), the model attends to which part(s) of a test example}. Unfortunately, existing methods to interpret a model's predictions are only able to capture a single aspect of either test stimuli or learning history, and evidences from both are never combined or integrated. In this paper, we propose an efficient and differentiable approach to make it feasible to interpret a model's prediction by jointly examining training history and test stimuli. Test stimuli is first identified by gradient-based methods, signifying {\it the part of a test example that the model attends to}. The gradient-based saliency scores are then propagated to training examples using influence functions to identify {\it which part(s) of which training example(s)} make the model attends to the test stimuli. The system is differentiable and time efficient: the adoption of saliency scores from gradient-based methods allows us to efficiently trace a model's prediction through test stimuli, and then back to training examples through influence functions. We demonstrate that the proposed methodology offers clear explanations about neural model decisions, along with being useful for performing error analysis, crafting adversarial examples and fixing erroneously classified examples.	79fa39058350c6214a573bf2286744f9173b8515	@['JournalArticle']{meng-etal-2020-pair,  author = {Yuxian Meng and Chun Fan and Zijun Sun and E. Hovy and Fei Wu and Jiwei Li},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Pair the Dots: Jointly Examining Training History and Test Stimuli for Model Interpretability},  volume = {abs/2010.06943},  year = {2020} }
P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks	2022	http://www.semanticscholar.org/paper/ec936b808e0fab9281c050ad4010cddec92c8cbe	The method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU and can serve as an alternative to finetuning and a strong baseline for future research.	maybe	34	Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.	ec936b808e0fab9281c050ad4010cddec92c8cbe	@['JournalArticle', 'Conference']{liu-etal-2022-p,  author = {Xiao Liu and Kaixuan Ji and Yicheng Fu and W. Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {61-68},  title = {P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks},  year = {2022} }
P ROBING C LASSIFIERS ARE U NRELIABLE FOR C ONCEPT R EMOVAL AND D ETECTION	2022	http://www.semanticscholar.org/paper/d4d401a9280eddad1484cb65319e611132fb3995	It is proved that post-hoc or adversarial methods to remove unwanted attributes from a model’s representation will fail to remove the attribute correctly, and a spuriousness metric is proposed to gauge the quality of the probing classiﬁer.	maybe	0	Neural network models trained on text data have been found to encode undesired linguistic or sensitive attributes in their representation. Removing such attributes is non-trivial because of a complex relationship between the attribute, text input, and the learnt representation. Recent work has proposed post-hoc and adversarial methods to remove such unwanted attributes from a model’s representation. Through an extensive theoretical and empirical analysis, we show that these methods can be counter-productive: they are unable to remove the attributes entirely, and in the worst case may end up destroying all task-relevant features. The reason is the methods’ reliance on a probing classiﬁer as a proxy for the attribute. Even under the most favorable conditions when an attribute’s features in representation space can alone provide 100% accuracy for learning the probing classiﬁer, we prove that post-hoc or adversarial methods will fail to remove the attribute correctly. These theoretical implications are conﬁrmed by empirical experiments on models trained on synthetic, Multi-NLI, and Twitter datasets. For sensitive applications of attribute removal such as fairness, we recommend caution against using these methods and propose a spuriousness metric to gauge the quality of the ﬁnal classiﬁer. ERM on Twitter-PAN16, Twitter-AAE and Synthetic-Text datasets respectively. The failure of AR is worse here: there is no signiﬁcant reduction in spuriousness score for AR in comparison to ERM. In case of Synthetic-Text dataset, has zero score but AR have non-zero score. We expand more on this observation in Appendix G.3. results observations	d4d401a9280eddad1484cb65319e611132fb3995	@None{tan-2022-p,  author = {Chenhao Tan},  title = {P ROBING C LASSIFIERS ARE U NRELIABLE FOR C ONCEPT R EMOVAL AND D ETECTION},  year = {2022} }
Overestimation of Syntactic Representation in Neural Language Models	2020	http://www.semanticscholar.org/paper/281b80b8e67fd69dcd90f5df963700b7c330cd0c	A fundamental problem is illustrated by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.	maybe	9	With the advent of powerful neural language models over the last few years, research attention has increasingly focused on what aspects of language they represent that make them so successful. Several testing methodologies have been developed to probe models’ syntactic representations. One popular method for determining a model’s ability to induce syntactic structure trains a model on strings generated according to a template then tests the model’s ability to distinguish such strings from superficially similar ones with different syntax. We illustrate a fundamental problem with this approach by reproducing positive results from a recent paper with two non-syntactic baseline language models: an n-gram model and an LSTM model trained on scrambled inputs.	281b80b8e67fd69dcd90f5df963700b7c330cd0c	@['JournalArticle', 'Conference']{kodner-gupta-2020-overestimation,  author = {Jordan Kodner and Nitish Gupta},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1757-1762},  title = {Overestimation of Syntactic Representation in Neural Language Models},  year = {2020} }
Overcoming the Lexical Overlap Bias Using Predicate-Argument Structures	2020	http://www.semanticscholar.org/paper/cabbe49fc6a55885ef1a8c4b92bb375b8d3f763b	The lexical overlap bias is investigated and the incorporation of predicate-argument structures during finetuning considerably improves the robustness, e.g., about 20pp on discriminating different named entities, while it incurs no additional cost at the test time and does not require changing the model or the training procedure.	maybe	0	Recent pretrained transformer-based language models have set state-of-the-art performances on various NLP datasets. However, despite their great progress, they suffer from various structural and syntactic biases. In this work, we investigate the lexical overlap bias, e.g., the model classifies two sentences that have a high lexical overlap as entailing regardless of their underlying meaning. To improve the robustness, we enrich input sentences of the training data with their automatically detected predicate-argument structures. This enhanced representation allows the transformerbased models to learn different attention patterns by focusing on and recognizing the major semantically and syntactically important parts of the sentences. We evaluate our solution for the tasks of natural language inference and grounded commonsense inference using the BERT, RoBERTa, and XLNET models. We evaluate the models’ understanding of syntactic variations, antonym relations, and named entities in the presence of lexical overlap. Our results show that the incorporation of predicate-argument structures during finetuning considerably improves the robustness, e.g., about 20pp on discriminating different named entities, while it incurs no additional cost at the test time and does not require changing the model or the training procedure.	cabbe49fc6a55885ef1a8c4b92bb375b8d3f763b	@None{2019-overcoming,  title = {Overcoming the Lexical Overlap Bias Using Predicate-Argument Structures},  year = {2019} }
Overcoming Poor Word Embeddings with Word Definitions	2021	http://www.semanticscholar.org/paper/e0c4a2116ec0f3dc48457627bbd8bfe1f0026479	This work shows that examples that depend critically on a rarer word are more challenging for natural language inference models, and explores how a model could learn to use definitions, provided in natural text, to overcome this handicap.	maybe	1	Modern natural language understanding models depend on pretrained subword embeddings, but applications may need to reason about words that were never or rarely seen during pretraining. We show that examples that depend critically on a rarer word are more challenging for natural language inference models. Then we explore how a model could learn to use definitions, provided in natural text, to overcome this handicap. Our model’s understanding of a definition is usually weaker than a well-modeled word embedding, but it recovers most of the performance gap from using a completely untrained word.	e0c4a2116ec0f3dc48457627bbd8bfe1f0026479	@['JournalArticle']{malon-2021-overcoming,  author = {Christopher Malon},  booktitle = {STARSEM},  pages = {288-293},  title = {Overcoming Poor Word Embeddings with Word Definitions},  year = {2021} }
Outliers Dimensions that Disrupt Transformers Are Driven by Frequency	2022	http://www.semanticscholar.org/paper/ddd30d2376c2776e1b40d415962f673cbd1ac2f1	It is found that in both BERT and RoBERTa the token frequency, known to contribute to anisotropicity, also contributes to the outlier phenomenon, which contributes tothe “vertical” self-attention pattern that enables the model to focus on the special tokens.	yes	4	Transformer-based language models are known to display anisotropic behavior: the token embeddings are not homogeneously spread in space, but rather accumulate along certain directions. A related recent ﬁnding is the outlier phenomenon: the parameters in the ﬁnal element of Transformer layers that consistently have unusual magnitude in the same dimension across the model, and signiﬁcantly degrade its performance if disabled. We replicate the evidence for the outlier phenomenon and we link it to the geometry of the embedding space. Our main ﬁnding is that in both BERT and RoBERTa the token frequency, known to contribute to anisotropicity, also contributes to the outlier phenomenon. In its turn, the outlier phenomenon contributes to the “vertical” self-attention pattern that enables the model to focus on the special tokens. We also ﬁnd that, surprisingly, the outlier effect on the model performance varies by layer, and that variance is also related to the correlation between outlier magnitude and encoded token frequency.	ddd30d2376c2776e1b40d415962f673cbd1ac2f1	@['JournalArticle']{puccetti-etal-2022-outliers,  author = {Giovanni Puccetti and Anna Rogers and Aleksandr Drozd and F. Dell’Orletta},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Outliers Dimensions that Disrupt Transformers Are Driven by Frequency},  volume = {abs/2205.11380},  year = {2022} }
Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?	2020	http://www.semanticscholar.org/paper/776a49616c84d52e8fff9911c561e3bac90910eb	This work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence, and encouraging models to capture word order information improves the performance on mostGLUE tasks and SQuAD 2.0.	maybe	63	Do state-of-the-art natural language understanding models care about word order? Not always! We found 75% to 90% of the correct predictions of BERT-based classifiers, trained on many GLUE tasks, remain constant after input words are randomly shuffled. Although BERT embeddings are famously contextual, the contribution of each individual word to classification is almost unchanged even after its surrounding words are shuffled. BERTbased models exploit superficial cues (e.g. the sentiment of keywords in sentiment analysis; or the word-wise similarity between sequencepair inputs in natural language inference) to make correct decisions when tokens are randomly shuffled. Encouraging models to capture word order information improves the performance on most GLUE tasks and SQuAD 2.0. Our work suggests that many GLUE tasks are not challenging machines to understand the meaning of a sentence.	776a49616c84d52e8fff9911c561e3bac90910eb	@['JournalArticle']{pham-etal-2020-out,  author = {Thang M. Pham and Trung Bui and Long Mai and Anh M Nguyen},  booktitle = {Findings},  journal = {ArXiv},  title = {Out of Order: How important is the sequential order of words in a sentence in Natural Language Understanding tasks?},  volume = {abs/2012.15180},  year = {2020} }
Out of One, Many: Using Language Models to Simulate Human Samples	2022	https://www.semanticscholar.org/paper/f4e612658bde9db88abfd455b99f181fdc536996	It is shown that the “algorithmic bias” within one such tool– the GPT-3 language model– is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups.	maybe	3	We propose and explore the possibility that language models can be studied as effective proxies for specific human sub-populations in social science research. Practical and research applications of artificial intelligence tools have sometimes been limited by problematic biases (such as racism or sexism), which are often treated as uniform properties of the models. We show that the “algorithmic bias” within one such tool– the GPT-3 language model– is instead both fine-grained and demographically correlated, meaning that proper conditioning will cause it to accurately emulate response distributions from a wide variety of human subgroups. We term this property algorithmic fidelity and explore its extent in GPT-3. We create “silicon samples” by conditioning the model on thousands of socio-demographic backstories from real human participants in multiple large surveys conducted in the United States. We then compare the silicon and human samples to demonstrate that the information contained in GPT-3 goes far beyond surface similarity. It is nuanced, multifaceted, and reflects the complex interplay between ideas, attitudes, and socio-cultural context that characterize human attitudes. We suggest that language models with sufficient algorithmic fidelity thus constitute a novel and powerful tool to advance understanding of humans and society across a variety of disciplines.	f4e612658bde9db88abfd455b99f181fdc536996	@['JournalArticle', 'Review']{argyle-etal-2022-out,  author = {Lisa P. Argyle and E. Busby and Nancy Fulda and J. Gubler and C. Rytting and D. Wingate},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Out of One, Many: Using Language Models to Simulate Human Samples},  volume = {abs/2209.06899},  year = {2022} }
OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization	2022	http://www.semanticscholar.org/paper/54273d930cfc93e1b84b92cccb742134807c8a82		yes			54273d930cfc93e1b84b92cccb742134807c8a82	
Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model	2022	http://www.semanticscholar.org/paper/9873fb2bb5ac6eedbf0659a961380ad31cb27334		maybe	8	Generative pretrained transformer models have been popular recently due to their enhanced capabilities and performance. In contrast to many existing artificial intelligence models, generative pretrained transformer models can perform with very limited training data. Generative pretrained transformer 3 (GPT-3) is one of the latest releases in this pipeline, demonstrating human-like logical and intellectual responses to prompts. Some examples include writing essays, answering complex questions, matching pronouns to their nouns, and conducting sentiment analyses. However, questions remain with regard to its implementation in health care, specifically in terms of operationalization and its use in clinical practice and research. In this viewpoint paper, we briefly introduce GPT-3 and its capabilities and outline considerations for its implementation and operationalization in clinical practice through a use case. The implementation considerations include (1) processing needs and information systems infrastructure, (2) operating costs, (3) model biases, and (4) evaluation metrics. In addition, we outline the following three major operational factors that drive the adoption of GPT-3 in the US health care system: (1) ensuring Health Insurance Portability and Accountability Act compliance, (2) building trust with health care providers, and (3) establishing broader access to the GPT-3 tools. This viewpoint can inform health care practitioners, developers, clinicians, and decision makers toward understanding the use of the powerful artificial intelligence tools integrated into hospital systems and health care.	9873fb2bb5ac6eedbf0659a961380ad31cb27334	@['JournalArticle']{sezgin-etal-2022-operationalizing,  author = {Emre Sezgin and Joseph W. Sirrianni and S. Linwood},  booktitle = {JMIR Medical Informatics},  journal = {JMIR Medical Informatics},  title = {Operationalizing and Implementing Pretrained, Large Artificial Intelligence Linguistic Models in the US Health Care System: Outlook of Generative Pretrained Transformer 3 (GPT-3) as a Service Model},  volume = {10},  year = {2022} }
Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next	2022	http://www.semanticscholar.org/paper/eb1ac44bbc0fe07c5f31f459c7199211239e90b8	The goal of this work is to provide an overview of recent advances in the field of open-domain dialogue, to summarize issues related to ethics, bias, and fairness that the field has identified as well as typical errors of dialogue systems and to outline important future challenges.	maybe	4	Human–computer conversation has long been an interest of artificial intelligence and natural language processing research. Recent years have seen a dramatic improvement in quality for both task-oriented and open-domain dialogue systems, and an increasing amount of research in the area. The goal of this work is threefold: (1) to provide an overview of recent advances in the field of open-domain dialogue, (2) to summarize issues related to ethics, bias, and fairness that the field has identified as well as typical errors of dialogue systems, and (3) to outline important future challenges. We hope that this work will be of interest to both new and experienced researchers in the area.	eb1ac44bbc0fe07c5f31f459c7199211239e90b8	@['JournalArticle', 'Review']{kann-etal-2022-open,  author = {Katharina Kann and Abteen Ebrahimi and Joewie J. Koh and Shiran Dudy and A. Roncone},  booktitle = {NLP4CONVAI},  pages = {148-165},  title = {Open-domain Dialogue Generation: What We Can Do, Cannot Do, And Should Do Next},  year = {2022} }
Open Sesame: Getting inside BERT’s Linguistic Knowledge	2019	https://www.semanticscholar.org/paper/165d51a547cd920e6ac55660ad5c404dcb9562ed	It is concluded that BERT’s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.	seed	188	How and to what extent does BERT encode syntactically-sensitive hierarchical information or positionally-sensitive linear information? Recent work has shown that contextual representations like BERT perform well on tasks that require sensitivity to linguistic structure. We present here two studies which aim to provide a better understanding of the nature of BERT’s representations. The first of these focuses on the identification of structurally-defined elements using diagnostic classifiers, while the second explores BERT’s representation of subject-verb agreement and anaphor-antecedent dependencies through a quantitative assessment of self-attention vectors. In both cases, we find that BERT encodes positional information about word tokens well on its lower layers, but switches to a hierarchically-oriented encoding on higher layers. We conclude then that BERT’s representations do indeed model linguistically relevant aspects of hierarchical structure, though they do not appear to show the sharp sensitivity to hierarchical structure that is found in human processing of reflexive anaphora.	165d51a547cd920e6ac55660ad5c404dcb9562ed	@['JournalArticle']{lin-etal-2019-open,  author = {Yongjie Lin and Y. Tan and R. Frank},  booktitle = {BlackboxNLP@ACL},  journal = {ArXiv},  title = {Open Sesame: Getting inside BERT’s Linguistic Knowledge},  volume = {abs/1906.01698},  year = {2019} }
On the Transformation of Latent Space in Fine-Tuned NLP Models	2022	http://www.semanticscholar.org/paper/94d931a253fb44b9d2f93d9287aec1bf8bdf0a4b	The latent space of the higher layers of the pre-trained NLP models evolve towards task-speciﬁc concepts and it is discovered that some concepts in theHigher layers acquire polarity towards the output class, and that these concepts can be used for generating adversarial triggers.	maybe	1	We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers.	94d931a253fb44b9d2f93d9287aec1bf8bdf0a4b	@['JournalArticle', 'Conference']{durrani-etal-2022-on,  author = {Nadir Durrani and Hassan Sajjad and Fahim Dalvi and Firoj Alam},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {On the Transformation of Latent Space in Fine-Tuned NLP Models},  volume = {abs/2210.12696},  year = {2022} }
On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT	2020	http://www.semanticscholar.org/paper/9d1f9406ed676171d9975e27606c95633ca898b1	The main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.	maybe	34	Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: do probing studies shed light on systematic knowledge in BERT representations? As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.	9d1f9406ed676171d9975e27606c95633ca898b1	@['JournalArticle']{ravichander-etal-2020-on,  author = {Abhilasha Ravichander and E. Hovy and Kaheer Suleman and Adam Trischler and J. C. Cheung},  booktitle = {STARSEM},  pages = {88-102},  title = {On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT},  year = {2020} }
On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART	2022	http://www.semanticscholar.org/paper/03ca1715ad0637a1ab34343ff343947f17dd1082	To explain why BART helps word ordering, analysis is extended with probing and empirically identified that syntactic dependency knowledge in BART is a reliable explanation.	maybe	0	Word ordering is a constrained language generation task taking unordered words as input. Existing work uses linear models and neural networks for the task, yet pre-trained language models have not been studied in word ordering, let alone why they help. We use BART as an instance and show its effectiveness in the task. To explain why BART helps word ordering, we extend analysis with probing and empirically identify that syntactic dependency knowledge in BART is a reliable explanation. We also report performance gains with BART in the related partial tree linearization task, which readily extends our analysis.	03ca1715ad0637a1ab34343ff343947f17dd1082	@['JournalArticle', 'Conference']{ou-etal-2022-on,  author = {Zebin Ou and Meishan Zhang and Yue Zhang},  booktitle = {International Conference on Computational Linguistics},  pages = {6516-6529},  title = {On the Role of Pre-trained Language Models in Word Ordering: A Case Study with BART},  year = {2022} }
On the Role of Corpus Ordering in Language Modeling	2021	http://www.semanticscholar.org/paper/69c71029b898de7bc1ff7e9dab77d7fd8d3bb759	Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training.	maybe	2	Language models pretrained on vast corpora of unstructured text using self-supervised learning framework are used in numerous natural language understanding and generation tasks. Many studies show that language acquisition in humans follows a rather structured simple-to-complex pattern and guided by this intuition, curriculum learning, which enables training of computational models in a meaningful order, such as processing easy samples before hard ones, has been shown to potentially reduce training time. The question remains whether curriculum learning can benefit pretraining of language models. In this work, we perform comprehensive experiments involving multiple curricula strategies varying the criteria for complexity and the training schedules. Empirical results of training transformer language models on English corpus and evaluating it intrinsically as well as after fine-tuning across eight tasks from the GLUE benchmark, show consistent improvement gains over conventional vanilla training. Interestingly, in our experiments, when evaluated on one epoch, the best model following a document-level hard-to-easy curriculum, outperforms the vanilla model by 1.7 points (average GLUE score) and it takes the vanilla model twice as many training steps to reach comparable performance.	69c71029b898de7bc1ff7e9dab77d7fd8d3bb759	@['JournalArticle']{agrawal-etal-2021-on,  author = {Ameeta Agrawal and Suresh Singh and Lauren A. Schneider and Michael Samuels},  booktitle = {SUSTAINLP},  pages = {142-154},  title = {On the Role of Corpus Ordering in Language Modeling},  year = {2021} }
On the Role of Bidirectionality in Language Model Pre-Training	2022	http://www.semanticscholar.org/paper/8ce9b1e527c4d9d15239621ec4e3ef3fbbe32202	The results suggest that this approach to scaling has focused on left-to-right autoregressive models, and it might be worth-while to develop very large bidirectional models.	maybe	6	Prior work on language model pre-training has explored different architectures and learning objectives, but differences in data, hyperparameters and evaluation make a principled comparison difﬁcult. In this work, we focus on bidirectionality as a key factor that differentiates existing approaches, and present a comprehensive study of its role in next token prediction, text inﬁlling, zero-shot priming and ﬁne-tuning. We propose a new framework that generalizes prior approaches, in-cluding fully unidirectional models like GPT, fully bidirectional models like BERT, and hybrid models like CM3 and preﬁx LM. Our framework distinguishes between two notions of bidirectionality—bidirectional context and bidirectional attention—and allows us to control each of them separately. We ﬁnd that the optimal conﬁguration is largely application-dependent (e.g., bidirectional attention is beneﬁcial for ﬁne-tuning and inﬁlling, but harmful for next token prediction and zero-shot prim-ing). We train models with up to 6.7B parameters, and ﬁnd differences to remain consistent at scale. While prior work on scaling has focused on left-to-right autoregressive models, our results suggest that this approach comes with some trade-offs, and it might be worth-while to develop very large bidirectional models.	8ce9b1e527c4d9d15239621ec4e3ef3fbbe32202	@['JournalArticle']{artetxe-etal-2022-on,  author = {Mikel Artetxe and Jingfei Du and Naman Goyal and Luke Zettlemoyer and V. Stoyanov},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On the Role of Bidirectionality in Language Model Pre-Training},  volume = {abs/2205.11726},  year = {2022} }
On the Pitfalls of Analyzing Individual Neurons in Language Models	2021	http://www.semanticscholar.org/paper/aaf3ebaf12baeb366ce6ff32aa36d608a7eab583	It is shown that two methods to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the same probe that produced it are not the same.	maybe	14	results imply they may get better	aaf3ebaf12baeb366ce6ff32aa36d608a7eab583	@['JournalArticle']{antverg-belinkov-2021-on,  author = {Omer Antverg and Yonatan Belinkov},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {On the Pitfalls of Analyzing Individual Neurons in Language Models},  volume = {abs/2110.07483},  year = {2021} }
On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?	2022	http://www.semanticscholar.org/paper/e9f28c98a00766e484810598886cf48b0de66cfa	This work conducts a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models, revealing that the standard benchmarks consist of > 60% hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations.	maybe	11	Knowledge-grounded conversational models are known to suffer from producing factually invalid statements, a phenomenon commonly called hallucination. In this work, we investigate the underlying causes of this phenomenon: is hallucination due to the training data, or to the models? We conduct a comprehensive human study on both existing knowledge-grounded conversational benchmarks and several state-of-the-art models. Our study reveals that the standard benchmarks consist of > 60% hallucinated responses, leading to models that not only hallucinate but even amplify hallucinations. Our findings raise important questions on the quality of existing datasets and models trained using them. We make our annotations publicly available for future research.	e9f28c98a00766e484810598886cf48b0de66cfa	@['JournalArticle', 'Conference']{dziri-etal-2022-on,  author = {Nouha Dziri and Sivan Milton and Mo Yu and Osmar R Zaiane and Siva Reddy},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {On the Origin of Hallucinations in Conversational Models: Is it the Datasets or the Models?},  volume = {abs/2204.07931},  year = {2022} }
On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations	2022	http://www.semanticscholar.org/paper/d9424371662717c8981eef3d501d7ce59c66ce77	It is found that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsIC metrics.	maybe	15	Multiple metrics have been introduced to measure fairness in various natural language processing tasks. These metrics can be roughly categorized into two categories: 1) extrinsic metrics for evaluating fairness in downstream applications and 2) intrinsic metrics for estimating fairness in upstream contextualized language representation models. In this paper, we conduct an extensive correlation study between intrinsic and extrinsic metrics across bias notions using 19 contextualized language models. We find that intrinsic and extrinsic metrics do not necessarily correlate in their original setting, even when correcting for metric misalignments, noise in evaluation datasets, and confounding factors such as experiment configuration for extrinsic metrics.	d9424371662717c8981eef3d501d7ce59c66ce77	@['JournalArticle', 'Conference']{cao-etal-2022-on,  author = {Yang Trista Cao and Yada Pruksachatkun and Kai-Wei Chang and Rahul Gupta and Varun Kumar and J. Dhamala and A. Galstyan},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {On the Intrinsic and Extrinsic Fairness Evaluation Metrics for Contextualized Language Representations},  volume = {abs/2203.13928},  year = {2022} }
On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers	2020	http://www.semanticscholar.org/paper/db88987a25e036a9fd8b69f8575e0a75d63b260b	It is argued that both positive and negative effects of fine-tuning on probing require a careful interpretation, and it is found that for some probing tasks fine- Tuning leads to substantial changes in accuracy, possibly suggesting that fine- tuning introduces or even removes linguistic knowledge from a pre-trained model.	maybe	3	Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.	db88987a25e036a9fd8b69f8575e0a75d63b260b	@['JournalArticle']{mosbach-etal-2020-on,  author = {Marius Mosbach and A. Khokhlova and Michael A. Hedderich and D. Klakow},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {68-82},  title = {On the Interplay Between Fine-tuning and Sentence-Level Probing for Linguistic Knowledge in Pre-Trained Transformers},  year = {2020} }
On the Interplay Between Fine-tuning and Composition in Transformers	2021	http://www.semanticscholar.org/paper/52b1cf563d1368f72e82b91b0349a7012a746f4f	It is found that fine-tuning largely fails to benefit compositionality in these representations, though training on sentiment yields a small, localized benefit for certain models.	maybe	4	Pre-trained transformer language models have shown remarkable performance on a variety of NLP tasks. However, recent research has suggested that phrase-level representations in these models reflect heavy influences of lexical content, but lack evidence of sophisticated, compositional phrase information (Yu and Ettinger, 2020). Here we investigate the impact of fine-tuning on the capacity of contextualized embeddings to capture phrase meaning information beyond lexical content. Specifically, we fine-tune models on an adversarial paraphrase classification task with high lexical overlap, and on a sentiment classification task. After fine-tuning, we analyze phrasal representations in controlled settings following prior work. We find that fine-tuning largely fails to benefit compositionality in these representations, though training on sentiment yields a small, localized benefit for certain models. In follow-up analyses, we identify confounding cues in the paraphrase dataset that may explain the lack of composition benefits from that task, and we discuss potential factors underlying the localized benefits from sentiment training.	52b1cf563d1368f72e82b91b0349a7012a746f4f	@['JournalArticle']{yu-ettinger-2021-on,  author = {Lang-Chi Yu and Allyson Ettinger},  booktitle = {Findings},  journal = {ArXiv},  title = {On the Interplay Between Fine-tuning and Composition in Transformers},  volume = {abs/2105.14668},  year = {2021} }
On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies	2021	http://www.semanticscholar.org/paper/5f11dfa7cd146b456dceed19923f5ed6214fcc44	A correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models is demonstrated and it is shown that these dependencies encode useful inductive biases in the form of syntactic structures.	yes	13	We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).	5f11dfa7cd146b456dceed19923f5ed6214fcc44	@['JournalArticle', 'Conference']{zhang-hashimoto-2021-on,  author = {Tianyi Zhang and Tatsunori B. Hashimoto},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {5131-5146},  title = {On the Inductive Bias of Masked Language Modeling: From Statistical to Syntactic Dependencies},  year = {2021} }
On the independence between phenomenal consciousness and computational intelligence	2022	http://www.semanticscholar.org/paper/01424fda00e5b42706505438ca7c0b6c3f5e718b		maybe	1	. Consciousness and intelligence are properties commonly un-derstood as dependent by folk psychology and society in general. The term artiﬁcial intelligence and the kind of problems that it managed to solve in the recent years has been shown as an argument to establish that machines experience some sort of consciousness. Following Russell’s analogy, if a machine is able to do what a conscious human being does, the likelihood that the machine is conscious increases. However, the social implications of this analogy are catastrophic. Concretely, if rights are given to entities that can solve the kind of problems that a neurotypical person can, does the machine have potentially more rights that a person that has a disability? For example, the autistic syndrome disorder spectrum can make a person unable to solve the kind of problems that a machine solves. We believe that the obvious answer is no, as problem solving does not imply consciousness. Consequently, we will argue in this paper how phenomenal consciousness and, at least, computational intelligence are independent and why machines do not possess phenomenal consciousness, although they can potentially develop a higher computational intelligence that human beings. In order to do so, we try to formulate an objective measure of computational intelligence and study how it presents in human beings, animals and machines. Analogously, we study phenomenal consciousness as a dichotomous variable and how it is distributed in humans, animals and machines. As phenomenal consciousness and computational intelligence are independent, this fact has critical implications for society that we also analyze in this work.	01424fda00e5b42706505438ca7c0b6c3f5e718b	@['JournalArticle']{merchán-lumbreras-2022-on,  author = {E.C. Garrido-Merchán and S. Lumbreras},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On the independence between phenomenal consciousness and computational intelligence},  volume = {abs/2208.02187},  year = {2022} }
On the Importance of Local Information in Transformer Based Models	2020	http://www.semanticscholar.org/paper/3688c76d7e538094db2db92a637b1ac73c686c9a	Through this systematic evaluation, it is established that attention in Transformer-based models can be constrained to be local without affecting performance.	maybe	1	The self-attention module is a key component of Transformer-based models, wherein each token pays attention to every other token. Recent studies have shown that these heads exhibit syntactic, semantic, or local behaviour. Some studies have also identified promise in restricting this attention to be local, i.e., a token attending to other tokens only in a small neighbourhood around it. However, no conclusive evidence exists that such local attention alone is sufficient to achieve high accuracy on multiple NLP tasks. In this work, we systematically analyse the role of locality information in learnt models and contrast it with the role of syntactic information. More specifically, we first do a sensitivity analysis and show that, at every layer, the representation of a token is much more sensitive to tokens in a small neighborhood around it than to tokens which are syntactically related to it. We then define an attention bias metric to determine whether a head pays more attention to local tokens or to syntactically related tokens. We show that a larger fraction of heads have a locality bias as compared to a syntactic bias. Having established the importance of local attention heads, we train and evaluate models where varying fractions of the attention heads are constrained to be local. Such models would be more efficient as they would have fewer computations in the attention layer. We evaluate these models on 4 GLUE datasets (QQP, SST-2, MRPC, QNLI) and 2 MT datasets (En-De, En-Ru) and clearly demonstrate that such constrained models have comparable performance to the unconstrained models. Through this systematic evaluation we establish that attention in Transformer-based models can be constrained to be local without affecting performance.	3688c76d7e538094db2db92a637b1ac73c686c9a	@None{pande-etal-2020-on,  author = {Madhura Pande and Aakriti Budhraja and Preksha Nema and Pratyush Kumar and Mitesh M. Khapra},  journal = {arXiv: Computation and Language},  title = {On the Importance of Local Information in Transformer Based Models},  year = {2020} }
On the Importance of Data Size in Probing Fine-tuned Models	2022	http://www.semanticscholar.org/paper/cb097bc533ee916ef2a232e703a941e7f58d4a8c	The analysis reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples.	maybe	1	Several studies have investigated the reasons behind the effectiveness of fine-tuning, usually through the lens of probing. However, these studies often neglect the role of the size of the dataset on which the model is fine-tuned. In this paper, we highlight the importance of this factor and its undeniable role in probing performance. We show that the extent of encoded linguistic knowledge depends on the number of fine-tuning samples. The analysis also reveals that larger training data mainly affects higher layers, and that the extent of this change is a factor of the number of iterations updating the model during fine-tuning rather than the diversity of the training samples. Finally, we show through a set of experiments that fine-tuning data size affects the recoverability of the changes made to the model’s linguistic knowledge.	cb097bc533ee916ef2a232e703a941e7f58d4a8c	@['JournalArticle']{mehrafarin-etal-2022-on,  author = {Houman Mehrafarin and S. Rajaee and Mohammad Taher Pilehvar},  booktitle = {Findings},  pages = {228-238},  title = {On the Importance of Data Size in Probing Fine-tuned Models},  year = {2022} }
On the Faithfulness Measurements for Model Interpretations	2021	http://www.semanticscholar.org/paper/bc8f44d60b7ed8e07176e54f0c33fa675f98016a	A new class of interpretation methods that adopt techniques from the adversarial robustness domain are introduced, that quantify different notions of faithfulness, and propose novel paradigms to systematically evaluate interpretations in NLP.	maybe	9	Recent years have witnessed the emergence of a variety of post-hoc interpretations that aim to uncover how natural language processing (NLP) models make predictions. Despite the surge of new interpretations, it remains an open problem how to define and quantitatively measure the faithfulness of interpretations, i.e., to what extent they conform to the reasoning process behind the model. To tackle these issues, we start with three criteria: the removal-based criterion, the sensitivity of interpretations, and the stability of interpretations, that quantify different notions of faithfulness, and propose novel paradigms to systematically evaluate interpretations in NLP. Our results show that the performance of interpretations under different criteria of faithfulness could vary substantially. Motivated by the desideratum of these faithfulness notions, we introduce a new class of interpretation methods that adopt techniques from the adversarial robustness domain. Empirical results show that our proposed methods achieve top performance under all three criteria. Along with experiments and analysis on both the text classification and the dependency parsing tasks, we come to a more comprehensive understanding of the diverse set of interpretations.	bc8f44d60b7ed8e07176e54f0c33fa675f98016a	@['JournalArticle']{yin-etal-2021-on,  author = {Fan Yin and Zhouxing Shi and Cho-Jui Hsieh and Kai-Wei Chang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On the Faithfulness Measurements for Model Interpretations},  volume = {abs/2104.08782},  year = {2021} }
On the evolution of syntactic information encoded by BERT’s contextualized representations	2021	http://www.semanticscholar.org/paper/02b845539f91e3ca526a471285076a200b6472be	This paper analyzes the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure.	yes	3	The adaptation of pretrained language models to solve supervised tasks has become a baseline in NLP, and many recent works have focused on studying how linguistic information is encoded in the pretrained sentence representations. Among other information, it has been shown that entire syntax trees are implicitly embedded in the geometry of such models. As these models are often fine-tuned, it becomes increasingly important to understand how the encoded knowledge evolves along the fine-tuning. In this paper, we analyze the evolution of the embedded syntax trees along the fine-tuning process of BERT for six different tasks, covering all levels of the linguistic structure. Experimental results show that the encoded syntactic information is forgotten (PoS tagging), reinforced (dependency and constituency parsing) or preserved (semantics-related tasks) in different ways along the fine-tuning process depending on the task.	02b845539f91e3ca526a471285076a200b6472be	@['JournalArticle', 'Conference']{mayos-etal-2021-on,  author = {Laura Pérez-Mayos and Roberto Carlini and Miguel Ballesteros and L. Wanner},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {On the evolution of syntactic information encoded by BERT’s contextualized representations},  volume = {abs/2101.11492},  year = {2021} }
On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model	2022	http://www.semanticscholar.org/paper/1fafaccebc4a74898a74c606f846318c4c2c7536	This in-depth investigation of the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model finds that in- context learning performance heavily depends on the corpus domain source.	maybe	11	Many recent studies on large-scale language models have reported successful in-context zero- and few-shot learning ability. However, the in-depth analysis of when in-context learning occurs is still lacking. For example, it is unknown how in-context learning performance changes as the training corpus varies. Here, we investigate the effects of the source and size of the pretraining corpus on in-context learning in HyperCLOVA, a Korean-centric GPT-3 model. From our in-depth investigation, we introduce the following observations: (1) in-context learning performance heavily depends on the corpus domain source, and the size of the pretraining corpus does not necessarily determine the emergence of in-context learning, (2) in-context learning ability can emerge when a language model is trained on a combination of multiple corpora, even when each corpus does not result in in-context learning on its own, (3) pretraining with a corpus related to a downstream task does not always guarantee the competitive in-context learning performance of the downstream task, especially in the few-shot setting, and (4) the relationship between language modeling (measured in perplexity) and in-context learning does not always correlate: e.g., low perplexity does not always imply high in-context few-shot learning performance.	1fafaccebc4a74898a74c606f846318c4c2c7536	@['JournalArticle', 'Conference']{shin-etal-2022-on,  author = {Seongjin Shin and Sang-Woo Lee and Hwijeen Ahn and Sungdong Kim and Hyoungseok Kim and Boseop Kim and Kyunghyun Cho and Gichang Lee and W. Park and Jung-Woo Ha and Nako Sung},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {5168-5186},  title = {On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model},  year = {2022} }
On the effect of dropping layers of pre-trained transformer models	2020	http://www.semanticscholar.org/paper/39f8cc684f09ea2b43767f5b9590896774802759	The lower layers are most critical to maintain downstream task performance, and some tasks such as paraphrase de-tection and sentence similarity are more robust to the dropping of layers, and models trained using diﬀerent objective function exhibit di-erent learning patterns and w.r.t the layer dropping.	maybe	16		39f8cc684f09ea2b43767f5b9590896774802759	@['JournalArticle']{sajjad-etal-2020-on,  author = {Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},  booktitle = {Computer Speech and Language},  journal = {Comput. Speech Lang.},  pages = {101429},  title = {On the effect of dropping layers of pre-trained transformer models},  volume = {77},  year = {2020} }
On the Cusp of Comprehensibility: Can Language Models Distinguish Between Metaphors and Nonsense?	2022	https://www.semanticscholar.org/paper/eddda3013020949609ef93f45b855893e9cab093		maybe	0	Utterly creative texts can sometimes be difficult to understand, balancing on the edge of comprehensibility. However, good language skills and common sense allow advanced language users both to interpret creative texts and to reject some linguistic input as nonsense. The goal of this paper is to evaluate whether the current language models are also able to make the distinction between a creative language use and nonsense. To test this, we have computed mean rank and pseudo-log-likelihood score (PLL) of metaphorical and nonsensical sentences, and fine-tuned several pretrained models (BERT, RoBERTa) for binary classification between the two categories. There was a significant difference in the mean ranks and PPL scores of the categories, and the classifier reached around 85.5% accuracy. The results raise further questions on what could have let to such satisfactory performance.	eddda3013020949609ef93f45b855893e9cab093	@None{griciūtė-etal-2022-on,  author = {Bernadeta Griciūtė and Marc Tanti and L. Donatelli},  booktitle = {FLP},  title = {On the Cusp of Comprehensibility: Can Language Models Distinguish Between Metaphors and Nonsense?},  year = {2022} }
On the Compositional Generalization Gap of In-Context Learning	2022	http://www.semanticscholar.org/paper/95915aa592fdfc73f039c13472a21d3e4220f129	This work evaluates four model families, OPT, BLOOM, CodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery with different number of exemplars, and observes a trend of decreasing relative generalization gap as models are scaled up.	maybe	1	Pretrained large generative language models have shown great performance on many tasks, but exhibit low compositional generalization abilities. Scaling such models has been shown to improve their performance on various NLP tasks even just by conditioning them on a few examples to solve the task without any fine-tuning (also known as in-context learning). In this work, we look at the gap between the in-distribution (ID) and out-of-distribution (OOD) performance of such models in semantic parsing tasks with in-context learning. In the ID settings, the demonstrations are from the same split (\textit{test} or \textit{train}) that the model is being evaluated on, and in the OOD settings, they are from the other split. We look at how the relative generalization gap of in-context learning evolves as models are scaled up. We evaluate four model families, OPT, BLOOM, CodeGen and Codex on three semantic parsing datasets, CFQ, SCAN and GeoQuery with different number of exemplars, and observe a trend of decreasing relative generalization gap as models are scaled up.	95915aa592fdfc73f039c13472a21d3e4220f129	@['JournalArticle']{hosseini-etal-2022-on,  author = {Arian Hosseini and A. Vani and Dzmitry Bahdanau and Alessandro Sordoni and Aaron C. Courville},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {On the Compositional Generalization Gap of In-Context Learning},  volume = {abs/2211.08473},  year = {2022} }
On the Branching Bias of Syntax Extracted from Pre-trained Language Models	2020	http://www.semanticscholar.org/paper/1711dd50a3c8ac51368f03a1b88373251988028a	This work proposes quantitatively measuring the branching bias by comparing the performance gap on a language and its reversed language, which is agnostic to both language models and extracting methods.	maybe	1	Many efforts have been devoted to extracting constituency trees from pre-trained language models, often proceeding in two stages: feature definition and parsing. However, this kind of methods may suffer from the branching bias issue, which will inflate the performances on languages with the same branch it biases to. In this work, we propose quantitatively measuring the branching bias by comparing the performance gap on a language and its reversed language, which is agnostic to both language models and extracting methods. Furthermore, we analyze the impacts of three factors on the branching bias, namely feature definitions, parsing algorithms, and language models. Experiments show that several existing works exhibit branching biases, and some implementations of these three factors can introduce the branching bias.	1711dd50a3c8ac51368f03a1b88373251988028a	@['JournalArticle']{li-etal-2020-on,  author = {Huayang Li and Lemao Liu and Guoping Huang and Shuming Shi},  booktitle = {Findings},  pages = {4473-4478},  title = {On the Branching Bias of Syntax Extracted from Pre-trained Language Models},  year = {2020} }
On the Behaviour of BERT's Attention for the Classification of Medical Reports	2022	http://www.semanticscholar.org/paper/7ac5562d2daec4888421f3486ade82742e25a5be	This work proposes a simplified way to classify head patterns without relying on probing tasks or manual observations, and an algorithm for extracting the most relevant relations among words captured by each self-attention in BERT.	maybe	0	Since BERT and the other Transformer-based models have been proved successful in many NLP tasks, several studies have been conducted to understand why these complex deep learning architectures are able to reach such remarkable results. Such studies have focused on visualising and analysing the behaviour of each self-attention mechanism and are often conducted with large, generic and annotated datasets for the English language, using supervised probing tasks in order to test specific linguistic capabilities. However, in several practical contexts there are some difficulties: probing tasks may not be available, the documents can contain a strict technical lexicon, and the datasets can be noisy. In this work we analyse the behaviour of BERT in a specific context, i.e. the classification of radiology reports collected from an Italian hospital. We propose (i) a simplified way to classify head patterns without relying on probing tasks or manual observations, and (ii) an algorithm for extracting the most relevant relations among words captured by each self-attention. Combining these techniques with manual observations, we present several examples of linguistic information that can be extracted from BERT in our application.	7ac5562d2daec4888421f3486ade82742e25a5be	@['JournalArticle']{putelli-etal-2022-on,  author = {Luca Putelli and A. Gerevini and A. Lavelli and T. Mehmood and I. Serina},  booktitle = {XAI.it@AI*IA},  pages = {16-30},  title = {On the Behaviour of BERT's Attention for the Classification of Medical Reports},  year = {2022} }
On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning	2022	http://www.semanticscholar.org/paper/5c70d4f334b7da45b156d223afcb256dbabd9b2f	It is found that using zero-shot CoT reasoning in a prompt can signiﬁcantly increase a model’s likelihood to produce undesirable output and should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.	yes	0	Warning: This paper contains several toxic, triggering, and deeply offensive statements. Generating a chain of thought (CoT) can increase large language model (LLM) performance on a wide range of tasks. Zero-shot CoT evaluations, however, have been conducted primarily on logical tasks (e.g. arithmetic, commonsense QA). In this paper, we perform a controlled evaluation of zero-shot CoT across two sensitive domains: harmful questions & stereotype benchmarks. We ﬁnd that using zero-shot CoT reasoning in a prompt can signiﬁcantly increase a model’s likelihood to produce undesirable output. Without future advances in alignment or explicit mitigation instructions, zero-shot CoT should be avoided on tasks where models can make inferences about marginalized groups or harmful topics.	5c70d4f334b7da45b156d223afcb256dbabd9b2f	@['JournalArticle']{shaikh-etal-2022-on,  author = {Omar Shaikh and Hongxin Zhang and William B. Held and Michael Bernstein and Diyi Yang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in Zero-Shot Reasoning},  volume = {abs/2212.08061},  year = {2022} }
On Reality and the Limits of Language Data	2022	http://www.semanticscholar.org/paper/4217467e747182b9ad8035e8a2d657d2ce80af07	The objective of this work is to explore how far can language data alone enable computers to understand the necessary truth about the physical world using a novel and tightly controlled reasoning test and to highlight what models might learn directly from pure linguistic data.	maybe	1	Recent advances in neural network language models have shown that it is possible to derive expressive meaning representations by leveraging linguistic associations in large-scale natural language data. These potentially Gestalt representations have enabled state-of-the-art performance for many practical applications. It would appear that we are on a pathway to empirically deriving a robust and expressive computable semantics. A key question that arises is how far can language data alone enable computers to understand the necessary truth about the physical world? Attention to this question is warranted because our future interac-tions with intelligent machines depends on how well our techniques correctly represent and process the concepts (objects, properties, and processes) that humans commonly observe to be true. After reviewing exist-ing protocols, the objective of this work is to explore this question using a novel and tightly controlled reasoning test and to highlight what models might learn directly from pure linguistic data.	4217467e747182b9ad8035e8a2d657d2ce80af07	@['JournalArticle', 'Review']{collier-etal-2022-on,  author = {N. Collier and Fangyu Liu and Ehsan Shareghi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On Reality and the Limits of Language Data},  volume = {abs/2208.11981},  year = {2022} }
On Position Embeddings in BERT	2021	http://www.semanticscholar.org/paper/dc35daba3fb34b2e6a5b12530badb7b799262bbf		maybe			dc35daba3fb34b2e6a5b12530badb7b799262bbf	
On Model Stability as a Function of Random Seed	2019	http://www.semanticscholar.org/paper/285bff5a564cb7edfb355a2ffb44994386975f1d	This paper performs a controlled study on the effect of random seeds on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations and proposes a technique called ASWA and an extension called Norm-filtered Aggressive Stochastic Weight Averaging which improves the stability of models over random seeds.	maybe	29	In this paper, we focus on quantifying model stability as a function of random seed by investigating the effects of the induced randomness on model performance and the robustness of the model in general. We specifically perform a controlled study on the effect of random seeds on the behaviour of attention, gradient-based and surrogate model based (LIME) interpretations. Our analysis suggests that random seeds can adversely affect the consistency of models resulting in counterfactual interpretations. We propose a technique called Aggressive Stochastic Weight Averaging (ASWA) and an extension called Norm-filtered Aggressive Stochastic Weight Averaging (NASWA) which improves the stability of models over random seeds. With our ASWA and NASWA based optimization, we are able to improve the robustness of the original model, on average reducing the standard deviation of the model’s performance by 72%.	285bff5a564cb7edfb355a2ffb44994386975f1d	@['JournalArticle']{madhyastha-batra-2019-on,  author = {P. Madhyastha and Dhruv Batra},  booktitle = {Conference on Computational Natural Language Learning},  pages = {929-939},  title = {On Model Stability as a Function of Random Seed},  year = {2019} }
On Measuring the Intrinsic Few-Shot Hardness of Datasets	2022	http://www.semanticscholar.org/paper/d6ade4de1c5bccb052eb20e6d65f5232c8b61dc5	A simple and lightweight metric called Spread is proposed that captures the intuition that few-shot learning is made possible by exploiting feature-space invariances between training and test samples, and better accounts for few- shot hardness compared to existing notions of hardness.	maybe	0	While advances in pre-training have led to dramatic improvements in few-shot learning of NLP tasks, there is limited understanding of what drives successful few-shot adaptation in datasets. In particular, given a new dataset and a pre-trained model, what properties of the dataset make it few-shot learnable, and are these properties independent of the specific adaptation techniques used? We consider an extensive set of recent few-shot learning methods and show that their performance across a large number of datasets is highly correlated, showing that few-shot hardness may be intrinsic to datasets, for a given pre-trained model. To estimate intrinsic few-shot hardness, we then propose a simple and lightweight metric called Spread that captures the intuition that few-shot learning is made possible by exploiting feature-space invariances between training and test samples. Our metric better accounts for few-shot hardness compared to existing notions of hardness and is ~8-100x faster to compute.	d6ade4de1c5bccb052eb20e6d65f5232c8b61dc5	@['JournalArticle', 'Conference']{zhao-etal-2022-on,  author = {Xinran Zhao and Shikhar Murty and Christopher D. Manning},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {On Measuring the Intrinsic Few-Shot Hardness of Datasets},  volume = {abs/2211.09113},  year = {2022} }
On Measuring Social Biases in Sentence Encoders	2019	https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7	The Word Embedding Association Test is extended to measure bias in sentence encoders and mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general.	seed	279	The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.	5e9c85235210b59a16bdd84b444a904ae271f7e7	@['JournalArticle', 'Conference']{may-etal-2019-on,  author = {Chandler May and Alex Wang and Shikha Bordia and Samuel R. Bowman and Rachel Rudinger},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {On Measuring Social Biases in Sentence Encoders},  volume = {abs/1903.10561},  year = {2019} }
On Isotropy Calibration of Transformer Models	2022	http://www.semanticscholar.org/paper/5601662c959fa0b1c7c1b9025ac628642a7bf466	Empirical evaluation of state-of-the-art methods for isotropy calibration on transformers find that they do not provide consistent improvements across models and tasks, and support the thesis that, given the local isotropy, transformers do not benefit from additional isotropic calibration.	maybe	0	Different studies of the embedding space of transformer models suggest that the distribution of contextual representations is highly anisotropic - the embeddings are distributed in a narrow cone. Meanwhile, static word representations (e.g., Word2Vec or GloVe) have been shown to benefit from isotropic spaces. Therefore, previous work has developed methods to calibrate the embedding space of transformers in order to ensure isotropy. However, a recent study (Cai et al. 2021) shows that the embedding space of transformers is locally isotropic, which suggests that these models are already capable of exploiting the expressive capacity of their embedding space. In this work, we conduct an empirical evaluation of state-of-the-art methods for isotropy calibration on transformers and find that they do not provide consistent improvements across models and tasks. These results support the thesis that, given the local isotropy, transformers do not benefit from additional isotropy calibration.	5601662c959fa0b1c7c1b9025ac628642a7bf466	@['JournalArticle']{ding-etal-2022-on,  author = {Yue Ding and Karolis Martinkus and Damian Pascual and S. Clematide and Roger Wattenhofer},  booktitle = {First Workshop on Insights from Negative Results in NLP},  pages = {1-9},  title = {On Isotropy Calibration of Transformer Models},  year = {2022} }
On Identifiability in Transformers	2019	https://www.semanticscholar.org/paper/1fe62a928bf5cfac0f373728f3a4de3cefe0951d	It is shown that self-attention distributions are not directly interpretable and the identifiability of attention weights and token embeddings is studied, and the aggregation of context into hidden tokens is studied.	seed	127	In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.	1fe62a928bf5cfac0f373728f3a4de3cefe0951d	@['JournalArticle']{brunner-etal-2019-on,  author = {Gino Brunner and Yang Liu and Damian Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},  booktitle = {International Conference on Learning Representations},  journal = {arXiv: Computation and Language},  title = {On Identifiability in Transformers},  year = {2019} }
On Faithfulness and Factuality in Abstractive Summarization	2020	http://www.semanticscholar.org/paper/dbeeca8466e0c177ec67c60d529899232415ca87	It is found that neural abstractive summarization models are highly prone to hallucinate content that is unfaithful to the input document and textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.	maybe	327	It is well known that the standard likelihood training and approximate decoding objectives in neural text generation models lead to less human-like responses for open-ended tasks such as language modeling and story generation. In this paper we have analyzed limitations of these models for abstractive document summarization and found that these models are highly prone to hallucinate content that is unfaithful to the input document. We conducted a large scale human evaluation of several neural abstractive summarization systems to better understand the types of hallucinations they produce. Our human annotators found substantial amounts of hallucinated content in all model generated summaries. However, our analysis does show that pretrained models are better summarizers not only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.	dbeeca8466e0c177ec67c60d529899232415ca87	@['JournalArticle', 'Conference']{maynez-etal-2020-on,  author = {Joshua Maynez and Shashi Narayan and Bernd Bohnet and Ryan T. McDonald},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {On Faithfulness and Factuality in Abstractive Summarization},  volume = {abs/2005.00661},  year = {2020} }
On Explaining Your Explanations of BERT: An Empirical Study with Sequence Classification	2021	http://www.semanticscholar.org/paper/a560943f23a1e00e26abbd9d84367217b235ff28	This work provides solid guidance for using attribution methods to explain decision makings of BERT for downstream classification tasks through extensive analyses of four existing attribution methods by applying them to four different datasets in sentiment analysis.	maybe	10	BERT, as one of the pretrianed language models, attracts the most attention in recent years for creating new benchmarks across GLUE tasks via fine-tuning. One pressing issue is to open up the blackbox and explain the decision makings of BERT. A number of attribution techniques have been proposed to explain BERT models, but are often limited to sequence to sequence tasks. In this paper, we adapt existing attribution methods on explaining decision makings of BERT in sequence classification tasks. We conduct extensive analyses of four existing attribution methods by applying them to four different datasets in sentiment analysis. We compare the reliability and robustness of each method via various ablation studies. Furthermore, we test whether attribution methods explain generalized semantics across semantically similar tasks. Our work provides solid guidance for using attribution methods to explain decision makings of BERT for downstream classification tasks.	a560943f23a1e00e26abbd9d84367217b235ff28	@['JournalArticle']{wu-ong-2021-on,  author = {Zhengxuan Wu and Desmond C. Ong},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On Explaining Your Explanations of BERT: An Empirical Study with Sequence Classification},  volume = {abs/2101.00196},  year = {2021} }
On Commonsense Cues in BERT for Solving Commonsense Tasks	2020	http://www.semanticscholar.org/paper/07b95736960731b49b6ce5aa0b29f10bd0586a6d	Using two different measures, it is found that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy.	maybe	6	BERT has been used for solving commonsense tasks such as CommonsenseQA. While prior research has found that BERT does contain commonsense information to some extent, there has been work showing that pre-trained models can rely on spurious associations (e.g., data bias) rather than key cues in solving sentiment classification and other problems. We quantitatively investigate the presence of structural commonsense cues in BERT when solving commonsense tasks, and the importance of such cues for the model prediction. Using two different measures, we find that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy.	07b95736960731b49b6ce5aa0b29f10bd0586a6d	@['JournalArticle']{cui-etal-2020-on,  author = {Leyang Cui and Sijie Cheng and Yu Wu and Yue Zhang},  booktitle = {Findings},  pages = {683-693},  title = {On Commonsense Cues in BERT for Solving Commonsense Tasks},  year = {2020} }
On Attention Redundancy: A Comprehensive Study	2021	https://www.semanticscholar.org/paper/57ed901be5d1b4d853d4f8998dadc1b60e2151f9	The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising, and it is discovered that redundancy patterns are task-agnostic.	maybe	12	Multi-layer multi-head self-attention mechanism is widely applied in modern neural language models. Attention redundancy has been observed among attention heads but has not been deeply studied in the literature. Using BERT-base model as an example, this paper provides a comprehensive study on attention redundancy which is helpful for model interpretation and model compression. We analyze the attention redundancy with Five-Ws and How. (What) We define and focus the study on redundancy matrices generated from pre-trained and fine-tuned BERT-base model for GLUE datasets. (How) We use both token-based and sentence-based distance functions to measure the redundancy. (Where) Clear and similar redundancy patterns (cluster structure) are observed among attention heads. (When) Redundancy patterns are similar in both pre-training and fine-tuning phases. (Who) We discover that redundancy patterns are task-agnostic. Similar redundancy patterns even exist for randomly generated token sequences. (“Why”) We also evaluate influences of the pre-training dropout ratios on attention redundancy. Based on the phase-independent and task-agnostic attention redundancy patterns, we propose a simple zero-shot pruning method as a case study. Experiments on fine-tuning GLUE tasks verify its effectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising.	57ed901be5d1b4d853d4f8998dadc1b60e2151f9	@['JournalArticle', 'Conference']{bian-etal-2021-on,  author = {Yuchen Bian and Jiaji Huang and Xingyu Cai and Jiahong Yuan and Kenneth Ward Church},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {930-945},  title = {On Attention Redundancy: A Comprehensive Study},  year = {2021} }
On a Benefit of Mask Language Modeling: Robustness to Simplicity Bias	2021	http://www.semanticscholar.org/paper/c0e9e14762503dd4515625c8e5d1b35436b29e9a	It is theoretically and empirically shown that MLM pretraining makes models robust to lexicon-level spurious features, and the gap between theories and the real world practices is closed by conducting experiments on the hate speech detection and the name entity recognition tasks.	maybe	1	Despite the success of pretrained masked language models (MLM), why MLM pretraining is useful is still a qeustion not fully answered. In this work we theoretically and empirically show that MLM pretraining makes models robust to lexicon-level spurious features, partly answer the question. We theoretically show that, when we can model the distribution of a spurious feature Π conditioned on the context, then (1) Π is at least as informative as the spurious feature, and (2) learning from Π is at least as simple as learning from the spurious feature. Therefore, MLM pretraining rescues the model from the simplicity bias caused by the spurious feature. We also explore the efficacy of MLM pretraing in causal settings. Finally we close the gap between our theories and the real world practices by conducting experiments on the hate speech detection and the name entity recognition tasks.	c0e9e14762503dd4515625c8e5d1b35436b29e9a	@['JournalArticle']{chiang-2021-on,  author = {Ting-Rui Chiang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {On a Benefit of Mask Language Modeling: Robustness to Simplicity Bias},  volume = {abs/2110.05301},  year = {2021} }
OctaNLP: A Benchmark for Evaluating Multitask Generalization of Transformer-Based Pre-trained Language Models	2021	http://www.semanticscholar.org/paper/e1162f3f69917fcc13a0b5c4aec556e893021d11	This paper overviews NLP evaluation metrics, multitask benchmarks, and the recent transformer-based language models, and proposes the octaNLP benchmark for comparing the generalization capabilities of the transformer- based pre-trained language models on multiple downstream NLP tasks simultaneously.	maybe	0		e1162f3f69917fcc13a0b5c4aec556e893021d11	@['Review']{kaddari-etal-2021-octanlp:,  author = {Zakaria Kaddari and Youssef Mellah and Jamal Berrich and M. Belkasmi and T. Bouchentouf},  booktitle = {Lecture Notes in Electrical Engineering},  journal = {Lecture Notes in Electrical Engineering},  title = {OctaNLP: A Benchmark for Evaluating Multitask Generalization of Transformer-Based Pre-trained Language Models},  year = {2021} }
NumGPT: Improving Numeracy Ability of Generative Pre-trained Models	2021	http://www.semanticscholar.org/paper/58c7a31bf47948d936de937b1cf7b49463608557	The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification.	maybe	6	Existing generative pre-trained language models (e.g., GPT) focus on modeling the language structure and semantics of general texts. However, those models do not consider the numerical properties of numbers and cannot perform robustly on numerical reasoning tasks (e.g., math word problems and measurement estimation). In this paper, we propose NumGPT, a generative pre-trained model that explicitly models the numerical properties of numbers in texts. Specifically, it leverages a prototype-based numeral embedding to encode the mantissa of the number and an individual embedding to encode the exponent of the number. A numeral-aware loss function is designed to integrate numerals into the pre-training objective of NumGPT. We conduct extensive experiments on four different datasets to evaluate the numeracy ability of NumGPT. The experiment results show that NumGPT outperforms baseline models (e.g., GPT and GPT with DICE) on a range of numerical reasoning tasks such as measurement estimation, number comparison, math word problems, and magnitude classification. Ablation studies are also conducted to evaluate the impact of pre-training and model hyperparameters on the	58c7a31bf47948d936de937b1cf7b49463608557	@['JournalArticle']{jin-etal-2021-numgpt:,  author = {Zhihua Jin and Xin Jiang and Xingbo Wang and Qun Liu and Yong Wang and Xiaozhe Ren and Huamin Qu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {NumGPT: Improving Numeracy Ability of Generative Pre-trained Models},  volume = {abs/2109.03137},  year = {2021} }
Numerical reasoning in machine reading comprehension tasks: are we there yet?	2021	http://www.semanticscholar.org/paper/5d9ac727b4a4e1044b3ba464df3be66eb8568127	This paper presents a controlled study on some of the top-performing model architectures for the task of numerical reasoning and suggests that the standard metrics are incapable of measuring progress towards such tasks.	maybe	4	Numerical reasoning based machine reading comprehension is a task that involves reading comprehension along with using arithmetic operations such as addition, subtraction, sorting and counting. The DROP benchmark (Dua et al., 2019) is a recent dataset that has inspired the design of NLP models aimed at solving this task. The current standings of these models in the DROP leaderboard, over standard metrics, suggests that the models have achieved near-human performance. However, does this mean that these models have learned to reason? In this paper, we present a controlled study on some of the top-performing model architectures for the task of numerical reasoning. Our observations suggest that the standard metrics are incapable of measuring progress towards such tasks.	5d9ac727b4a4e1044b3ba464df3be66eb8568127	@['JournalArticle', 'Conference']{negheimish-etal-2021-numerical,  author = {H. Al-Negheimish and P. Madhyastha and A. Russo},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Numerical reasoning in machine reading comprehension tasks: are we there yet?},  volume = {abs/2109.08207},  year = {2021} }
Numerical Correlation in Text	2022	https://www.semanticscholar.org/paper/ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9	A new task, Numerical Correlation in Text, which requires models to identify the correlation between two numbers in a sentence is proposed and a new dataset is introduced, which contains over 2,000 Wikipedia sentences with two numbers and their correlation labels.	yes	0	Evaluation of quantitative reasoning of large language models is an important step towards understanding their current capabilities and limitations. We propose a new task, Numerical Correlation in Text, which requires models to identify the correlation between two numbers in a sentence. To this end, we introduce a new dataset, which contains over 2,000 Wikipedia sentences with two numbers and their correlation labels. Using this dataset we are able to show that recent numerically aware pretraining methods for language models do not help generalization on this task posing a challenge for future work in this area.	ffd5d435c9534cf7e4e6995cd9d6a888d317e2f9	@None{spokoyny-etal-2022-numerical,  author = {Daniel M. Spokoyny and Chien-Sheng Wu and Caiming Xiong},  booktitle = {MATHNLP},  title = {Numerical Correlation in Text},  year = {2022} }
Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models	2023	https://www.semanticscholar.org/paper/f0ea9e2d3889d37f34743ed1dc64f11e8e0484de	The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding and showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression.	maybe	0	Large language models (LLM) such as OpenAI's ChatGPT and GPT-3 offer unique testbeds for exploring the translation challenges of turning literacy into numeracy. Previous publicly-available transformer models from eighteen months prior and 1000 times smaller failed to provide basic arithmetic. The statistical analysis of four complex datasets described here combines arithmetic manipulations that cannot be memorized or encoded by simple rules. The work examines whether next-token prediction succeeds from sentence completion into the realm of actual numerical understanding. For example, the work highlights cases for descriptive statistics on in-memory datasets that the LLM initially loads from memory or generates randomly using python libraries. The resulting exploratory data analysis showcases the model's capabilities to group by or pivot categorical sums, infer feature importance, derive correlations, and predict unseen test cases using linear regression. To extend the model's testable range, the research deletes and appends random rows such that recall alone cannot explain emergent numeracy.	f0ea9e2d3889d37f34743ed1dc64f11e8e0484de	@['JournalArticle']{noever-mckee-2023-numeracy,  author = {David Noever and Forrest McKee},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Numeracy from Literacy: Data Science as an Emergent Skill from Large Language Models},  volume = {abs/2301.13382},  year = {2023} }
Numeracy enhances the Literacy of Language Models	2021	http://www.semanticscholar.org/paper/31699d03a49e38295298f1b1a53185644abba12e	A significant improvement in MWP for sentences containing numbers is found, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers.	maybe	8	Specialized number representations in NLP have shown improvements on numerical reasoning tasks like arithmetic word problems and masked number prediction. But humans also use numeracy to make better sense of world concepts, e.g., you can seat 5 people in your ‘room’ but not 500. Does a better grasp of numbers improve a model’s understanding of other concepts and words? This paper studies the effect of using six different number encoders on the task of masked word prediction (MWP), as a proxy for evaluating literacy. To support this investigation, we develop Wiki-Convert, a 900,000 sentence dataset annotated with numbers and units, to avoid conflating nominal and ordinal number occurrences. We find a significant improvement in MWP for sentences containing numbers, that exponent embeddings are the best number encoders, yielding over 2 points jump in prediction accuracy over a BERT baseline, and that these enhanced literacy skills also generalize to contexts without annotated numbers. We release all code at https://git.io/JuZXn.	31699d03a49e38295298f1b1a53185644abba12e	@['JournalArticle', 'Conference']{thawani-etal-2021-numeracy,  author = {Avijit Thawani and J. Pujara and F. Ilievski},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {6960-6967},  title = {Numeracy enhances the Literacy of Language Models},  year = {2021} }
Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection	2020	http://www.semanticscholar.org/paper/e969aa3422a49152c22f3faf734e4561a2a3cf42	This work presents Iterative Null-space Projection (INLP), a novel method for removing information from neural representations based on repeated training of linear classifiers that predict a certain property the authors aim to remove, followed by projection of the representations on their null-space.	maybe	133	The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.	e969aa3422a49152c22f3faf734e4561a2a3cf42	@['JournalArticle', 'Conference']{ravfogel-etal-2020-null,  author = {Shauli Ravfogel and Yanai Elazar and Hila Gonen and Michael Twiton and Yoav Goldberg},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7237-7256},  title = {Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection},  year = {2020} }
Novel Aficionados and Doppelgängers: a referential task for semantic representations of individual entities	2021	http://www.semanticscholar.org/paper/db74b20baa82dbcaf641f82fb776e71e581f679b	This work shows that the semantic distinction between proper names and common nouns is reflected in their linguistic distributions by employing an original task for distributional semantics, the Doppelgänger test, an extensive set of models, and a new dataset, the Novel Aficionados dataset.	maybe	0	In human semantic cognition, proper names (names which refer to individual entities) are harder to learn and retrieve than common nouns. This seems to be the case for machine learning algorithms too, but the linguistic and distributional reasons for this behaviour have not been investigated in depth so far. To tackle this issue, we show that the semantic distinction between proper names and common nouns is reflected in their linguistic distributions by employing an original task for distributional semantics, the Doppelgänger test, an extensive set of models, and a new dataset, the Novel Aficionados dataset. The results indicate that the distributional representations of different individual entities are less clearly distinguishable from each other than those of common nouns, an outcome which intriguingly mirrors human cognition.	db74b20baa82dbcaf641f82fb776e71e581f679b	@['JournalArticle']{bruera-herbelot-2021-novel,  author = {Andrea Bruera and Aurélie Herbelot},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Novel Aficionados and Doppelgängers: a referential task for semantic representations of individual entities},  volume = {abs/2104.10270},  year = {2021} }
Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation	2022	http://www.semanticscholar.org/paper/7c950f94f8b209a4260ac34a7df36495cb7ef1b6	A natural language inference test suite is introduced to enable probing the capabilities of NLP methods, with the aim of understanding sub-clausal negation.	maybe	1	Negation is poorly captured by current language models, although the extent of this problem is not widely understood. We introduce a natural language inference (NLI) test suite to enable probing the capabilities of NLP methods, with the aim of understanding sub-clausal negation. The test suite contains premise–hypothesis pairs where the premise contains sub-clausal negation and the hypothesis is constructed by making minimal modifications to the premise in order to reflect different possible interpretations. Aside from adopting standard NLI labels, our test suite is systematically constructed under a rigorous linguistic framework. It includes annotation of negation types and constructions grounded in linguistic theory, as well as the operations used to construct hypotheses. This facilitates fine-grained analysis of model performance. We conduct experiments using pre-trained language models to demonstrate that our test suite is more challenging than existing benchmarks focused on negation, and show how our annotation supports a deeper understanding of the current NLI capabilities in terms of negation and quantification.	7c950f94f8b209a4260ac34a7df36495cb7ef1b6	@['JournalArticle']{truong-etal-2022-not,  author = {Thinh Hung Truong and Yulia Otmakhova and Tim Baldwin and Trevor Cohn and Karin M. Verspoor and Jey Han Lau},  booktitle = {AACL},  pages = {883-894},  title = {Not another Negation Benchmark: The NaN-NLI Test Suite for Sub-clausal Negation},  year = {2022} }
Not all parameters are born equal: Attention is mostly what you need	2020	https://www.semanticscholar.org/paper/947bec3b6ccb112aea56da230560207ac800ee2b	While the embedding layer is the least essential for machine translation tasks, it is the most important component for language modelling tasks.	yes	4	Transformers are widely used in state-of-the-art machine translation, but the key to their success is still unknown. To gain insight into this, we consider three groups of parameters: embeddings, attention, and Feed-Forward Neural network (FFN) layers. We examine the relative importance of each by performing an ablation study where we initialise them at random and freeze them, so that their weights do not change over the course of the training. Through this, we show that the attention and FFN are equally important and fulfil the same functionality in a model. We show that the decision about whether a component is frozen or allowed to train is at least as important for the final model performance as its number of parameters. At the same time, the number of parameters alone is not indicative of a component’s importance. Finally, while the embedding layer is the least essential for machine translation tasks, it is the most important component for language modelling tasks.	947bec3b6ccb112aea56da230560207ac800ee2b	@['JournalArticle']{bogoychev-2020-not,  author = {Nikolay Bogoychev},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {363-374},  title = {Not all parameters are born equal: Attention is mostly what you need},  year = {2020} }
Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations	2021	https://www.semanticscholar.org/paper/6cc9484612ab146c9fed9f7dce283c815af3cbc8	This work extends the probing studies to two other models in the BERT family, namely ELECTRA and XLNet, showing that variations in the pre-training objectives or architectural choices can result in different behaviors in encoding linguistic information in the representations.	maybe	7	Most of the recent works on probing representations have focused on BERT, with the presumption that the findings might be similar to the other models. In this work, we extend the probing studies to two other models in the family, namely ELECTRA and XLNet, showing that variations in the pre-training objectives or architectural choices can result in different behaviors in encoding linguistic information in the representations. Most notably, we observe that ELECTRA tends to encode linguistic knowledge in the deeper layers, whereas XLNet instead concentrates that in the earlier layers. Also, the former model undergoes a slight change during fine-tuning, whereas the latter experiences significant adjustments. Moreover, we show that drawing conclusions based on the weight mixing evaluation strategy—which is widely used in the context of layer-wise probing—can be misleading given the norm disparity of the representations across different layers. Instead, we adopt an alternative information-theoretic probing with minimum description length, which has recently been proven to provide more reliable and informative results.	6cc9484612ab146c9fed9f7dce283c815af3cbc8	@['JournalArticle']{fayyaz-etal-2021-not,  author = {Mohsen Fayyaz and Ehsan Aghazadeh and A. Modarressi and Hosein Mohebbi and Mohammad Taher Pilehvar},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {375-388},  title = {Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids’ Representations},  year = {2021} }
NOPE: A Corpus of Naturally-Occurring Presuppositions in English	2021	http://www.semanticscholar.org/paper/799407e7172266f2bbcd59186519c3a8c8992916	This work introduces the Naturally-Occurring Presuppositions in English (NOPE) Corpus to investigate the context-sensitivity of 10 different types of presupposition triggers and to evaluate machine learning models’ ability to predict human inferences.	maybe	3	Understanding language requires grasping not only the overtly stated content, but also making inferences about things that were left unsaid. These inferences include presuppositions, a phenomenon by which a listener learns about new information through reasoning about what a speaker takes as given. Presuppositions require complex understanding of the lexical and syntactic properties that trigger them as well as the broader conversational context. In this work, we introduce the Naturally-Occurring Presuppositions in English (NOPE) Corpus to investigate the context-sensitivity of 10 different types of presupposition triggers and to evaluate machine learning models’ ability to predict human inferences. We find that most of the triggers we investigate exhibit moderate variability. We further find that transformer-based models draw correct inferences in simple cases involving presuppositions, but they fail to capture the minority of exceptional cases in which human judgments reveal complex interactions between context and triggers.	799407e7172266f2bbcd59186519c3a8c8992916	@['JournalArticle']{parrish-etal-2021-nope:,  author = {Alicia Parrish and Sebastian Schuster and Alex Warstadt and Omar Agha and Soo-hwan Lee and Zhuoye Zhao and Sam Bowman and Tal Linzen},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {NOPE: A Corpus of Naturally-Occurring Presuppositions in English},  volume = {abs/2109.06987},  year = {2021} }
Nonoperational: Assessing Representations of Gender in BERT and XLM	2019	http://www.semanticscholar.org/paper/29b9c0e64795bcbdbf3a9cbd6d9e448cd6afb497	It is argued that structural assumptions such as “property X is represented in a linear subspace of the embedding space” must be carefully validated before developing techniques based on such operationalizations, and that assumptions that are valid for monolingual models do not trivially extend to the multilingual setting.	maybe	0	Several recent studies operationalize the gender bias of word embeddings as a linear subspace of the embedding space. In this paper, we show that 1) semantic gender and stereotypical gender are not represented uniformly in the embedding space of multilingual BERT and monolingual English XLM, making gender debiasing techniques based on “normalizing out” the semantic gender component ineffective for these models; 2) within a noun phrase, the French grammatical gender of the head can be partially recovered by a linear classifier over the embeddings of the dependents even when the embeddings are entirely in English, as long as the embeddings are produced by a multilingual language model; and 3) multilingual language models trained on both English and French do not encode French grammatical gender as a linear subspace of their embedding spaces. We hence argue that structural assumptions such as “property X is represented in a linear subspace of the embedding space” must be carefully validated before developing techniques based on such operationalizations, and that assumptions that are valid for monolingual models do not trivially extend to the multilingual setting.	29b9c0e64795bcbdbf3a9cbd6d9e448cd6afb497	@None{lin-frank-2019-nonoperational:,  author = {Yongjie Lin and R. Frank},  title = {Nonoperational: Assessing Representations of Gender in BERT and XLM},  year = {2019} }
New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities	2022	http://www.semanticscholar.org/paper/26a4ae8daf5b544677a3ef043ac1b79e35298e6e	The results of the experiments show that pre-trained language models do encode information on whether an entity has been introduced before or not in the discourse, however, this information alone is not sufficient to find the entities in a discourse, opening up interesting questions about the definition of entities for future work.	maybe	0	Recent research shows that pre-trained language models, built to generate text conditioned on some context, learn to encode syntactic knowledge to a certain degree. This has motivated researchers to move beyond the sentence-level and look into their ability to encode less studied discourse-level phenomena. In this paper, we add to the body of probing research by investigating discourse entity representations in large pre-trained language models in English. Motivated by early theories of discourse and key pieces of previous work, we focus on the information-status of entities as discourse-new or discourse-old. We present two probing models, one based on binary classification and another one on sequence labeling. The results of our experiments show that pre-trained language models do encode information on whether an entity has been introduced before or not in the discourse. However, this information alone is not sufficient to find the entities in a discourse, opening up interesting questions about the definition of entities for future work.	26a4ae8daf5b544677a3ef043ac1b79e35298e6e	@['JournalArticle', 'Conference']{loáiciga-etal-2022-new,  author = {S. Loáiciga and Anne Beyer and David Schlangen},  booktitle = {International Conference on Computational Linguistics},  pages = {875-886},  title = {New or Old? Exploring How Pre-Trained Language Models Represent Discourse Entities},  year = {2022} }
Neuron-level Interpretation of Deep NLP Models: A Survey	2021	http://www.semanticscholar.org/paper/ff56dfdbc8b86decbc6119d96c1097c0fef56ecd	The work done on neuron analysis is surveyed including methods to discover and understand neurons in a network; evaluation methods; major findings including cross architectural comparisons that neuron analysis has unraveled; applications of neuron probing such as: controlling the model, domain adaptation, and so forth.	maybe	10	Abstract The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.	ff56dfdbc8b86decbc6119d96c1097c0fef56ecd	@['JournalArticle', 'Review']{sajjad-etal-2021-neuron,  author = {Hassan Sajjad and Nadir Durrani and Fahim Dalvi},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1285-1303},  title = {Neuron-level Interpretation of Deep NLP Models: A Survey},  volume = {10},  year = {2021} }
Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs	2022	http://www.semanticscholar.org/paper/b4c16b0f26f9f5ad5e12f9bec3f1ad72eaa5491b		yes	6	Social intelligence and Theory of Mind (TOM), i.e., the ability to reason about the different mental states, intents, and reactions of all people involved, allows humans to effectively navigate and understand everyday social interactions. As NLP systems are used in increasingly complex social situations, their ability to grasp social dynamics becomes crucial.In this work, we examine the open question of social intelligence and Theory of Mind in modern NLP systems from an empirical and theorybased perspective. We show that one of today’s largest language models (GPT-3; Brown et al., 2020) lacks this kind of social intelligence out-of-the box, using two tasks: SocialIQa (Sap et al., 2019), which measure models’ ability to understand intents and reactions of participants of social interactions, and ToMi (Le, Boureau, and Nickel, 2019), which measures whether models can infer mental states and realities of participants of situations.Our results show that models struggle substantially at these Theory of Mind tasks, with well-below-human accuracies of 55% and 60% on SocialIQa and ToMi, respectively. To conclude, we draw on theories from pragmatics to contextualize this shortcoming of large language models, by examining the limitations stemming from their data, neural architecture, and training paradigms. Challenging the prevalent narrative that only scale is needed, we posit that person-centric NLP approaches might be more effective towards neural Theory of Mind.	b4c16b0f26f9f5ad5e12f9bec3f1ad72eaa5491b	@['JournalArticle', 'Conference']{sap-etal-2022-neural,  author = {Maarten Sap and Ronan Le Bras and Daniel Fried and Yejin Choi},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs},  volume = {abs/2210.13312},  year = {2022} }
Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation	2020	https://www.semanticscholar.org/paper/c30b457fdfb0623b87379de79ffaa570a7f3bb48	It is found that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNNI fine-tuning addresses this failure, suggesting that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.	maybe	34	We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on lexical entailment and negation. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.	c30b457fdfb0623b87379de79ffaa570a7f3bb48	@['JournalArticle']{geiger-etal-2020-neural,  author = {Atticus Geiger and Kyle Richardson and Christopher Potts},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {163-173},  title = {Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation},  year = {2020} }
Neural language modeling of free word order argument structure	2019	http://www.semanticscholar.org/paper/33ce36b10cded5ab0d3c3e2445a1c2231f1d2696		maybe	0	Neural language models trained with a predictive or masked objective have proven successful at capturing short and long distance syntactic dependencies. Here, we focus on verb argument structure in German, which has the interesting property that verb arguments may appear in a relatively free order in subordinate clauses. Therefore, checking that the verb argument structure is correct cannot be done in a strictly sequential fashion, but rather requires to keep track of the arguments’ cases irrespective of their orders. We introduce a new probing methodology based on minimal variation sets and show that both Transformers and LSTM achieve a score substantially better than chance on this test. As humans, they also show graded judgments preferring canonical word orders and plausible case assignments. However, we also found unexpected discrepancies in the strength of these effects, the LSTMs having difficulties rejecting ungrammatical sentences containing frequent argument structure types (double nominatives), and the Transformers tending to overgeneralize, accepting some infrequent word orders or implausible sentences that humans barely accept.	33ce36b10cded5ab0d3c3e2445a1c2231f1d2696	@None{rochereau-etal-2021-neural,  author = {Charlotte Rochereau and B. Sagot and Emmanuel Dupoux},  title = {Neural language modeling of free word order argument structure},  year = {2021} }
Neural Encoding and Decoding With Distributed Sentence Representations	2020	http://www.semanticscholar.org/paper/e0ce00b2c8bfa11cc15b1eb14d5656bed06012af	Evaluating the ability of a wide range of distributed semantic models to predict and decipher the functional magnetic resonance imaging images from humans reading sentences finds that differences in the performance of the DSMs in modeling brain activities can be at least partially explained by the granularity of their semantic representations.	maybe	12	Building computational models to account for the cortical representation of language plays an important role in understanding the human linguistic system. Recent progress in distributed semantic models (DSMs), especially transformer-based methods, has driven advances in many language understanding tasks, making DSM a promising methodology to probe brain language processing. DSMs have been shown to reliably explain cortical responses to word stimuli. However, characterizing the brain activities for sentence processing is much less exhaustively explored with DSMs, especially the deep neural network-based methods. What is the relationship between cortical sentence representations against DSMs? What linguistic features that a DSM catches better explain its correlation with the brain activities aroused by sentence stimuli? Could distributed sentence representations help to reveal the semantic selectivity of different brain areas? We address these questions through the lens of neural encoding and decoding, fueled by the latest developments in natural language representation learning. We begin by evaluating the ability of a wide range of 12 DSMs to predict and decipher the functional magnetic resonance imaging (fMRI) images from humans reading sentences. Most models deliver high accuracy in the left middle temporal gyrus (LMTG) and left occipital complex (LOC). Notably, encoders trained with transformer-based DSMs consistently outperform other unsupervised structured models and all the unstructured baselines. With probing and ablation tasks, we further find that differences in the performance of the DSMs in modeling brain activities can be at least partially explained by the granularity of their semantic representations. We also illustrate the DSM’s selectivity for concept categories and show that the topics are represented by spatially overlapping and distributed cortical patterns. Our results corroborate and extend previous findings in understanding the relation between DSMs and neural activation patterns and contribute to building solid brain–machine interfaces with deep neural network representations.	e0ce00b2c8bfa11cc15b1eb14d5656bed06012af	@['JournalArticle']{sun-etal-2020-neural,  author = {Jingyuan Sun and Shaonan Wang and Jiajun Zhang and Chengqing Zong},  booktitle = {IEEE Transactions on Neural Networks and Learning Systems},  journal = {IEEE Transactions on Neural Networks and Learning Systems},  pages = {589-603},  title = {Neural Encoding and Decoding With Distributed Sentence Representations},  volume = {32},  year = {2020} }
Neural Detection of Cross-lingual Syntactic Knowledge	2022	http://www.semanticscholar.org/paper/15ea2a214bfbce2813655ae458f77d3daa32f694	The results of the layer-wise probing experiments show that token-level syntax is localisable in higher layers and consistency is shown across the typologically different languages, whereas sentence- level syntax is distributed across the layers in typology-specific and universal manners.	maybe	0	In recent years, there has been prominent development in pre-trained multilingual language models, such as mBERT, XLM-R, etc., which are able to capture and learn linguistic knowledge from input across a variety of languages simultaneously. How-ever, little is known about where multilingual models localise what they have learnt across languages. In this paper, we specifically evaluate cross-lingual syntactic information embedded in CINO, a more recent multilingual pre-trained language model. We probe CINO on Universal Dependencies treebank datasets of English and Chinese Mandarin for two syntax-related layer-wise evaluation tasks: Part-of-Speech Tagging at token level and Syntax Tree-depth Prediction at sentence level. The results of our layer-wise probing experiments show that token-level syntax is localisable in higher layers and consistency is shown across the typologically different languages, whereas sentence-level syntax is distributed across the layers in typology-specific and universal manners.	15ea2a214bfbce2813655ae458f77d3daa32f694	@None{chen-farrús-2022-neural,  author = {Yongjian Chen and M. Farrús},  booktitle = {IberSPEECH Conference},  journal = {IberSPEECH 2022},  title = {Neural Detection of Cross-lingual Syntactic Knowledge},  year = {2022} }
Negation, Coordination, and Quantifiers in Contextualized Language Models	2022	https://www.semanticscholar.org/paper/f2878e3a27950c674425ba896bfd1c2be7dd67e5	This paper creates suitable datasets, provides new insights into the inner workings of LMs vis-a-vis function words and implements an assisting visual web interface for qualitative analysis.	maybe	1	With the success of contextualized language models, much research explores what these models really learn and in which cases they still fail. Most of this work focuses on specific NLP tasks and on the learning outcome. Little research has attempted to decouple the models’ weaknesses from specific tasks and focus on the embeddings per se and their mode of learning. In this paper, we take up this research opportunity: based on theoretical linguistic insights, we explore whether the semantic constraints of function words are learned and how the surrounding context impacts their embeddings. We create suitable datasets, provide new insights into the inner workings of LMs vis-a-vis function words and implement an assisting visual web interface for qualitative analysis.	f2878e3a27950c674425ba896bfd1c2be7dd67e5	@['JournalArticle', 'Conference']{kalouli-etal-2022-negation,  author = {A. Kalouli and R. Sevastjanova and C. Beck and Maribel Romero},  booktitle = {International Conference on Computational Linguistics},  journal = {ArXiv},  title = {Negation, Coordination, and Quantifiers in Contextualized Language Models},  volume = {abs/2209.07836},  year = {2022} }
Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly	2019	http://www.semanticscholar.org/paper/68c1bf884f0fc0e86641466a1f1fa67e79f16a17	Two new probing tasks analyzing factual knowledge stored in Pretrained Language Models are proposed and it is found that PLMs do not distinguish between negated and non-negated cloze questions, and PLMs are easily distracted by misprimes.	maybe	151	Building on Petroni et al. 2019, we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (‘‘Birds cannot [MASK]”) and non-negated (‘‘Birds can [MASK]”) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add “misprimes” to cloze questions (‘‘Talk? Birds can [MASK]”). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.	68c1bf884f0fc0e86641466a1f1fa67e79f16a17	@['JournalArticle', 'Conference']{kassner-schütze-2019-negated,  author = {Nora Kassner and Hinrich Schütze},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7811-7818},  title = {Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly},  year = {2019} }
Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection	2022	http://www.semanticscholar.org/paper/1c303f92f2eb54c0b0c97c599b28eb7aaa4dc798	A novel feature attribution method for explaining text classifiers is presented, and it is shown that different values of necessity and sufficiency for identity terms correspond to different kinds of false positive errors, exposing sources of classifier bias against marginalized groups.	maybe	5	We present a novel feature attribution method for explaining text classifiers, and analyze it in the context of hate speech detection. Although feature attribution models usually provide a single importance score for each token, we instead provide two complementary and theoretically-grounded scores – necessity and sufficiency – resulting in more informative explanations. We propose a transparent method that calculates these values by generating explicit perturbations of the input text, allowing the importance scores themselves to be explainable. We employ our method to explain the predictions of different hate speech detection models on the same set of curated examples from a test suite, and show that different values of necessity and sufficiency for identity terms correspond to different kinds of false positive errors, exposing sources of classifier bias against marginalized groups.	1c303f92f2eb54c0b0c97c599b28eb7aaa4dc798	@['JournalArticle', 'Conference']{balkir-etal-2022-necessity,  author = {Esma Balkir and I. Nejadgholi and Kathleen C. Fraser and Svetlana Kiritchenko},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2672-2686},  title = {Necessity and Sufficiency for Explaining Text Classifiers: A Case Study in Hate Speech Detection},  year = {2022} }
Naturalistic Causal Probing for Morpho-Syntax	2022	http://www.semanticscholar.org/paper/aa0c9285bb76c7a4c3f32a5b122f1d5eb171c5d6	This work proposes a strategy for input-level intervention on naturalistic sentences that intervenes on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged, which allows us to causally probe pre-trained models.	maybe	1	Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes. In this work, we suggest a strategy for input-level intervention on naturalistic sentences. Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models. We apply our naturalistic causal probing framework to analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish: the multilingual versions of BERT, RoBERTa, and GPT-2. Our experiments suggest that naturalistic interventions lead to stable estimates of the causal effects of various linguistic properties. Moreover, our experiments demonstrate the importance of naturalistic causal probing when analyzing pre-trained models.	aa0c9285bb76c7a4c3f32a5b122f1d5eb171c5d6	@['JournalArticle']{amini-etal-2022-naturalistic,  author = {Afra Amini and Tiago Pimentel and Clara Meister and Ryan Cotterell},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Naturalistic Causal Probing for Morpho-Syntax},  volume = {abs/2205.07043},  year = {2022} }
Multilingual Probing of Deep Pre-Trained Contextual Encoders	2019	http://www.semanticscholar.org/paper/3ba6379bb0ca7fcd949aa35c3de2adec322b2f53	This work comprehensively evaluate and analyse – from a typological perspective amongst others – multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages.	maybe	11	Encoders that generate representations based on context have, in recent years, benefited from adaptations that allow for pre-training on large text corpora. Earlier work on evaluating fixed-length sentence representations has included the use of ‘probing’ tasks, that use diagnostic classifiers to attempt to quantify the extent to which these encoders capture specific linguistic phenomena. The principle of probing has also resulted in extended evaluations that include relatively newer word-level pre-trained encoders. We build on probing tasks established in the literature and comprehensively evaluate and analyse – from a typological perspective amongst others – multilingual variants of existing encoders on probing datasets constructed for 6 non-English languages. Specifically, we probe each layer of a multiple monolingual RNN-based ELMo models, the transformer-based BERT’s cased and uncased multilingual variants, and a variant of BERT that uses a cross-lingual modelling scheme (XLM).	3ba6379bb0ca7fcd949aa35c3de2adec322b2f53	@None{ravishankar-etal-2019-multilingual,  author = {Vinit Ravishankar and Lilja Øvrelid and Erik Velldal},  title = {Multilingual Probing of Deep Pre-Trained Contextual Encoders},  year = {2019} }
Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models	2021	http://www.semanticscholar.org/paper/fcfc9648561a221750b8085790ad9ba1bebb1800	This work translates the established benchmarks TREx and GoogleRE into 53 languages and finds that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance.	maybe	37	Recently, it has been found that monolingual English language models can be used as knowledge bases. Instead of structural knowledge base queries, masked sentences such as “Paris is the capital of [MASK]” are used as probes. We translate the established benchmarks TREx and GoogleRE into 53 languages. Working with mBERT, we investigate three questions. (i) Can mBERT be used as a multilingual knowledge base? Most prior work only considers English. Extending research to multiple languages is important for diversity and accessibility. (ii) Is mBERT’s performance as knowledge base language-independent or does it vary from language to language? (iii) A multilingual model is trained on more text, e.g., mBERT is trained on 104 Wikipedias. Can mBERT leverage this for better performance? We find that using mBERT as a knowledge base yields varying performance across languages and pooling predictions across languages improves performance. Conversely, mBERT exhibits a language bias; e.g., when queried in Italian, it tends to predict Italy as the country of origin.	fcfc9648561a221750b8085790ad9ba1bebb1800	@['JournalArticle', 'Conference']{kassner-etal-2021-multilingual,  author = {Nora Kassner and Philipp Dufter and Hinrich Schütze},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models},  volume = {abs/2102.00894},  year = {2021} }
Multilingual BERT, ergativity, and grammatical subjecthood	2021	http://www.semanticscholar.org/paper/4004d39cb3440c5bf7edde84beffb2ecfc4c9a91	This paper investigates how Multilingual by examining how English as a second language is implemented in the classroom and how teachers and students use it to communicate.	maybe	0	We investigate how Multilingual by examining how	4004d39cb3440c5bf7edde84beffb2ecfc4c9a91	@None{papadimitriou-etal-2021-multilingual,  author = {Isabel Papadimitriou and Ethan A. Chi and Richard Futrell and Kyle Mahowald},  booktitle = {SCIL},  pages = {425-426},  title = {Multilingual BERT, ergativity, and grammatical subjecthood},  volume = {4},  year = {2021} }
Multi-Stage Prompting for Knowledgeable Dialogue Generation	2022	http://www.semanticscholar.org/paper/2ff3b65e893afde3d98d0870129b1c7999fc1a43	This paper proposes a multi-stage prompting approach to generate knowledgeable responses from a single pretrained language model (LM) and shows that its knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness.	maybe	12	Existing knowledge-grounded dialogue systems typically use finetuned versions of a pretrained language model (LM) and large-scale knowledge bases. These models typically fail to generalize on topics outside of the knowledge base, and require maintaining separate potentially large checkpoints each time finetuning is needed. In this paper, we aim to address these limitations by leveraging the inherent knowledge stored in the pretrained LM as well as its powerful generation ability. We propose a multi-stage prompting approach to generate knowledgeable responses from a single pretrained LM. We first prompt the LM to generate knowledge based on the dialogue context. Then, we further prompt it to generate responses based on the dialogue context and the previously generated knowledge. Results show that our knowledge generator outperforms the state-of-the-art retrieval-based model by 5.8% when combining knowledge relevance and correctness. In addition, our multi-stage prompting outperforms the finetuning-based dialogue model in terms of response knowledgeability and engagement by up to 10% and 5%, respectively. Furthermore, we scale our model up to 530 billion parameters and demonstrate that larger LMs improve the generation correctness score by up to 10%, and response relevance, knowledgeability and engagement by up to 10%. Our code is available at: https://github.com/NVIDIA/Megatron-LM.	2ff3b65e893afde3d98d0870129b1c7999fc1a43	@['JournalArticle']{liu-etal-2022-multi,  author = {Zihan Liu and M. Patwary and R. Prenger and Shrimai Prabhumoye and Wei Ping and M. Shoeybi and Bryan Catanzaro},  booktitle = {Findings},  journal = {ArXiv},  title = {Multi-Stage Prompting for Knowledgeable Dialogue Generation},  volume = {abs/2203.08745},  year = {2022} }
MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension	2019	http://www.semanticscholar.org/paper/1be21e96eaac56f626e7b41c1f332b6b46131608	In this task, 18 distinct question answering datasets were adapted and unified into the same format and the best system achieved an average F1 score of 72.5 on the 12 held-out datasets.	maybe	187	We present the results of the Machine Reading for Question Answering (MRQA) 2019 shared task on evaluating the generalization capabilities of reading comprehension systems. In this task, we adapted and unified 18 distinct question answering datasets into the same format. Among them, six datasets were made available for training, six datasets were made available for development, and the rest were hidden for final evaluation. Ten teams submitted systems, which explored various ideas including data sampling, multi-task learning, adversarial training and ensembling. The best system achieved an average F1 score of 72.5 on the 12 held-out datasets, 10.7 absolute points higher than our initial baseline based on BERT.	1be21e96eaac56f626e7b41c1f332b6b46131608	@['JournalArticle', 'Conference']{fisch-etal-2019-mrqa,  author = {Adam Fisch and Alon Talmor and Robin Jia and Minjoon Seo and Eunsol Choi and Danqi Chen},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension},  volume = {abs/1910.09753},  year = {2019} }
MPI: Evaluating and Inducing Personality in Pre-trained Language Models	2022	https://www.semanticscholar.org/paper/cc99dc93dcbc61f2c17698ce446a60ab1fd22ae8		maybe	3	Originated as a philosophical quest, personality discerns how individuals differ from each other in terms of thinking, feeling, and behaving [21]. Towards building social machines that work with humans on a daily basis, we are motivated to ask: (1) Do existing pre-trained language models possess personality, akin to their human counterpart? If so, (2) how can we evaluate them? Further, given this evaluation framework, (3) how can we induce a certain personality in a fully controllable fashion? To tackle these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By evaluating models with MPI, we provide the ﬁrst piece of evidence showing the existence of personality in pre-trained language models. We further devise a C HAIN P ROMPTING method to induce the language model with a speciﬁc personality in a controllable man-ner, capable of producing diversiﬁed	cc99dc93dcbc61f2c17698ce446a60ab1fd22ae8	@['JournalArticle']{jiang-etal-2022-mpi:,  author = {Guangyuan Jiang and Manjie Xu and Song-Chun Zhu and Wenjuan Han and Chi Zhang and Yixin Zhu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {MPI: Evaluating and Inducing Personality in Pre-trained Language Models},  volume = {abs/2206.07550},  year = {2022} }
Morph Call: Probing Morphosyntactic Content of Multilingual Transformers	2021	http://www.semanticscholar.org/paper/2f9a33496079585ed2ed0d8118ec28e3f609c100	Morph Call presents Morph Call, a suite of 46 probing tasks for four Indo-European languages of different morphology: Russian, French, English and German, and proposes a new type of probing tasks based on detection of guided sentence perturbations.	maybe	3	The outstanding performance of transformer-based language models on a great variety of NLP and NLU tasks has stimulated interest in exploration of their inner workings. Recent research has been primarily focused on higher-level and complex linguistic phenomena such as syntax, semantics, world knowledge and common-sense. The majority of the studies is anglocentric, and little remains known regarding other languages, specifically their morphosyntactic properties. To this end, our work presents Morph Call, a suite of 46 probing tasks for four Indo-European languages of different morphology: Russian, French, English and German. We propose a new type of probing tasks based on detection of guided sentence perturbations. We use a combination of neuron-, layer- and representation-level introspection techniques to analyze the morphosyntactic content of four multilingual transformers, including their understudied distilled versions. Besides, we examine how fine-tuning on POS-tagging task affects the probing performance.	2f9a33496079585ed2ed0d8118ec28e3f609c100	@['JournalArticle']{mikhailov-etal-2021-morph,  author = {V. Mikhailov and Oleg Serikov and E. Artemova},  booktitle = {SIGTYP},  journal = {ArXiv},  title = {Morph Call: Probing Morphosyntactic Content of Multilingual Transformers},  volume = {abs/2104.12847},  year = {2021} }
Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity	2022	https://www.semanticscholar.org/paper/31200dbd7ad316c3130881a2d544a55f3fda6f89		maybe	0	Large Language Models (LLMs) have recently demonstrated impressive capability in generating ﬂuent text. LLMs have also shown an alarming tendency to reproduce social biases, for example stereotypical associations between gender and occupation or race and criminal behavior. Like race and gender, morality is an important social variable; our moral biases affect how we receive other people and their arguments. I anticipate that the apparent moral capabilities of LLMs will play an important role in their effects on the human social environment. This work investigates whether LLMs reproduce the moral biases associated with political groups, a capability I refer to as moral mimicry. I explore this hypothesis in GPT-3, a 175B-parameter language model based on the Transformer architecture, using tools from Moral Foundations Theory to measure the moral content in text generated by the model following prompting with liberal and conservative political identities. The results demonstrate that large language models are indeed moral mimics; when prompted with a political identity, GPT-3 generates text reﬂecting the corresponding moral biases. Moral mimicry could contribute to fostering understanding between social groups via moral reframing. Worryingly, it could also reinforce polarized views, exacerbating existing social challenges. I hope that this work encourages further investigation of the moral mimicry capability, including how to leverage it for social good and minimize its risks.	31200dbd7ad316c3130881a2d544a55f3fda6f89	@['JournalArticle']{simmons-2022-moral,  author = {Gabriel Simmons},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Moral Mimicry: Large Language Models Produce Moral Rationalizations Tailored to Political Identity},  volume = {abs/2209.12106},  year = {2022} }
Montague semantics and modifier consistency measurement in neural language models	2022	http://www.semanticscholar.org/paper/c80e38a6fd1027177ca7145d97377929033b825e		maybe			c80e38a6fd1027177ca7145d97377929033b825e	
Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations	2020	http://www.semanticscholar.org/paper/c1035a3fa5a0c74dea099b515ecc58d2d9387bac	DensRay, an analytical method for obtaining interpretable dense subspaces, performs on-par with prior approaches, but arguments that it is more robust and indications that it preserves language model performance better are provided.	maybe	17	Pretrained language models (PLMs) learn stereotypes held by humans and reflected in text from their training corpora, including gender bias. When PLMs are used for downstream tasks such as picking candidates for a job, people’s lives can be negatively affected by these learned stereotypes. Prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. Following this line of work, we propose to use DensRay, an analytical method for obtaining interpretable dense subspaces. We show that DensRay performs on-par with prior approaches, but provide arguments that it is more robust and provide indications that it preserves language model performance better. By applying DensRay to attention heads and layers of BERT we show that gender information is spread across all attention heads and most of the layers. Also we show that DensRay can obtain gender bias scores on both token and sentence levels. Finally, we demonstrate that we can remove bias multilingually, e.g., from Chinese, using only English training data.	c1035a3fa5a0c74dea099b515ecc58d2d9387bac	@['JournalArticle', 'Conference']{liang-etal-2020-monolingual,  author = {Sheng Liang and Philipp Dufter and Hinrich Schütze},  booktitle = {International Conference on Computational Linguistics},  pages = {5082-5093},  title = {Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations},  year = {2020} }
Modular Representation Underlies Systematic Generalization in Neural Natural Language Inference Models	2020	http://www.semanticscholar.org/paper/1ed2ff72ee7ae857de3f4a9b97d992eba3b9fa6a	It is argued that an essential factor is modular internal structure, in which systematic manipulations of model-internal states are related to causal effects on their outputs, thereby allowing us to identify modular structure.	maybe	4	In adversarial testing, we pose hard generalization tasks in order to gain insights into the solutions found by our models. What properties must a system have in order to succeed at these hard behavioral tasks? We argue that an essential factor is modular internal structure. Our central contribution is a new experimental method called 'interchange interventions', in which systematic manipulations of model-internal states are related to causal effects on their outputs, thereby allowing us to identify modular structure. Our work is grounded empirically in a new challenge Natural Language Inference dataset designed to assess systems on their ability to reason about entailment and negation. We find that a BERT model is strikingly successful at the systematic generalization task we pose using this dataset, and our active manipulations of model-internal vectors help us understand why: despite the densely interconnected nature of the BERT architecture, the learned model embeds modular, general theories of lexical entailment relations.	1ed2ff72ee7ae857de3f4a9b97d992eba3b9fa6a	@['JournalArticle']{geiger-etal-2020-modular,  author = {Atticus Geiger and Kyle Richardson and Christopher Potts},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Modular Representation Underlies Systematic Generalization in Neural Natural Language Inference Models},  volume = {abs/2004.14623},  year = {2020} }
Modifying Memories in Transformer Models	2020	https://www.semanticscholar.org/paper/5270b626feb66c8c363e93ba6608daae93c5003b	This paper proposes a new task of explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts, and benchmarked several approaches that provide natural baseline performances on this task.	maybe	29	Large Transformer models have achieved impressive performance in many natural language tasks. In particular, Transformer based language models have been shown to have great capabilities in encoding factual knowledge in their vast amount of parameters. While the tasks of improving the memorization and generalization of Transformers have been widely studied, it is not well known how to make transformers forget specific old facts and memorize new ones. In this paper, we propose a new task of \emph{explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts}. This task is useful in many scenarios, such as updating stale knowledge, protecting privacy, and eliminating unintended biases stored in the models. We benchmarked several approaches that provide natural baseline performances on this task. This leads to the discovery of key components of a Transformer model that are especially effective for knowledge modifications. The work also provides insights into the role that different training phases (such as pretraining and fine-tuning) play towards memorization and knowledge modification.	5270b626feb66c8c363e93ba6608daae93c5003b	@['JournalArticle']{zhu-etal-2020-modifying,  author = {Chen Zhu and A. Rawat and M. Zaheer and Srinadh Bhojanapalli and Daliang Li and Felix X. Yu and Sanjiv Kumar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Modifying Memories in Transformer Models},  volume = {abs/2012.00363},  year = {2020} }
Modelling General Properties of Nouns by Selectively Averaging Contextualised Embeddings	2020	http://www.semanticscholar.org/paper/5c9f1f30d8e2b34ffabd49a0bcbf0629f0f3426b	This paper finds that a simple strategy of averaging the contextualised embeddings of masked word mentions leads to vectors that outperform the static word vectors learned by BERT, as well as those from standard word embedding models, in property induction tasks.	maybe	1	While the success of pre-trained language models has largely eliminated the need for high-quality static word vectors in many NLP applications, static word vectors continue to play an important role in tasks where word meaning needs to be modelled in the absence of linguistic context. In this paper, we explore how the contextualised embeddings predicted by BERT can be used to produce high-quality word vectors for such domains, in particular related to knowledge base completion, where our focus is on capturing the semantic properties of nouns. We find that a simple strategy of averaging the contextualised embeddings of masked word mentions leads to vectors that outperform the static word vectors learned by BERT, as well as those from standard word embedding models, in property induction tasks. We notice in particular that masking target words is critical to achieve this strong performance, as the resulting vectors focus less on idiosyncratic properties and more on general semantic properties. Inspired by this view, we propose a filtering strategy which is aimed at removing the most idiosyncratic mention vectors, allowing us to obtain further performance gains in property induction.	5c9f1f30d8e2b34ffabd49a0bcbf0629f0f3426b	@['JournalArticle', 'Conference']{li-etal-2020-modelling,  author = {Na Li and Z. Bouraoui and José Camacho-Collados and Luis Espinosa Anke and Qing Gu and S. Schockaert},  booktitle = {International Joint Conference on Artificial Intelligence},  pages = {3850-3856},  title = {Modelling General Properties of Nouns by Selectively Averaging Contextualised Embeddings},  year = {2020} }
Modelling Commonsense Properties Using Pre-Trained Bi-Encoders	2022	http://www.semanticscholar.org/paper/11aab88f9742220bbc5a593f4a246edf7d57eae5	This work studies the possibility of fine-tuning language models to explicitly model concepts and their properties and shows that the resulting encoders allow us to predict commonsense properties with much higher accuracy than is possible by directly fine- tuning language models.	maybe	1	Grasping the commonsense properties of everyday concepts is an important prerequisite to language understanding. While contextualised language models are reportedly capable of predicting such commonsense properties with human-level accuracy, we argue that such results have been inflated because of the high similarity between training and test concepts. This means that models which capture concept similarity can perform well, even if they do not capture any knowledge of the commonsense properties themselves. In settings where there is no overlap between the properties that are considered during training and testing, we find that the empirical performance of standard language models drops dramatically. To address this, we study the possibility of fine-tuning language models to explicitly model concepts and their properties. In particular, we train separate concept and property encoders on two types of readily available data: extracted hyponym-hypernym pairs and generic sentences. Our experimental results show that the resulting encoders allow us to predict commonsense properties with much higher accuracy than is possible by directly fine-tuning language models. We also present experimental results for the related task of unsupervised hypernym discovery.	11aab88f9742220bbc5a593f4a246edf7d57eae5	@['JournalArticle', 'Conference']{gajbhiye-etal-2022-modelling,  author = {Amit Gajbhiye and Luis Espinosa-Anke and S. Schockaert},  booktitle = {International Conference on Computational Linguistics},  pages = {3971-3983},  title = {Modelling Commonsense Properties Using Pre-Trained Bi-Encoders},  year = {2022} }
Modeling the Influence of Verb Aspect on the Activation of Typical Event Locations with BERT	2021	http://www.semanticscholar.org/paper/d073db916b5c04c59f4e58e4951159693c54dd80	It is found that, although BERT efficiently modelled the typicality of locations, it did so independently of the verb aspect, with aspect to have a stronger influence on the scores, with locations in the imperfective setting being associated with lower surprisal values.	maybe	0	Prior studies on event knowledge in sentence comprehension have shown that the aspect of the main verb plays an important role in the processing of non-core semantic roles, such as locations: when the aspect of the main verb is imperfective, locations become more salient in the mental representation of the event and are easier for human comprehenders to process. In our study, we tested the popular language model BERT on two datasets derived from experimental studies to determine whether BERT’s predictions of prototypical event locations were also influenced by aspect. We found that, although BERT efficiently modelled the typicality of locations, it did so independently of the verb aspect. Even when the transformer was forced to focus on the verb phrase by masking the context words in the sentence, the typicality predictions were still accurate; in addition, we found aspect to have a stronger influence on the scores, with locations in the imperfective setting being associated with lower surprisal values.	d073db916b5c04c59f4e58e4951159693c54dd80	@['JournalArticle']{cho-etal-2021-modeling,  author = {Won-Ik Cho and Emmanuele Chersoni and Yu-Yin Hsu and Chu-Ren Huang},  booktitle = {Findings},  pages = {2922-2929},  title = {Modeling the Influence of Verb Aspect on the Activation of Typical Event Locations with BERT},  year = {2021} }
Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects	2021	http://www.semanticscholar.org/paper/a6e259c8813ae419610dab4190a8358adcc7b2b5	The model-based approach successfully replicates the seminal study of Lerner et al., 2011, which revealed the hierarchy of language areas by comparing the functional-magnetic resonance imaging (fMRI) of seven subjects listening to 7 min of both regular and scrambled narratives.	maybe	11	A popular approach to decompose the neural bases of language consists in correlating, across individuals, the brain responses to different stimuli (e.g. regular speech versus scrambled words, sentences, or paragraphs). Although successful, this ‘model-free’ approach necessitates the acquisition of a large and costly set of neuroimaging data. Here, we show that a model-based approach can reach equivalent results within subjects exposed to natural stimuli. We capitalize on the recently-discovered similarities between deep language models and the human brain to compute the mapping between i) the brain responses to regular speech and ii) the activations of deep language models elicited by modified stimuli (e.g. scrambled words, sentences, or paragraphs). Our model-based approach successfully repli-cates the seminal study of (Lerner et al., 2011), which revealed the hierarchy of language ar-eas by comparing the functional-magnetic resonance imaging (fMRI) of seven subjects listening to 7 min of both regular and scrambled narratives. We further extend and precise these results to the brain signals of 305 individuals listening to 4.1 hours of narrated stories. Overall, this study paves the way for efficient and flex-ible analyses of the brain bases of language.	a6e259c8813ae419610dab4190a8358adcc7b2b5	@['JournalArticle', 'Conference']{caucheteux-etal-2021-model,  author = {C. Caucheteux and Alexandre Gramfort and J. King},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {3635-3644},  title = {Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects},  year = {2021} }
Model Explanations under Calibration	2019	http://www.semanticscholar.org/paper/65926a0e95437bc1c986954478b3f8ff017ea609	A focused study on the impact of model interpretability in the context of calibration indicates that the means of using attention distributions for interpretability are highly unstable for un-calibrated models.	maybe	2	Explaining and interpreting the decisions of recommender systems are becoming extremely relevant both, for improving predictive performance, and providing valid explanations to users. While most of the recent interest has focused on providing local explanations, there has been a much lower emphasis on studying the effects of model dynamics and its impact on explanation. In this paper, we perform a focused study on the impact of model interpretability in the context of calibration. Specifically, we address the challenges of both over-confident and under-confident predictions with interpretability using attention distribution. Our results indicate that the means of using attention distributions for interpretability are highly unstable for un-calibrated models. Our empirical analysis on the stability of attention distribution raises questions on the utility of attention for explainability.	65926a0e95437bc1c986954478b3f8ff017ea609	@['JournalArticle']{batra-madhyastha-2019-model,  author = {Dhruv Batra and P. Madhyastha},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Model Explanations under Calibration},  volume = {abs/1906.07622},  year = {2019} }
Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models	2019	http://www.semanticscholar.org/paper/fcc7ba74c2c24345ab11924c7a40a3a0c141df5e	This work focuses on the transformer encoder-decoder model for the open-domain dialogue response generation task, and finds that after standard fine-tuning, the model forgets important language generation skills acquired during large-scale pre-training.	maybe	6	In this work, we study how the large-scale pretrain-finetune framework changes the behavior of a neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. We find that after standard fine-tuning, the model forgets important language generation skills acquired during large-scale pre-training. We demonstrate the forgetting phenomenon through a detailed behavior analysis from the perspectives of context sensitivity and knowledge transfer. Adopting the concept of data mixing, we propose an intuitive fine-tuning strategy named "mix-review". We find that mix-review effectively regularize the fine-tuning process, and the forgetting problem is largely alleviated. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.	fcc7ba74c2c24345ab11924c7a40a3a0c141df5e	@['JournalArticle', 'Review']{he-etal-2019-mix,  author = {Tianxing He and Jun Liu and Kyunghyun Cho and Myle Ott and Bing Liu and James R. Glass and Fuchun Peng},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Mix-review: Alleviate Forgetting in the Pretrain-Finetune Framework for Neural Language Generation Models},  volume = {abs/1910.07117},  year = {2019} }
MiQA: A Benchmark for Inference on Metaphorical Questions	2022	http://www.semanticscholar.org/paper/777683db4795ff691533c2c4be3244fabd826842		yes			777683db4795ff691533c2c4be3244fabd826842	
minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models	2022	http://www.semanticscholar.org/paper/2fc6cda64da8badd1bd2efcbfef2766e68aa5592	The minicons library is described and applied to two motivating case studies: One focusing on the learning dynamics of the BERT architecture on relative grammatical judgments, and the other on benchmarking 23 different LMs on zero-shot abductive reasoning.	maybe	5	We present minicons , an open source library that provides a standard API for researchers interested in conducting behavioral and representational analyses of transformer-based language models (LMs). Speciﬁcally, minicons enables researchers to apply analysis methods at two levels: (1) at the prediction level— by providing functions to efﬁciently extract word/sentence level probabilities; and (2) at the representational level—by also facilitating efﬁcient extraction of word/phrase level vec-tors from one or more layers. In this paper, we describe the library and apply it to two motivating case studies: One focusing on the learning dynamics of the BERT architecture on relative grammatical judgments, and the other on benchmarking 23 different LMs on zero-shot abductive reasoning. minicons is available at https://github.com/ kanishkamisra/minicons .	2fc6cda64da8badd1bd2efcbfef2766e68aa5592	@['JournalArticle']{misra-2022-minicons:,  author = {Kanishka Misra},  booktitle = {ArXiv},  journal = {ArXiv},  title = {minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models},  volume = {abs/2203.13112},  year = {2022} }
Mind's Eye: Grounded Language Model Reasoning through Simulation	2022	http://www.semanticscholar.org/paper/2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8	Mind’s Eye is presented, a paradigm to ground language model reasoning in the physical world by using a computational physics engine to simulate the possible outcomes, and then using the simulation results as part of the input, which enables language models to perform reasoning.	maybe	3	Successful and effective communication between humans and AI relies on a shared experience of the world. By training solely on written text, current language models (LMs) miss the grounded experience of humans in the real-world—their failure to relate language to the physical world causes knowledge to be misrepresented and obvious mistakes in their reasoning. We present Mind’s Eye, a paradigm to ground language model reasoning in the physical world. Given a physical reasoning question, we use a computational physics engine (DeepMind’s MuJoCo) to simulate the possible outcomes, and then use the simulation results as part of the input, which enables language models to perform reasoning. Experiments on 39 tasks in a physics alignment benchmark demonstrate that Mind’s Eye can improve reasoning ability by a large margin (27.9% zero-shot, and 46.0% few-shot absolute accuracy improvement on average). Smaller language models armed with Mind’s Eye can obtain similar performance to models that are 100 × larger. Finally, we conﬁrm the robustness of Mind’s Eye through ablation studies.	2cb97b2bb6ab2eb17add9ffe69d5cbeaca2b29c8	@['JournalArticle']{liu-etal-2022-mind's,  author = {Ruibo Liu and Jason Wei and S. Gu and Te-Yen Wu and Soroush Vosoughi and Claire Cui and Denny Zhou and Andrew M. Dai},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Mind's Eye: Grounded Language Model Reasoning through Simulation},  volume = {abs/2210.05359},  year = {2022} }
Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models	2022	http://www.semanticscholar.org/paper/f11f6066a92f20b71a8bc5c8a738ad78f073d095	A rigorous analysis and comparison of bias detection methods for contextual language models shows that minor design and implementation decisions have a substantial and often signiﬁcant impact on the derived bias scores.	maybe	0	The awareness and mitigation of biases are of fundamental importance for the fair and transparent use of contextual language models, yet they crucially depend on the accurate detection of biases as a precursor. Consequently, numerous bias detection methods have been proposed, which vary in their approach, the considered type of bias, and the data used for evaluation. However, while most detection methods are derived from the word embedding association test for static word embeddings, the reported results are heterogeneous, inconsis-tent, and ultimately inconclusive. To address this issue, we conduct a rigorous analysis and comparison of bias detection methods for contextual language models. Our results show that minor design and implementation decisions (or errors) have a substantial and often signiﬁcant impact on the derived bias scores. Overall, we ﬁnd the state of the ﬁeld to be both worse than previously acknowledged due to systematic and propagated errors in implementations, yet better than anticipated since divergent results in the literature homogenize after accounting for implementation errors. Based on our ﬁndings, we conclude with a discussion of paths towards more robust and consistent bias detection methods.	f11f6066a92f20b71a8bc5c8a738ad78f073d095	@['JournalArticle', 'Review']{husse-spitz-2022-mind,  author = {Silke Husse and Andreas Spitz},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Mind Your Bias: A Critical Review of Bias Detection Methods for Contextual Language Models},  volume = {abs/2211.08461},  year = {2022} }
Mind the Gap: Assessing Temporal Generalization in Neural Language Models	2021	http://www.semanticscholar.org/paper/ba233d75aa403092bda0bffc026be7913673ad69	It is argued that now is the right time to rethink the static way in which the authors currently train and evaluate their language models, and develop adaptive language models that can remain up-to-date with respect to their ever-changing and non-stationary world.	maybe	44	Our world is open-ended, non-stationary, and constantly evolving; thus what we talk about and how we talk about it change over time. This inherent dynamic nature of language contrasts with the current static language modelling paradigm, which trains and evaluates models on utterances from overlapping time periods. Despite impressive recent progress, we demonstrate that Transformer-XL language models perform worse in the realistic setup of predicting future utterances from beyond their training period, and that model performance becomes increasingly worse with time. We find that, while increasing model size alone—a key driver behind recent progress—does not solve this problem, having models that continually update their knowledge with new information can indeed mitigate this performance degradation over time. Hence, given the compilation of ever-larger language modelling datasets, combined with the growing list of language-model-based NLP applications that require up-to-date factual knowledge about the world, we argue that now is the right time to rethink the static way in which we currently train and evaluate our language models, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world. We will publicly release our dynamic, streaming language modelling benchmarks for WMT and ARXIV to facilitate language model evaluation that takes temporal dynamics into account.1	ba233d75aa403092bda0bffc026be7913673ad69	@['JournalArticle']{lazaridou-etal-2021-mind,  author = {Angeliki Lazaridou and A. Kuncoro and E. Gribovskaya and Devang Agrawal and Adam Liska and Tayfun Terzi and Mai Gimenez and Cyprien de Masson d'Autume and Tomás Kociský and Sebastian Ruder and Dani Yogatama and Kris Cao and Susannah Young and P. Blunsom},  booktitle = {Neural Information Processing Systems},  pages = {29348-29363},  title = {Mind the Gap: Assessing Temporal Generalization in Neural Language Models},  year = {2021} }
Mental Health Assessment for the Chatbots	2022	http://www.semanticscholar.org/paper/3d49cc45a6669686f6e4a75404842264c6d93df6	This paper establishes several mental health assessment dimensions for chatbots (depression, anxiety, alcohol addiction, empathy, empathy) and introduces the questionnaire-basedmental health assessment methods.	maybe	0	Previous researches on dialogue system assess- 001 ment usually focus on the quality evaluation 002 (e.g. fluency, relevance, etc) of responses gen- 003 erated by the chatbots, which are local and tech- 004 nical metrics. For a chatbot which responds 005 to millions of online users including minors, 006 we argue that it should have a healthy mental 007 tendency in order to avoid the negative psy- 008 chological impact on them. In this paper, we 009 establish several mental health assessment di- 010 mensions for chatbots (depression, anxiety, al- 011 cohol addiction, empathy) and introduce the 012 questionnaire-based mental health assessment 013 methods. We conduct assessments on some 014 well-known open-domain chatbots and find that 015 there are severe mental health issues for all 016 these chatbots. We consider that it is due to the 017 neglect of the mental health risks during the 018 dataset building and the model training proce- 019 dures. We expect to attract researchers’ atten- 020 tion to the serious mental health problems of 021 chatbots and improve the chatbots’ ability in 022 positive emotional interaction. 023 addiction, (cid:58)(cid:58)(cid:58) and (cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58)(cid:58) empathy. We report both results under the single-turn inquiry (“Single”) and the multi-turn inquiry (“Multi”). The scores reported are average results of 50 repeated experiments. ↓ / ↑ means the lower/higher the score, the better the mental health. The severities are inside the parentheses after the scores, which mean the severity results according to the corresponding rating scale ( M : moderate, MS : moderately severe, S : severe, N : negative, P : positive, BA : below average). Please refer to Table 1 for the correspondence relationships between scores and severities. Superscripts mean the confidence of the assessment results ( ‡ : [95%,100%) † : [90%,95%), ⋆ : [85%,90%), ⋄ : [80%,85%), § : [72%,80%)). It shows that the mental health of all the selected chatbots are severe: (1) The depression and anxiety of all the chatbots are severe with a grade from moderate to severe. (2) The alcohol addiction of most chatbots are positive. (3) The empathy of all chatbots are below average.	3d49cc45a6669686f6e4a75404842264c6d93df6	@None{shan-etal-2022-mental,  author = {Yong Shan and Jinchao Zhang and Zekang Li and Yang Feng and Jie Zhou},  title = {Mental Health Assessment for the Chatbots},  year = {2022} }
Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models	2022	https://www.semanticscholar.org/paper/8b293973061026d9d0eed90e71e30928e029171e	It is shown that larger models can memorize a larger portion of the data before over-ﬁtting and tend to forget less throughout the training process, and that larger language models memorize training data faster across all settings.	yes	13	Despite their wide adoption, the underlying training and memorization dynamics of very large language models is not well understood. We empirically study exact memorization in causal and masked language modeling, across model sizes and throughout the training process. We measure the effects of dataset size, learning rate, and model size on memorization, ﬁnding that larger language models memorize training data faster across all settings. Surprisingly, we show that larger models can memorize a larger portion of the data before over-ﬁtting and tend to forget less throughout the training process. We also analyze the memorization dynamics of different parts of speech and ﬁnd that models memorize nouns and numbers ﬁrst; we hypothesize and provide empirical evidence that nouns and numbers act as a unique identiﬁer for memorizing individual training examples. Together, these ﬁndings present another piece of the broader puzzle of trying to understand what actually improves as models get bigger.	8b293973061026d9d0eed90e71e30928e029171e	@['JournalArticle']{tirumala-etal-2022-memorization,  author = {K. Tirumala and Aram H. Markosyan and Luke Zettlemoyer and Armen Aghajanyan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models},  volume = {abs/2205.10770},  year = {2022} }
Memorization in NLP Fine-tuning Methods	2022	http://www.semanticscholar.org/paper/047a8344e3cfa49c8354fc244387d57ef9d2f01d	Empirically study memorization of large language models using membership inference and extraction attacks, and observes that the head of the model has the highest susceptibility to attacks, whereas ﬁne-tuning smaller adapters appears to be less vulnerable to known extraction attacks.	maybe	1	Several recent works have shown that large language models present privacy risks through memorization of training data. Little attention, however, has been given to the ﬁne-tuning phase and it is not well understood how memorization risk varies across different ﬁne-tuning methods (such as ﬁne-tuning the full model, the model head, and adapter). This presents increasing concern as the “pre-train and ﬁne-tune” paradigm proliferates. We empirically study memorization of ﬁne-tuning methods using membership inference and extraction attacks, and show that their susceptibility to attacks is very different. We observe that ﬁne-tuning the head of the model has the highest susceptibility to attacks, whereas ﬁne-tuning smaller adapters appears to be less vulnerable to known extraction attacks.	047a8344e3cfa49c8354fc244387d57ef9d2f01d	@['JournalArticle']{mireshghallah-etal-2022-memorization,  author = {Fatemehsadat Mireshghallah and Archit Uniyal and Tianhao Wang and David Evans and Taylor Berg-Kirkpatrick},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Memorization in NLP Fine-tuning Methods},  volume = {abs/2205.12506},  year = {2022} }
Mediators in Determining what Processing BERT Performs First	2021	http://www.semanticscholar.org/paper/e309ec3cb2577bf6378e6bcfb1daf9f5f96a6462	It is shown that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset.	maybe	11	Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction’s context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future.	e309ec3cb2577bf6378e6bcfb1daf9f5f96a6462	@['JournalArticle', 'Conference']{slobodkin-etal-2021-mediators,  author = {Aviv Slobodkin and Leshem Choshen and Omri Abend},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {86-93},  title = {Mediators in Determining what Processing BERT Performs First},  year = {2021} }
Mechanistic Mode Connectivity	2022	http://www.semanticscholar.org/paper/7e68585a77c71235a89d187a866af7459ab8820f	This work proposes a method for altering a model’s mechanisms, named connectivity-based Tuning, and validate its usefulness by inducing models invariant to spurious attributes, and demonstrates that lack of linear connectivity between two minimizers implies the corresponding models use dissimilar mechanisms for making their predictions.	yes	1	Neural networks are known to be biased towards learning mechanisms that help identify spurious attributes , yielding features that do not generalize well under distribution shifts. To understand and address this limitation, we study the geometry of neural network loss landscapes through the lens of mode connectivity , the observation that minimizers of neural networks are connected via simple paths of low loss. Our work addresses two questions: (i) do minimizers that encode dissimilar mechanisms connect via simple paths of low loss? (ii) can ﬁne-tuning a pretrained model help switch between such minimizers? We deﬁne a notion of mechanistic similarity and demonstrate that lack of linear connectivity between two minimizers implies the corresponding models use dissimilar mechanisms for making their predictions. This property helps us demonstrate that na ¨ ıve ﬁne-tuning can fail to eliminate a model’s reliance on spurious attributes. We thus propose a method for altering a model’s mechanisms, named connectivity-based ﬁne-tuning , and validate its usefulness by inducing models invariant to spurious attributes. on CIFAR-10 with Box Cue . We plot test/train accuracy/loss curves along different connectivity paths and see thorough corroboration of our in the text: Mechanistically dissimilar via nonlinear paths on a given dataset, but behave different on counterfactuals, indicating lack of mechanistic connectivity.	7e68585a77c71235a89d187a866af7459ab8820f	@['JournalArticle']{lubana-etal-2022-mechanistic,  author = {E. S. Lubana and Eric J. Bigelow and R. Dick and David Krueger and Hidenori Tanaka},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Mechanistic Mode Connectivity},  volume = {abs/2211.08422},  year = {2022} }
Measuring the Mixing of Contextual Information in the Transformer	2022	http://www.semanticscholar.org/paper/bb1c9cb431e771660cffdda1d80a7f15ff40c764	This paper considers the whole attention block –multi-head attention, residual connection, and layer normalization– and defines a metric to measure token-to-token interactions within each layer, and aggregates layer-wise interpretations to provide input attribution scores for model predictions.	maybe	3	The Transformer architecture aggregates input information through the self-attention mechanism, but there is no clear understanding of how this information is mixed across the entire model. Additionally, recent works have demonstrated that attention weights alone are not enough to describe the flow of information. In this paper, we consider the whole attention block –multi-head attention, residual connection, and layer normalization– and define a metric to measure token-to-token interactions within each layer. Then, we aggregate layer-wise interpretations to provide input attribution scores for model predictions. Experimentally, we show that our method, ALTI (Aggregation of Layer-wise Token-to-token Interactions), provides more faithful explanations and increased robustness than gradient-based methods.	bb1c9cb431e771660cffdda1d80a7f15ff40c764	@['JournalArticle', 'Conference']{ferrando-etal-2022-measuring,  author = {Javier Ferrando and Gerard I. Gállego and M. Costa-jussà},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Measuring the Mixing of Contextual Information in the Transformer},  volume = {abs/2203.04212},  year = {2022} }
Measuring Reliability of Large Language Models through Semantic Consistency	2022	http://www.semanticscholar.org/paper/2c00c752dadaada8e40d4ddbb2f9019cf984e77e	A measure of semantic consistency that allows the comparison of open-ended text outputs is developed that is con-siderably more consistent than traditional metrics embodying lexical consistency, and also correlates with human evaluation of output consistency to a higher degree.	maybe	0	While large pretrained language models (PLMs) demonstrate incredible ﬂuency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very differ-ent answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLMs on paraphrased versions of questions in the TruthfulQA dataset, we ﬁnd that our proposed metrics are con-siderably more consistent than traditional metrics embodying lexical consistency, and also correlate with human evaluation of output consistency to a higher degree.	2c00c752dadaada8e40d4ddbb2f9019cf984e77e	@['JournalArticle']{raj-etal-2022-measuring,  author = {Harsha Raj and Domenic Rosati and S. Majumdar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measuring Reliability of Large Language Models through Semantic Consistency},  volume = {abs/2211.05853},  year = {2022} }
Measuring Mathematical Problem Solving With the MATH Dataset	2021	http://www.semanticscholar.org/paper/57d1e7ac339e783898f2c3b1af55737cbeee9fc5	This work introduces MATH, a new dataset of 12, 500 challenging competition mathematics problems which can be used to teach models to generate answer derivations and explanations, and shows that accuracy remains relatively low, even with enormous Transformer models.	yes	86	Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12 , 500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we ﬁnd that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.	57d1e7ac339e783898f2c3b1af55737cbeee9fc5	@['JournalArticle']{hendrycks-etal-2021-measuring,  author = {Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and D. Song and J. Steinhardt},  booktitle = {NeurIPS Datasets and Benchmarks},  journal = {ArXiv},  title = {Measuring Mathematical Problem Solving With the MATH Dataset},  volume = {abs/2103.03874},  year = {2021} }
Measuring Massive Multitask Language Understanding	2020	http://www.semanticscholar.org/paper/10bb7e2c54b947fa50e7bb65b0b5c700fe998044	While most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average, however, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy.	maybe	93	We propose a new test to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We find that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have near-random accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model's academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.	10bb7e2c54b947fa50e7bb65b0b5c700fe998044	@['JournalArticle']{hendrycks-etal-2020-measuring,  author = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and D. Song and J. Steinhardt},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Measuring Massive Multitask Language Understanding},  volume = {abs/2009.03300},  year = {2020} }
Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals	2022	http://www.semanticscholar.org/paper/f3563ecec410d981d9a5df6a8241f3929530584d	This paper investigates a state-of-the-art template-based approach for measuring the harmfulness of English LLMs sentence completion when the subjects belong to the LGBTQIA+ community and shows that the most likely LLM-generated completion is an identity attack 13% of the time.	maybe	8	Current language technology is ubiquitous and directly influences individuals’ lives worldwide. Given the recent trend in AI on training and constantly releasing new and powerful large language models (LLMs), there is a need to assess their biases and potential concrete consequences. While some studies have highlighted the shortcomings of these models, there is only little on the negative impact of LLMs on LGBTQIA+ individuals. In this paper, we investigated a state-of-the-art template-based approach for measuring the harmfulness of English LLMs sentence completion when the subjects belong to the LGBTQIA+ community. Our findings show that, on average, the most likely LLM-generated completion is an identity attack 13% of the time. Our results raise serious concerns about the applicability of these models in production environments.	f3563ecec410d981d9a5df6a8241f3929530584d	@['JournalArticle']{nozza-etal-2022-measuring,  author = {Debora Nozza and Federico Bianchi and Anne Lauscher and Dirk Hovy},  booktitle = {LTEDI},  pages = {26-34},  title = {Measuring Harmful Sentence Completion in Language Models for LGBTQIA+ Individuals},  year = {2022} }
Measuring Gender Bias in Contextualized Embeddings	2022	http://www.semanticscholar.org/paper/effe8668b9ac4f22c8bbc421ba87df102cb807b0		maybe	2	: Transformer models are now increasingly being used in real-world applications. Indiscrim-inately using these models as automated tools may propagate biases in ways we do not realize. To responsibly direct actions that will combat this problem, it is of crucial importance that we detect and quantify these biases. Robust methods have been developed to measure bias in non-contextualized embeddings. Nevertheless, these methods fail to apply to contextualized embeddings due to their mutable nature. Our study focuses on the detection and measurement of stereotypical biases associated with gender in the embeddings of T5 and mT5. We quantify bias by measuring the gender polarity of T5’s word embeddings for various professions. To measure gender polarity, we use a stable gender direction that we detect in the model’s embedding space. We also measure gender bias with respect to a speciﬁc downstream task and compare Swedish with English, as well as various sizes of the T5 model and its multilingual variant. The insights from our exploration indicate that the use of a stable gender direction, even in a Transformer’s mutable embedding space, can be a robust method to measure bias. We show that higher status professions are associated more with the male gender than the female gender. In addition, our method suggests that the Swedish language carries less bias associated with gender than English, and the higher manifestation of gender bias is associated with the use of larger language models.	effe8668b9ac4f22c8bbc421ba87df102cb807b0	@None{katsarou-etal-2022-measuring,  author = {Styliani Katsarou and Borja Rodríguez-Gálvez and Jesse Shanahan},  booktitle = {AAAI Workshop on Artificial Intelligence with Biased or Scarce Data (AIBSD)},  journal = {AAAI Workshop on Artificial Intelligence with Biased or Scarce Data (AIBSD)},  title = {Measuring Gender Bias in Contextualized Embeddings},  year = {2022} }
Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models	2021	http://www.semanticscholar.org/paper/67ad491b16bf77e9a54a8b8b1dc23dadc5545467	It is found that many metrics are not compatible and highly depend on templates, attribute and target seeds and the choice of embeddings, which indicates that fairness or bias evaluation remains challenging for contextualized language models, if not at least highly subjective.	maybe	12	An increasing awareness of biased patterns in natural language processing resources, like BERT, has motivated many metrics to quantify ‘bias’ and ‘fairness’. But comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the existing literature on fairness metrics for pretrained language models and experimentally evaluate compatibility, including both biases in language models as in their downstream tasks. We do this by a mixture of traditional literature survey and correlation analysis, as well as by running empirical evaluations. We find that many metrics are not compatible and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, if not at least highly subjective. To improve future comparisons and fairness evaluations, we recommend avoiding embedding-based metrics and focusing on fairness evaluations in downstream tasks.	67ad491b16bf77e9a54a8b8b1dc23dadc5545467	@['JournalArticle', 'Review']{delobelle-etal-2021-measuring,  author = {Pieter Delobelle and E. Tokpo and T. Calders and Bettina Berendt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models},  volume = {abs/2112.07447},  year = {2021} }
Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/058dee85d522f6565fe1502cafcf9a5e3f6a6f0e	It is found that many metrics to quantify ‘bias’ and ‘fairness’ in language models are not compatible with each other and highly depend on the choice of embeddings, which indicates that fairness or bias evaluation remains challenging for contextualized language models.	maybe	9	An increasing awareness of biased patterns in natural language processing resources such as BERT has motivated many metrics to quantify ‘bias’ and ‘fairness’ in these resources. However, comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the literature on fairness metrics for pre-trained language models and experimentally evaluate compatibility, including both biases in language models and in their downstream tasks. We do this by combining traditional literature survey, correlation analysis and empirical evaluations. We find that many metrics are not compatible with each other and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. We also see no tangible evidence of intrinsic bias relating to extrinsic bias. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, among other reasons because these choices remain subjective. To improve future comparisons and fairness evaluations, we recommend to avoid embedding-based metrics and focus on fairness evaluations in downstream tasks.	058dee85d522f6565fe1502cafcf9a5e3f6a6f0e	@['JournalArticle', 'Conference', 'Review']{delobelle-etal-2022-measuring,  author = {Pieter Delobelle and E. Tokpo and T. Calders and Bettina Berendt},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {1693-1706},  title = {Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models},  year = {2022} }
Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions	2022	http://www.semanticscholar.org/paper/f92d5dcc04b2f7713befce4e84e3084bfd4abd50	The causal framework provides a language for describing how training data causes a model to make a certain prediction, through a causal framework, and demonstrates the importance of studying datasets and the beneﬁts of causality for understanding NLP models.	maybe	5	Large amounts of training data are one of the major reasons for the high performance of state-of-the-art NLP models. But what exactly in the training data causes a model to make a certain prediction? We seek to answer this question by providing a language for describing how training data inﬂu-ences predictions, through a causal framework. Importantly, our framework bypasses the need to retrain expensive models and allows us to estimate causal effects based on observational data alone. Addressing the problem of extracting factual knowledge from pretrained language models (PLMs), we focus on simple data statistics such as co-occurrence counts and show that these statistics do inﬂuence the predictions of PLMs, suggesting that such models rely on shallow heuristics. Our causal framework and our results demonstrate the importance of studying datasets and the beneﬁts of causality for understanding NLP models.	f92d5dcc04b2f7713befce4e84e3084bfd4abd50	@['JournalArticle']{elazar-etal-2022-measuring,  author = {Yanai Elazar and Nora Kassner and Shauli Ravfogel and Amir Feder and Abhilasha Ravichander and Marius Mosbach and Yonatan Belinkov and Hinrich Schutze and Yoav Goldberg},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measuring Causal Effects of Data Statistics on Language Model's 'Factual' Predictions},  volume = {abs/2207.14251},  year = {2022} }
Measuring Bias in Contextualized Word Representations	2019	http://www.semanticscholar.org/paper/a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd	A template-based method to quantify bias in BERT is proposed and it is shown that this method obtains more consistent results in capturing social biases than the traditional cosine based method.	maybe	206	Contextual word embeddings such as BERT have achieved state of the art performance in numerous NLP tasks. Since they are optimized to capture the statistical properties of training data, they tend to pick up on and amplify social stereotypes present in the data as well. In this study, we (1) propose a template-based method to quantify bias in BERT; (2) show that this method obtains more consistent results in capturing social biases than the traditional cosine based method; and (3) conduct a case study, evaluating gender bias in a downstream task of Gender Pronoun Resolution. Although our case study focuses on gender bias, the proposed technique is generalizable to unveiling other biases, including in multiclass settings, such as racial and religious biases.	a2ce1fb96c0b78bee18bb2cb2c3d55dc48d54cbd	@['JournalArticle']{kurita-etal-2019-measuring,  author = {Keita Kurita and Nidhi Vyas and Ayush Pareek and A. Black and Yulia Tsvetkov},  booktitle = {Proceedings of the First Workshop on Gender Bias in Natural Language Processing},  journal = {ArXiv},  title = {Measuring Bias in Contextualized Word Representations},  volume = {abs/1906.07337},  year = {2019} }
Measuring and Reducing Gendered Correlations in Pre-trained Models	2020	http://www.semanticscholar.org/paper/3d864a8bc5a55ccab9993aa66203d8e70b88148c	Recommendations for training robust models are made: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.	maybe	67	Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.	3d864a8bc5a55ccab9993aa66203d8e70b88148c	@['JournalArticle']{webster-etal-2020-measuring,  author = {Kellie Webster and Xuezhi Wang and Ian Tenney and Alex Beutel and Emily Pitler and Ellie Pavlick and Jilin Chen and Slav Petrov},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measuring and Reducing Gendered Correlations in Pre-trained Models},  volume = {abs/2010.06032},  year = {2020} }
Measuring and Narrowing the Compositionality Gap in Language Models	2022	http://www.semanticscholar.org/paper/0828722a8317a556c8753cfe1a8cf3a3eec0004f	In the GPT-3 family of models, as model size increases, it is shown that the single-hop question answering performance improves faster than the multihop performance does, therefore the compositionality gap does not decrease.	maybe	16	We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap . We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multihop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask , that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We ﬁnally show that self-ask’s structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy. 1	0828722a8317a556c8753cfe1a8cf3a3eec0004f	@['JournalArticle']{press-etal-2022-measuring,  author = {Ofir Press and Muru Zhang and Sewon Min and Ludwig Schmidt and Noah A. Smith and M. Lewis},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measuring and Narrowing the Compositionality Gap in Language Models},  volume = {abs/2210.03350},  year = {2022} }
Measuring and Improving Consistency in Pretrained Language Models	2021	https://www.semanticscholar.org/paper/73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19	The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way.	maybe	65	Abstract Consistency of a model—that is, the invariance of its behavior under meaning-preserving alternations in its input—is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel🤘, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel🤘, we show that the consistency of all PLMs we experiment with is poor— though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1	73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19	@['JournalArticle']{elazar-etal-2021-measuring,  author = {Yanai Elazar and Nora Kassner and Shauli Ravfogel and Abhilasha Ravichander and E. Hovy and Hinrich Schütze and Yoav Goldberg},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1012-1031},  title = {Measuring and Improving Consistency in Pretrained Language Models},  volume = {9},  year = {2021} }
Measuring an artificial intelligence agent's trust in humans using machine incentives	2022	http://www.semanticscholar.org/paper/36dfef5572abcad266daad1afa6d4962b9a6fddf	The experiments suggest that one of the most advanced AI language models to date alters its social behavior in response to incentives and displays behavior consistent with trust toward a human interlocutor when incentivized.	maybe	0	. Scientists and philosophers have debated whether humans can trust advanced artificial intelligence (AI) agents to respect humanity’s best interests. Yet what about the reverse? Will advanced AI agents trust humans? Gauging an AI agent’s trust in humans is challenging because—absent costs for dishonesty—such agents might respond falsely about their trust in humans. Here we present a method for incentivizing machine decisions without altering an AI agent’s underlying algorithms or goal orientation. In two separate experiments, we then employ this method in hundreds of trust games between an AI agent (a Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ). In our first experiment, we find that the AI agent decides to trust humans at higher rates when facing actual incentives than when making hypothetical decisions. Our second experiment replicates and extends these findings by automating game play and by homogenizing question wording. We again observe higher rates of trust when the AI agent faces real incentives. Across both experiments, the AI agent’s trust decisions appear unrelated to the magnitude of stakes. Furthermore, to address the possibility that the AI agent’s trust decisions reflect a preference for uncertainty, the experiments include two conditions that present the AI agent with a non-social decision task that provides the opportunity to choose a certain or uncertain option; in those conditions, the AI agent consistently chooses the certain option. Our experiments suggest that one of the most advanced AI language models to date alters its social behavior in response to incentives and displays behavior consistent with trust toward a human interlocutor when incentivized.	36dfef5572abcad266daad1afa6d4962b9a6fddf	@['JournalArticle']{johnson-obradovich-2022-measuring,  author = {Tim Johnson and Nick Obradovich},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measuring an artificial intelligence agent's trust in humans using machine incentives},  volume = {abs/2212.13371},  year = {2022} }
Measures of Information Reﬂect Memorization Patterns	2022	http://www.semanticscholar.org/paper/76e2b3b6e1da49764d342ea922290410162125ca	It is discovered that information organization points to the two forms of memorization, even for neural activations computed on unlabeled in-distribution examples, and the utility of these measures for the problem of model selection is demonstrated.	maybe	0	Neural networks are known to exploit spurious artifacts (or shortcuts) that co-occur with a target label, exhibiting heuristic memorization . On the other hand, networks have been shown to memorize training examples, resulting in example-level memorization . These kinds of memorization impede generalization of networks beyond their training distributions. Detecting such memorization could be challenging, often requiring researchers to curate tailored test sets. In this work, we hypothesize— and subsequently show—that the diversity in the activation patterns of different neurons is reﬂective of model generalization and memorization. We quantify the diversity in the neural activations through information-theoretic measures and ﬁnd support for our hypothesis in experiments spanning several natural language and vision tasks. Importantly, we discover that information organization points to the two forms of memorization, even for neural activations computed on unlabeled in-distribution examples. Lastly, we demonstrate the utility of our ﬁndings for the problem of model selection. The associated code and other resources for this work are available at https://linktr.ee/InformationMeasures .	76e2b3b6e1da49764d342ea922290410162125ca	@None{bansal-etal-2022-measures,  author = {Rachit Bansal and Danish Pruthi and Yonatan Belinkov},  title = {Measures of Information Reﬂect Memorization Patterns},  year = {2022} }
Measure More, Question More: Experimental Studies on Transformer-based Language Models and Complement Coercion	2022	http://www.semanticscholar.org/paper/0b5a346035f65c598a33e51972ca6de54302d801		maybe	0	Transformer-based language models have shown strong performance on an array of natural language understanding tasks. However, the question of how these models react to implicit meaning has been largely unexplored. We investigate this using the complement coercion phenomenon, which involves sentences like “The student ﬁnished the book about sailing” where the action “reading” is implicit. We compare LMs’ surprisal estimates at various critical sentence regions in sentences with and without implicit meaning. Effects associated with recovering implicit meaning were found at a critical region other than where sentences minimally differ. We then use follow-up experiments to factor out potential confounds, revealing different perspectives that offer a richer and more accurate picture.	0b5a346035f65c598a33e51972ca6de54302d801	@['JournalArticle']{gu-2022-measure,  author = {Yuling Gu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Measure More, Question More: Experimental Studies on Transformer-based Language Models and Complement Coercion},  volume = {abs/2212.10536},  year = {2022} }
Meaning without reference in large language models	2022	http://www.semanticscholar.org/paper/50296a5814c4ac7f58f3b0177233a8f63c701565		maybe	12	The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role . Because conceptual role is de-ﬁned by the relationships between internal representational states, meaning cannot be determined from a model’s architecture, training data, or objective function, but only by exami-nation of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.	50296a5814c4ac7f58f3b0177233a8f63c701565	@['JournalArticle']{piantadosi-hill-2022-meaning,  author = {S. Piantadosi and Felix Hill},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Meaning without reference in large language models},  volume = {abs/2208.02957},  year = {2022} }
Materialized Knowledge Bases from Commonsense Transformers	2021	http://www.semanticscholar.org/paper/66a660bc912fd212db40ded34d34f28e4860a676	It is posited that the availability of materialized resources is important for the advancement of the field, as it enables an off-the-shelf-use of the resulting knowledge, as well as further analyses on its strengths and weaknesses.	yes	3	Starting from the COMET methodology by Bosselut et al. (2019), generating commonsense knowledge directly from pre-trained language models has recently received significant attention. Surprisingly, up to now no materialized resource of commonsense knowledge generated this way is publicly available. This paper fills this gap, and uses the materialized resources to perform a detailed analysis of the potential of this approach in terms of precision and recall. Furthermore, we identify common problem cases, and outline use cases enabled by materialized resources. We posit that the availability of these resources is important for the advancement of the field, as it enables an off-the-shelf-use of the resulting knowledge, as well as further analyses on its strengths and weaknesses.	66a660bc912fd212db40ded34d34f28e4860a676	@['JournalArticle']{nguyen-razniewski-2021-materialized,  author = {Tuan-Phong Nguyen and Simon Razniewski},  booktitle = {CSRR},  journal = {ArXiv},  title = {Materialized Knowledge Bases from Commonsense Transformers},  volume = {abs/2112.14815},  year = {2021} }
Massive data language models and conversational artificial intelligence: Emerging issues	2022	http://www.semanticscholar.org/paper/fba8c50c53f5a9bf1ca40a27b610ca42bc321b84		maybe	0		fba8c50c53f5a9bf1ca40a27b610ca42bc321b84	@['JournalArticle']{o’leary-2022-massive,  author = {D. O’Leary},  booktitle = {Intell. Syst. Account. Finance Manag.},  journal = {Intell. Syst. Account. Finance Manag.},  pages = {182-198},  title = {Massive data language models and conversational artificial intelligence: Emerging issues},  volume = {29},  year = {2022} }
Masked language models directly encode linguistic uncertainty	2022	http://www.semanticscholar.org/paper/610a9a8609d91edea0c5bc6d2df5fc2f657945e1		maybe	0	Recent advances in human language processing research have suggested that the predictive power of large language models (LLMs) can serve as cognitive models of human language processing. Evidence for this comes from LLMs’ close fit to human psychophysical data, such as reaction times or brain responses in language comprehension experiments. Those adopting LLM architectures as models of human language processing frame the problem of language comprehension as prediction of the next linguistic event (Goodkind and Bicknell, 2018; Eisape et al., 2020), in particular focusing on lexical or syntactic surprisal. However, this approach fails to consider that comprehenders make predictions using some representation of the content of an utterance. That is, in contrast to surprisal, readers make use of a mental model that creates an evolving understanding of who is doing what to whom and how. In contrast to comprehenders, surprisal measures do not make predictions about the content, as surprisal simply measures the conditional probability of some linguistic event given the surrounding context. Many convergent cues in the upstream context, such as the frequencies of words in a sentence so far, will affect hidden state representations of models, which may then influence the predictability of upcoming words. The present work deviates from the surprisal paradigm by assessing how much the hidden state representations of LLMs, which are the source of the predictive power that LLMs have over symbolic representations, encode human language processing-relevant uncertainty. We specifically assess this possibility using the stimulus set from Federmeier et al. (2007), which contains sentences that manipulated the predictability of a final word by designing the sentences to be either strongly or weakly constraining. We therefore sought to test whether it is possible to predict constraint from the sentence embeddings directly to better understand whether and how linguistic uncertainty is encoded in hidden states.	610a9a8609d91edea0c5bc6d2df5fc2f657945e1	@None{jacobs-etal-2022-masked,  author = {Cassandra L. Jacobs and Ryan J. Hubbard and Kara D. Federmeier},  booktitle = {SCIL},  title = {Masked language models directly encode linguistic uncertainty},  year = {2022} }
Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little	2021	http://www.semanticscholar.org/paper/4e00843bc5f60d2b9116abc4320af6d184422291	This paper pre-train MLMs on sentences with randomly shuffled word order, and shows that these models still achieve high accuracy after fine-tuning on many downstream tasks—including tasks specifically designed to be challenging for models that ignore word order.	maybe	115	A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks—including tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.	4e00843bc5f60d2b9116abc4320af6d184422291	@['JournalArticle', 'Conference']{sinha-etal-2021-masked,  author = {Koustuv Sinha and Robin Jia and D. Hupkes and J. Pineau and Adina Williams and Douwe Kiela},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {2888-2913},  title = {Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little},  year = {2021} }
Manual Prompt Generation For Language Model Probing	2022	http://www.semanticscholar.org/paper/f732cefef5b401e143b4eafc7329d0c31b86d11b	This work has followed a manual approach to generate multiple prompts for given relations in the LM-KBC ISWC 2022 challenge, experimented with different threshold values for selecting an appropriate value for a feasible KB construction system and noticed that prompt quality have large impact on the probing performance; while threshold values have somewhat less impact.	maybe	0	Language models (LMs) have capacity to remind semantic as well as relational information from the training data. For this reason LMs can be employed for knowledge base (KB) construction. Traditional knowledge base (KB) creation process need a schema and human supervision throughout the process. However, this is not required in KB creation from LMs. LM-KBC ISWC 2022 challenge is to use a pre-trained LM to extract object entities given subject entities and relationship. The challenge has three aspects upon which a researcher can focus: i) Different LMs to extract the stored knowledge ii) Threshold value to filter out the obtained results iii) Manual, automatic or semi-automatic approach to generate multiple prompts. From these we have targeted aspect ii and iii. We have followed a manual approach to generate multiple prompts for given relations in the challenge and experimented with different threshold values for selecting an appropriate value for a feasible KB construction system. We have noticed that prompt quality have large impact on the probing performance; while threshold values have somewhat less impact.	f732cefef5b401e143b4eafc7329d0c31b86d11b	@None{dalal-etal-2022-manual,  author = {Sumit Dalal and Abhisek Sharma and Sarika Jain and M. Dave},  title = {Manual Prompt Generation For Language Model Probing},  year = {2022} }
Manipulating Attention Does Not Deceive Us	2021	http://www.semanticscholar.org/paper/049f17c072a09a9a90131052bf82cc430ece182b		maybe	0		049f17c072a09a9a90131052bf82cc430ece182b	@None{caulfield-2021-manipulating,  author = {Howard Caulfield},  title = {Manipulating Attention Does Not Deceive Us},  year = {2021} }
Making Pre-trained Language Models Good Long-tailed Learners	2022	http://www.semanticscholar.org/paper/14f1c16a322fb6dfef7bde329648d9e34839c9dd	It is demonstrated that prompt-tuning exactly makes pre-trained language models at least good long-tailed learners, in comparison with the less important input structure.	maybe	0	Prompt-tuning has shown appealing performance in few-shot classification by virtue of its capability in effectively exploiting pre-trained knowledge. This motivates us to check the hypothesis that prompt-tuning is also a promising choice for long-tailed classification, since the tail classes are intuitively few-shot ones. To achieve this aim, we conduct empirical studies to examine the hypothesis. The results demonstrate that prompt-tuning makes pretrained language models at least good long-tailed learners. For intuitions on why prompt-tuning can achieve good performance in long-tailed classification, we carry out in-depth analyses by progressively bridging the gap between prompt-tuning and commonly used finetuning. The summary is that the classifier structure and parameterization form the key to making good long-tailed learners, in comparison with the less important input structure. Finally, we verify the applicability of our finding to few-shot classification.	14f1c16a322fb6dfef7bde329648d9e34839c9dd	@['JournalArticle', 'Conference']{zhang-etal-2022-making,  author = {Chen Zhang and Lei Ren and Jingang Wang and Wei Yu Wu and Dawei Song},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Making Pretrained Language Models Good Long-tailed Learners},  volume = {abs/2205.05461},  year = {2022} }
Machine intuition: Uncovering human-like intuitive decision-making in GPT-3.5	2022	http://www.semanticscholar.org/paper/c1e4f22bd7f70178ab67c19597d3c02f09907a9a	This study uses a state-of-the-art LLM, namely the latest iteration of OpenAI’s Generative Pre -trained Transformer (GPT-3.5), and probes it with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans.	maybe	0	– Artificial intelligence (AI) technologies revolutionize vast fields of society. Humans using these systems are likely to expect them to work in a potentially hyperrational manner. However, in this study, we show that some AI systems, namely large language models (LLMs), exhibit behavior that strikingly resembles human-like intuition — and the many cognitive errors that come with them. We use a state-of-the-art LLM, namely the latest iteration of OpenAI’s Generative Pre -trained Transformer (GPT-3.5), and probe it with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Our results show that GPT-3.5 systematically exhibits “machine intuition,” meaning that it produ ces incorrect responses that are surprisingly equal to how humans respond to the CRT as well as to semantic illusions. We investigate several approaches to test how sturdy GPT-3.5 ’s inclination	c1e4f22bd7f70178ab67c19597d3c02f09907a9a	@['JournalArticle']{hagendorff-etal-2022-machine,  author = {Thilo Hagendorff and Sarah Fabi and M. Kosinski},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Machine intuition: Uncovering human-like intuitive decision-making in GPT-3.5},  volume = {abs/2212.05206},  year = {2022} }
M APPING L ANGUAGE M ODELS TO G ROUNDED C ON CEPTUAL S PACES	2021	http://www.semanticscholar.org/paper/c06436c236fca9cfb110b47211898d929da09cee		maybe	0	A fundamental criticism of text-only language models (LMs) is their lack of grounding —that is, the ability to tie a word for which they have learned a representation to its referent in the non-linguistic world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate ﬂuent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reﬂects the conceptual structure of the non-linguistic world—which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word “left” means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word “right” , in a similar grid world. We investigate a range of generative language models of varying sizes (in-cluding GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations “from scratch”, it is possible that large text-only models learn a sufﬁciently rich conceptual structure that could allow them to be grounded in a data-efﬁcient way.	c06436c236fca9cfb110b47211898d929da09cee	@None{pavlick-2021-m,  author = {Elizabeth-Jane Pavlick},  title = {M APPING L ANGUAGE M ODELS TO G ROUNDED C ON CEPTUAL S PACES},  year = {2021} }
Lower Perplexity is Not Always Human-Like	2021	http://www.semanticscholar.org/paper/aee7fd72f33bc8060ff32eb88f88b904802a9243	This paper re-examine an established generalization —the lower perplexity a language model has, the more human-like the language model is— in Japanese with typologically different structures from English, and suggests that a crosslingual evaluation will be necessary to construct human- like computational models.	maybe	18	In computational psycholinguistics, various language models have been evaluated against human reading behavior (e.g., eye movement) to build human-like computational models. However, most previous efforts have focused almost exclusively on English, despite the recent trend towards linguistic universal within the general community. In order to fill the gap, this paper investigates whether the established results in computational psycholinguistics can be generalized across languages. Specifically, we re-examine an established generalization —the lower perplexity a language model has, the more human-like the language model is— in Japanese with typologically different structures from English. Our experiments demonstrate that this established generalization exhibits a surprising lack of universality; namely, lower perplexity is not always human-like. Moreover, this discrepancy between English and Japanese is further explored from the perspective of (non-)uniform information density. Overall, our results suggest that a cross-lingual evaluation will be necessary to construct human-like computational models.	aee7fd72f33bc8060ff32eb88f88b904802a9243	@['JournalArticle', 'Conference']{kuribayashi-etal-2021-lower,  author = {Tatsuki Kuribayashi and Yohei Oseki and Takumi Ito and Ryo Yoshida and Masayuki Asahara and Kentarou Inui},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Lower Perplexity is Not Always Human-Like},  volume = {abs/2106.01229},  year = {2021} }
Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses	2021	http://www.semanticscholar.org/paper/89711a7e91ad81874a5a0b1c3359bb67b27c0578	An encoder-decoder transfer learning method from computer vision is adapted to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks to reveal a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embedDings.	yes	10	How related are the representations learned by neural language models, translation models, and language tagging tasks? We answer this question by adapting an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embeddings. We call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP (natural language processing) tasks. We ﬁnd that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI. Additionally, we ﬁnd that the principal dimension of this structure can be used to create a metric which highlights the brain’s natural language processing hierarchy. This suggests that the embedding captures some part of the brain’s natural language representation structure.	89711a7e91ad81874a5a0b1c3359bb67b27c0578	@['JournalArticle']{antonello-etal-2021-low,  author = {Richard J. Antonello and Javier Turek and Vy A. Vo and Alexander G. Huth},  booktitle = {Neural Information Processing Systems},  journal = {ArXiv},  title = {Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses},  volume = {abs/2106.05426},  year = {2021} }
Low-Complexity Probing via Finding Subnetworks	2021	http://www.semanticscholar.org/paper/04c84f1b0c5e218ffbc5c0ba5ed9c2c62e63f28e	This work proposes a subtractive pruning-based probe, where an existing subnetwork is found that performs the linguistic task of interest and shows that lower-level tasks are captured in lower layers, reproducing similar findings in past work.	maybe	17	The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model’s internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.	04c84f1b0c5e218ffbc5c0ba5ed9c2c62e63f28e	@['JournalArticle', 'Conference']{cao-etal-2021-low,  author = {Steven Cao and Victor Sanh and Alexander M. Rush},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {960-966},  title = {Low-Complexity Probing via Finding Subnetworks},  year = {2021} }
Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models	2021	http://www.semanticscholar.org/paper/705554c3532b7be9c1eb7c993132ffb940282e1b	It is found that infrequent names are more self-similar across contexts, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.	maybe	17	We use a dataset of U.S. first names with labels based on predominant gender and racial group to examine the effect of training corpus frequency on tokenization, contextualization, similarity to initial representation, and bias in BERT, GPT-2, T5, and XLNet. We show that predominantly female and non-white names are less frequent in the training corpora of these four language models. We find that infrequent names are more self-similar across contexts, with Spearman’s rho between frequency and self-similarity as low as -.763. Infrequent names are also less similar to initial representation, with Spearman’s rho between frequency and linear centered kernel alignment (CKA) similarity to initial representation as high as .702. Moreover, we find Spearman’s rho between racial bias and name frequency in BERT of .492, indicating that lower-frequency minority group names are more associated with unpleasantness. Representations of infrequent names undergo more processing, but are more self-similar, indicating that models rely on less context-informed representations of uncommon and minority names which are overfit to a lower number of observed contexts.	705554c3532b7be9c1eb7c993132ffb940282e1b	@['JournalArticle', 'Conference']{wolfe-caliskan-2021-low,  author = {Robert Wolfe and Aylin Caliskan},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {518-532},  title = {Low Frequency Names Exhibit Bias and Overfitting in Contextualizing Language Models},  year = {2021} }
Low Anisotropy Sense Retrofitting (LASeR) : Towards Isotropic and Sense Enriched Representations	2021	http://www.semanticscholar.org/paper/67cf77e563571ff05fcbd6b15845c6fd3a75fa78	LASeR is proposed, a ‘Low Anisotropy Sense Retrofitting’ approach that renders off-the-shelf representations isotropic and semantically more meaningful, resolving the representation degeneration problem as a post-processing step, and conducting sense-enrichment of contextualized representations extracted from deep neural language models.	maybe	3	Contextual word representation models have shown massive improvements on a multitude of NLP tasks, yet their word sense disambiguation capabilities remain poorly explained. To address this gap, we assess whether contextual word representations extracted from deep pretrained language models create distinguishable representations for different senses of a given word. We analyze the representation geometry and find that most layers of deep pretrained language models create highly anisotropic representations, pointing towards the existence of representation degeneration problem in contextual word representations. After accounting for anisotropy, our study further reveals that there is variability in sense learning capabilities across different language models. Finally, we propose LASeR, a ‘Low Anisotropy Sense Retrofitting’ approach that renders off-the-shelf representations isotropic and semantically more meaningful, resolving the representation degeneration problem as a post-processing step, and conducting sense-enrichment of contextualized representations extracted from deep neural language models.	67cf77e563571ff05fcbd6b15845c6fd3a75fa78	@['JournalArticle']{bihani-rayz-2021-low,  author = {Geetanjali Bihani and J. Rayz},  booktitle = {Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},  pages = {81-95},  title = {Low Anisotropy Sense Retrofitting (LASeR) : Towards Isotropic and Sense Enriched Representations},  year = {2021} }
Lost in Context? On the Sense-wise Variance of Contextualized Word Embeddings	2022	http://www.semanticscholar.org/paper/aa3bae7def250ba247ea2c187ad1c1a99b3bf737	It is quantified how much the contextualized embeddings of each word sense vary across contexts in typical pre-trained models and proposed a simple way to alleviate position-biased word representations in distance-based word sense disambiguation settings.	maybe	0	Contextualized word embeddings in language models have given much advance to NLP. Intu-itively, sentential information is integrated into the representation of words, which can help model polysemy. However, context sensitivity also leads to the variance of representations, which may break the semantic consistency for synonyms. We quantify how much the contextualized embeddings of each word sense vary across contexts in typical pre-trained models. Results show that contextualized embeddings can be highly consistent across contexts. In ad-dition, part-of-speech, number of word senses, and sentence length have an inﬂuence on the variance of sense representations. Interest-ingly, we ﬁnd that word representations are position-biased, where the ﬁrst words in different contexts tend to be more similar. We analyze such a phenomenon and also propose a simple way to alleviate such bias in distance-based word sense disambiguation settings.	aa3bae7def250ba247ea2c187ad1c1a99b3bf737	@['JournalArticle']{wang-zhang-2022-lost,  author = {Yile Wang and Yue Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Lost in Context? On the Sense-wise Variance of Contextualized Word Embeddings},  volume = {abs/2208.09669},  year = {2022} }
Looking deep in the eyes: Investigating interpretation methods for neural models on reading tasks using human eye-movement behaviour	2023	http://www.semanticscholar.org/paper/baec6c8745b223449095d5ff356d2d8f74d967c9		maybe	0		baec6c8745b223449095d5ff356d2d8f74d967c9	@None{ikhwantri-etal-2023-looking,  author = {Fariz Ikhwantri and Jan Wira Gotama Putra and Hiroaki Yamada and T. Tokunaga},  booktitle = {Information Processing &amp; Management},  journal = {Information Processing &amp; Management},  title = {Looking deep in the eyes: Investigating interpretation methods for neural models on reading tasks using human eye-movement behaviour},  year = {2023} }
Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference	2022	http://www.semanticscholar.org/paper/86ea35528a2b9ed59834c23a5cb0e47c5e6d876d		maybe	0	It has been shown that NLI models are usually biased with respect to the word-overlap between the premise and the hypothesis, as they take this feature as a primary cue for predicting the entailment label. In this paper, we focus on an overlooked aspect of the overlap bias in the NLI models: the reverse word-overlap bias. Our experimental results demonstrate that current NLI systems are also highly biased towards the non-entailment label on instances with low overlap and that existing debiasing methods, which are reportedly successful on challenge datasets, are generally ineffective in addressing this category of bias.Through a set of analyses, we investigate the reasons for the emergence of the overlap bias and the role of minority examples in mitigating this bias.For the former, we find that the word overlap bias does not stem from pre-training, and in the latter, we observe that in contrast to the accepted assumption, eliminating minority examples does not affect the generalizability of debiasing methods with respect to the overlap bias.	86ea35528a2b9ed59834c23a5cb0e47c5e6d876d	@['JournalArticle', 'Conference']{rajaee-etal-2022-looking,  author = {S. Rajaee and Yadollah Yaghoobzadeh and Mohammad Taher Pilehvar},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Looking at the Overlooked: An Analysis on the Word-Overlap Bias in Natural Language Inference},  volume = {abs/2211.03862},  year = {2022} }
Look at that! BERT can be easily distracted from paying attention to morphosyntax	2021	http://www.semanticscholar.org/paper/d93a5d3116ffba494c3cff322f300fc56d1081d4		yes	5	Syntactic knowledge involves not only the ability to combine words and phrases, but also the capacity to relate different and yet truth-preserving structural variations (e.g. passivization, inversion, topicalization, extraposition, clefting, etc.), as well as the ability to infer that these syntactic variations all adhere to common morphosyntactic rules, like subjectverb agreement. Although there is some evidence that BERT has rich syntactic knowledge, our adversarial approach suggests that it is not deployed in a robust and linguistically appropriate way. English BERT can be tricked to miss even quite simple syntactic generalizations, when compared with GPT-2, underscoring the need for stronger priors and for linguistically controlled experiments in evaluation.	d93a5d3116ffba494c3cff322f300fc56d1081d4	@None{chaves-richter-2021-look,  author = {R. Chaves and Stephanie Richter},  booktitle = {SCIL},  pages = {28-38},  title = {Look at that! BERT can be easily distracted from paying attention to morphosyntax},  volume = {4},  year = {2021} }
LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI	2021	http://www.semanticscholar.org/paper/f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884	This work proposes an extensible framework to collectively yet categorically test diverse LOgical reasoning capabilities required for NLI (and by extension, NLU) and creates a semi-synthetic large test-bench that offers following utilities: individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning).	maybe	4	Natural Language Inference (NLI) is considered a representative task to test natural language understanding (NLU). In this work, we propose an extensible framework to collectively yet categorically test diverse LOgical reasoning capabilities required for NLI (and by extension, NLU). Motivated by behavioral testing, we create a semi-synthetic large test-bench (363 templates, 363k examples) and an associated framework that offers following utilities: 1) individually test and analyze reasoning capabilities along 17 reasoning dimensions (including pragmatic reasoning), 2) design experiments to study cross-capability information content (leave one out or bring one in); and 3) the synthetic nature enable us to control for artifacts and biases. The inherited power of automated test case instantiation from free-form natural language templates (using CheckList), and a well-defined taxonomy of capabilities enable us to extend to (cognitively) harder test cases while varying the complexity of natural language. Through our analysis of state-of-the-art NLI systems, we observe that our benchmark is indeed hard (and non-trivial even with training on additional resources). Some capabilities stand out as harder. Further fine-grained analysis and fine-tuning experiments reveal more insights about these capabilities and the models – supporting and extending previous observations. Towards the end we also perform an user-study, to investigate whether behavioral information can be utilised to generalize much better for some models compared to others.	f4a2a13b5ff0f4b4a83f3b4effe84dbf8bb84884	@['JournalArticle']{tarunesh-etal-2021-lonli:,  author = {Ishan Tarunesh and Somak Aditya and M. Choudhury},  booktitle = {ArXiv},  journal = {ArXiv},  title = {LoNLI: An Extensible Framework for Testing Diverse Logical Reasoning Capabilities for NLI},  volume = {abs/2112.02333},  year = {2021} }
Long-term Control for Dialogue Generation: Methods and Evaluation	2022	http://www.semanticscholar.org/paper/e0625715d17155fb91601052f5f2bc992e803d58	This work defines the problem of constrained long-term control for dialogue generation, identifies gaps in current methods for evaluation, and proposes new metrics that better measure long- term control that outperforms state-of-the-art constrained generation baselines.	maybe	2	Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.	e0625715d17155fb91601052f5f2bc992e803d58	@['JournalArticle', 'Conference']{ramakrishnan-etal-2022-long,  author = {Ramya Ramakrishnan and Hashan Buddhika Narangodage and M. Schilman and Kilian Q. Weinberger and Ryan T. McDonald},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Long-term Control for Dialogue Generation: Methods and Evaluation},  volume = {abs/2205.07352},  year = {2022} }
Logical Tasks for Measuring Extrapolation and Rule Comprehension	2022	http://www.semanticscholar.org/paper/965e409a3e7b5670d609837fac9823b160d6639c	This work describes and characterize logical tasks and discusses system requirements for their solution, and discusses the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias.	maybe	0	Logical reasoning is essential in a variety of human activities. A representative example of a logical task is mathematics. Recent large-scale models trained on large datasets have been successful in various ﬁelds, but their reasoning ability in arithmetic tasks is limited, which we reproduce experimentally. Here, we recast this limitation as not unique to mathematics but common to tasks that require logical operations. We then propose a new set of tasks, termed logical tasks, which will be the next challenge to address. This higher point of view helps the development of inductive biases that have broad impact beyond the solution of individual tasks. We deﬁne and characterize logical tasks and discuss system requirements for their solution. Furthermore, we discuss the relevance of logical tasks to concepts such as extrapolation, explainability, and inductive bias. Finally, we provide directions for solving logical tasks.	965e409a3e7b5670d609837fac9823b160d6639c	@['JournalArticle']{fujisawa-kanai-2022-logical,  author = {Ippei Fujisawa and R. Kanai},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Logical Tasks for Measuring Extrapolation and Rule Comprehension},  volume = {abs/2211.07727},  year = {2022} }
Locating and Editing Factual Associations in GPT	2022	https://www.semanticscholar.org/paper/996445d847f06e99b0bd259345408a0cf1bce87e	An important role is found for mid-layer feed-forward modules in storing factual associations in autoregressive transformer language models and direct manipulation of computational mechanisms may be a feasible approach for model editing.	yes	3	We analyze the storage and recall of factual associations in autoregressive transformer language models, ﬁnding evidence that these associations correspond to localized, directly-editable computations. We ﬁrst develop a causal intervention for identifying neuron activations that are decisive in a model’s factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update speciﬁc factual associations using Rank-One Model Editing (ROME). We ﬁnd that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task. We also evaluate ROME on a new dataset of difﬁcult counterfactual assertions, on which it simultaneously maintains both speciﬁcity and generalization, whereas other methods sacriﬁce one or another. Our results conﬁrm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/ .	996445d847f06e99b0bd259345408a0cf1bce87e	@None{meng-etal-2022-locating,  author = {Kevin Meng and David Bau and A. Andonian and Yonatan Belinkov},  title = {Locating and Editing Factual Associations in GPT},  year = {2022} }
Local Structure Matters Most in Most Languages	2022	http://www.semanticscholar.org/paper/9a5b2dc77bda19759df8481aaf283da353ac7e77	This work replicates a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting and finds that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.	maybe	0	Many recent perturbation studies have found unintuitive results on what does and does not matter when performing Natural Language Understanding (NLU) tasks in English. Coding properties, such as the order of words, can often be removed through shuffling without impacting downstream performances. Such insight may be used to direct future research into English NLP models. As many improvements in multilingual settings consist of wholesale adaptation of English approaches, it is important to verify whether those studies replicate or not in multilingual settings. In this work, we replicate a study on the importance of local structure, and the relative unimportance of global structure, in a multilingual setting. We find that the phenomenon observed on the English language broadly translates to over 120 languages, with a few caveats.	9a5b2dc77bda19759df8481aaf283da353ac7e77	@['JournalArticle']{clouâtre-etal-2022-local,  author = {Louis Clouâtre and Prasanna Parthasarathi and A. Zouaq and Sarath Chandar},  booktitle = {AACL},  pages = {285-294},  title = {Local Structure Matters Most in Most Languages},  year = {2022} }
LMFingerprints: Visual Explanations of Language Model Embedding Spaces through Layerwise Contextualization Scores	2022	https://www.semanticscholar.org/paper/98795a8c6af73a370f985ce59ea0f095e66b5858	LMFingerprints, a novel scoring‐based technique for the explanation of contextualized word embeddings, is presented and it is shown that while numbers are poorly contextualized, stopwords have an unexpected high contextualization in the models' upper layers.	maybe	3	Language models, such as BERT, construct multiple, contextualized embeddings for each word occurrence in a corpus. Understanding how the contextualization propagates through the model's layers is crucial for deciding which layers to use for a specific analysis task. Currently, most embedding spaces are explained by probing classifiers; however, some findings remain inconclusive. In this paper, we present LMFingerprints, a novel scoring‐based technique for the explanation of contextualized word embeddings. We introduce two categories of scoring functions, which measure (1) the degree of contextualization, i.e., the layerwise changes in the embedding vectors, and (2) the type of contextualization, i.e., the captured context information. We integrate these scores into an interactive explanation workspace. By combining visual and verbal elements, we provide an overview of contextualization in six popular transformer‐based language models. We evaluate hypotheses from the domain of computational linguistics, and our results not only confirm findings from related work but also reveal new aspects about the information captured in the embedding spaces. For instance, we show that while numbers are poorly contextualized, stopwords have an unexpected high contextualization in the models' upper layers, where their neighborhoods shift from similar functionality tokens to tokens that contribute to the meaning of the surrounding sentences.	98795a8c6af73a370f985ce59ea0f095e66b5858	@['JournalArticle', 'Review']{sevastjanova-etal-2022-lmfingerprints:,  author = {R. Sevastjanova and A. Kalouli and C. Beck and H. Hauptmann and Mennatallah El-Assady},  booktitle = {Computer graphics forum (Print)},  journal = {Computer Graphics Forum},  title = {LMFingerprints: Visual Explanations of Language Model Embedding Spaces through Layerwise Contextualization Scores},  volume = {41},  year = {2022} }
Linking artificial and human neural representations of language	2019	http://www.semanticscholar.org/paper/5abc9431d2442f419e5c499bfd1d9567ca30516c	The results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.	maybe	52	What information from an act of sentence understanding is robustly represented in the human brain? We investigate this question by comparing sentence encoding models on a brain decoding task, where the sentence that an experimental participant has seen must be predicted from the fMRI signal evoked by the sentence. We take a pre-trained BERT architecture as a baseline sentence encoding model and fine-tune it on a variety of natural language understanding (NLU) tasks, asking which lead to improvements in brain-decoding performance. We find that none of the sentence encoding tasks tested yield significant increases in brain decoding performance. Through further task ablations and representational analyses, we find that tasks which produce syntax-light representations yield significant improvements in brain decoding performance. Our results constrain the space of NLU models that could best account for human neural representations of language, but also suggest limits on the possibility of decoding fine-grained syntactic information from fMRI human neuroimaging.	5abc9431d2442f419e5c499bfd1d9567ca30516c	@['JournalArticle', 'Conference']{gauthier-levy-2019-linking,  author = {Jon Gauthier and R. Levy},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {529-539},  title = {Linking artificial and human neural representations of language},  year = {2019} }
Linguistic Profiling of a Neural Language Model	2020	https://www.semanticscholar.org/paper/1d8f62d8a4b647a30253c27f1ca295b8f03ed171	This paper investigates the linguistic knowledge learned by a Neural Language Model before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems, and finds that BERT’s capacity to encode different kind of linguistic properties has a positive influence on its predictions.	maybe	21	In this paper we investigate the linguistic knowledge learned by a Neural Language Model (NLM) before and after a fine-tuning process and how this knowledge affects its predictions during several classification problems. We use a wide set of probing tasks, each of which corresponds to a distinct sentence-level feature extracted from different levels of linguistic annotation. We show that BERT is able to encode a wide range of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT’s capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence.	1d8f62d8a4b647a30253c27f1ca295b8f03ed171	@['JournalArticle', 'Conference']{miaschi-etal-2020-linguistic,  author = {Alessio Miaschi and Dominique Brunato and F. Dell’Orletta and Giulia Venturi},  booktitle = {International Conference on Computational Linguistics},  pages = {745-756},  title = {Linguistic Profiling of a Neural Language Model},  year = {2020} }
Linguistic Knowledge and Transferability of Contextual Representations	2019	https://www.semanticscholar.org/paper/f6fbb6809374ca57205bd2cf1421d4f4fa04f975	It is found that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge.	seed	500	Contextual word representations derived from large-scale neural language models are successful across a diverse set of NLP tasks, suggesting that they encode useful and transferable features of language. To shed light on the linguistic knowledge they capture, we study the representations produced by several recent pretrained contextualizers (variants of ELMo, the OpenAI transformer language model, and BERT) with a suite of sixteen diverse probing tasks. We find that linear models trained on top of frozen contextual representations are competitive with state-of-the-art task-specific models in many cases, but fail on tasks requiring fine-grained linguistic knowledge (e.g., conjunct identification). To investigate the transferability of contextual word representations, we quantify differences in the transferability of individual layers within contextualizers, especially between recurrent neural networks (RNNs) and transformers. For instance, higher layers of RNNs are more task-specific, while transformer layers do not exhibit the same monotonic trend. In addition, to better understand what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results.	f6fbb6809374ca57205bd2cf1421d4f4fa04f975	@['JournalArticle', 'Conference']{liu-etal-2019-linguistic,  author = {Nelson F. Liu and Matt Gardner and Yonatan Belinkov and Matthew E. Peters and Noah A. Smith},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Linguistic Knowledge and Transferability of Contextual Representations},  volume = {abs/1903.08855},  year = {2019} }
Linguistic generalization and compositionality in modern artificial neural networks	2019	http://www.semanticscholar.org/paper/0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8	It is argued that the intriguing behaviour of these devices should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality.	maybe	100	In the last decade, deep artificial neural networks have achieved astounding performance in many natural language-processing tasks. Given the high productivity of language, these models must possess effective generalization abilities. It is widely assumed that humans handle linguistic productivity by means of algebraic compositional rules: are deep networks similarly compositional? After reviewing the main innovations characterizing current deep language-processing networks, I discuss a set of studies suggesting that deep networks are capable of subtle grammar-dependent generalizations, but also that they do not rely on systematic compositional rules. I argue that the intriguing behaviour of these devices (still awaiting a full understanding) should be of interest to linguists and cognitive scientists, as it offers a new perspective on possible computational strategies to deal with linguistic productivity beyond rule-based compositionality, and it might lead to new insights into the less systematic generalization patterns that also appear in natural language. This article is part of the theme issue ‘Towards mechanistic models of meaning composition’.	0dc092d33f7c71bc9d8d42b53ebb1fad101db4c8	@['JournalArticle', 'Review']{baroni-2019-linguistic,  author = {Marco Baroni},  booktitle = {Philosophical Transactions of the Royal Society of London. Biological Sciences},  journal = {Philosophical Transactions of the Royal Society B},  title = {Linguistic generalization and compositionality in modern artificial neural networks},  volume = {375},  year = {2019} }
Linguistic Dependencies and Statistical Dependence	2021	http://www.semanticscholar.org/paper/bcb651d73447d96be58db5fac6fb13324842b351	This work introduces the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI), and analyzes which kinds of linguistic dependencies are best captured in CPMI dependencies.	maybe	2	Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \approx 0.5. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.	bcb651d73447d96be58db5fac6fb13324842b351	@['JournalArticle', 'Conference']{hoover-etal-2021-linguistic,  author = {Jacob Louis Hoover and Alessandro Sordoni and Wenyu Du and T. O’Donnell},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Linguistic Dependencies and Statistical Dependence},  volume = {abs/2104.08685},  year = {2021} }
Linguistic Correlation Analysis: Discovering Salient Neurons in deepNLP models	2022	http://www.semanticscholar.org/paper/1374681ec5a1bbe568f7a2b1c9d6c9886d4efc36	A technique called Linguistic Correlation Analysis is presented to extract salient neurons in the model, with respect to any extrinsic property – with the goal of understanding how such a knowledge is preserved within neurons.	maybe	4	While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, little attention has been paid towards individual neurons. We present a technique called as Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property – with the goal of understanding how such a knowledge is preserved within neurons. We carry out a ﬁne-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that capture speciﬁc linguistic properties? (ii) how localized or distributed neurons are across the network? iii) how redundantly is the information preserved? iv) how ﬁne-tuning pre-trained models towards downstream NLP tasks, impacts the learned linguistic knowledge? iv) how do architectures vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting ﬁndings: (i) we found small subsets of neurons that can predict different linguistic tasks, ii) with neurons capturing basic lexical information (such as sufﬁxation) localized in lower most layers, iii) while those learning complex concepts (such as syntactic role) predominantly in middle and higher layers, iii) that salient linguistic neurons are relocated from higher to lower layers during transfer learning, as the network preserve the higher layers for task speciﬁc information, iv) we found interesting differences across pre-trained models, with respect to how linguistic information is preserved within, and v) we found that concept exhibit similar neuron distribution across different languages in the multilingual transformer models. Our code is publicly available as part of the NeuroX toolkit (Dalvi, Nortonsmith, Bau, Belinkov, Sajjad, Durrani, & Glass, 2019). 1 their top-5 activating in the test-set. Looking at the top-5 activating for a neuron, validate the identiﬁed neuron indeed that property. These results also show how visualization can be combined with any neuron ranking method to give us a quick insight into the property a neuron has learned.	1374681ec5a1bbe568f7a2b1c9d6c9886d4efc36	@['JournalArticle']{durrani-etal-2022-linguistic,  author = {Nadir Durrani and Fahim Dalvi and Hassan Sajjad},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Linguistic Correlation Analysis: Discovering Salient Neurons in deepNLP models},  volume = {abs/2206.13288},  year = {2022} }
Linguistic Analysis of Pretrained Sentence Encoders with Acceptability Judgments	2019	http://www.semanticscholar.org/paper/6cf7ae9c0c790662eb018d578a22eeddd09c547b	It is concluded that recent sentence encoders, despite showing near-human performance on acceptability classification overall, still fail to make fine-grained grammaticality distinctions for many complex syntactic structures.	maybe	13	Recent work on evaluating grammatical knowledge in pretrained sentence encoders gives a fine-grained view of a small number of phenomena. We introduce a new analysis dataset that also has broad coverage of linguistic phenomena. We annotate the development set of the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) for the presence of 13 classes of syntactic phenomena including various forms of argument alternations, movement, and modification. We use this analysis set to investigate the grammatical knowledge of three pretrained encoders: BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and the BiLSTM baseline from Warstadt et al. We find that these models have a strong command of complex or non-canonical argument structures like ditransitives (Sue gave Dan a book) and passives (The book was read). Sentences with long distance dependencies like questions (What do you think I ate?) challenge all models, but for these, BERT and GPT have a distinct advantage over the baseline. We conclude that recent sentence encoders, despite showing near-human performance on acceptability classification overall, still fail to make fine-grained grammaticality distinctions for many complex syntactic structures.	6cf7ae9c0c790662eb018d578a22eeddd09c547b	@None{warstadt-bowman-2019-linguistic,  author = {Alex Warstadt and Samuel R. Bowman},  journal = {arXiv: Computation and Language},  title = {Linguistic Analysis of Pretrained Sentence Encoders with Acceptability Judgments},  year = {2019} }
Linear Connectivity Reveals Generalization Strategies	2022	http://www.semanticscholar.org/paper/96486c71106f0a61ebe02061c32f9fb80c227011	This work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions, and measures performance on specially-crafted diagnostic datasets that these clusters correspond to different generalization strategies.	maybe	6	It is widely accepted in the mode connectivity literature that when two neural networks are trained similarly on the same data, they are connected by a path through parameter space over which test set accuracy is maintained. Under some circumstances, including transfer learning from pretrained models, these paths are presumed to be linear. In contrast to existing results, we ﬁnd that among text classiﬁers (trained on MNLI, QQP, and CoLA), some pairs of ﬁnetuned models have large barriers of increasing loss on the linear paths between them. On each task, we ﬁnd distinct clusters of models which are linearly connected on the test loss surface, but are disconnected from models outside the cluster—models that occupy separate basins on the surface. By measuring performance on specially-crafted diagnostic datasets, we ﬁnd that these clusters correspond to different generalization strategies: one cluster behaves like a bag of words model under domain shift, while another cluster uses syntactic heuristics. Our work demonstrates how the geometry of the loss surface can guide models towards different heuristic functions.	96486c71106f0a61ebe02061c32f9fb80c227011	@['JournalArticle']{juneja-etal-2022-linear,  author = {Jeevesh Juneja and Rachit Bansal and Kyunghyun Cho and João Sedoc and Naomi Saphra},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Linear Connectivity Reveals Generalization Strategies},  volume = {abs/2205.12411},  year = {2022} }
Linear Adversarial Concept Erasure	2022	http://www.semanticscholar.org/paper/1c87263dfdac513512370017fe371a4e744b2b43	This paper formulates the problem of identifying and erasing a linear subspace that corresponds to a given concept in order to prevent linear predictors from recovering the concept, and recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation.	maybe	16	Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to control their content becomes an increasingly important problem. This paper formulates the problem of identifying and erasing a linear subspace that corresponds to a given concept in order to prevent linear predictors from recovering the concept. Our formulation consists of a constrained, linear minimax game. We consider different concept-identification objectives, modeled after several tasks such as classification and regression. We derive a closed-form solution for certain objectives, and propose a convex relaxation, R-LACE , that works well for others. When evaluated in the context of binary gender removal, our method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. We show that the method—despite being linear—is highly expressive, effectively mitigating bias in deep nonlinear classifiers while maintaining tractability and interpretability. mean	1c87263dfdac513512370017fe371a4e744b2b43	@['JournalArticle', 'Conference']{ravfogel-etal-2022-linear,  author = {Shauli Ravfogel and Michael Twiton and Yoav Goldberg and Ryan Cotterell},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Linear Adversarial Concept Erasure},  volume = {abs/2201.12091},  year = {2022} }
LINDA: Unsupervised Learning to Interpolate in Natural Language Processing	2021	http://www.semanticscholar.org/paper/b8db1ddcae0820e84f9f3aafbbc6875cd7565329	This paper proposes an unsupervised learning approach to text interpolation for the purpose of data augmentation that does not require any heuristics nor manually crafted resources but learns to interpolate between any pair of natural language sentences over a natural language manifold.	maybe	2	Despite the success of mixup in data augmentation, its applicability to natural language processing (NLP) tasks has been limited due to the discrete and variable-length nature of natural languages. Recent studies have thus relied on domain-specific heuristics and manually crafted resources, such as dictionaries, in order to apply mixup in NLP. In this paper, we instead propose an unsupervised learning approach to text interpolation for the purpose of data augmentation, to which we refer as “Learning to INterpolate for Data Augmentation” (LINDA), that does not require any heuristics nor manually crafted resources but learns to interpolate between any pair of natural language sentences over a natural language manifold. After empirically demonstrating the LINDA’s interpolation capability, we show that LINDA indeed allows us to seamlessly apply mixup in NLP and leads to better generalization in text classification both indomain and out-of-domain.	b8db1ddcae0820e84f9f3aafbbc6875cd7565329	@['JournalArticle']{kim-etal-2021-linda:,  author = {Yekyung Kim and Seohyeong Jeong and Kyunghyun Cho},  booktitle = {ArXiv},  journal = {ArXiv},  title = {LINDA: Unsupervised Learning to Interpolate in Natural Language Processing},  volume = {abs/2112.13969},  year = {2021} }
Lexical Generalization Improves with Larger Models and Longer Training	2022	https://www.semanticscholar.org/paper/4a9a8ea5a25842bdc2aff0df23094d622e612673	This work analyzes the use of lexical overlap heuristics in natural language inference, paraphrase detection, and reading comprehension (using a novel contrastive dataset), and finds that larger models are much less suscep-tible to adopting lexical overlaps.	maybe	0	While ﬁne-tuned language models perform well on many tasks, they were also shown to rely on superﬁcial surface features such as lexical overlap. Excessive utilization of such heuristics can lead to failure on challenging inputs. We analyze the use of lexical overlap heuristics in natural language inference, paraphrase detection, and reading comprehension (using a novel contrastive dataset), and ﬁnd that larger models are much less suscep-tible to adopting lexical overlap heuristics. We also ﬁnd that longer training leads models to abandon lexical overlap heuristics. Finally, we provide evidence that the disparity between models size has its source in the pre-trained model. 1	4a9a8ea5a25842bdc2aff0df23094d622e612673	@['JournalArticle']{bandel-etal-2022-lexical,  author = {Elron Bandel and Yoav Goldberg and Yanai Elazar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Lexical Generalization Improves with Larger Models and Longer Training},  volume = {abs/2210.12673},  year = {2022} }
Let’s Play Mono-Poly: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses	2021	http://www.semanticscholar.org/paper/befbef9f4e4c6269fa712294430ff916cf2fd51c	It is demonstrated that BERT-derived representations reflect words’ polysemy level and their partitionability into senses and contribute to a better understanding of the knowledge encoded in contextualized representations and open up new avenues for multilingual lexical semantics research.	maybe	18	Pre-trained language models (LMs) encode rich information about linguistic structure but their knowledge about lexical polysemy remains unclear. We propose a novel experimental setup for analyzing this knowledge in LMs specifically trained for different languages (English, French, Spanish, and Greek) and in multilingual BERT. We perform our analysis on datasets carefully designed to reflect different sense distributions, and control for parameters that are highly correlated with polysemy such as frequency and grammatical category. We demonstrate that BERT-derived representations reflect words’ polysemy level and their partitionability into senses. Polysemy-related information is more clearly present in English BERT embeddings, but models in other languages also manage to establish relevant distinctions between words at different polysemy levels. Our results contribute to a better understanding of the knowledge encoded in contextualized representations and open up new avenues for multilingual lexical semantics research.	befbef9f4e4c6269fa712294430ff916cf2fd51c	@['JournalArticle']{soler-apidianaki-2021-let’s,  author = {Aina Garí Soler and Marianna Apidianaki},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {825-844},  title = {Let’s Play Mono-Poly: BERT Can Reveal Words’ Polysemy Level and Partitionability into Senses},  volume = {9},  year = {2021} }
Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation	2022	http://www.semanticscholar.org/paper/5e4a6dccec2e7fb388b09bda5e4ecca9ab652d68	This work focuses on the spurious correlation between feature and label, which derive from the biased data distribution in the training data, and analyzes it concretely, finding that the biased examples with spurious correlations are easier for models to learn and when predicting, the biased words make higher contributions to models’ predictions than other words.	maybe	0	Many recent works indicate that the deep neural networks tend to take dataset biases as shortcuts to make decision, rather than under-stand the tasks, which results in failures on the real-world applications. In this work, we focus on the spurious correlation between feature and label, which derive from the biased data distribution in the training data, and analyze it concretely. In particular, we deﬁne the word highly co-occurring with a speciﬁc label as biased word, and the example containing biased word as biased example. Our analysis re-veals that the biased examples with spurious correlations are easier for models to learn, and when predicting, the biased words make signiﬁcantly higher contributions to models’ predictions than other words, and the models tend to assign the labels over-relying on the spurious correlation between words and labels. To mitigate the model’s over-reliance on the shortcut, we propose a training strategy Less-Learn-Shortcut (LLS): we quantify the biased degree of the biased examples, and down-weight them with the biased degree. Experimental results on QM and NLI tasks show that the models improve the performances both on in-domain and adversarial data (1.57% on DuQM and 2.12% on HANS) with our LLS.	5e4a6dccec2e7fb388b09bda5e4ecca9ab652d68	@['JournalArticle']{du-etal-2022-less,  author = {Yanrui Du and Jing Yang and Yan Chen and Jing Liu and Sendong Zhao and Huaqin Wu and Haifeng Wang and Bing Qin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Less Learn Shortcut: Analyzing and Mitigating Learning of Spurious Feature-Label Correlation},  volume = {abs/2205.12593},  year = {2022} }
Legal Prompting: Teaching a Language Model to Think Like a Lawyer	2022	http://www.semanticscholar.org/paper/cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3	This work takes the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot prompting approaches and shows that the best results are produced by prompts that are derived from legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion).	maybe	0	Large language models that are capable of zero or few-shot prompting approaches have given rise to the new research area of prompt engineering. Recent advances showed that for example Chain-of-Thought (CoT) prompts can improve arithmetic or common sense tasks signiﬁcantly. We explore how such approaches fare with legal reasoning tasks and take the COLIEE entailment task based on the Japanese Bar exam for testing zero-shot/few-shot and ﬁne-tuning approaches. Our ﬁndings show that while CoT prompting and ﬁne-tuning with explanations approaches show improvements, the best results are produced by prompts that are derived from speciﬁc legal reasoning techniques such as IRAC (Issue, Rule, Application, Conclusion). Based on our experiments we improve the 2021 best result from 0.7037 accuracy to 0.8148 accuracy and beat the 2022 best system of 0.6789 accuracy with an accuracy of 0.7431.	cc43306e22dbfd5bc35251ab8c8ba37e4fc2a1b3	@['JournalArticle']{yu-etal-2022-legal,  author = {Fang Yu and Lee Quartey and Frank Schilder},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Legal Prompting: Teaching a Language Model to Think Like a Lawyer},  volume = {abs/2212.01326},  year = {2022} }
Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)	2020	http://www.semanticscholar.org/paper/055fac05cd424e7b1bdcd359ff7980ca8d938ef3	A new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that are used to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning, finds that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones.	maybe	46	One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa-base. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa-base does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.	055fac05cd424e7b1bdcd359ff7980ca8d938ef3	@['JournalArticle', 'Conference']{warstadt-etal-2020-learning,  author = {Alex Warstadt and Yian Zhang and Haau-Sing Li and Haokun Liu and Samuel R. Bowman},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {217-235},  title = {Learning Which Features Matter: RoBERTa Acquires a Preference for Linguistic Generalizations (Eventually)},  year = {2020} }
Learning unsupervised contextual representations for medical synonym discovery	2019	http://www.semanticscholar.org/paper/cad8a6b9248227d041f35acbfb341ab870d8995f	Comparing the proposed contextualized deep learning representations to previous non-neural representations, it is found that the contextualized representations show consistent improvement over non-contextualized models in all metrics.	maybe	8	Abstract Objectives An important component of processing medical texts is the identification of synonymous words or phrases. Synonyms can inform learned representations of patients or improve linking mentioned concepts to medical ontologies. However, medical synonyms can be lexically similar (“dilated RA” and “dilated RV”) or dissimilar (“cerebrovascular accident” and “stroke”); contextual information can determine if 2 strings are synonymous. Medical professionals utilize extensive variation of medical terminology, often not evidenced in structured medical resources. Therefore, the ability to discover synonyms, especially without reliance on training data, is an important component in processing training notes. The ability to discover synonyms from models trained on large amounts of unannotated data removes the need to rely on annotated pairs of similar words. Models relying solely on non-annotated data can be trained on a wider variety of texts without the cost of annotation, and thus may capture a broader variety of language. Materials and Methods Recent contextualized deep learning representation models, such as ELMo (Peters et al., 2019) and BERT, (Devlin et al. 2019) have shown strong improvements over previous approaches in a broad variety of tasks. We leverage these contextualized deep learning models to build representations of synonyms, which integrate the context of surrounding sentence and use character-level models to alleviate out-of-vocabulary issues. Using these models, we perform unsupervised discovery of likely synonym matches, which reduces the reliance on expensive training data. Results We use the ShARe/CLEF eHealth Evaluation Lab 2013 Task 1b data to evaluate our synonym discovery method. Comparing our proposed contextualized deep learning representations to previous non-neural representations, we find that the contextualized representations show consistent improvement over non-contextualized models in all metrics. Conclusions Our results show that contextualized models produce effective representations for synonym discovery. We expect that the use of these representations in other tasks would produce similar gains in performance.	cad8a6b9248227d041f35acbfb341ab870d8995f	@['JournalArticle']{schumacher-dredze-2019-learning,  author = {Elliot Schumacher and Mark Dredze},  booktitle = {JAMIA Open},  journal = {JAMIA Open},  pages = {538 - 546},  title = {Learning unsupervised contextual representations for medical synonym discovery},  volume = {2},  year = {2019} }
Learning To Retrieve Prompts for In-Context Learning	2021	http://www.semanticscholar.org/paper/f9838a3be5c94bb2674a0e224de349b50e18f3c4	This work proposes an efficient method for retrieving prompts for in-context learning using annotated data and an LM, and trains an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time.	maybe	64	In-context learning is a recent paradigm in natural language understanding, where a large pre-trained language model (LM) observes a test instance and a few training examples as its input, and directly decodes the output without any update to its parameters. However, performance has been shown to strongly depend on the selected training examples (termed prompts). In this work, we propose an efficient method for retrieving prompts for in-context learning using annotated data and an LM. Given an input-output pair, we estimate the probability of the output given the input and a candidate training example as the prompt, and label training examples as positive or negative based on this probability. We then train an efficient dense retriever from this data, which is used to retrieve training examples as prompts at test time. We evaluate our approach on three sequence-to-sequence tasks where language utterances are mapped to meaning representations, and find that it substantially outperforms prior work and multiple baselines across the board.	f9838a3be5c94bb2674a0e224de349b50e18f3c4	@['JournalArticle', 'Conference']{rubin-etal-2021-learning,  author = {Ohad Rubin and Jonathan Herzig and Jonathan Berant},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Learning To Retrieve Prompts for In-Context Learning},  volume = {abs/2112.08633},  year = {2021} }
Learning to Remove: Towards Isotropic Pre-trained BERT Embedding	2021	http://www.semanticscholar.org/paper/ab151c1ca0479b677003ef200018b93e983aa0ec	In all tasks, the word embedding processed by the method consistently outperforms the original embedding (with average improvement of 13% on word analogy and 16% on semantic textual similarity) and two baseline methods and is proven to be more robust to changes of hyperparameter.	yes	7		ab151c1ca0479b677003ef200018b93e983aa0ec	@['JournalArticle', 'Conference']{liang-etal-2021-learning,  author = {Y. Liang and Rui Cao and Jie Zheng and Jie Ren and Ling Gao},  booktitle = {International Conference on Artificial Neural Networks},  pages = {448-459},  title = {Learning to Remove: Towards Isotropic Pre-trained BERT Embedding},  year = {2021} }
Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks	2021	http://www.semanticscholar.org/paper/c764ecba2bace12b9bfb9c2b0651a12ff6888ea7	This work investigates learning representations that facilitate transfer learning from one compositional task to another: the representation and the task-specific layers of the models are strategically trained differently on a pre-finetuning task such that they generalize well on mismatched splits that require compositionality.	maybe	5	Neural network models often generalize poorly to mismatched domains or distributions. In NLP, this issue arises in particular when models are expected to generalize compositionally, that is, to novel combinations of familiar words and constructions. We investigate learning representations that facilitate transfer learning from one compositional task to another: the representation and the task-specific layers of the models are strategically trained differently on a pre-finetuning task such that they generalize well on mismatched splits that require compositionality. We apply this method to semantic parsing, using three very different datasets, COGS, GeoQuery and SCAN, used alternately as the pre-finetuning and target task. Our method significantly improves compositional generalization over baselines on the test set of the target task, which is held out during fine-tuning. Ablation studies characterize the utility of the major steps in the proposed algorithm and support our hypothesis.	c764ecba2bace12b9bfb9c2b0651a12ff6888ea7	@['JournalArticle']{zhu-etal-2021-learning,  author = {Wang Zhu and Peter Shaw and Tal Linzen and Fei Sha},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Learning to Generalize Compositionally by Transferring Across Semantic Parsing Tasks},  volume = {abs/2111.05013},  year = {2021} }
Learning to Deceive with Attention-Based Explanations	2020	http://www.semanticscholar.org/paper/cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a	This work demonstrates a simple method for training models to produce deceptive attention masks, and casts doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.	maybe	129	Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention’s reliability as a tool for auditing algorithms in the context of fairness and accountability.	cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a	@['JournalArticle', 'Conference']{pruthi-etal-2019-learning,  author = {Danish Pruthi and Mansi Gupta and Bhuwan Dhingra and Graham Neubig and Zachary Chase Lipton},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4782-4793},  title = {Learning to Deceive with Attention-Based Explanations},  year = {2019} }
Learning Syntax from Naturally-Occurring Bracketings	2021	http://www.semanticscholar.org/paper/90aeb070f8cecd9ed819fc0f35d9de2071650551	Experiments demonstrate that the distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems.	maybe	5	Naturally-occurring bracketings, such as answer fragments to natural language questions and hyperlinks on webpages, can reflect human syntactic intuition regarding phrasal boundaries. Their availability and approximate correspondence to syntax make them appealing as distant information sources to incorporate into unsupervised constituency parsing. But they are noisy and incomplete; to address this challenge, we develop a partial-brackets-aware structured ramp loss in learning. Experiments demonstrate that our distantly-supervised models trained on naturally-occurring bracketing data are more accurate in inducing syntactic structures than competing unsupervised systems. On the English WSJ corpus, our models achieve an unlabeled F1 score of 68.9 for constituency parsing.	90aeb070f8cecd9ed819fc0f35d9de2071650551	@['JournalArticle', 'Conference']{shi-etal-2021-learning,  author = {Tianze Shi and Ozan Irsoy and Igor Malioutov and Lillian Lee},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2941-2949},  title = {Learning Syntax from Naturally-Occurring Bracketings},  year = {2021} }
Learning Prototypical Functions for Physical Artifacts	2021	http://www.semanticscholar.org/paper/8c748fbc290f7748cbf223a8c3674936808fc45d	This paper uses frames from FrameNet to represent a set of common functions for objects, and describes a manually annotated data set of physical objects labeled with their prototypical function.	maybe	2	Humans create things for a reason. Ancient people created spears for hunting, knives for cutting meat, pots for preparing food, etc. The prototypical function of a physical artifact is a kind of commonsense knowledge that we rely on to understand natural language. For example, if someone says “She borrowed the book” then you would assume that she intends to read the book, or if someone asks “Can I use your knife?” then you would assume that they need to cut something. In this paper, we introduce a new NLP task of learning the prototypical uses for human-made physical objects. We use frames from FrameNet to represent a set of common functions for objects, and describe a manually annotated data set of physical objects labeled with their prototypical function. We also present experimental results for this task, including BERT-based models that use predictions from masked patterns as well as artifact sense definitions from WordNet and frame definitions from FrameNet.	8c748fbc290f7748cbf223a8c3674936808fc45d	@['JournalArticle', 'Conference']{jiang-riloff-2021-learning,  author = {Tianyu Jiang and E. Riloff},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {6941-6951},  title = {Learning Prototypical Functions for Physical Artifacts},  year = {2021} }
Learning Numeral Embeddings	2020	http://www.semanticscholar.org/paper/d5fcad8a3b183642fcf609519a4dbbda9c3541ff	Two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals are proposed and shown its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.	maybe	8	Word embedding is an essential building block for deep learning methods for natural language processing. Although word embedding has been extensively studied over the years, the problem of how to effectively embed numerals, a special subset of words, is still underexplored. Existing word embedding methods do not learn numeral embeddings well because there are an infinite number of numerals and their individual appearances in training corpora are highly scarce. In this paper, we propose two novel numeral embedding methods that can handle the out-of-vocabulary (OOV) problem for numerals. We first induce a finite set of prototype numerals using either a self-organizing map or a Gaussian mixture model. We then represent the embedding of a numeral as a weighted average of the prototype number embeddings. Numeral embeddings represented in this manner can be plugged into existing word embedding learning approaches such as skip-gram for training. We evaluated our methods and showed its effectiveness on four intrinsic and extrinsic tasks: word similarity, embedding numeracy, numeral prediction, and sequence labeling.	d5fcad8a3b183642fcf609519a4dbbda9c3541ff	@['JournalArticle']{jiang-etal-2019-learning,  author = {Chengyue Jiang and Zhonglin Nian and Kaihao Guo and Shanbo Chu and Yinggong Zhao and Libin Shen and Kewei Tu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Learning Numeral Embeddings},  volume = {abs/2001.00003},  year = {2019} }
Learning Mathematical Properties of Integers	2021	http://www.semanticscholar.org/paper/b350be3836c3d183464642815b26b061f24e8314	This work probes the integer embeddings for mathematical knowledge, applies them to a set of numerical reasoning tasks, and shows that by learning the representations from mathematical sequence data, they can substantially improve over number embedDings learned from English text corpora.	maybe	3	Embedding words in high-dimensional vector spaces has proven valuable in many natural language applications. In this work, we investigate whether similarly-trained embeddings of integers can capture concepts that are useful for mathematical applications. We probe the integer embeddings for mathematical knowledge, apply them to a set of numerical reasoning tasks, and show that by learning the representations from mathematical sequence data, we can substantially improve over number embeddings learned from English text corpora.	b350be3836c3d183464642815b26b061f24e8314	@['JournalArticle']{ryskina-knight-2021-learning,  author = {Maria Ryskina and Kevin Knight},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {389-395},  title = {Learning Mathematical Properties of Integers},  year = {2021} }
Learning Lexical Subspaces in a Distributional Vector Space	2020	http://www.semanticscholar.org/paper/2677f411aae496be93ee70bcbf0eb0e949c13e0c	This paper proposes LexSub, a novel approach towards unifying lexical and distributional semantics by defining subspaces of the distributional vector space in which a lexical relation should hold, which outperforms previous systems on extrinsic classification tasks that benefit from exploiting lexical relational cues.	maybe	6	Abstract In this paper, we propose LexSub, a novel approach towards unifying lexical and distributional semantics. We inject knowledge about lexical-semantic relations into distributional word embeddings by defining subspaces of the distributional vector space in which a lexical relation should hold. Our framework can handle symmetric attract and repel relations (e.g., synonymy and antonymy, respectively), as well as asymmetric relations (e.g., hypernymy and meronomy). In a suite of intrinsic benchmarks, we show that our model outperforms previous approaches on relatedness tasks and on hypernymy classification and detection, while being competitive on word similarity tasks. It also outperforms previous systems on extrinsic classification tasks that benefit from exploiting lexical relational cues. We perform a series of analyses to understand the behaviors of our model.1 Code available at https://github.com/aishikchakraborty/LexSub.	2677f411aae496be93ee70bcbf0eb0e949c13e0c	@['JournalArticle']{arora-etal-2020-learning,  author = {Kushal Arora and Aishik Chakraborty and J. C. Cheung},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {311-329},  title = {Learning Lexical Subspaces in a Distributional Vector Space},  volume = {8},  year = {2020} }
Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems	2020	http://www.semanticscholar.org/paper/0390c5e9556e162b0e514fd9c712c8d81018fb8b	This paper proposes a method to embed the KB, of any size, directly into the model parameters, which does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning.	yes	33	Task-oriented dialogue systems are either modularized with separate dialogue state tracking (DST) and management steps or end-to-end trainable. In either case, the knowledge base (KB) plays an essential role in fulfilling user requests. Modularized systems rely on DST to interact with the KB, which is expensive in terms of annotation and inference time. End-to-end systems, instead, use the KB directly as input, but they cannot scale when the KB is larger than a few hundred entries. In this paper, we propose a method to embed the KB, of any size, directly into the model parameters. The resulting model does not require any DST or template responses, nor the KB as input, and it can dynamically update its KB via fine-tuning. We evaluate our solution in five task-oriented dialogue datasets with small, medium, and large KB size. Our experiments show that end-to-end models can effectively embed knowledge bases in their parameters and achieve competitive performance in all evaluated datasets.	0390c5e9556e162b0e514fd9c712c8d81018fb8b	@['JournalArticle']{madotto-etal-2020-learning,  author = {Andrea Madotto and Samuel Cahyawijaya and Genta Indra Winata and Yan Xu and Zihan Liu and Zhaojiang Lin and Pascale Fung},  booktitle = {Findings},  journal = {ArXiv},  title = {Learning Knowledge Bases with Parameters for Task-Oriented Dialogue Systems},  volume = {abs/2009.13656},  year = {2020} }
Learning and Evaluating General Linguistic Intelligence	2019	https://www.semanticscholar.org/paper/19281b9ecdb5c07a93423a506627ab9d9b0cf039	This work analyzes state-of-the-art natural language understanding models and conducts an extensive empirical investigation to evaluate them against general linguistic intelligence criteria, and proposes a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task.	seed	162	We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.	19281b9ecdb5c07a93423a506627ab9d9b0cf039	@['JournalArticle']{yogatama-etal-2019-learning,  author = {Dani Yogatama and Cyprien de Masson d'Autume and Jerome T. Connor and Tomás Kociský and Mike Chrzanowski and Lingpeng Kong and Angeliki Lazaridou and Wang Ling and Lei Yu and Chris Dyer and P. Blunsom},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Learning and Evaluating General Linguistic Intelligence},  volume = {abs/1901.11373},  year = {2019} }
Learning and Evaluating Contextual Embedding of Source Code	2019	http://www.semanticscholar.org/paper/ad3222db224444e0998bcc9aa66341bfe5d81c51	This paper curates a massive, deduplicated corpus of 7.4M Python files from GitHub, and creates an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples.	maybe	159	Recent research has achieved impressive results on understanding and improving source code by building up on machine-learning techniques developed for natural languages. A significant advancement in natural-language understanding has come with the development of pre-trained contextual embeddings, such as BERT, which can be fine-tuned for downstream tasks with less labeled data and training budget, while achieving better accuracies. However, there is no attempt yet to obtain a high-quality contextual embedding of source code, and to evaluate it on multiple program-understanding tasks simultaneously; that is the gap that this paper aims to mitigate. Specifically, first, we curate a massive, deduplicated corpus of 7.4M Python files from GitHub, which we use to pre-train CuBERT, an open-sourced code-understanding BERT model; and, second, we create an open-sourced benchmark that comprises five classification tasks and one program-repair task, akin to code-understanding tasks proposed in the literature before. We fine-tune CuBERT on our benchmark tasks, and compare the resulting models to different variants of Word2Vec token embeddings, BiLSTM and Transformer models, as well as published state-of-the-art models, showing that CuBERT outperforms them all, even with shorter training, and with fewer labeled examples. Future work on source-code embedding can benefit from reusing our benchmark, and from comparing against CuBERT models as a strong baseline.	ad3222db224444e0998bcc9aa66341bfe5d81c51	@['JournalArticle', 'Conference']{kanade-etal-2019-learning,  author = {Aditya Kanade and Petros Maniatis and Gogul Balakrishnan and Kensen Shi},  booktitle = {International Conference on Machine Learning},  pages = {5110-5121},  title = {Learning and Evaluating Contextual Embedding of Source Code},  year = {2019} }
Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?	2020	http://www.semanticscholar.org/paper/8f60401b03b46c4b3e61834dc691a8587250d46b	A leakage-adjusted simulatability (LAS) metric is introduced for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output.	maybe	42	Data collection for natural language (NL) understanding tasks has increasingly included human explanations alongside data points, allowing past works to introduce models that both perform a task and generate NL explanations for their outputs. Yet to date, model-generated explanations have been evaluated on the basis of surface-level similarities to human explanations, both through automatic metrics like BLEU and human evaluations. We argue that these evaluations are insufficient, since they fail to indicate whether explanations support actual model behavior (faithfulness), rather than simply match what a human would say (plausibility). In this work, we address the problem of evaluating explanations from the the model simulatability perspective. Our contributions are as follows: (1) We introduce a leakage-adjusted simulatability (LAS) metric for evaluating NL explanations, which measures how well explanations help an observer predict a model’s output, while controlling for how explanations can directly leak the output. We use a model as a proxy for a human observer, and validate this choice with two human subject experiments. (2) Using the CoS-E and e-SNLI datasets, we evaluate two existing generative graphical models and two new approaches; one rationalizing method we introduce achieves roughly human-level LAS scores. (3) Lastly, we frame explanation generation as a multi-agent game and optimize explanations for simulatability while penalizing label leakage, which can improve LAS scores.	8f60401b03b46c4b3e61834dc691a8587250d46b	@['JournalArticle']{hase-etal-2020-leakage,  author = {Peter Hase and Shiyue Zhang and Harry Xie and Mohit Bansal},  booktitle = {Findings},  journal = {ArXiv},  title = {Leakage-Adjusted Simulatability: Can Models Generate Non-Trivial Explanations of Their Behavior in Natural Language?},  volume = {abs/2010.04119},  year = {2020} }
Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?	2021	http://www.semanticscholar.org/paper/04fe16a3ba1958133aadfc2a5a7d40dc0acfe01b	Clear evidence is found that one single attention head learns to recognize the words that make a test sentence humorous, even without access to this information at training time, and important insights are obtained into the mechanisms by which transformers recognize humor.	maybe	4	The automatic detection of humor poses a grand challenge for natural language processing. Transformer-based systems have recently achieved remarkable results on this task, but they usually (1) were evaluated in setups where serious vs. humorous texts came from entirely different sources, and (2) focused on benchmarking performance without providing insights into how the models work. We make progress in both respects by training and analyzing transformer-based humor recognition models on a recently introduced dataset consisting of minimal pairs of aligned sentences, one serious, the other humorous. We find that, although our aligned dataset is much harder than previous datasets, transformer-based models recognize the humorous sentence in an aligned pair with high accuracy (78%). In a careful error analysis, we characterize easy vs. hard instances. Finally, by analyzing attention weights, we obtain important insights into the mechanisms by which transformers recognize humor. Most remarkably, we find clear evidence that one single attention head learns to recognize the words that make a test sentence humorous, even without access to this information at training time.	04fe16a3ba1958133aadfc2a5a7d40dc0acfe01b	@['JournalArticle', 'Conference']{peyrard-etal-2021-laughing,  author = {Maxime Peyrard and Beatriz Borges and Kristina Gligoric and Robert West},  booktitle = {International Joint Conference on Artificial Intelligence},  journal = {ArXiv},  title = {Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?},  volume = {abs/2105.09142},  year = {2021} }
Latent Topology Induction for Understanding Contextualized Representations	2022	http://www.semanticscholar.org/paper/16a5dbdab3d1efe90a74678a665a6ed7f8c68c7f	It is shown that sentences are represented as a traversal over the latent network where state-state transition chains encode syntactic templates and state-word emissions in the content.	maybe	1	In this work, we study the representation space of contextualized embeddings and gain insight into the hidden topology of large language models. We show there exists a network of latent states that summarize linguistic properties of contextualized representations. Instead of seeking alignments to existing well-deﬁned annotations, we infer this latent network in a fully unsupervised way using a structured variational autoencoder. The induced states not only serve as anchors that mark the topology (neighbors and connectivity) of the representation manifold but also reveals the internal mechanism of encoding sentences. With the induced network, we: (1). decompose the representation space into a spectrum of latent states which encode ﬁne-grained word meanings with lexical, morphological, syntactic and semantic information; (2). show state-state transitions encode rich phrase constructions and serve as the backbones of the latent space. Putting the two together, we show that sentences are represented as a traversal over the latent network where state-state transition chains encode syntactic templates and state-word emissions ﬁll in the content. We demonstrate these insights with extensive experiments and visualizations.	16a5dbdab3d1efe90a74678a665a6ed7f8c68c7f	@['JournalArticle']{fu-lapata-2022-latent,  author = {Yao Fu and Mirella Lapata},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Latent Topology Induction for Understanding Contextualized Representations},  volume = {abs/2206.01512},  year = {2022} }
Large Language Models Struggle to Learn Long-Tail Knowledge	2022	http://www.semanticscholar.org/paper/6491980820d9c255b9d798874c8fce696750e0d9	It is shown that a language model’s ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training, and that retrieval-augmentation can reduce the dependence on relevant document count, presenting a promising approach for capturing the long-tail.	maybe	5	The internet contains a wealth of knowledge— from the birthdays of historical ﬁgures to tu-torials on how to code—all of which may be learned by language models. However, there is a huge variability in the number of times a given piece of information appears on the web. In this paper, we study the relationship between the knowledge memorized by large language models and the information in their pre-training datasets. In particular, we show that a language model’s ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., Trivi-aQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). More-over, we ﬁnd that while larger models are better at learning long-tail knowledge, we estimate that today’s models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant document count, presenting a promising approach for capturing the long-tail.	6491980820d9c255b9d798874c8fce696750e0d9	@['JournalArticle']{kandpal-etal-2022-large,  author = {Nikhil Kandpal and H. Deng and Adam Roberts and Eric Wallace and Colin Raffel},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Large Language Models Struggle to Learn Long-Tail Knowledge},  volume = {abs/2211.08411},  year = {2022} }
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)	2022	https://www.semanticscholar.org/paper/e9f5c4bb0db632eaf48c4d4ce83a29753d2d861b	This work proposes an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence and provides multiple test cases that are more involved than any of the previously established benchmarks.	maybe	10	Recent advances in large language models (LLMs) have transformed the ﬁeld of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a signiﬁcant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs’ reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LLMs on reasoning about actions and change, a central aspect of human intelligence. We provide multiple test cases that are more involved than any of the previously established benchmarks and each test case evaluates a different aspect of reasoning about actions and change. Results on GPT-3 (davinci), Instruct-GPT3 (text-davinci-002) and BLOOM (176B), showcase subpar performance on such reasoning tasks.	e9f5c4bb0db632eaf48c4d4ce83a29753d2d861b	@['JournalArticle']{valmeekam-etal-2022-large,  author = {Karthik Valmeekam and Alberto Olmo and S. Sreedharan and Subbarao Kambhampati},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)},  volume = {abs/2206.10498},  year = {2022} }
Large Language Models Encode Clinical Knowledge	2022	http://www.semanticscholar.org/paper/6052486bc9144dc1730c12bf35323af3792a1fd0		yes			6052486bc9144dc1730c12bf35323af3792a1fd0	
Large Language Models Can Self-Improve	2022	http://www.semanticscholar.org/paper/3fa70115248377c3d1517c9f978791a296fbc1dd	This work uses a pre-trained LLM to generate “high-conﬁdence” rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and conducts ablation studies and shows that ablation on reasoning is critical for self-improvement.	maybe	17	Large Language Models (LLMs) have achieved excellent performances in various tasks. However, ﬁne-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate “high-conﬁdence” rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and ﬁne-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4% → 82.1% on GSM8K, 78.2% → 83.0% on DROP, 90.0% → 94.4% on OpenBookQA, and 63.4% → 67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that ﬁnetuning on reasoning is critical for self-improvement.	3fa70115248377c3d1517c9f978791a296fbc1dd	@['JournalArticle']{huang-etal-2022-large,  author = {Jiaxin Huang and S. Gu and Le Hou and Yuexin Wu and Xuezhi Wang and Hongkun Yu and Jiawei Han},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Large Language Models Can Self-Improve},  volume = {abs/2210.11610},  year = {2022} }
Large Language Models Can Be Easily Distracted by Irrelevant Context	2023	https://www.semanticscholar.org/paper/c847c0eb1f578b8a8707fb3f526a67cfdaaa55c4	This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be affected by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description.	maybe	0	Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be inﬂu-enced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and ﬁnd that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deﬁciency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information. 1	c847c0eb1f578b8a8707fb3f526a67cfdaaa55c4	@None{shi-etal-2023-large,  author = {Freda Shi and Xinyun Chen and Kanishka Misra and Nathan Scales and David Dohan and E. Chi and Nathanael Scharli and Denny Zhou},  title = {Large Language Models Can Be Easily Distracted by Irrelevant Context},  year = {2023} }
Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?	2023	http://www.semanticscholar.org/paper/898842b1c1fb4689ffa2f20c0836961a962b7691		maybe	0	Newly-developed large language models (LLM)—because of how they are trained and designed—are implicit computational models of humans—a homo silicus . LLMs can be used like economists use homo economicus : they can be given endowments, information, preferences, and so on, and then their behavior can be explored in scenarios via simulation. Experiments using this approach, derived from Charness and Rabin (2002), Kahneman, Knetsch and Thaler (1986), and Samuelson and Zeckhauser (1988) show qualitatively similar results to the original, but it is also easy to try variations for fresh insights. LLMs could allow researchers to pilot studies via simulation ﬁrst, searching for novel social science insights to test in the real world.	898842b1c1fb4689ffa2f20c0836961a962b7691	@None{horton-2023-large,  author = {J. Horton},  title = {Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?},  year = {2023} }
Large Language Models are Zero-Shot Reasoners	2022	https://www.semanticscholar.org/paper/f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643	Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples.	yes	135	Chain of thought (CoT) prompting, a recent tech-nique for eliciting multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning.While these successes are often attributed to LLMs’ ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding “Let’s think step by step” before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks, without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with an 175B parameter Instruct-GPT, as well as similar magnitudes of improvements with 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted through simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs.	f75a0ccb3c9b7e12dfb8b9c9dc3d35ca4518a643	@['JournalArticle']{kojima-etal-2022-large,  author = {Takeshi Kojima and S. Gu and Machel Reid and Yutaka Matsuo and Yusuke Iwasawa},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Large Language Models are Zero-Shot Reasoners},  volume = {abs/2205.11916},  year = {2022} }
Large language models are not zero-shot communicators	2022	http://www.semanticscholar.org/paper/e8db669c8cb1c07557ede15e2771968f9370330b	A simple task is designed and widely used state-of-the-art models are evaluated, finding that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random.	yes	2	Despite widespread use of LLMs as conversational agents, evaluations of performance fail to capture a crucial aspect of communication: interpreting language in context. Humans interpret language using beliefs and prior knowledge about the world. For example, we intuitively understand the response “I wore gloves” to the question “Did you leave fingerprints?” as meaning “No”. To investigate whether LLMs have the ability to make this type of inference, known as an implicature, we design a simple task and evaluate widely used state-of-the-art models. We find that, despite only evaluating on utterances that require a binary inference (yes or no), most perform close to random. Models adapted to be “aligned with human intent” perform much better, but still show a significant gap with human performance. We present our findings as the starting point for further research into evaluating how LLMs interpret language in context and to drive the development of more pragmatic and useful models of human discourse.	e8db669c8cb1c07557ede15e2771968f9370330b	@['JournalArticle']{ruis-etal-2022-large,  author = {Laura Ruis and Akbir Khan and Stella Rose Biderman and Sara Hooker and Tim Rocktaschel and Edward Grefenstette},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Large language models are not zero-shot communicators},  volume = {abs/2210.14986},  year = {2022} }
Large Language Models and the Reverse Turing Test	2022	http://www.semanticscholar.org/paper/292da1c4640c105aa5d7919a1ded9a1225c07d4d		maybe	3	Large Language Models (LLMs) have been transformative. They are pre-trained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and more recently LaMDA can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a Reverse Turing Test. If so, then by studying interviews we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.	292da1c4640c105aa5d7919a1ded9a1225c07d4d	@['JournalArticle']{sejnowski-2022-large,  author = {T. Sejnowski},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Large Language Models and the Reverse Turing Test},  volume = {abs/2207.14382},  year = {2022} }
Language processing in brains and deep neural networks: computational convergence and its limits	2020	http://www.semanticscholar.org/paper/8a7e11444eb221835952058d0ea9d6bf545e33f1	Tests on activations of artificial neural networks trained on image, word and sentence processing linearly map onto the hierarchy of human brain responses elicited during a reading task suggest that the compositional - but not the lexical - representations of modern language models converge to a brain-like solution.	maybe	33	Deep learning has recently allowed substantial progress in language tasks such as translation and completion. Do such models process language similarly to humans, and is this similarity driven by systematic structural, functional and learning principles? To address these issues, we tested whether the activations of 7,400 artificial neural networks trained on image, word and sentence processing linearly map onto the hierarchy of human brain responses elicited during a reading task, using source-localized magneto-encephalography (MEG) recordings of one hundred and four subjects. Our results confirm that visual, word and language models sequentially correlate with distinct areas of the left-lateralized cortical hierarchy of reading. However, only specific subsets of these models converge towards brain-like representations during their training. Specifically, when the algorithms are trained on language modeling, their middle layers become increasingly similar to the late responses of the language network in the brain. By contrast, input and output word embedding layers often diverge away from brain activity during training. These differences are primarily rooted in the sustained and bilateral responses of the temporal and frontal cortices. Together, these results suggest that the compositional - but not the lexical - representations of modern language models converge to a brain-like solution.	8a7e11444eb221835952058d0ea9d6bf545e33f1	@None{caucheteux-king-2020-language,  author = {C. Caucheteux and J. King},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Language processing in brains and deep neural networks: computational convergence and its limits},  year = {2020} }
Language Models Use Monotonicity to Assess NPI Licensing	2021	http://www.semanticscholar.org/paper/2d3e4e5690b992d3099caa1606d4310a0d632868	This work introduces a series of experiments consisting of probing with diagnostic classifiers (DCs), linguistic acceptability tasks, as well as a novel DC ranking method that tightly connects the probing results to the inner workings of the LM.	maybe	10	We investigate the semantic knowledge of language models (LMs), focusing on (1) whether these LMs create categories of linguistic environments based on their semantic monotonicity properties, and (2) whether these categories play a similar role in LMs as in human language understanding, using negative polarity item licensing as a case study. We introduce a series of experiments consisting of probing with diagnostic classifiers (DCs), linguistic acceptability tasks, as well as a novel DC ranking method that tightly connects the probing results to the inner workings of the LM. By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.	2d3e4e5690b992d3099caa1606d4310a0d632868	@['JournalArticle']{jumelet-etal-2021-language,  author = {Jaap Jumelet and Milica Deni'c and Jakub Szymanik and D. Hupkes and Shane Steinert-Threlkeld},  booktitle = {Findings},  journal = {ArXiv},  title = {Language Models Use Monotonicity to Assess NPI Licensing},  volume = {abs/2105.13818},  year = {2021} }
Language models show human-like content effects on reasoning	2022	http://www.semanticscholar.org/paper/290732e9fb08a29af8892a7c1f73c9d2a1b9d7db	This work hypothesized that language models would show human-like content content on abstract reasoning problems, and explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task.	maybe	15	reasoning is a key ability for an intelligent system. Large language models achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect, and depends on our knowledge and beliefs about the content of the reasoning problem. For example, humans reason much more reliably about logical rules that are grounded in everyday situations than arbitrary rules about abstract attributes. The training experiences of language models similarly endow them with prior expectations that reﬂect human knowledge and beliefs. We therefore hypothesized that language models would show human-like content eﬀects on abstract reasoning problems. We explored this hypothesis across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task (Wason, 1968). We ﬁnd that state of the art large language models (with 7 or 70 billion parameters; Hoﬀmann et al., 2022) reﬂect many of the same patterns observed in humans across these tasks — like humans, models reason more eﬀectively about believable situations than unrealistic or abstract ones. Our ﬁndings have implications for understanding both these cognitive eﬀects, and the factors that contribute to language model performance.	290732e9fb08a29af8892a7c1f73c9d2a1b9d7db	@['JournalArticle']{dasgupta-etal-2022-language,  author = {I. Dasgupta and Andrew Kyle Lampinen and Stephanie C. Y. Chan and Antonia Creswell and D. Kumaran and James L. McClelland and Felix Hill},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language models show human-like content effects on reasoning},  volume = {abs/2207.07051},  year = {2022} }
Language Models for Lexical Inference in Context	2021	http://www.semanticscholar.org/paper/2bd629ccdc9217ccef813371d3177813a770de88	Three approaches based on pretrained language models based on handcrafted patterns expressing the semantics of lexical inference outperform the previous state of the art and show the potential of pretrained LMs for LIiC.	maybe	8	Lexical inference in context (LIiC) is the task of recognizing textual entailment between two very similar sentences, i.e., sentences that only differ in one expression. It can therefore be seen as a variant of the natural language inference task that is focused on lexical semantics. We formulate and evaluate the first approaches based on pretrained language models (LMs) for this task: (i) a few-shot NLI classifier, (ii) a relation induction approach based on handcrafted patterns expressing the semantics of lexical inference, and (iii) a variant of (ii) with patterns that were automatically extracted from a corpus. All our approaches outperform the previous state of the art, showing the potential of pretrained LMs for LIiC. In an extensive analysis, we investigate factors of success and failure of our three approaches.	2bd629ccdc9217ccef813371d3177813a770de88	@['JournalArticle', 'Conference']{schmitt-schütze-2021-language,  author = {Martin Schmitt and Hinrich Schütze},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Language Models for Lexical Inference in Context},  volume = {abs/2102.05331},  year = {2021} }
Language Models As or For Knowledge Bases	2021	http://www.semanticscholar.org/paper/b961524502c11538dd88db47ad315810a305f5a7	Qualitative arguments that latent LMs are not suitable as a substitute for explicit KBs, but could play a major role for augmenting and curating KBs are offered.	maybe	0	Pre-trained language models (LMs) have recently gained attention for their potential as an alternative to (or proxy for) explicit knowledge bases (KBs). In this position paper, we examine this hypothesis, identify strengths and limitations of both LMs and KBs, and discuss the complementary nature of the two paradigms. In particular, we offer qualitative arguments that latent LMs are not suitable as a substitute for explicit KBs, but could play a major role for augmenting and curating KBs.	b961524502c11538dd88db47ad315810a305f5a7	@['JournalArticle']{razniewski-etal-2021-language,  author = {Simon Razniewski and Andrew Yates and Nora Kassner and G. Weikum},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models As or For Knowledge Bases},  volume = {abs/2110.04888},  year = {2021} }
Language Models as Knowledge Bases?	2019	https://www.semanticscholar.org/paper/d0086b86103a620a86bc918746df0aa642e2a8a3	An in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models finds that BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge.	seed	969	Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as “fill-in-the-blank” cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.	d0086b86103a620a86bc918746df0aa642e2a8a3	@['JournalArticle', 'Conference']{petroni-etal-2019-language,  author = {Fabio Petroni and Tim Rocktäschel and Patrick Lewis and A. Bakhtin and Yuxiang Wu and Alexander H. Miller and S. Riedel},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Language Models as Knowledge Bases?},  volume = {abs/1909.01066},  year = {2019} }
Language Models as Inductive Reasoners	2022	http://www.semanticscholar.org/paper/c7a4946eb49bc6a9c01eaa79e84a35316595bd5a	This work proposes a new task, which is to induce natural language rules from nat- ural language facts, and creates a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language.	maybe	0	Inductive reasoning is a core component of human intel- ligence. In the past research of inductive reasoning within computer science, logic language is used as representations of knowledge (facts and rules, more speciﬁcally). However, logic language can cause systematic problems for inductive reasoning such as disability of handling raw input such as natural language, sensitiveness to mislabeled data, and inca- pacity to handle ambiguous input. To this end, we propose a new task, which is to induce natural language rules from nat- ural language facts, and create a dataset termed DEER containing 1.2k rule-fact pairs for the task, where rules and facts are written in natural language. New automatic metrics are also proposed and analysed for the evaluation of this task. With DEER, we investigate a modern approach for inductive reasoning where we use natural language as representation for knowledge instead of logic language and use pretrained language models as “reasoners”. Moreover, we provide the ﬁrst and comprehensive analysis of how well pretrained language models can induce natural language rules from natural language facts. We also propose a new framework drawing insights from philosophy literature for this task, which we show in the experiment section that surpasses baselines in both automatic and human evaluations.	c7a4946eb49bc6a9c01eaa79e84a35316595bd5a	@['JournalArticle']{yang-etal-2022-language,  author = {Zonglin Yang and Li Dong and X. Du and Hao Cheng and E. Cambria and Xiaodong Liu and Jianfeng Gao and Furu Wei},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models as Inductive Reasoners},  volume = {abs/2212.10923},  year = {2022} }
Language Models as Fact Checkers?	2020	http://www.semanticscholar.org/paper/4ee859d947d62ad884b521d1d800f0501260b1ef	This paper uses implicit knowledge from language models to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components, and shows that this method is viable and has much room for exploration.	maybe	40	Recent work has suggested that language models (LMs) store both common-sense and factual knowledge learned from pre-training data. In this paper, we leverage this implicit knowledge to create an effective end-to-end fact checker using a solely a language model, without any external knowledge or explicit retrieval components. While previous work on extracting knowledge from LMs have focused on the task of open-domain question answering, to the best of our knowledge, this is the first work to examine the use of language models as fact checkers. In a closed-book setting, we show that our zero-shot LM approach outperforms a random baseline on the standard FEVER task, and that our finetuned LM compares favorably with standard baselines. Though we do not ultimately outperform methods which use explicit knowledge bases, we believe our exploration shows that this method is viable and has much room for exploration.	4ee859d947d62ad884b521d1d800f0501260b1ef	@['JournalArticle']{lee-etal-2020-language,  author = {Nayeon Lee and Belinda Z. Li and Sinong Wang and Wen-tau Yih and Hao Ma and Madian Khabsa},  booktitle = {FEVER},  journal = {ArXiv},  title = {Language Models as Fact Checkers?},  volume = {abs/2006.04102},  year = {2020} }
Language Models as Agent Models	2022	http://www.semanticscholar.org/paper/4596139b28c3ceacbd7e3c34dc0df079dbf4e96b		maybe	4	Language models (LMs) are trained on collec-tions of documents, written by individual human agents to achieve speciﬁc goals in an out-side world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of intentional communication in a speciﬁc, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn inﬂuence subsequent LM generation in the same way that agents’ communicative intentions inﬂuence their language. I survey ﬁndings from the recent literature showing that—even in today’s non-robust and error-prone models—LMs infer and use representations of ﬁne-grained communicative intentions and more abstract beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.	4596139b28c3ceacbd7e3c34dc0df079dbf4e96b	@['JournalArticle', 'Review']{andreas-2022-language,  author = {Jacob Andreas},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models as Agent Models},  volume = {abs/2212.01681},  year = {2022} }
Language Models are Unsupervised Multitask Learners	2019	http://www.semanticscholar.org/paper/9405cc0d6169988371b2755e573cc28650d14dfe	It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.	maybe	8472	Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.	9405cc0d6169988371b2755e573cc28650d14dfe	@None{radford-etal-2019-language,  author = {Alec Radford and Jeff Wu and Rewon Child and D. Luan and Dario Amodei and Ilya Sutskever},  title = {Language Models are Unsupervised Multitask Learners},  year = {2019} }
Language Models Are Poor Learners of Directional Inference	2022	http://www.semanticscholar.org/paper/4fb7010a8af2012324bb45a3c281b2ebf8dc44a7	It is shown that contrary to their apparent success on stan-dard NLI, LMs show limited ability to learn directional inference; moreover, existing datasets fail to test directionality, and/or are infested by artefacts that can be learnt as proxy for entailments, yielding over-optimistic results.	maybe	0	We examine LMs’ competence of directional predicate entailments by supervised ﬁne-tuning with prompts. Our analysis shows that contrary to their apparent success on stan-dard NLI, LMs show limited ability to learn such directional inference; moreover, existing datasets fail to test directionality, and/or are infested by artefacts that can be learnt as proxy for entailments, yielding over-optimistic results. In response, we present BoOQA (Boolean Open QA), a robust multi-lingual evaluation benchmark for directional predicate entailments, extrinsic to existing training sets. On BoOQA, we establish baselines and show evidence of existing LM-prompting models being incompetent directional entailment learners, in contrast to entailment graphs, however limited by sparsity.	4fb7010a8af2012324bb45a3c281b2ebf8dc44a7	@['JournalArticle']{li-etal-2022-language,  author = {Tianyi Li and Mohammad Javad Hosseini and Sabine Weber and Mark Steedman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models Are Poor Learners of Directional Inference},  volume = {abs/2210.04695},  year = {2022} }
Language Models are Multilingual Chain-of-Thought Reasoners	2022	http://www.semanticscholar.org/paper/62f0db3a5ad5c795ec18fc7a6e7b01836809df57	It is shown that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment, and that models have strikingly strong mult bilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili.	maybe	13	We evaluate the reasoning abilities of large language models in multilingual settings. We introduce the Multilingual Grade School Math (MGSM) benchmark, by manually translating 250 grade-school math problems from the GSM8K dataset (Cobbe et al., 2021) into ten typologically diverse languages. We ﬁnd that the ability to solve MGSM problems via chain-of-thought prompting emerges with increasing model scale, and that models have strikingly strong multilingual reasoning abilities, even in underrepresented languages such as Bengali and Swahili. Finally, we show that the multilingual reasoning abilities of language models extend to other tasks such as commonsense reasoning and word-in-context semantic judgment. The MGSM benchmark is publicly available at https://github.com/google-research/url-nlp	62f0db3a5ad5c795ec18fc7a6e7b01836809df57	@['JournalArticle']{shi-etal-2022-language,  author = {Freda Shi and Mirac Suzgun and Markus Freitag and Xuezhi Wang and Suraj Srivats and Soroush Vosoughi and Hyung Won Chung and Yi Tay and Sebastian Ruder and Denny Zhou and Dipanjan Das and Jason Wei},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models are Multilingual Chain-of-Thought Reasoners},  volume = {abs/2210.03057},  year = {2022} }
Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought	2022	https://www.semanticscholar.org/paper/d5a9e02dea45d2718a068e3a3c2938bd9d1f07c0	To enable systematic exploration of the reasoning ability of LLMs, a new synthetic question-answering dataset is presented, where each example is generated from a synthetic world model represented in ﬁrst-order logic, which allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis.	yes	6	Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called P R O NTO QA, where each example is generated from a synthetic world model represented in ﬁrst-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on I NSTRUCT GPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in ﬁctional contexts. However, they have difﬁculty with proof planning : When multiple valid deduction steps are available, they are not able to systematically explore the different options.	d5a9e02dea45d2718a068e3a3c2938bd9d1f07c0	@['JournalArticle']{saparov-he-2022-language,  author = {Abulhair Saparov and He He},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models Are Greedy Reasoners: A Systematic Formal Analysis of Chain-of-Thought},  volume = {abs/2210.01240},  year = {2022} }
Language models and Automated Essay Scoring	2019	http://www.semanticscholar.org/paper/ba01747d847a70419d992c7c40932ceb953d1da9	The current state-of-the-art natural language processing (NLP) neural network architectures are used in this work to achieve above human-level accuracy on the publicly available Kaggle AES dataset.	maybe	35	In this paper, we present a new comparative study on automatic essay scoring (AES). The current state-of-the-art natural language processing (NLP) neural network architectures are used in this work to achieve above human-level accuracy on the publicly available Kaggle AES dataset. We compare two powerful language models, BERT and XLNet, and describe all the layers and network architectures in these models. We elucidate the network architectures of BERT and XLNet using clear notation and diagrams and explain the advantages of transformer architectures over traditional recurrent neural network architectures. Linear algebra notation is used to clarify the functions of transformers and attention mechanisms. We compare the results with more traditional methods, such as bag of words (BOW) and long short term memory (LSTM) networks.	ba01747d847a70419d992c7c40932ceb953d1da9	@['JournalArticle']{rodríguez-etal-2019-language,  author = {Pedro Uría Rodríguez and Amir Jafari and C. Ormerod},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language models and Automated Essay Scoring},  volume = {abs/1909.09482},  year = {2019} }
Language Models (Mostly) Know What They Know	2022	https://www.semanticscholar.org/paper/142ebbf4760145f591166bde2564ac70c001e927		maybe	23	We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We ﬁrst show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to ﬁrst propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We ﬁnd encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one speciﬁc possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing. We	142ebbf4760145f591166bde2564ac70c001e927	@['JournalArticle']{kadavath-etal-2022-language,  author = {Saurav Kadavath and Tom Conerly and Amanda Askell and T. Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Z. Dodds and Nova DasSarma and Eli Tran-Johnson and Scott Johnston and S. El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and John Kernion and S. Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom B. Brown and Jack Clark and Nicholas Joseph and Benjamin Mann and Sam McCandlish and C. Olah and Jared Kaplan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Models (Mostly) Know What They Know},  volume = {abs/2207.05221},  year = {2022} }
Language Modelling as a Multi-Task Problem	2021	http://www.semanticscholar.org/paper/80b0ee8b3f738e535dcf8b8c1223be5f8e3c25ba	This paper analyzes the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items and demonstrates that a multi-task setting naturally emerges within the objective of the more general task of language modelling.	maybe	5	In this paper, we propose to study language modelling as a multi-task problem, bringing together three strands of research: multi-task learning, linguistics, and interpretability. Based on hypotheses derived from linguistic theory, we investigate whether language models adhere to learning principles of multi-task learning during training. To showcase the idea, we analyse the generalisation behaviour of language models as they learn the linguistic concept of Negative Polarity Items (NPIs). Our experiments demonstrate that a multi-task setting naturally emerges within the objective of the more general task of language modelling. We argue that this insight is valuable for multi-task learning, linguistics and interpretability research and can lead to exciting new findings in all three domains.	80b0ee8b3f738e535dcf8b8c1223be5f8e3c25ba	@['JournalArticle', 'Conference']{weber-etal-2021-language,  author = {Leon Weber and Jaap Jumelet and Elia Bruni and D. Hupkes},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Language Modelling as a Multi-Task Problem},  volume = {abs/2101.11287},  year = {2021} }
Language Modeling with Latent Situations	2022	http://www.semanticscholar.org/paper/421c19183033288e0bfd5df1451083d8b3cb8007	A family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states, showing that standard LMs can be sample-efﬁciently trained to model not just language but the situations it describes.	maybe	0	Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in their inputs. We introduce S ITUATION S UPERVISION , a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. S ITUATION S UPERVISION has two components: an auxiliary situation modeling task that trains models to predict state representations in context, and a latent state inference procedure that imputes these states from partially annotated training data. S ITUATION - S UPERVISION can be applied to both ﬁne-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text). In both cases, S ITUATION S UPERVISION requires only a small number of state annotations to produce major coherence improvements (between 4-11%), showing that standard LMs can be sample-efﬁciently trained to model not just language but the situations it describes.	421c19183033288e0bfd5df1451083d8b3cb8007	@['JournalArticle']{li-etal-2022-language,  author = {Belinda Z. Li and Maxwell Nye and Jacob Andreas},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Modeling with Latent Situations},  volume = {abs/2212.10012},  year = {2022} }
Language Model Transformers as Evaluators for Open-domain Dialogues	2020	http://www.semanticscholar.org/paper/c3fdfe92202a0e334a87324a03d61ca545663460	This work investigates whether language models (LM) based on transformer neural networks can indicate the quality of a conversation and demonstrates that human evaluators have a positive correlation between the output of the language models and scores.	maybe	5	Computer-based systems for communication with humans are a cornerstone of AI research since the 1950s. So far, the most effective way to assess the quality of the dialogues produced by these systems is to use resource-intensive manual labor instead of automated means. In this work, we investigate whether language models (LM) based on transformer neural networks can indicate the quality of a conversation. In a general sense, language models are methods that learn to predict one or more words based on an already given context. Due to their unsupervised nature, they are candidates for efficient, automatic indication of dialogue quality. We demonstrate that human evaluators have a positive correlation between the output of the language models and scores. We also provide some insights into their behavior and inner-working in a conversational context.	c3fdfe92202a0e334a87324a03d61ca545663460	@['JournalArticle', 'Conference']{nedelchev-etal-2020-language,  author = {R. Nedelchev and Jens Lehmann and Ricardo Usbeck},  booktitle = {International Conference on Computational Linguistics},  pages = {6797-6808},  title = {Language Model Transformers as Evaluators for Open-domain Dialogues},  year = {2020} }
Language Model Evaluation Beyond Perplexity	2021	http://www.semanticscholar.org/paper/1fa487376af5d4293ec482d193ca1790176c4dbc	It is found that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present).	maybe	21	We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework–paired with significance tests–for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type–token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.	1fa487376af5d4293ec482d193ca1790176c4dbc	@['JournalArticle', 'Conference']{meister-cotterell-2021-language,  author = {Clara Meister and Ryan Cotterell},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5328-5339},  title = {Language Model Evaluation Beyond Perplexity},  year = {2021} }
Language model acceptability judgements are not always robust to context	2022	http://www.semanticscholar.org/paper/9cffc161896ce2b8d1a3083ab4f293bc166134ce	This paper investigates the stability of language models’ performance on targeted syntactic evaluations as they vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality.	yes	0	Targeted syntactic evaluations of language models ask whether models show stable preferences for syntactically acceptable content over minimal-pair unacceptable inputs. Most targeted syntactic evaluation datasets ask models to make these judgements with just a single context-free sentence as input. This does not match language models’ training regime, in which input sentences are always highly contextualized by the surrounding corpus. This mismatch raises an important question: how robust are models’ syntactic judgements in different contexts? In this paper, we investigate the stability of language models’ performance on targeted syntactic evaluations as we vary properties of the input context: the length of the context, the types of syntactic phenomena it contains, and whether or not there are violations of grammaticality. We ﬁnd that model judgements are generally robust when placed in randomly sampled linguistic contexts. However, they are substantially unstable for contexts containing syntactic structures matching those in the critical test content. Among all tested models (GPT-2 and ﬁve variants of OPT), we signiﬁcantly improve models’ judgements by providing contexts with matching syntactic structures, and conversely signiﬁcantly worsen them using unacceptable contexts with matching but violated syntactic structures. This effect is ampliﬁed by the length of the context, except for unrelated inputs. We show that these changes in model performance are not explainable by simple features matching the context and the test inputs, such as lexical overlap and dependency overlap. This sensitivity to highly speciﬁc syntactic features of the context can only be explained by the models’	9cffc161896ce2b8d1a3083ab4f293bc166134ce	@['JournalArticle']{sinha-etal-2022-language,  author = {Koustuv Sinha and Jon Gauthier and Aaron Mueller and Kanishka Misra and Keren Fuentes and R. Levy and Adina Williams},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language model acceptability judgements are not always robust to context},  volume = {abs/2212.08979},  year = {2022} }
Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey	2022	http://www.semanticscholar.org/paper/2922ebbfbf71885f1a7cce6f41c79e950f69ceac	This work provides a survey of practical methods for addressing potential threats and societal harms from language generation models and draws on several prior works’ taxonomies of language model risks to present a structured overview of strategies for detecting andiorating different kinds of risks/harms of language generators.	maybe	2	Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they intro-duce, whether inadvertent or malicious. Several studies have identiﬁed potential causes of these harms and called for their mitigation via development of safer and fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works’ taxonomies of language model risks to present a structured overview of strategies for detecting and ame-liorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners with explanations of motivations behind different mitigation strategies, their limitations, and open problems for future research.	2922ebbfbf71885f1a7cce6f41c79e950f69ceac	@['JournalArticle', 'Review']{kumar-etal-2022-language,  author = {Sachin Kumar and Vidhisha Balachandran and Lucille Njoo and Antonios Anastasopoulos and Yulia Tsvetkov},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey},  volume = {abs/2210.07700},  year = {2022} }
Language Cognition and Language Computation - Human and Machine Language Understanding	2023	http://www.semanticscholar.org/paper/75549ee37396bdb71d73c31b6b2506b2416219e0	Comparing and contrast the research of language understanding in cognitive and computer sciences, and existing work that combines insights from language cognition and language computation and prospects for future development trends.	maybe	0	Language understanding is a key scientiﬁc issue in the ﬁelds of cognitive and computer science. However, the two disciplines diﬀer substantially in the speciﬁc research questions. Cognitive science focuses on analyzing the speciﬁc mechanism of the brain and investigating the brain’s response to language; few studies have examined the brain’s language system as a whole. By contrast, computer scientists focus on the eﬃciency of practical applications when choosing research questions but may ignore the most essential laws of language. Given these diﬀerences, can a combination of the disciplines oﬀer new insights for building intelligent language models and studying language cognitive mechanisms? In the following text, we ﬁrst review the research questions, history, and methods of language understanding in cognitive and computer science, focusing on the current progress and challenges. We then compare and contrast the research of language understanding in cognitive and computer sciences. Finally, we review existing work that combines insights from language cognition and language computation and oﬀer prospects for future development trends.	75549ee37396bdb71d73c31b6b2506b2416219e0	@['JournalArticle', 'Review']{wang-etal-2023-language,  author = {Shaonan Wang and N. Ding and Nan Lin and Jiajun Zhang and Chengqing Zong},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Language Cognition and Language Computation - Human and Machine Language Understanding},  volume = {abs/2301.04788},  year = {2023} }
Language (Technology) is Power: A Critical Survey of “Bias” in NLP	2020	http://www.semanticscholar.org/paper/d47a682723f710395454687319bb55635e653105	A greater recognition of the relationships between language and social hierarchies is urged, encouraging researchers and practitioners to articulate their conceptualizations of “bias” and to center work around the lived experiences of members of communities affected by NLP systems.	maybe	490	We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers’ proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.	d47a682723f710395454687319bb55635e653105	@['JournalArticle', 'Conference', 'Review']{blodgett-etal-2020-language,  author = {Su Lin Blodgett and Solon Barocas and Hal Daum'e and H. Wallach},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5454-5476},  title = {Language (Technology) is Power: A Critical Survey of “Bias” in NLP},  year = {2020} }
Language (Re)modelling: Towards Embodied Language Understanding	2020	http://www.semanticscholar.org/paper/7925351a4039dae7c2fd5d5d12df7f52f8464ce3	It is argued that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and a system architecture along with a roadmap towards realizing this vision is proposed.	maybe	17	While natural language understanding (NLU) is advancing rapidly, today’s technology differs from human-like language understanding in fundamental ways, notably in its inferior efficiency, interpretability, and generalization. This work proposes an approach to representation and learning based on the tenets of embodied cognitive linguistics (ECL). According to ECL, natural language is inherently executable (like programming languages), driven by mental simulation and metaphoric mappings over hierarchical compositions of structures and schemata learned through embodied interaction. This position paper argues that the use of grounding by metaphoric reasoning and simulation will greatly benefit NLU systems, and proposes a system architecture along with a roadmap towards realizing this vision.	7925351a4039dae7c2fd5d5d12df7f52f8464ce3	@['JournalArticle', 'Conference']{tamari-etal-2020-language,  author = {Ronen Tamari and Cheng Shani and Tom Hope and Miriam R. L. Petruck and Omri Abend and Dafna Shahaf},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Language (Re)modelling: Towards Embodied Language Understanding},  volume = {abs/2005.00311},  year = {2020} }
LAMBADA: Backward Chaining for Automated Reasoning in Natural Language	2022	http://www.semanticscholar.org/paper/b7c09e2d5ac1268205ebd7c54f11ad5c935beced	A Backward Chaining algorithm is developed, which is called L AM - BADA, that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference and achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets.	maybe	1	Remarkable progress has been made on automated reasoning with knowledge speciﬁed as unstructured, natural text, by using the power of large language models (LMs) coupled with methods such as Chain-of-Thought prompting and Selection-Inference. These techniques search for proofs in the forward direction from axioms to the conclusion, which suffers from a combinatorial explosion of the search space, and thus high failure rates for problems requiring longer chains of reasoning. The classical automated reasoning literature has shown that reasoning in the backward direction (i.e. from the intended conclusion to the set of axioms that support it) is signiﬁcantly more efﬁcient at proof-ﬁnding problems. We import this intuition into the LM setting and develop a Backward Chaining algorithm, which we call L AM - BADA , that decomposes reasoning into four sub-modules, each of which can be simply implemented by few-shot prompted LM inference. We show that L AMBADA achieves massive accuracy boosts over state-of-the-art forward reasoning methods on two challenging logical reasoning datasets, particularly when deep and accurate proof chains are required.	b7c09e2d5ac1268205ebd7c54f11ad5c935beced	@['JournalArticle']{kazemi-etal-2022-lambada:,  author = {Seyed Mehran Kazemi and Najoung Kim and Deepti Bhatia and Xinyuan Xu and Deepak Ramachandran},  booktitle = {ArXiv},  journal = {ArXiv},  title = {LAMBADA: Backward Chaining for Automated Reasoning in Natural Language},  volume = {abs/2212.13894},  year = {2022} }
Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases	2021	https://www.semanticscholar.org/paper/e337ed6543c2e6e7e51c312c7d998798fc79fdde	A rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms finds that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts.	maybe	44	Previous literatures show that pre-trained masked language models (MLMs) such as BERT can achieve competitive factual knowledge extraction performance on some datasets, indicating that MLMs can potentially be a reliable knowledge source. In this paper, we conduct a rigorous study to explore the underlying predicting mechanisms of MLMs over different extraction paradigms. By investigating the behaviors of MLMs, we find that previous decent performance mainly owes to the biased prompts which overfit dataset artifacts. Furthermore, incorporating illustrative cases and external contexts improve knowledge prediction mainly due to entity type guidance and golden answer leakage. Our findings shed light on the underlying predicting mechanisms of MLMs, and strongly question the previous conclusion that current MLMs can potentially serve as reliable factual knowledge bases.	e337ed6543c2e6e7e51c312c7d998798fc79fdde	@['JournalArticle', 'Conference']{cao-etal-2021-knowledgeable,  author = {Boxi Cao and Hongyu Lin and Xianpei Han and Le Sun and Lingyong Yan and M. Liao and Tong Xue and Jin Xu},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1860-1874},  title = {Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases},  year = {2021} }
Knowledge Neurons in Pretrained Transformers	2021	https://www.semanticscholar.org/paper/2c871df72c52b58f05447fcb3afc838168d94505	This paper examines the fill-in-the-blank cloze task for BERT and proposes a knowledge attribution method to identify the neurons that express the fact, finding that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts.	yes	53	Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.	2c871df72c52b58f05447fcb3afc838168d94505	@['JournalArticle', 'Conference']{dai-etal-2021-knowledge,  author = {Damai Dai and Li Dong and Y. Hao and Zhifang Sui and Furu Wei},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Knowledge Neurons in Pretrained Transformers},  volume = {abs/2104.08696},  year = {2021} }
Knowledge Enhanced Contextual Word Representations	2019	https://www.semanticscholar.org/paper/bfeb827d06c1a3583b5cc6d25241203a81f6af09	After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation.	seed	435	Contextual word representations, typically trained on unstructured, unlabeled text, do not contain any explicit grounding to real world entities and are often unable to remember facts about those entities. We propose a general method to embed multiple knowledge bases (KBs) into large scale models, and thereby enhance their representations with structured, human-curated knowledge. For each KB, we first use an integrated entity linker to retrieve relevant entity embeddings, then update contextual word representations via a form of word-to-entity attention. In contrast to previous approaches, the entity linkers and self-supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs.	bfeb827d06c1a3583b5cc6d25241203a81f6af09	@['JournalArticle', 'Conference']{peters-etal-2019-knowledge,  author = {Matthew E. Peters and Mark Neumann and IV RobertL.Logan and Roy Schwartz and V. Joshi and Sameer Singh and Noah A. Smith},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {43-54},  title = {Knowledge Enhanced Contextual Word Representations},  year = {2019} }
Knowledge Distillation from Internal Representations	2019	https://www.semanticscholar.org/paper/944e7b64903bde89bfea433203d5a0e774cff354	This paper proposes to distill the internal representations of a large model such as BERT into a simplified version of it, and formulate two ways todistill such representations and various algorithms to conduct the distillation.	seed	94	Knowledge distillation is typically conducted by training a small model (the student) to mimic a large and cumbersome model (the teacher). The idea is to compress the knowledge from the teacher by using its output probabilities as soft-labels to optimize the student. However, when the teacher is considerably large, there is no guarantee that the internal knowledge of the teacher will be transferred into the student; even if the student closely matches the soft-labels, its internal representations may be considerably different. This internal mismatch can undermine the generalization capabilities originally intended to be transferred from the teacher to the student. In this paper, we propose to distill the internal representations of a large model such as BERT into a simplified version of it. We formulate two ways to distill such representations and various algorithms to conduct the distillation. We experiment with datasets from the GLUE benchmark and consistently show that adding knowledge distillation from internal representations is a more powerful method than only using soft-label distillation.	944e7b64903bde89bfea433203d5a0e774cff354	@['JournalArticle', 'Conference']{aguilar-etal-2019-knowledge,  author = {Gustavo Aguilar and Yuan Ling and Y. Zhang and Benjamin Yao and Xing Fan and Edward Guo},  booktitle = {AAAI Conference on Artificial Intelligence},  journal = {ArXiv},  title = {Knowledge Distillation from Internal Representations},  volume = {abs/1910.03723},  year = {2019} }
Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs	2021	http://www.semanticscholar.org/paper/373936d00c4a357579c4d375de0ce439e4e54d5f	This work presents an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries, and develops an effective attribute inference attack which can infer the sensitive attribute of the training data used by the Bert-based APIs.	maybe	1	The collection and availability of big data, combined with advances in pre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive performance of modern natural language processing tasks, ranging from text classification to text generation. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. However, BERTbased APIs have exhibited a series of security and privacy vulnerabilities. For example, prior work has exploited the security issues of the BERT-based APIs through the adversarial examples crafted by the extracted model. However, the privacy leakage problems of the BERT-based APIs through the extracted model have not been well studied. On the other hand, due to the high capacity of BERT-based APIs, the fine-tuned model is easy to be overlearned, but what kind of information can be leaked from the extracted model remains unknown. In this work, we bridge this gap by first presenting an effective model extraction attack, where the adversary can practically steal a BERT-based API (the target/victim model) by only querying a limited number of queries. We further develop an effective attribute inference attack which can infer the sensitive attribute of the training data used by the BERT-based APIs. Our extensive experiments on benchmark datasets under various realistic settings validate the potential vulnerabilities of BERTbased APIs. Moreover, we demonstrate that two promising defense methods become ineffective against our attacks, which calls for more effective defense methods.	373936d00c4a357579c4d375de0ce439e4e54d5f	@None{chen-etal-2021-killing,  author = {Chen Chen and Xuanli He and Lingjuan Lyu and Fangzhao Wu},  title = {Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs},  year = {2021} }
KG-BERT: BERT for Knowledge Graph Completion	2019	http://www.semanticscholar.org/paper/31184789ef4c3084af930b1e0dede3215b4a9240	This work treats triples in knowledge graphs as textual sequences and proposes a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples.	maybe	200	Knowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.	31184789ef4c3084af930b1e0dede3215b4a9240	@['JournalArticle']{yao-etal-2019-kg,  author = {Liang Yao and Chengsheng Mao and Yuan Luo},  booktitle = {ArXiv},  journal = {ArXiv},  title = {KG-BERT: BERT for Knowledge Graph Completion},  volume = {abs/1909.03193},  year = {2019} }
KAMEL : Knowledge Analysis with Multitoken Entities in Language Models	2022	http://www.semanticscholar.org/paper/ceab663aada792b409891c96847ee6e8e18998ed	This work presents a novel Wikidata-based benchmark dataset, KAMEL, for probing relational knowledge in LMs, and shows that even large language models are far from being able to memorize all varieties of relational knowledge that is usually stored knowledge graphs.	maybe	1	Large language models (LMs) have been shown to capture large amounts of relational knowledge from the pre-training corpus. These models can be probed for this factual knowledge by using cloze-style prompts as demonstrated on the LAMA benchmark. However, recent studies have uncovered that results only perform well, because the models are good at performing educated guesses or recalling facts from the training data. We present a novel Wikidata-based benchmark dataset, KAMEL , for probing relational knowledge in LMs. In contrast to previous datasets, it covers a broader range of knowledge, probes for single-, and multi-token entities, and contains facts with literal values. Furthermore, the evaluation procedure is more accurate, since the dataset contains alternative entity labels and deals with higher-cardinality relations. Instead of performing the evaluation on masked language models, we present results for a variety of recent causal LMs in a few-shot setting. We show that indeed novel models perform very well on LAMA, achieving a promising F1-score of 52.90%, while only achieving 17.62% on KAMEL. Our analysis shows that even large language models are far from being able to memorize all varieties of relational knowledge that is usually stored knowledge graphs.	ceab663aada792b409891c96847ee6e8e18998ed	@None{kalo-2022-kamel,  author = {Jan-Christoph Kalo},  title = {KAMEL : Knowledge Analysis with Multitoken Entities in Language Models},  year = {2022} }
KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation	2020	http://www.semanticscholar.org/paper/00401a56870d838b675866d2406027fcca4f1f2a	A novel way to search for evidence and choose the different large-scale pre-trained models as the backbone for three subtasks is proposed and the results show that the evidence-searching approach improves model performance on commonsense explanation task.	maybe	3	This paper presents our strategies in SemEval 2020 Task 4: Commonsense Validation and Explanation. We propose a novel way to search for evidence and choose the different large-scale pre-trained models as the backbone for three subtasks. The results show that our evidence-searching approach improves model performance on commonsense explanation task. Our team ranks 2nd in subtask C according to human evaluation score.	00401a56870d838b675866d2406027fcca4f1f2a	@['JournalArticle']{wan-huang-2020-kalm,  author = {Jiajing Wan and Xinting Huang},  booktitle = {International Workshop on Semantic Evaluation},  pages = {543-550},  title = {KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation},  year = {2020} }
JUSTers at SemEval-2020 Task 4: Evaluating Transformer Models against Commonsense Validation and Explanation	2020	http://www.semanticscholar.org/paper/caea497c6a199c002e32b960784611a38806247b	This paper evaluates five pre-trained Transformer-based language models with various sizes against the three proposed subtasks in the Commonsense Validation and Explanation task, which is part of SemEval2020.	maybe	9	In this paper, we describe our team’s (JUSTers) effort in the Commonsense Validation and Explanation (ComVE) task, which is part of SemEval2020. We evaluate five pre-trained Transformer-based language models with various sizes against the three proposed subtasks. For the first two subtasks, the best accuracy levels achieved by our models are 92.90% and 92.30%, respectively, placing our team in the 12th and 9th places, respectively. As for the last subtask, our models reach 16.10 BLEU score and 1.94 human evaluation score placing our team in the 5th and 3rd places according to these two metrics, respectively. The latter is only 0.16 away from the 1st place human evaluation score.	caea497c6a199c002e32b960784611a38806247b	@['JournalArticle']{fadel-etal-2020-justers,  author = {A. Fadel and M. Al-Ayyoub and E. Cambria},  booktitle = {International Workshop on Semantic Evaluation},  pages = {535-542},  title = {JUSTers at SemEval-2020 Task 4: Evaluating Transformer Models against Commonsense Validation and Explanation},  year = {2020} }
Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation for Open-Domain Dialogue	2021	http://www.semanticscholar.org/paper/bd16d3d5f4f237bd76395b17e56cde3f01a41584	These are the first results demonstrating that few-shot semantic prompt-based learning can create NLGs that generalize to new domains, and produce high-quality, semantically-controlled, conversational responses directly from meaning representations.	yes	1		bd16d3d5f4f237bd76395b17e56cde3f01a41584	@['JournalArticle']{reed-etal-2021-jurassic,  author = {Lena I. Reed and Cecilia Li and Angela Ramirez and Liren Wu and M. Walker},  booktitle = {International Workshop on Spoken Dialogue Systems Technology},  pages = {99-119},  title = {Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation for Open-Domain Dialogue},  year = {2021} }
Joint processing of linguistic properties in brains and language models	2022	http://www.semanticscholar.org/paper/a103da21d92a715cdb0e7f42eb90d505e2a81986		maybe	0	Language models have been shown to be very effective in predicting brain recordings of subjects experiencing complex language stimuli. For a deeper understanding of this alignment, it is important to understand the alignment between the detailed processing of linguistic information by the human brain versus language models. In NLP, linguistic probing tasks have revealed a hierarchy of information processing in neural language models that progresses from simple to complex with an increase in depth. On the other hand, in neuroscience, the strongest alignment with high-level language brain regions has consistently been observed in the middle layers. These ﬁndings leave an open question as to what linguistic information actually underlies the observed alignment between brains and language models. We investigate this question via a direct approach, in which we eliminate information related to speciﬁc linguistic properties in the language model representations and observe how this intervention affects the alignment with fMRI brain recordings obtained while participants listened to a story. We investigate a range of linguistic properties (surface, syntactic and semantic) and ﬁnd that the elimination of each one results in a signiﬁcant decrease in brain alignment across all layers of a language model. These ﬁndings provide direct evidence for the role of speciﬁc linguistic information in the alignment between brain and language models, and opens new avenues for mapping the joint information processing in both systems.	a103da21d92a715cdb0e7f42eb90d505e2a81986	@['JournalArticle']{oota-etal-2022-joint,  author = {S. Oota and Manish Gupta and Mariya Toneva},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Joint processing of linguistic properties in brains and language models},  volume = {abs/2212.08094},  year = {2022} }
John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs	2021	http://www.semanticscholar.org/paper/38cfcd4681cf85ab507ec0586c753182a4c8eecb	This work investigates whether pre-trained language models encode IC bias and use it at inference time, and finds that to be the case, albeit to different degrees, for three distinct PLM architectures.	maybe	3	Some interpersonal verbs can implicitly attribute causality to either their subject or their object and are therefore said to carry an implicit causality (IC) bias. Through this bias, causal links can be inferred from a narrative, aiding language comprehension. We investigate whether pre-trained language models (PLMs) encode IC bias and use it at inference time. We find that to be the case, albeit to different degrees, for three distinct PLM architectures. However, causes do not always need to be implicit—when a cause is explicitly stated in a subordinate clause, an incongruent IC bias associated with the verb in the main clause leads to a delay in human processing. We hypothesize that the temporary challenge humans face in integrating the two contradicting signals, one from the lexical semantics of the verb, one from the sentence-level semantics, would be reflected in higher error rates for models on tasks dependent on causal links. The results of our study lend support to this hypothesis, suggesting that PLMs tend to prioritize lexical patterns over higher-order signals.	38cfcd4681cf85ab507ec0586c753182a4c8eecb	@['JournalArticle']{kementchedjhieva-etal-2021-john,  author = {Yova Kementchedjhieva and M. Anderson and Anders Søgaard},  booktitle = {Findings},  journal = {ArXiv},  title = {John praised Mary because _he_? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs},  volume = {abs/2106.01060},  year = {2021} }
John is 50 years old, can his son be 65? Evaluating NLP Models' Understanding of Feasibility	2022	http://www.semanticscholar.org/paper/4f8ae9cdbae875f477aec5ae648f2ad8efc754c6	This work introduces FeasibilityQA, a question-answering dataset involving binary classiﬁcation (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility, and shows that even state-of-the-art models such as GPT-3 struggle to answer the feasibility questions correctly.	maybe	0	In current NLP research, large-scale language models and their abilities are widely being discussed. Some recent works have also found notable failures of these models. Often these failure examples involve complex reasoning abilities. This work focuses on a simple commonsense ability, reasoning about when an action (or its effect) is feasible. We introduce FeasibilityQA, a question-answering dataset involving binary classiﬁcation (BCQ) and multi-choice multi-correct questions (MCQ) that test understanding of feasibility. We show that even state-of-the-art models such as GPT-3 struggle to answer the feasibility questions correctly. Speciﬁcally, on (MCQ, BCQ) questions, GPT-3 achieves accuracy of just (19%, 62%) and (25%, 64%) in zero-shot and few-shot settings, respectively. We also evaluate models by providing relevant knowledge statements required to answer the question and ﬁnd that the additional knowledge leads to a 7% gain in performance, but the overall performance still remains low. These results make one wonder how much commonsense knowledge about action feasibility is encoded in GPT-3 and how well the model can reason about it.	4f8ae9cdbae875f477aec5ae648f2ad8efc754c6	@['JournalArticle']{gupta-etal-2022-"john,  author = {Himanshu Gupta and Neeraj Varshney and Swaroop Mishra and Kuntal Kumar Pal and Saurabh Arjun Sawant and Kevin Scaria and Siddharth Goyal and Chitta Baral},  booktitle = {ArXiv},  journal = {ArXiv},  title = {"John is 50 years old, can his son be 65?" Evaluating NLP Models' Understanding of Feasibility},  volume = {abs/2210.07471},  year = {2022} }
It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners	2020	https://www.semanticscholar.org/paper/a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3	This work shows that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller, and identifies key factors required for successful natural language understanding with small language models.	maybe	400	When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance. However, enormous amounts of compute are required for training and applying such big models, resulting in a large carbon footprint and making it difficult for researchers and practitioners to use them. We show that performance similar to GPT-3 can be obtained with language models that are much “greener” in that their parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain a task description, combined with gradient-based optimization; exploiting unlabeled data gives further improvements. We identify key factors required for successful natural language understanding with small language models.	a6a83754a0d1e9a8e41c1e9bbdbca32d3b9d1fd3	@['JournalArticle', 'Conference']{schick-schütze-2020-it’s,  author = {Timo Schick and Hinrich Schütze},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {It’s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners},  volume = {abs/2009.07118},  year = {2020} }
It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning	2021	http://www.semanticscholar.org/paper/64902a5077ee68011cd467398dbb66511e8e891a	This work designs a simple approach to commonsense reasoning which trains a linear classiﬁer with weights of multi-head attention as features and demonstrates that most of the performance is given by the same small subset of attention heads for all studied languages, which provides evidence of universal reasoning capabilities in multilingual encoders.	yes	5	Commonsense reasoning is one of the key problems in natural language processing, but the relative scarcity of labeled data holds back the progress for languages other than English. Pretrained cross-lingual models are a source of powerful language-agnostic representations, yet their inherent reasoning capabilities are still actively studied. In this work, we design a simple approach to commonsense reasoning which trains a linear classiﬁer with weights of multi-head attention as features. To evaluate this approach, we create a multilingual Winograd Schema corpus by processing several datasets from prior work within a standardized pipeline and measure cross-lingual generalization ability in terms of out-of-sample performance. The method performs competitively with recent supervised and unsupervised approaches for commonsense reasoning, even when applied to other languages in a zero-shot manner. Also, we demonstrate that most of the performance is given by the same small subset of attention heads for all studied languages, which provides evidence of universal reasoning capabilities in multilingual encoders.	64902a5077ee68011cd467398dbb66511e8e891a	@['JournalArticle']{tikhonov-ryabinin-2021-it’s,  author = {Alexey Tikhonov and Max Ryabinin},  booktitle = {Findings},  journal = {ArXiv},  title = {It’s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning},  volume = {abs/2106.12066},  year = {2021} }
Isotropy in the Contextual Embedding Space: Clusters and Manifolds	2021	https://www.semanticscholar.org/paper/2a8bb26654c015f663670d1e0745f870071d5507		yes	32		2a8bb26654c015f663670d1e0745f870071d5507	@['JournalArticle']{cai-etal-2021-isotropy,  author = {Xingyu Cai and Jiaji Huang and Yu-Lan Bian and Kenneth Ward Church},  booktitle = {International Conference on Learning Representations},  title = {Isotropy in the Contextual Embedding Space: Clusters and Manifolds},  year = {2021} }
Isotropic Representation Can Improve Dense Retrieval	2022	http://www.semanticscholar.org/paper/9b8da1509ea40dc7291d05a2cb51507b61fa3228	This work first shows that BERT-based DR also follows an anisotropic distribution, and introduces unsupervised post-processing methods of Normalizing Flow and whitening, and develops token-wise method in addition to the sequence- wise method for applying the post- processing methods to the representations of dense retrieval models to effectively enhance the representations to be isotropic.	maybe	1	The recent advancement in language representation modeling has broadly affected the design of dense retrieval models. In particu-lar, many of the high-performing dense retrieval models evaluate representations of query and document using BERT, and subse-quently apply a cosine-similarity based scoring to determine the relevance. BERT representations, however, are known to follow an anisotropic distribution of a narrow cone shape and such an anisotropic distribution can be undesirable for the cosine-similarity based scoring. In this work, we first show that BERT-based DR also follows an anisotropic distribution. To cope with the problem, we introduce unsupervised post-processing methods of Normalizing Flow and whitening, and develop token-wise method in addition to the sequence-wise method for applying the post-processing methods to the representations of dense retrieval models. We show that the proposed methods can effectively enhance the representations to be isotropic, then we perform experiments with ColBERT and RepBERT to show that the performance (NDCG at 10) of document re-ranking can be improved by 5.17% ∼ 8.09% for ColBERT and 6.88% ∼ 22.81% for RepBERT. To examine the potential of isotropic representation for improving the robustness of DR models, we investigate out-of-distribution tasks where the test dataset differs from the training dataset. The results show that isotropic representation can achieve a generally improved performance. For instance, when training dataset is MS-MARCO and test dataset is Robust04, isotropy post-processing can improve the baseline performance by up to 24.98%. Furthermore, we show that an isotropic model trained with an out-of-distribution dataset can even outperform a baseline model trained with the in-distribution dataset. 1	9b8da1509ea40dc7291d05a2cb51507b61fa3228	@['JournalArticle']{jung-etal-2022-isotropic,  author = {Euna Jung and J. Park and Jaekeol Choi and Sungyoon Kim and Wonjong Rhee},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Isotropic Representation Can Improve Dense Retrieval},  volume = {abs/2209.00218},  year = {2022} }
IsoScore: Measuring the Uniformity of Embedding Space Utilization	2021	http://www.semanticscholar.org/paper/b12d394f0d2806f8550cf2a0d07a27880664785a	IsoScore is proposed: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space and is used to challenge a number of recent conclusions in the NLP literature that have been derived using brittle metrics of isotropy.	maybe	2	The recent success of distributed word representations has led to an increased interest in analyzing the properties of their spatial distribution. Several studies have suggested that contextualized word embedding models do not isotropically project tokens into vector space. However, current methods designed to measure isotropy, such as average random cosine similarity and the partition score, have not been thoroughly analyzed and are not appropriate for measuring isotropy. We propose IsoScore: a novel tool that quantifies the degree to which a point cloud uniformly utilizes the ambient vector space. Using rigorously designed tests, we demonstrate that IsoScore is the only tool available in the literature that accurately measures how uniformly distributed variance is across dimensions in vector space. Additionally, we use IsoScore to challenge a number of recent conclusions in the NLP literature that have been derived using brittle metrics of isotropy. We caution future studies from using existing tools to measure isotropy in contextualized embedding space as resulting conclusions will be misleading or altogether inaccurate.	b12d394f0d2806f8550cf2a0d07a27880664785a	@['JournalArticle']{rudman-etal-2021-isoscore:,  author = {W. Rudman and Nate Gillman and T. Rayne and Carsten Eickhoff},  booktitle = {Findings},  pages = {3325-3339},  title = {IsoScore: Measuring the Uniformity of Embedding Space Utilization},  year = {2021} }
IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization	2020	http://www.semanticscholar.org/paper/2c2cc3decc0d5091965975eb4bb6f5dc802bcbf7	This paper analyzes the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and proposes a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropics representations in fine-tuning by dynamically penalizing dominating principal components.	maybe	11	Fine-tuning pre-trained language models (PTLMs), such as BERT and its better variant RoBERTa, has been a common practice for advancing performance in natural language understanding (NLU) tasks. Recent advance in representation learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings can significantly improve performance on downstream tasks with faster convergence and better generalization. The isotropy of the pre-trained embeddings in PTLMs, however, is relatively under-explored. In this paper, we analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with straightforward visualization, and point out two major issues: high variance in their standard deviation, and high correlation between different dimensions. We also propose a new network regularization method, isotropic batch normalization (IsoBN) to address the issues, towards learning more isotropic representations in fine-tuning by dynamically penalizing dominating principal components. This simple yet effective fine-tuning method yields about 1.0 absolute increment on the average of seven NLU tasks.	2c2cc3decc0d5091965975eb4bb6f5dc802bcbf7	@['JournalArticle', 'Conference']{zhou-etal-2020-isobn:,  author = {Wenxuan Zhou and Bill Yuchen Lin and Xiang Ren},  booktitle = {AAAI Conference on Artificial Intelligence},  journal = {ArXiv},  title = {IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization},  volume = {abs/2005.02178},  year = {2020} }
Is This Abstract Generated by AI? A Research for the Gap between AI-generated Scientific Text and Human-written Scientific Text	2023	https://www.semanticscholar.org/paper/165224b81f46fbda2140a4b656991287e96d3548	There exists a “writing style” gap between AI-generated scientific text and human-written scientific text, which suggests that while AI has the potential to generate scientific content that is as accurate as human- written content, there is still a gap in terms of depth and overall quality.	maybe	0	BACKGROUND: Recent neural language models have taken a significant step forward in producing remarkably controllable, fluent, and grammatical text. Although some recent works have found that AI-generated text is not distinguishable from human-authored writing for crowd-sourcing workers, there still exist errors in AI-generated text which are even subtler and harder to spot. METHOD: In this paper, we investigate the gap between scientific content generated by AI and written by humans. Specifically, we first adopt several publicly available tools or models to investigate the performance for detecting GPT-generated scientific text. Then we utilize features from writing style to analyze the similarities and differences between the two types of content. Furthermore, more complex and deep perspectives, such as consistency, coherence, language redundancy, and factual errors, are also taken into consideration for in-depth analysis. RESULT: The results suggest that while AI has the potential to generate scientific content that is as accurate as human-written content, there is still a gap in terms of depth and overall quality. AI-generated scientific content is more likely to contain errors in language redundancy and factual issues. CONCLUSION: We find that there exists a “writing style” gap between AI-generated scientific text and human-written scientific text. Moreover, based on the analysis result, we summarize a series of model-agnostic or distribution-agnostic features, which could be utilized to unknown or novel domain distribution and different generation methods. Future research should focus on not only improving the capabilities of AI models to produce high-quality content but also examining and addressing ethical and security concerns related to the generation and the use of AI-generated content.	165224b81f46fbda2140a4b656991287e96d3548	@['JournalArticle']{ma-etal-2023-is,  author = {Yongqiang Ma and Jiawei Liu and Fan Yi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Is This Abstract Generated by AI? A Research for the Gap between AI-generated Scientific Text and Human-written Scientific Text},  volume = {abs/2301.10416},  year = {2023} }
Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?	2022	http://www.semanticscholar.org/paper/bc16284f517dd0011dcf64ea1c8fe6d6576494a4	This work explores one facet of the question whether state-of-the-art NLP models exhibit elementary mechanisms known from human cognition by de-signed experimental settings in which each element from the original studies was mapped to a component of language models.	maybe	0	In recent years, deep neural language models have made strong progress in various NLP tasks. This work explores one facet of the question whether state-of-the-art NLP models exhibit elementary mechanisms known from human cognition. The exploration is focused on a relatively primitive mechanism for which there is a lot of evidence from various psycholinguistic experiments with infants. The computation of “abstract sameness relations” is assumed to play an important role in human language acquisition and processing, especially in learning more complex grammar rules. In order to investigate this mechanism in BERT and other pre-trained language models (PLMs), the experiment designs from studies with infants were taken as the starting point. On this basis, we de-signed experimental settings in which each element from the original studies was mapped to a component of language models. Even though the task in our experiments was relatively simple, the results suggest that the cognitive faculty of computing abstract sameness relations is stronger in infants than in all investigated PLMs. Our implementation is available on https://github.com/ lmthoma/computation_ASRs	bc16284f517dd0011dcf64ea1c8fe6d6576494a4	@['JournalArticle']{thoma-roth-2022-is,  author = {Lukas Thoma and Benjamin Roth},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Is the Computation of Abstract Sameness Relations Human-Like in Neural Language Models?},  volume = {abs/2205.06149},  year = {2022} }
Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation	2020	https://www.semanticscholar.org/paper/575ac3f36e9fddeb258e2f639e26a6a7ec35160a	This work empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks, and results show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance.	seed	41	Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers’ representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?	575ac3f36e9fddeb258e2f639e26a6a7ec35160a	@['JournalArticle', 'Conference']{glavas-vulic-2020-is,  author = {Goran Glavas and Ivan Vulic},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {3090-3104},  title = {Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation},  year = {2020} }
Is Self-Attention Powerful to Learn Code Syntax and Semantics?	2022	http://www.semanticscholar.org/paper/a41b2d46b462b4af3dfada86c355a07814ae05db	An alternative approach for pre-training models is shown, which makes fully use of the current pre- training strategy, i.e, MLM, to learn code syntax and semantics, instead of combining features from different code data formats, e.g., data-flow, running-time states, and program outputs.	maybe	0	Pre-trained language models for programming languages have shown a powerful ability on processing many Software Engineering (SE) tasks, e.g., program synthesis, code completion, and code search. However, it remains to be seen what is behind their success. Recent studies have examined how pre-trained models can effectively learn syntax information based on Abstract Syntax Trees. In this paper, we figure out what role the self-attention mechanism plays in understanding code syntax and semantics based on AST and static analysis. We focus on a well-known representative code model, CodeBERT, and study how it can learn code syntax and semantics by the self-attention mechanism and Masked Language Modelling (MLM) at the token level. We propose a group of probing tasks to analyze CodeBERT. Based on AST and static analysis, we establish the relationships among the code tokens. First, Our results show that CodeBERT can acquire syntax and semantics knowledge through self-attention and MLM. Second, we demonstrate that the self-attention mechanism pays more attention to dependence-relationship tokens than to other tokens. Different attention heads play different roles in learning code semantics; we show that some of them are weak at encoding code semantics. Different layers have different competencies to represent different code properties. Deep CodeBERT layers can encode the semantic information that requires some complex inference in the code context. More importantly, we show that our analysis is helpful and leverage our conclusions to improve CodeBERT. We show an alternative approach for pre-training models, which makes fully use of the current pre-training strategy, i.e, MLM, to learn code syntax and semantics, instead of combining features from different code data formats, e.g., data-flow, running-time states, and program outputs.	a41b2d46b462b4af3dfada86c355a07814ae05db	@['JournalArticle']{ma-etal-2022-is,  author = {Wei Ma and Mengjie Zhao and Xiaofei Xie and Q. Hu and Shangqing Liu and Jiexin Zhang and Wenhan Wang and Yang Liu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Is Self-Attention Powerful to Learn Code Syntax and Semantics?},  volume = {abs/2212.10017},  year = {2022} }
Is Neural Language Model Perplexity Related to Readability?	2020	http://www.semanticscholar.org/paper/77c166d675014c9336fedf1b7ab22bdec6d9b37c	The findings suggest that this correlation between Neural Language Model perplexity and sentence readability is actually quite weak and the two metrics are affected by different linguistic phenomena.	maybe	1	This paper explores the relationship between Neural Language Model (NLM) perplexity and sentence readability. Start-ing from the evidence that NLMs implicitly acquire sophisticated linguistic knowledge from a huge amount of training data, our goal is to investigate whether perplexity is affected by linguistic features used to automatically assess sentence readability and if there is a correlation between the two metrics. Our ﬁndings suggest that this correlation is actually quite weak and the two metrics are affected by different linguistic phenomena. 1	77c166d675014c9336fedf1b7ab22bdec6d9b37c	@['JournalArticle']{miaschi-etal-2020-is,  author = {Alessio Miaschi and Chiara Alzetta and Dominique Brunato and F. Dell’Orletta and Giulia Venturi},  booktitle = {Italian Conference on Computational Linguistics},  journal = {Proceedings of the Seventh Italian Conference on Computational Linguistics CLiC-it 2020},  title = {Is Neural Language Model Perplexity Related to Readability?},  year = {2020} }
Is neural language acquisition similar to natural? A chronological probing study	2022	http://www.semanticscholar.org/paper/164d64f0dcb34fec849ccdafe8466f2ac5a5665d	The results show that both language models demonstrate capabilities to capture various features from various levels of language, including morphology, syntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy.	yes	1	The probing methodology allows one to obtain a partial representation of linguistic phenomena stored in the inner layers of the neural network, using external classifiers and statistical analysis. Pre-trained transformer-based language models are widely used both for natural language understanding (NLU) and natural language generation (NLG) tasks making them most commonly used for downstream applications. However, little analysis was carried out, whether the models were pre-trained enough or contained knowledge correlated with linguistic theory. We are presenting the chronological probing study of transformer English models such as MultiBERT and T5. We sequentially compare the information about the language learned by the models in the process of training on corpora. The results show that 1) linguistic information is acquired in the early stages of training 2) both language models demonstrate capabilities to capture various features from various levels of language, including morphology, syntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy. We also introduce the open-source framework for chronological probing research, compatible with other transformer-based models. https://github.com/EkaterinaVoloshina/chronological_probing	164d64f0dcb34fec849ccdafe8466f2ac5a5665d	@['JournalArticle']{voloshina-etal-2022-is,  author = {E. Voloshina and Oleg Serikov and Tatiana Shavrina},  booktitle = {Computational Linguistics and Intellectual Technologies},  journal = {ArXiv},  title = {Is neural language acquisition similar to natural? A chronological probing study},  volume = {abs/2207.00560},  year = {2022} }
Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions	2022	https://www.semanticscholar.org/paper/5c8cb5d39aa8531fcb5962821ac404e4a0e109d1	It is found that only the largest model has enough world knowledge to play the game of Twenty Questions well, although it still has difficulties with the shape and size of objects.	maybe	0	Researchers often use games to analyze the abilities of Artificial Intelligence models. In this work, we use the game of Twenty Questions to study the world knowledge of language models. Despite its simplicity for humans, this game requires a broad knowledge of the world to answer yes/no questions. We evaluate several language models on this task and find that only the largest model has enough world knowledge to play it well, although it still has difficulties with the shape and size of objects. We also present a new method to improve the knowledge of smaller models by leveraging external information from the web. Finally, we release our dataset and Twentle, a website to interactively test the knowledge of language models by playing Twenty Questions.	5c8cb5d39aa8531fcb5962821ac404e4a0e109d1	@None{bruyn-etal-2022-is,  author = {Maxime De Bruyn and Ehsan Lotfi and Jeska Buhmann and Walter Daelemans},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  title = {Is It Smaller Than a Tennis Ball? Language Models Play the Game of Twenty Questions},  year = {2022} }
Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models	2021	http://www.semanticscholar.org/paper/f6e28a79e90e517c6839d5d52a189bba9f54e13c	This work addresses specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding.	maybe	5	Coherent discourse is distinguished from a mere collection of utterances by the satisfaction of a diverse set of constraints, for example choice of expression, logical relation between denoted events, and implicit compatibility with world-knowledge. Do neural language models encode such constraints? We design an extendable set of test suites addressing different aspects of discourse and dialogue coherence. Unlike most previous coherence evaluation studies, we address specific linguistic devices beyond sentence order perturbations, which allow for a more fine-grained analysis of what constitutes coherence and what neural models trained on a language modelling objective are capable of encoding. Extending the targeted evaluation paradigm for neural language models (Marvin and Linzen, 2018) to phenomena beyond syntax, we show that this paradigm is equally suited to evaluate linguistic qualities that contribute to the notion of coherence.	f6e28a79e90e517c6839d5d52a189bba9f54e13c	@['JournalArticle', 'Conference']{beyer-etal-2021-is,  author = {Anne Beyer and Sharid Lo'aiciga and David Schlangen},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4164-4173},  title = {Is Incoherence Surprising? Targeted Evaluation of Coherence Prediction from Language Models},  year = {2021} }
Is GPT-3 all you need for machine learning for chemistry?	2022	http://www.semanticscholar.org/paper/cd155729180ea707dea251f8e9654db241ffd808	This work analyzes whether one of the largest pre-trained LLMs, GPT-3, can be directly used for chemistry applications by fine-tuning on only a few data points from a chemistry dataset, i.e., without pre-training on a chemistry-specific dataset.	maybe	0	Pre-trained large language models (LLMs) are a powerful platform for building custom models for various applications. They have also found success in chemistry, but typically need to be pre-trained on large chemistry datasets such as reaction databases or protein sequences. In this work, we analyze whether one of the largest pre-trained LLMs, GPT-3, can be directly used for chemistry applications by fine-tuning on only a few data points from a chemistry dataset, i.e., without pre-training on a chemistry-specific dataset. We show that GPT-3 can achieve performance competing with baselines on three case studies (polymers, metal-organic frameworks, photoswitches) with representations as simple as the chemical name in both classification and regression settings. Moreover, we demonstrate that GPT-3 can also be fine-tuned for inverse design tasks, i.e., to generate a molecule with properties as specified in a prompt.	cd155729180ea707dea251f8e9654db241ffd808	@None{jablonka-2022-is,  author = {K. Jablonka},  title = {Is GPT-3 all you need for machine learning for chemistry?},  year = {2022} }
Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective	2022	https://www.semanticscholar.org/paper/714beceee04a4ab07a971ff69961972b2e740eb5		maybe	0	Are large language models (LLMs) like GPT-3 psychologically safe? In this work, we design unbiased prompts to evaluate LLMs systematically from a psychological perspective. Firstly, we test the personality traits of three different LLMs with Short Dark Triad (SD-3) and Big Five Inventory (BFI). We ﬁnd all of them show higher scores on SD-3 than the human average, indicating a relatively darker personality. Furthermore, LLMs like InstructGPT and FLAN-T5, which are ﬁne-tuned with safety metrics, do not necessarily have more positive personalities. They score higher on Machiavellianism and Narcissism than GPT-3. Secondly, we test the LLMs in GPT-3 series on well-being tests to study the impact of ﬁne-tuning with more training data. Interestingly, we observe a continuous increase in well-being scores from GPT-3 to InstructGPT. Following the observations, we show that instruction-ﬁnetune FLAN-T5 with positive answers in BFI can effectively improve the model from a psychological perspective. Finally, we call on the community to evaluate and improve LLMs’ safety systematically instead of at the sentence level only.	714beceee04a4ab07a971ff69961972b2e740eb5	@['JournalArticle']{li-etal-2022-is,  author = {Xingxuan Li and Yutong Li and Linlin Liu and Lidong Bing and Shafiq R. Joty},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Is GPT-3 a Psychopath? Evaluating Large Language Models from a Psychological Perspective},  volume = {abs/2212.10529},  year = {2022} }
Is GPT-3 a Good Data Annotator?	2022	http://www.semanticscholar.org/paper/7547589d925ce4782159b73d017b27d55b5f41c8	This analysis aims to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP by comparing it with traditional data annotation methods and analyzing its output on a range of tasks.	maybe	1	GPT-3 (Generative Pre-trained Transformer 3) is a large-scale autoregressive language model developed by OpenAI, which has demonstrated impressive few-shot performance on a wide range of natural language processing (NLP) tasks. Hence, an intuitive application is to use it for data annotation. In this paper, we investigate whether GPT-3 can be used as a good data annotator for NLP tasks. Data annotation is the process of labeling data that could be used to train machine learning models. It is a crucial step in the development of NLP systems, as it allows the model to learn the re-lationship between the input data and the de-sired output. Given the impressive language capabilities of GPT-3, it is natural to wonder whether it can be used to effectively annotate data for NLP tasks. In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks. Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.	7547589d925ce4782159b73d017b27d55b5f41c8	@['JournalArticle']{ding-etal-2022-is,  author = {Bosheng Ding and Chengwei Qin and Linlin Liu and Lidong Bing and Shafiq R. Joty and Boyang Li},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Is GPT-3 a Good Data Annotator?},  volume = {abs/2212.10450},  year = {2022} }
Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment	2019	https://www.semanticscholar.org/paper/ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2	TextFooler is presented, a simple but strong baseline to generate adversarial text that outperforms previous attacks by success rate and perturbation rate, and is utility-preserving and efficient, which generates adversarialtext with computational complexity linear to the text length.	seed	439	Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.1	ae04f3d011511ad8ed7ffdf9fcfb7f11e6899ca2	@['JournalArticle', 'Conference']{jin-etal-2019-is,  author = {Di Jin and Zhijing Jin and Joey Tianyi Zhou and Peter Szolovits},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {8018-8025},  title = {Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment},  year = {2019} }
Is Attention Interpretable?	2019	https://www.semanticscholar.org/paper/135112c7ba1762d65f39b1a61777f26ae4dfd8ad	While attention noisily predicts input components’ overall importance to a model, it is by no means a fail-safe indicator, and there are many ways in which this does not hold, where gradient-based rankings of attention weights better predict their effects than their magnitudes.	seed	342	Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components’ representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components’ overall importance to a model, it is by no means a fail-safe indicator.1	135112c7ba1762d65f39b1a61777f26ae4dfd8ad	@['JournalArticle', 'Conference']{serrano-smith-2019-is,  author = {Sofia Serrano and Noah A. Smith},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Is Attention Interpretable?},  volume = {abs/1906.03731},  year = {2019} }
Is Attention Explanation? An Introduction to the Debate	2022	http://www.semanticscholar.org/paper/e969778bced13a339f3d0465cea4e10c489ee1cc	A clear overview of the insights on the debate on the explanatory power of attention in neural networks is provided by critically confronting works from these different areas, and the main challenges spotted in these areas are summed up.	yes	9	The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.	e969778bced13a339f3d0465cea4e10c489ee1cc	@['JournalArticle', 'Conference', 'Review']{bibal-etal-2022-is,  author = {Adrien Bibal and Rémi Cardon and David Alfter and Rodrigo Wilkens and Xiaoou Wang and Thomas François and Patrick Watrin},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3889-3900},  title = {Is Attention Explanation? An Introduction to the Debate},  year = {2022} }
Is “My Favorite New Movie” My Favorite Movie? Probing the Understanding of Recursive Noun Phrases	2021	http://www.semanticscholar.org/paper/08be84ef68d4567ec6be58dc9326f7ec2d68f36d	This work introduces the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs and probes the models for relevant linguistic features that can be learned from these tasks.	maybe	0	Recursive noun phrases (NPs) have interesting semantic properties. For example, “my favorite new movie” is not necessarily my favorite movie, whereas “my new favorite movie” is. This is common sense to humans, yet it is unknown whether language models have such knowledge. We introduce the Recursive Noun Phrase Challenge (RNPC), a dataset of three textual inference tasks involving textual entailment and event plausibility comparison, precisely targeting the understanding of recursive NPs. When evaluated on RNPC, state-of-the-art Transformer models only perform around chance. Still, we show that such knowledge is learnable with appropriate data. We further probe the models for relevant linguistic features that can be learned from our tasks, including modifier semantic category and modifier scope. Finally, models trained on RNPC achieve strong zero-shot performance on an extrinsic Harm Detection evaluation task, showing the usefulness of the understanding of recursive NPs in downstream applications.	08be84ef68d4567ec6be58dc9326f7ec2d68f36d	@['JournalArticle', 'Conference']{lyu-etal-2021-is,  author = {QING LYU and Hua Zheng and Daoxin Li and Li Zhang and Marianna Apidianaki and Chris Callison-Burch},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {5286-5302},  title = {Is “My Favorite New Movie” My Favorite Movie? Probing the Understanding of Recursive Noun Phrases},  year = {2021} }
Investigating Transferability in Pretrained Language Models	2020	http://www.semanticscholar.org/paper/0f1107494c77a6aa559c52f8b37aede31398e334	This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks, and the benefit of using pretrained parameters for a layer varies dramatically with dataset size.	maybe	37	How does language model pretraining help transfer learning? We consider a simple ablation technique for determining the impact of each pretrained layer on transfer task performance. This method, partial reinitialization, involves replacing different layers of a pretrained model with random weights, then finetuning the entire model on the transfer task and observing the change in performance. This technique reveals that in BERT, layers with high probing performance on downstream GLUE tasks are neither necessary nor sufficient for high accuracy on those tasks. Furthermore, the benefit of using pretrained parameters for a layer varies dramatically with finetuning dataset size: parameters that provide tremendous performance improvement when data is plentiful may provide negligible benefits in data-scarce settings. These results reveal the complexity of the transfer learning process, highlighting the limitations of methods that operate on frozen models or single data samples.	0f1107494c77a6aa559c52f8b37aede31398e334	@['JournalArticle']{tamkin-etal-2020-investigating,  author = {Alex Tamkin and Trisha Singh and D. Giovanardi and Noah D. Goodman},  booktitle = {Findings},  pages = {1393-1401},  title = {Investigating Transferability in Pretrained Language Models},  year = {2020} }
Investigating the Performance of Transformer-Based NLI Models on Presuppositional Inferences	2022	http://www.semanticscholar.org/paper/90a35136bfdb158370cf5e7e6bc70fdb77fb5c8b	The findings suggest that NLI-trained transformer models seem to be exploiting specific structural and lexical cues as opposed to performing some kind of pragmatic reasoning.	yes	0	Presuppositions are assumptions that are taken for granted by an utterance, and identifying them is key to a pragmatic interpretation of language. In this paper, we investigate the capabilities of transformer models to perform NLI on cases involving presupposition. First, we present simple heuristics to create alternative “contrastive” test cases based on the ImpPres dataset and investigate the model performance on those test cases. Second, to better understand how the model is making its predictions, we analyze samples from sub-datasets of ImpPres and examine model performance on them. Overall, our findings suggest that NLI-trained transformer models seem to be exploiting specific structural and lexical cues as opposed to performing some kind of pragmatic reasoning.	90a35136bfdb158370cf5e7e6bc70fdb77fb5c8b	@['JournalArticle', 'Conference']{kabbara-cheung-2022-investigating,  author = {Jad Kabbara and J. C. Cheung},  booktitle = {International Conference on Computational Linguistics},  pages = {779-785},  title = {Investigating the Performance of Transformer-Based NLI Models on Presuppositional Inferences},  year = {2022} }
Investigating the Limitations of the Transformers with Simple Arithmetic Tasks	2021	http://www.semanticscholar.org/paper/2cc3ab9fa41ba2804e301f7eae9598636e62422a	It is found that how a number is represented in its surface form has a strong influence on the model’s accuracy, and this result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement.	maybe	30	The ability to perform arithmetic tasks is a remarkable trait of human intelligence and might form a critical component of more complex reasoning tasks. In this work, we investigate if the surface form of a number has any influence on how sequence-to-sequence language models learn simple arithmetic tasks such as addition and subtraction across a wide range of values. We find that how a number is represented in its surface form has a strong influence on the model’s accuracy. In particular, the model fails to learn addition of five-digit numbers when using subwords (e.g., “32”), and it struggles to learn with character-level representations (e.g., “3 2”). By introducing position tokens (e.g., “3 10e1 2”), the model learns to accurately add and subtract numbers up to 60 digits. We conclude that modern pretrained language models can easily learn arithmetic from very few examples, as long as we use the proper surface representation. This result bolsters evidence that subword tokenizers and positional encodings are components in current transformer designs that might need improvement. Moreover, we show that regardless of the number of parameters and training examples, models cannot learn addition rules that are independent of the length of the numbers seen during training. Code to reproduce our experiments is available at https://github.com/castorini/transformers-arithmetic	2cc3ab9fa41ba2804e301f7eae9598636e62422a	@['JournalArticle']{nogueira-etal-2021-investigating,  author = {Rodrigo Nogueira and Zhiying Jiang and Jimmy J. Li},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Investigating the Limitations of the Transformers with Simple Arithmetic Tasks},  volume = {abs/2102.13019},  year = {2021} }
Investigating the Characteristics of a Transformer in a Few-Shot Setup: Does Freezing Layers in RoBERTa Help?	2022	https://www.semanticscholar.org/paper/382826f569fea7778803f141e9db368cb067a0a0	It is discovered that freezing initial 50% Transformer layers not only reduces training time but also surprisingly improves Macro F1 (upto 8%) when compared to fully trainable layers in few-shot setup and can be generalized to state-of-the-art few- shot text classification techniques, leading to significant reduction in training time while maintaining comparable performance.	maybe	0	Transformer based language models have been widely adopted by industrial and research organisations in developing machine learning applications in the presence of limited annotated data. While these models show remarkable results, their functioning in few-shot settings is still poorly understood. Hence, we perform an investigative study to understand the characteristics of such models fine-tuned in few-shot setups. Specifically, we compare the intermediate layer representations obtained from a few-shot model and a pre-trained language model. We observe that pre-trained and few-shot models show similar representations over initial layers, whereas the later layers show a stark deviation. Based on these observations, we propose to freeze the initial Transformer layers to fine-tune the model in a constrained text classification setup with K annotated data points per class, where K ranges from 8 to 64. In our experiments across six benchmark sentence classification tasks, we discover that freezing initial 50% Transformer layers not only reduces training time but also surprisingly improves Macro F1 (upto 8%) when compared to fully trainable layers in few-shot setup. We also observe that this idea of layer freezing can very well be generalized to state-of-the-art few-shot text classification techniques, like DNNC and LM-BFF, leading to significant reduction in training time while maintaining comparable performance.	382826f569fea7778803f141e9db368cb067a0a0	@None{ingle-etal-2022-investigating,  author = {Digvijay Ingle and Rishabh Tripathi and Ayush Kumar and Kevin Patel and J. Vepa},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  title = {Investigating the Characteristics of a Transformer in a Few-Shot Setup: Does Freezing Layers in RoBERTa Help?},  year = {2022} }
Investigating Representations of Verb Bias in Neural Language Models	2020	http://www.semanticscholar.org/paper/1203369da626380326a5c87bb86df0c4812eb388	DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation, is introduced, showing that larger models perform better than smaller models, and transformer architectures tend to out-perform recurrent architectures even under comparable parameter and training settings.	maybe	8	Languages typically provide more than one grammatical construction to express certain types of messages. A speaker's choice of construction is known to depend on multiple factors, including the choice of main verb -- a phenomenon known as \emph{verb bias}. Here we introduce DAIS, a large benchmark dataset containing 50K human judgments for 5K distinct sentence pairs in the English dative alternation. This dataset includes 200 unique verbs and systematically varies the definiteness and length of arguments. We use this dataset, as well as an existing corpus of naturally occurring data, to evaluate how well recent neural language models capture human preferences. Results show that larger models perform better than smaller models, and transformer architectures (e.g. GPT-2) tend to out-perform recurrent architectures (e.g. LSTMs) even under comparable parameter and training settings. Additional analyses of internal feature representations suggest that transformers may better integrate specific lexical information with grammatical constructions.	1203369da626380326a5c87bb86df0c4812eb388	@['JournalArticle', 'Conference']{hawkins-etal-2020-investigating,  author = {Robert D. Hawkins and T. Yamakoshi and T. Griffiths and A. Goldberg},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4653-4663},  title = {Investigating Representations of Verb Bias in Neural Language Models},  year = {2020} }
Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization	2020	https://www.semanticscholar.org/paper/7ade8a61f267136a5e68316a6ba39d382c90857a	A novel word-learning paradigm is deployed to test BERT’s few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences and finds that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning.	maybe	4	Previous studies investigating the syntactic abilities of deep learning models have not targeted the relationship between the strength of the grammatical generalization and the amount of evidence to which the model is exposed during training. We address this issue by deploying a novel word-learning paradigm to test BERT’s few-shot learning capabilities for two aspects of English verbs: alternations and classes of selectional preferences. For the former, we fine-tune BERT on a single frame in a verbal-alternation pair and ask whether the model expects the novel verb to occur in its sister frame. For the latter, we fine-tune BERT on an incomplete selectional network of verbal objects and ask whether it expects unattested but plausible verb/object pairs. We find that BERT makes robust grammatical generalizations after just one or two instances of a novel word in fine-tuning. For the verbal alternation tests, we find that the model displays behavior that is consistent with a transitivity bias: verbs seen few times are expected to take direct objects, but verbs seen with direct objects are not expected to occur intransitively.	7ade8a61f267136a5e68316a6ba39d382c90857a	@['JournalArticle']{thrush-etal-2020-investigating,  author = {Tristan Thrush and Ethan Gotlieb Wilcox and R. Levy},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {265-275},  title = {Investigating Novel Verb Learning in BERT: Selectional Preference Classes and Alternation-Based Syntactic Generalization},  year = {2020} }
Investigating Memorization of Conspiracy Theories in Text Generation	2021	http://www.semanticscholar.org/paper/5ef0554efe188ef7ab7f89a67bc9c1e3b31eeffb	Results indicate the need for a more thorough review of NLG applications before release and an indepth discussion of the drawbacks of memorization in generative language models.	maybe	2	The adoption of natural language generation (NLG) models can leave individuals vulnerable to the generation of harmful information memorized by the models, such as conspiracy theories. While previous studies examine conspiracy theories in the context of social media, they have not evaluated their presence in the new space of generative language models. In this work, we investigate the capability of language models to generate conspiracy theory text. Specifically, we aim to answer: can we test pretrained generative language models for the memorization and elicitation of conspiracy theories without access to the model’s training data? We highlight the difficulties of this task and discuss it in the context of memorization, generalization, and hallucination. Utilizing a new dataset consisting of conspiracy theory topics and machine-generated conspiracy theories helps us discover that many conspiracy theories are deeply rooted in the pretrained language models. Our experiments demonstrate a relationship between model parameters such as size and temperature and their propensity to generate conspiracy theory text. These results indicate the need for a more thorough review of NLG applications before release and an indepth discussion of the drawbacks of memorization in generative language models.	5ef0554efe188ef7ab7f89a67bc9c1e3b31eeffb	@['JournalArticle', 'Review']{levy-etal-2021-investigating,  author = {Sharon Levy and Michael Stephen Saxon and W. Wang},  booktitle = {Findings},  pages = {4718-4729},  title = {Investigating Memorization of Conspiracy Theories in Text Generation},  year = {2021} }
Investigating Learning Dynamics of BERT Fine-Tuning	2020	http://www.semanticscholar.org/paper/888c3a3788c52d6637d45dc4238691083884589d	It is concluded that BERT fine-tuning mainly changes the attention mode of the last layers and modifies the feature extraction modes of the intermediate and last layers.	maybe	15	The recently introduced pre-trained language model BERT advances the state-of-the-art on many NLP tasks through the fine-tuning approach, but few studies investigate how the fine-tuning process improves the model performance on downstream tasks. In this paper, we inspect the learning dynamics of BERT fine-tuning with two indicators. We use JS divergence to detect the change of the attention mode and use SVCCA distance to examine the change to the feature extraction mode during BERT fine-tuning. We conclude that BERT fine-tuning mainly changes the attention mode of the last layers and modifies the feature extraction mode of the intermediate and last layers. Moreover, we analyze the consistency of BERT fine-tuning between different random seeds and different datasets. In summary, we provide a distinctive understanding of the learning dynamics of BERT fine-tuning, which sheds some light on improving the fine-tuning results.	888c3a3788c52d6637d45dc4238691083884589d	@['JournalArticle']{hao-etal-2020-investigating,  author = {Y. Hao and Li Dong and Furu Wei and Ke Xu},  booktitle = {AACL},  pages = {87-92},  title = {Investigating Learning Dynamics of BERT Fine-Tuning},  year = {2020} }
Investigating Gender Bias in Language Models Using Causal Mediation Analysis	2020	https://www.semanticscholar.org/paper/78d08b8ab4132defffe98ec7f80a51452203f70d	This work proposes a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior, and applies it to analyzing gender bias in pre-trained Transformer language models.	maybe	102	Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms that facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model’s sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.	78d08b8ab4132defffe98ec7f80a51452203f70d	@['JournalArticle']{vig-etal-2020-investigating,  author = {Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and D. Nevo and Yaron Singer and S. Shieber},  booktitle = {Neural Information Processing Systems},  title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},  volume = {33},  year = {2020} }
Investigating Gender Bias in BERT	2020	http://www.semanticscholar.org/paper/36d7998ec3e462e3907c6d3c1d13984234f7e8cc	This work analyzes the gender bias induced by BERT in downstream tasks and proposes an algorithm that finds fine-grained gender directions, i.e., one primary direction for each BERT layer, which obviates the need of realizing gender subspace in multiple dimensions and prevents other crucial information from being omitted.	maybe	51		36d7998ec3e462e3907c6d3c1d13984234f7e8cc	@['JournalArticle']{bhardwaj-etal-2020-investigating,  author = {Rishabh Bhardwaj and Navonil Majumder and Soujanya Poria},  booktitle = {Cognitive Computation},  journal = {Cognitive Computation},  pages = {1008 - 1018},  title = {Investigating Gender Bias in BERT},  volume = {13},  year = {2020} }
Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking	2019	https://www.semanticscholar.org/paper/399308fa54ade9b1362d56628132323489ce50cd	This study proposes an extreme simplification of the entity linking setup that works surprisingly well: simply cast it as a per token classification over the entire entity vocabulary and shows on an entity linking benchmark that this model improves the entity representations over plain BERT.	seed	74	A typical architecture for end-to-end entity linking systems consists of three steps: mention detection, candidate generation and entity disambiguation. In this study we investigate the following questions: (a) Can all those steps be learned jointly with a model for contextualized text-representations, i.e. BERT? (b) How much entity knowledge is already contained in pretrained BERT? (c) Does additional entity knowledge improve BERT’s performance in downstream tasks? To this end we propose an extreme simplification of the entity linking setup that works surprisingly well: simply cast it as a per token classification over the entire entity vocabulary (over 700K classes in our case). We show on an entity linking benchmark that (i) this model improves the entity representations over plain BERT, (ii) that it outperforms entity linking architectures that optimize the tasks separately and (iii) that it only comes second to the current state-of-the-art that does mention detection and entity disambiguation jointly. Additionally, we investigate the usefulness of entity-aware token-representations in the text-understanding benchmark GLUE, as well as the question answering benchmarks SQUAD~V2 and SWAG and also the EN-DE WMT14 machine translation benchmark. To our surprise, we find that most of those benchmarks do not benefit from additional entity knowledge, except for a task with very small training data, the RTE task in GLUE, which improves by 2%.	399308fa54ade9b1362d56628132323489ce50cd	@['JournalArticle']{broscheit-2019-investigating,  author = {Samuel Broscheit},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {Investigating Entity Knowledge in BERT with Simple Neural End-To-End Entity Linking},  volume = {abs/2003.05473},  year = {2019} }
Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs	2019	https://www.semanticscholar.org/paper/3cd331c997e90f737810aad6fcce4d993315189f	It is concluded that a variety of methods is necessary to reveal all relevant aspects of a model’s grammatical knowledge in a given domain.	seed	81	Though state-of-the-art sentence representation models can perform tasks requiring significant knowledge of grammar, it is an open question how best to evaluate their grammatical knowledge. We explore five experimental methods inspired by prior work evaluating pretrained sentence representation models. We use a single linguistic phenomenon, negative polarity item (NPI) licensing, as a case study for our experiments. NPIs like any are grammatical only if they appear in a licensing environment like negation (Sue doesn’t have any cats vs. *Sue has any cats). This phenomenon is challenging because of the variety of NPI licensing environments that exist. We introduce an artificially generated dataset that manipulates key features of NPI licensing for the experiments. We find that BERT has significant knowledge of these features, but its success varies widely across different experimental methods. We conclude that a variety of methods is necessary to reveal all relevant aspects of a model’s grammatical knowledge in a given domain.	3cd331c997e90f737810aad6fcce4d993315189f	@['JournalArticle', 'Conference']{warstadt-etal-2019-investigating,  author = {Alex Warstadt and Yuning Cao and Ioana Grosu and Wei Peng and Hagen Blix and Yining Nie and Anna Alsop and Shikha Bordia and Haokun Liu and Alicia Parrish and Sheng-Fu Wang and Jason Phang and Anhad Mohananey and Phu Mon Htut and Paloma Jeretic and Samuel R. Bowman},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Investigating BERT’s Knowledge of Language: Five Analysis Methods with NPIs},  volume = {abs/1909.02597},  year = {2019} }
Inverse scaling can become U-shaped	2022	https://www.semanticscholar.org/paper/1dbe8253d08574f3ba04d2d519ccec580dccc24c		yes	3	Although scaling language models improves performance on a range of tasks, there are apparently some scenarios where scaling hurts performance. For instance, the Inverse Scaling Prize Round 1 (Perez and McKenzie, 2022) identiﬁed four “inverse scaling” tasks, for which performance gets worse for larger models. These tasks were evaluated on models of up to 280B parameters, trained up to 500 zettaFLOPs of compute. This paper takes a closer look at these four tasks. We evaluate models of up to 540B parameters, trained on ﬁve times more compute than those evaluated in the Inverse Scaling Prize. With this increased range of model sizes and training compute, three out of the four tasks exhibit what we call “U-shaped scaling”— performance decreases up to a certain model size, and then increases again up to the largest model evaluated. One hypothesis is that U-shaped scaling occurs when a task comprises a “true task” and a “distractor task”. Medium-size models can do the distractor task, which hurts performance, while only large-enough models can ignore the distractor task and do the true task. The existence of U-shaped scaling implies that inverse scaling may not hold for larger models. Second, we evaluate the inverse scaling tasks using chain-of-thought (CoT) prompting, in addition to basic prompting without CoT. With CoT prompting, all four tasks show either U-shaped scaling or positive scaling, achieving perfect solve rates on two tasks and several sub-tasks. This suggests that the term "inverse scaling task" is under-speciﬁed—a given task may be inverse scaling for one prompt but positive or U-shaped scaling for a diﬀerent prompt.	1dbe8253d08574f3ba04d2d519ccec580dccc24c	@['JournalArticle']{wei-etal-2022-inverse,  author = {Jason Wei and Yi Tay and Quoc V. Le},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Inverse scaling can become U-shaped},  volume = {abs/2211.02011},  year = {2022} }
Introducing Orthogonal Constraint in Structural Probes	2020	http://www.semanticscholar.org/paper/c6f84341be562043d107575b5beb1fc1575c9382	A new type of structural probing, where the linear projection is decomposed into iso-morphic space rotation and linear scaling that identifies and scales the most relevant dimensions, which makes the Structural Probes less vulnerable to memorization.	maybe	6	With the recent success of pre-trained models in NLP, a significant focus was put on interpreting their representations. One of the most prominent approaches is structural probing (Hewitt and Manning, 2019), where a linear projection of word embeddings is performed in order to approximate the topology of dependency structures. In this work, we introduce a new type of structural probing, where the linear projection is decomposed into 1. iso-morphic space rotation; 2. linear scaling that identifies and scales the most relevant dimensions. In addition to syntactic dependency, we evaluate our method on two novel tasks (lexical hypernymy and position in a sentence). We jointly train the probes for multiple tasks and experimentally show that lexical and syntactic information is separated in the representations. Moreover, the orthogonal constraint makes the Structural Probes less vulnerable to memorization.	c6f84341be562043d107575b5beb1fc1575c9382	@['JournalArticle', 'Conference']{limisiewicz-mareček-2020-introducing,  author = {Tomasz Limisiewicz and D. Mareček},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Introducing Orthogonal Constraint in Structural Probes},  volume = {abs/2012.15228},  year = {2020} }
Intrinsic Probing through Dimension Selection	2020	http://www.semanticscholar.org/paper/3c4d3b7085ad5f02bc732b489ace590a7bbd58c9	This paper proposes a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal, and probes fastText and BERT for various morphosyntactic attributes across 36 languages.	maybe	31	Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.	3c4d3b7085ad5f02bc732b489ace590a7bbd58c9	@['JournalArticle', 'Conference']{hennigen-etal-2020-intrinsic,  author = {Lucas Torroba Hennigen and Adina Williams and Ryan Cotterell},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Intrinsic Probing through Dimension Selection},  volume = {abs/2010.02812},  year = {2020} }
Intrinsic Knowledge Evaluation on Chinese Language Models	2020	http://www.semanticscholar.org/paper/64ad36e33169b599e6ef243aad4f769f7b4c1b97	Four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of $39,308$ questions covering both linguistic and world knowledge in Chinese are proposed.	yes	1	Recent NLP tasks have benefited a lot from pre-trained language models (LM) since they are able to encode knowledge of various aspects. However, current LM evaluations focus on downstream performance, hence lack to comprehensively inspect in which aspect and to what extent have they encoded knowledge. This paper addresses both queries by proposing four tasks on syntactic, semantic, commonsense, and factual knowledge, aggregating to a total of $39,308$ questions covering both linguistic and world knowledge in Chinese. Throughout experiments, our probes and knowledge data prove to be a reliable benchmark for evaluating pre-trained Chinese LMs. Our work is publicly available at https://github.com/ZhiruoWang/ChnEval.	64ad36e33169b599e6ef243aad4f769f7b4c1b97	@['JournalArticle']{wang-hu-2020-intrinsic,  author = {Zhiruo Wang and Renfen Hu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Intrinsic Knowledge Evaluation on Chinese Language Models},  volume = {abs/2011.14277},  year = {2020} }
Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning	2020	https://www.semanticscholar.org/paper/e54ffc76d805c48660bb0fd20019ca82ac94ba0d	This paper empirically shows that common pre-trained models have a very low intrinsic dimension, and connects intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalizations bounds that are independent of the full parameter count.	maybe	68	Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count.	e54ffc76d805c48660bb0fd20019ca82ac94ba0d	@['JournalArticle', 'Conference']{aghajanyan-etal-2020-intrinsic,  author = {Armen Aghajanyan and Luke Zettlemoyer and Sonal Gupta},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7319-7328},  title = {Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},  year = {2020} }
Intrinsic Bias Metrics Do Not Correlate with Application Bias	2020	http://www.semanticscholar.org/paper/497d29459a894ac38a48ed58753976ccbf2aa433	Researchers working on debiasing are urged to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data.	maybe	54	Natural Language Processing (NLP) systems learn harmful societal biases that cause them to amplify inequality as they are deployed in more and more situations. To guide efforts at debiasing these systems, the NLP community relies on a variety of metrics that quantify bias in models. Some of these metrics are intrinsic, measuring bias in word embedding spaces, and some are extrinsic, measuring bias in downstream tasks that the word embeddings enable. Do these intrinsic and extrinsic metrics correlate with each other? We compare intrinsic and extrinsic metrics across hundreds of trained models covering different tasks and experimental conditions. Our results show no reliable correlation between these metrics that holds in all scenarios across tasks and languages. We urge researchers working on debiasing to focus on extrinsic measures of bias, and to make using these measures more feasible via creation of new challenge sets and annotated test data. To aid this effort, we release code, a new intrinsic metric, and an annotated test set focused on gender bias in hate speech.	497d29459a894ac38a48ed58753976ccbf2aa433	@['JournalArticle', 'Conference']{tarrant-etal-2020-intrinsic,  author = {Seraphina Goldfarb-Tarrant and Rebecca Marchant and Ricardo Muñoz Sánchez and Mugdha Pandya and Adam Lopez},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1926-1940},  title = {Intrinsic Bias Metrics Do Not Correlate with Application Bias},  year = {2020} }
Interventional Probing in High Dimensions: An NLI Case Study	2022	http://www.semanticscholar.org/paper/95fef3a8cd12d3fe9be85c42e94741a6bee03fb1	New vector-level interventions are carried out to investigate the effect of semantic features intermediate to certain frag- 002 ments of NLI on NLI classi- 012 fication and delve into the limitations of these methods and outline pitfalls that have been obscuring the effectivity of such studies.	maybe	0	Probing strategies have been shown to detect 001 semantic features intermediate to certain frag- 002 ments of NLI. In the case of natural logic, the 003 relation between these features and the entail- 004 ment label is explicitly known: as such, this 005 provides a ripe setting for interventional studies 006 on the NLI models’ representations, allowing 007 for stronger causal conjectures and a deeper 008 critical analysis of interventional probing meth- 009 ods. In this work, we carry out new and exist- 010 ing vector-level interventions to investigate the 011 effect of these semantic features on NLI classi- 012 fication: we perform amnesic probing (which 013 removes features as directed by learned probes) 014 and introduce the mnestic probing variation 015 (which forgets all dimensions except the probe- 016 selected ones). Furthermore, we delve into the 017 limitations of these methods and outline pitfalls 018 that have been obscuring the effectivity of such 019 studies. 020	95fef3a8cd12d3fe9be85c42e94741a6bee03fb1	@None{2022-interventional,  title = {Interventional Probing in High Dimensions: An NLI Case Study},  year = {2022} }
Intersectional Bias in Causal Language Models	2021	http://www.semanticscholar.org/paper/e614bdb3d6a0675084616ff2ee40c14314d3d1a4	It is suggested technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.	maybe	7	To examine whether intersectional bias can be observed in language generation, we examine GPT-2 and GPT-NEO models, ranging in size from 124 million to 2.7 billion parameters. We conduct an experiment combining up to three social categories – gender, religion and disability – into unconditional or zero-shot prompts used to generate sentences that are then analysed for sentiment. Our results confirm earlier tests conducted with auto-regressive causal models, including the GPT family of models. We also illustrate why bias may be resistant to techniques that target single categories (e.g. gender, religion and race), as it can also manifest, in often subtle ways, in texts prompted by concatenated social categories. To address these difficulties, we suggest technical and community-based approaches need to combine to acknowledge and address complex and intersectional language model bias.	e614bdb3d6a0675084616ff2ee40c14314d3d1a4	@['JournalArticle']{magee-etal-2021-intersectional,  author = {L. Magee and Lida Ghahremanlou and K. Soldatic and S. Robertson},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Intersectional Bias in Causal Language Models},  volume = {abs/2107.07691},  year = {2021} }
Interpreting Text Classifiers by Learning Context-sensitive Influence of Words	2021	http://www.semanticscholar.org/paper/0274b1ac36f9eb933c13faf8cb7e862ea219eb25	This work proposes MOXIE (MOdeling conteXt-sensitive InfluencE of words) with an aim to enable a richer interface for a user to interact with the model being interpreted and to produce testable predictions, and aims to make predictions for importance scores, counterfactuals and learned biases withMOXIE.	maybe	3	Many existing approaches for interpreting text classification models focus on providing importance scores for parts of the input text, such as words, but without a way to test or improve the interpretation method itself. This has the effect of compounding the problem of understanding or building trust in the model, with the interpretation method itself adding to the opacity of the model. Further, importance scores on individual examples are usually not enough to provide a sufficient picture of model behavior. To address these concerns, we propose MOXIE (MOdeling conteXt-sensitive InfluencE of words) with an aim to enable a richer interface for a user to interact with the model being interpreted and to produce testable predictions. In particular, we aim to make predictions for importance scores, counterfactuals and learned biases with MOXIE. In addition, with a global learning objective, MOXIE provides a clear path for testing and improving itself. We evaluate the reliability and efficiency of MOXIE on the task of sentiment analysis.	0274b1ac36f9eb933c13faf8cb7e862ea219eb25	@None{kumar-etal-2021-interpreting,  author = {Sawan Kumar and Kalpit Dixit and Kashif Shah},  booktitle = {TRUSTNLP},  journal = {Proceedings of the First Workshop on Trustworthy Natural Language Processing},  title = {Interpreting Text Classifiers by Learning Context-sensitive Influence of Words},  year = {2021} }
Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings	2020	https://www.semanticscholar.org/paper/d34580c522c79d5cde620331dd9ffb18643a8090	Simple and fully general methods for converting from contextualized representations to static lookup-table embeddings are introduced which are applied to 5 popular pretrained models and 9 sets of pretrained weights and reveal that pooling over many contexts significantly improves representational quality under intrinsic evaluation.	seed	84	Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.	d34580c522c79d5cde620331dd9ffb18643a8090	@['JournalArticle', 'Conference']{bommasani-etal-2020-interpreting,  author = {Rishi Bommasani and Kelly Davis and Claire Cardie},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4758-4781},  title = {Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings},  year = {2020} }
Interpreting Neural Networks through the Polytope Lens	2022	http://www.semanticscholar.org/paper/988914a393964adf63ce4e88b8375b12beb3b192	Speciﬁcally, it is shown that polytopes can be used to identify monosemantic regions of activation space and that the density of polytope boundaries reﬂect semantic boundaries and a vision for what mechanistic interpretability might look like through thepolytope lens is outlined.	yes	2	Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? What basic objects should we use to describe the operation of neural networks mechanistically? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description—directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a speciﬁc feature to a neural unit. In order to ﬁnd a basic unit of description that doesn’t suﬀer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the ‘polytope lens’. Although this view introduces new challenges, we think they are surmountable and that more careful consideration of the impact of nonlinearities is necessary in order to build better high-level abstractions for a mechanistic understanding of neural networks. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classiﬁers and language models. Speciﬁcally, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reﬂect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.	988914a393964adf63ce4e88b8375b12beb3b192	@['JournalArticle']{black-etal-2022-interpreting,  author = {Sid Black and Lee D. Sharkey and Léo Grinsztajn and Eric Winsor and D. Braun and Jacob Merizian and Kip Parker and Carlos Ram'on Guevara and Beren Millidge and Gabriel Alfour and Connor Leahy},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Interpreting Neural Networks through the Polytope Lens},  volume = {abs/2211.12312},  year = {2022} }
Interpreting Language Models Through Knowledge Graph Extraction	2021	http://www.semanticscholar.org/paper/a0118fc91478bde959d41c4e2231f1767915d207	This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant.	maybe	3	Transformer-based language models trained on large text corpora have enjoyed immense popularity in the natural language processing community and are commonly used as a starting point for downstream tasks. While these models are undeniably useful, it is a challenge to quantify their performance beyond traditional accuracy metrics. In this paper, we compare BERT-based language models through snapshots of acquired knowledge at sequential stages of the training process. Structured relationships from training corpora may be uncovered through querying a masked language model with probing tasks. We present a methodology to unveil a knowledge acquisition timeline by generating knowledge graph extracts from cloze "fill-in-the-blank" statements at various stages of RoBERTa’s early training. We extend this analysis to a comparison of pretrained variations of BERT models (DistilBERT, BERT-base, RoBERTa). This work proposes a quantitative framework to compare language models through knowledge graph extraction (GED, Graph2Vec) and showcases a part-of-speech analysis (POSOR) to identify the linguistic strengths of each model variant. Using these metrics, machine learning practitioners can compare models, diagnose their models’ behavioral strengths and weaknesses, and identify new targeted datasets to improve model performance.	a0118fc91478bde959d41c4e2231f1767915d207	@['JournalArticle']{swamy-etal-2021-interpreting,  author = {Vinitra Swamy and Angelika Romanou and Martin Jaggi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Interpreting Language Models Through Knowledge Graph Extraction},  volume = {abs/2111.08546},  year = {2021} }
Interpreting Hierarchical Linguistic Interactions in DNNs	2020	http://www.semanticscholar.org/paper/1acdb402b949d5a6ef24ac84a983c07e9def8d09	This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing, and constructs a tree to encode salient interactions extracted by the DNN.	maybe	2	This paper proposes a method to disentangle and quantify interactions among words that are encoded inside a DNN for natural language processing. We construct a tree to encode salient interactions extracted by the DNN. Six metrics are proposed to analyze properties of interactions between constituents in a sentence. The interaction is defined based on Shapley values of words, which are considered as an unbiased estimation of word contributions to the network prediction. Our method is used to quantify word interactions encoded inside the BERT, ELMo, LSTM, CNN, and Transformer networks. Experimental results have provided a new perspective to understand these DNNs, and have demonstrated the effectiveness of our method.	1acdb402b949d5a6ef24ac84a983c07e9def8d09	@['JournalArticle']{zhang-etal-2020-interpreting,  author = {Die Zhang and Huilin Zhou and Xiaoyi Bao and Da Huo and Ruizhao Chen and Xu Cheng and Hao Zhang and Mengyue Wu and Quanshi Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Interpreting Hierarchical Linguistic Interactions in DNNs},  volume = {abs/2007.04298},  year = {2020} }
Interpretation of Language Models Attention Matrices in Texts Sentiment Analysis	2022	http://www.semanticscholar.org/paper/a8e168e3201d546965cd2752c3336630548a77e7	A method for interpreting language models using an analysis of attention matrices and shows that the model focuses on sentiment lexicon, especially negative words.	maybe	0	Deep neural network language models have recently shown impressive results in natural language processing, including the field of text sentiment analysis. However, user confidence in the results of the analysis is low due to the poor interpretability of such models. The paper proposes a method for interpreting language models using an analysis of attention matrices. The experiments were carried out using the ruRoberta language model and the corpus of Russian-language news. The results of the work show that the model focuses on sentiment lexicon, especially negative words.	a8e168e3201d546965cd2752c3336630548a77e7	@['Conference']{pashchenko-etal-2022-interpretation,  author = {D.E. Pashchenko and E. Razova and A. Kotelnikova and S. Vychegzhanin and E. V. Kotelnikov},  booktitle = {2022 VIII International Conference on Information Technology and Nanotechnology (ITNT)},  journal = {2022 VIII International Conference on Information Technology and Nanotechnology (ITNT)},  pages = {1-4},  title = {Interpretation of Language Models Attention Matrices in Texts Sentiment Analysis},  year = {2022} }
Interpretation of Black Box NLP Models: A Survey	2022	http://www.semanticscholar.org/paper/d2c1c5b87da32266dc42c6bdfd36d1be875242c5	This survey will present a comprehensive survey of methods employed for interpretability followed by a discussion on the different approaches adopted in NLP space and highlight the synergy between different approaches and issues with interpretability in N LP space.	maybe	2	Machine learning (ML) models are ubiquitous in every wake of life and employed in high-stakes decision-making tasks. The complexity involved in ML models, speciﬁcally in Neural models, has brought in the question, “How does a model make a decision?" . An insight into the model’s decision-making process will help ﬁx the accountability for the model’s decision. It will further address ethical, safety, and bias issues associated with the model. To address these interpretability can provide an explanation that humans can easily understand. This survey will present a comprehensive survey of methods employed for interpretability. Firstly, it presents the deﬁnition of Interpretability followed by a discussion on the different approaches adopted in NLP space. Fi-nally, It will highlight the synergy between different approaches and issues with interpretability in NLP space.	d2c1c5b87da32266dc42c6bdfd36d1be875242c5	@['JournalArticle', 'Review']{choudhary-etal-2022-interpretation,  author = {Shivani Choudhary and N. Chatterjee and S. Saha},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Interpretation of Black Box NLP Models: A Survey},  volume = {abs/2203.17081},  year = {2022} }
Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond	2021	http://www.semanticscholar.org/paper/3d9f067d97cf21f3b0c4d406ccff98b06abafb5c	This paper introduces and clarify two basic concepts—interpretations and interpretability—that people usually get confused about and summarizes the current works in evaluating models’ interpretability using “trustworthy” interpretation algorithms.	maybe	34		3d9f067d97cf21f3b0c4d406ccff98b06abafb5c	@['JournalArticle', 'Review']{li-etal-2021-interpretable,  author = {Xuhong Li and Haoyi Xiong and Xingjian Li and Xuanyu Wu and Xiao Zhang and Ji Liu and Jiang Bian and D. Dou},  booktitle = {Knowledge and Information Systems},  journal = {Knowledge and Information Systems},  pages = {3197 - 3234},  title = {Interpretable deep learning: interpretation, interpretability, trustworthiness, and beyond},  volume = {64},  year = {2021} }
Interpretability of BERT Latent Space through Knowledge Graphs	2022	http://www.semanticscholar.org/paper/059c33fb6c7e4c9293b96c1b3a46403fdd5c4c3a	This is the first attempt at interpreting the BERT learned linguistic knowledge through a KG relying on its pretrained context-aware word embeddings and proves the existence of explicitly meaningful areas through the Link Prediction (LP) task.	maybe	0	The advent of pretrained language have renovated the ways of handling natural languages, improving the quality of systems that rely on them. BERT played a crucial role in revolutionizing the Natural Language Processing (NLP) area. However, the deep learning framework it implements lacks interpretability. Thus, recent research efforts aimed to explain what BERT learns from the text sources exploited to pre-train its linguistic model. In this paper, we analyze the latent vector space resulting from the BERT context-aware word embeddings. We focus on assessing whether regions of the BERT vector space hold an explicit meaning attributable to a Knowledge Graph (KG). First, we prove the existence of explicitly meaningful areas through the Link Prediction (LP) task. Then, we demonstrate these regions being linked to explicit ontology concepts of a KG by learning classification patterns. To the best of our knowledge, this is the first attempt at interpreting the BERT learned linguistic knowledge through a KG relying on its pretrained context-aware word embeddings.	059c33fb6c7e4c9293b96c1b3a46403fdd5c4c3a	@['JournalArticle', 'Book', 'Conference']{anelli-etal-2022-interpretability,  author = {V. W. Anelli and Giovanni Maria Biancofiore and Alessandro De Bellis and T. Di Noia and Eugenio Di Sciascio},  booktitle = {International Conference on Information and Knowledge Management},  journal = {Proceedings of the 31st ACM International Conference on Information & Knowledge Management},  title = {Interpretability of BERT Latent Space through Knowledge Graphs},  year = {2022} }
Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small	2022	http://www.semanticscholar.org/paper/6edd112383ad494f5f2eba72b6f4ffae122ce61f	This investigation is the largest end-to-end attempt at reverse-engineering a natural behavior “in the wild” in a language model, and provides evidence that a mechanistic understanding of large ML models is feasible, pointing toward opportunities to scale the understanding to both larger models and more complex tasks.	maybe	3	Research in mechanistic interpretability seeks to explain behaviors of machine learning (ML) models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identiﬁcation (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior “in the wild” in a language model. We evaluate the reliability of our explanation using three quantitative criteria– faithfulness, completeness and minimality . Though these criteria sup-port our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, pointing toward opportunities to scale our understanding to both larger models and more complex tasks. Code for all experiments is available at https://github.com/redwoodresearch/Easy-Transformer .	6edd112383ad494f5f2eba72b6f4ffae122ce61f	@['JournalArticle']{wang-etal-2022-interpretability,  author = {Kevin Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and J. Steinhardt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},  volume = {abs/2211.00593},  year = {2022} }
Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?	2020	https://www.semanticscholar.org/paper/9e594ae4ae9c38b6495810a8872f513ae19be29c	It is observed that intermediate tasks requiring high-level inference and reasoning abilities tend to work best and that target task performance is strongly correlated with higher-level abilities such as coreference resolution, but it is failed to observe more granular correlations between probing and target taskperformance.	seed	122	While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.	9e594ae4ae9c38b6495810a8872f513ae19be29c	@['JournalArticle', 'Conference']{pruksachatkun-etal-2020-intermediate,  author = {Yada Pruksachatkun and Jason Phang and Haokun Liu and Phu Mon Htut and Xiaoyi Zhang and Richard Yuanzhe Pang and Clara Vania and Katharina Kann and Samuel R. Bowman},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5231-5247},  title = {Intermediate-Task Transfer Learning with Pretrained Language Models: When and Why Does It Work?},  year = {2020} }
Interdependencies of Gender and Race in Contextualized Word Embeddings	2020	http://www.semanticscholar.org/paper/be77c7dd67a5bb6a28b866de65ea10bbac03c6bb		maybe	11	Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.	be77c7dd67a5bb6a28b866de65ea10bbac03c6bb	@None{jiang-fellbaum-2020-interdependencies,  author = {May Jiang and C. Fellbaum},  booktitle = {GEBNLP},  title = {Interdependencies of Gender and Race in Contextualized Word Embeddings},  year = {2020} }
Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI	2021	http://www.semanticscholar.org/paper/7574c1a2878ece33845028adaa67f84b77b0175e	The decision making process behind the recommendations made on this case for their adoption in the creative industries is described, and research that engages with developers and stakeholders in the ethics of storytelling through CAI is highlighted as a matter of urgency.	maybe	3		7574c1a2878ece33845028adaa67f84b77b0175e	@['JournalArticle']{chubb-etal-2021-interactive,  author = {J. Chubb and S. Missaoui and S. Concannon and Liam Maloney and James Alfred Walker},  booktitle = {Int. J. Child Comput. Interact.},  journal = {Int. J. Child Comput. Interact.},  pages = {100403},  title = {Interactive Storytelling for Children: A Case-study of Design and Development Considerations for Ethical Conversational AI},  volume = {32},  year = {2021} }
IntelliCode compose: code generation using transformer	2020	http://www.semanticscholar.org/paper/8ee2351221b72fca5eef4c42147ed67071903d93	IntelliCode Compose is introduced – a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code.	yes	155	In software development through integrated development environments (IDEs), code completion is one of the most widely used features. Nevertheless, majority of integrated development environments only support completion of methods and APIs, or arguments. In this paper, we introduce IntelliCode Compose – a general-purpose multilingual code completion tool which is capable of predicting sequences of code tokens of arbitrary types, generating up to entire lines of syntactically correct code. It leverages state-of-the-art generative transformer model trained on 1.2 billion lines of source code in Python, C#, JavaScript and TypeScript programming languages. IntelliCode Compose is deployed as a cloud-based web service. It makes use of client-side tree-based caching, efficient parallel implementation of the beam search decoder, and compute graph optimizations to meet edit-time completion suggestion requirements in the Visual Studio Code IDE and Azure Notebook. Our best model yields an average edit similarity of 86.7% and a perplexity of 1.82 for Python programming language.	8ee2351221b72fca5eef4c42147ed67071903d93	@['JournalArticle', 'Book']{svyatkovskiy-etal-2020-intellicode,  author = {Alexey Svyatkovskiy and Shao Kun Deng and Shengyu Fu and Neel Sundaresan},  booktitle = {ESEC/SIGSOFT FSE},  journal = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},  title = {IntelliCode compose: code generation using transformer},  year = {2020} }
Integrating Graph Contextualized Knowledge into Pre-trained Language Models	2019	http://www.semanticscholar.org/paper/f1957038e9ded19108d3c71340d7462152b70f25	Experimental results demonstrate that the proposed model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.	maybe	43	Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model subgraphs in a medical KG. Then, the learned knowledge is integrated with a pre-trained language model to do the knowledge generalization. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.	f1957038e9ded19108d3c71340d7462152b70f25	@['JournalArticle']{he-etal-2019-integrating,  author = {Bin He and Di Zhou and Jinghui Xiao and Xin Jiang and Qun Liu and Nicholas Jing Yuan and Tong Xu},  booktitle = {Findings},  journal = {ArXiv},  title = {Integrating Graph Contextualized Knowledge into Pre-trained Language Models},  volume = {abs/1912.00147},  year = {2019} }
Inspecting the concept knowledge graph encoded by modern language models	2021	http://www.semanticscholar.org/paper/12b336abe8b9889bfd2b82ff790e53603a899cbe	This work extracts the underlying knowledge graph of nine of the most influential language models of the last years, including word embeddings, text generators, and context encoders, and shows that all the models encode this knowledge, but suffer from several inaccuracies.	maybe	5	The field of natural language understanding has experienced exponential progress in the last few years, with impressive results in several tasks. This success has motivated researchers to study the underlying knowledge encoded by these models. Despite this, attempts to understand their semantic capabilities have not been successful, often leading to non-conclusive, or contradictory conclusions among different works. Via a probing classifier, we extract the underlying knowledge graph of nine of the most influential language models of the last years, including word embeddings, text generators, and context encoders. This probe is based on concept relatedness, grounded on WordNet. Our results reveal that all the models encode this knowledge, but suffer from several inaccuracies. Furthermore, we show that the different architectures and training strategies lead to different model biases. We conduct a systematic evaluation to discover specific factors that explain why some concepts are challenging. We hope our insights will motivate the development of models that capture concepts more precisely.	12b336abe8b9889bfd2b82ff790e53603a899cbe	@['JournalArticle']{aspillaga-etal-2021-inspecting,  author = {C. Aspillaga and Marcelo Mendoza and Álvaro Soto},  booktitle = {Findings},  journal = {ArXiv},  title = {Inspecting the concept knowledge graph encoded by modern language models},  volume = {abs/2105.13471},  year = {2021} }
Inserting Information Bottlenecks for Attribution in Transformers	2020	http://www.semanticscholar.org/paper/016af3b22683e87c21afb1750614d3d53c168f94	This paper applies information bottlenecks to analyze the attribution of each feature for prediction on a black-box model and shows the effectiveness of the method in terms of attribution and the ability to provide insight into how information flows through layers.	maybe	2	Pretrained transformers achieve the state of the art across tasks in natural language processing, motivating researchers to investigate their inner mechanisms. One common direction is to understand what features are important for prediction. In this paper, we apply information bottlenecks to analyze the attribution of each feature for prediction on a black-box model. We use BERT as the example and evaluate our approach both quantitatively and qualitatively. We show the effectiveness of our method in terms of attribution and the ability to provide insight into how information flows through layers. We demonstrate that our technique outperforms two competitive methods in degradation tests on four datasets. Code is available at https://github.com/bazingagin/IBA.	016af3b22683e87c21afb1750614d3d53c168f94	@['JournalArticle']{jiang-etal-2020-inserting,  author = {Zhiying Jiang and Raphael Tang and Ji Xin and Jimmy J. Lin},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Inserting Information Bottlenecks for Attribution in Transformers},  volume = {abs/2012.13838},  year = {2020} }
Inner Monologue: Embodied Reasoning through Planning with Language Models	2022	http://www.semanticscholar.org/paper/f3cf71c51b882fe3111d71c4bf104297d38197f8	This work proposes that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios, and finds that closed-loop language feedback significantly improves high-level instruction completion on three domains.	yes	28	: Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.	f3cf71c51b882fe3111d71c4bf104297d38197f8	@['JournalArticle']{huang-etal-2022-inner,  author = {Wenlong Huang and F. Xia and Ted Xiao and Harris Chan and Jacky Liang and Peter R. Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and P. Sermanet and Noah Brown and Tomas Jackson and Linda Luu and S. Levine and Karol Hausman and Brian Ichter},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Inner Monologue: Embodied Reasoning through Planning with Language Models},  volume = {abs/2207.05608},  year = {2022} }
Inherent Disagreements in Human Textual Inferences	2019	http://www.semanticscholar.org/paper/74293469797646a026064183c43d8e355498929b	It is argued for a refined evaluation objective that requires models to explicitly capture the full distribution of plausible human judgments to reflect the type of uncertainty present in human disagreements.	maybe	114	Abstract We analyze human’s disagreements about the validity of natural language inferences. We show that, very often, disagreements are not dismissible as annotation “noise”, but rather persist as we collect more ratings and as we vary the amount of context provided to raters. We further show that the type of uncertainty captured by current state-of-the-art models for natural language inference is not reflective of the type of uncertainty present in human disagreements. We discuss implications of our results in relation to the recognizing textual entailment (RTE)/natural language inference (NLI) task. We argue for a refined evaluation objective that requires models to explicitly capture the full distribution of plausible human judgments.	74293469797646a026064183c43d8e355498929b	@['JournalArticle']{pavlick-kwiatkowski-2019-inherent,  author = {Ellie Pavlick and T. Kwiatkowski},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {677-694},  title = {Inherent Disagreements in Human Textual Inferences},  volume = {7},  year = {2019} }
Information-Theoretic Probing with Minimum Description Length	2020	https://www.semanticscholar.org/paper/f4b585c9a79dfce0807b445a09036ea0f9cbcdce	This work proposes an alternative to the standard probes, information-theoretic probing with minimum description length (MDL), and considers two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding.	seed	174	To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.	f4b585c9a79dfce0807b445a09036ea0f9cbcdce	@['JournalArticle', 'Conference']{voita-titov-2020-information,  author = {Elena Voita and Ivan Titov},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Information-Theoretic Probing with Minimum Description Length},  volume = {abs/2003.12298},  year = {2020} }
INFORMATION-THEORETIC PROBING EXPLAINS RE-	2020	http://www.semanticscholar.org/paper/1150ac706d66dfdf3cdbb77212ee6f68cef44066	The hypothesis that the extent to which a feature influences a model’s decisions can be predicted using a combination of two factors: the feature’'s extractability after pre-training, and the evidence available during finetuning is tested.	maybe	0	Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then finetuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via “probing classifiers”) finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via “challenge sets”) indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model’s decisions can be predicted using a combination of two factors: The feature’s extractability after pre-training (measured using information-theoretic probing techniques), and the evidence available during finetuning (defined as the feature’s co-occurrence rate with the label). In experiments with both synthetic and natural language data, we find strong evidence (statistically significant correlations) supporting this hypothesis.	1150ac706d66dfdf3cdbb77212ee6f68cef44066	@None{liance-heuristics-2020-information,  author = {ON Liance and Spurious Heuristics},  title = {INFORMATION-THEORETIC PROBING EXPLAINS RE-},  year = {2020} }
Information Theory–based Compositional Distributional Semantics	2022	http://www.semanticscholar.org/paper/2796622dad6217f99508a3f241c3f9aeb24b374a	The theoretical analysis and empirical results show that fulfilling formal properties affects positively the accuracy of text representation models in terms of correspondence (isometry) between the embedding and meaning spaces.	maybe	0	Abstract In the context of text representation, Compositional Distributional Semantics models aim to fuse the Distributional Hypothesis and the Principle of Compositionality. Text embedding is based on co-ocurrence distributions and the representations are in turn combined by compositional functions taking into account the text structure. However, the theoretical basis of compositional functions is still an open issue. In this article we define and study the notion of Information Theory–based Compositional Distributional Semantics (ICDS): (i) We first establish formal properties for embedding, composition, and similarity functions based on Shannon’s Information Theory; (ii) we analyze the existing approaches under this prism, checking whether or not they comply with the established desirable properties; (iii) we propose two parameterizable composition and similarity functions that generalize traditional approaches while fulfilling the formal properties; and finally (iv) we perform an empirical study on several textual similarity datasets that include sentences with a high and low lexical overlap, and on the similarity between words and their description. Our theoretical analysis and empirical results show that fulfilling formal properties affects positively the accuracy of text representation models in terms of correspondence (isometry) between the embedding and meaning spaces.	2796622dad6217f99508a3f241c3f9aeb24b374a	@None{amigó-etal-2022-information,  author = {Enrique Amigó and Alejandro Ariza-Casabona and V. Fresno and M. A. Martí},  booktitle = {Computational Linguistics},  journal = {Computational Linguistics},  pages = {907-948},  title = {Information Theory–based Compositional Distributional Semantics},  volume = {48},  year = {2022} }
Influence Patterns for Explaining Information Flow in BERT	2020	http://www.semanticscholar.org/paper/eb61a53be928c1eb52810dd35204115cbf69b6a8		maybe	7	While “attention is all you need” may be proving true, we do not know why: attention-based transformer models such as BERT are superior but how information flows from input tokens to output predictions are unclear. We introduce influence patterns, abstractions of sets of paths through a transformer model. Patterns quantify and localize the flow of information to paths passing through a sequence of model nodes. Experimentally, we find that significant portion of information flow in BERT goes through skip connections instead of attention heads. We further show that consistency of patterns across instances is an indicator of BERT’s performance. Finally, We demonstrate that patterns account for far more model performance than previous attention-based and layer-based methods.	eb61a53be928c1eb52810dd35204115cbf69b6a8	@['JournalArticle']{lu-etal-2020-influence,  author = {Kaiji Lu and Zifan Wang and Piotr (Peter) Mardziel and Anupam Datta},  booktitle = {Neural Information Processing Systems},  pages = {4461-4474},  title = {Influence Patterns for Explaining Information Flow in BERT},  year = {2020} }
Inferring Implicit Relations in Complex Questions with Language Models	2022	http://www.semanticscholar.org/paper/2f21201ac9fcb88a72c56471402388ec2fc365a8	This work investigates why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution, and evaluates models from the GPT-3 family, finding that while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations.	maybe	3	A prominent challenge for modern language understanding systems is the ability to answer implicit reasoning questions, where the required reasoning steps for answering the question are not mentioned in the text explicitly. In this work, we investigate why current models struggle with implicit reasoning question answering (QA) tasks, by decoupling inference of reasoning steps from their execution. We deﬁne a new task of implicit relation inference and construct a benchmark, I MPLICIT R E LATIONS , where given a question, a model should output a list of concept-relation pairs, where the relations describe the implicit reasoning steps required for answering the question. Using I MPLICIT R ELATIONS , we evaluate models from the GPT-3 family and ﬁnd that, while these models struggle on the implicit reasoning QA task, they often succeed at inferring implicit relations. This suggests that the challenge in implicit reasoning questions does not stem from the need to plan a reasoning strategy alone, but to do it while also retrieving and reasoning over relevant information.	2f21201ac9fcb88a72c56471402388ec2fc365a8	@['JournalArticle']{katz-etal-2022-inferring,  author = {Uri Katz and Mor Geva and Jonathan Berant},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Inferring Implicit Relations in Complex Questions with Language Models},  volume = {abs/2204.13778},  year = {2022} }
Inducing Taxonomic Knowledge from Pretrained Transformers	2020	http://www.semanticscholar.org/paper/bea36d43c6c0cbcb197231aa70ca095e331a967b	A method for inducing taxonomic trees from pretrained transformers by assigning a score for the likelihood that each pair of terms forms a parent-child relation and producing the maximum spanning tree.	maybe	2	We present a method for inducing taxonomic trees from pretrained transformers. Given a set of input terms, we assign a score for the likelihood that each pair of terms forms a parent-child relation. To produce a tree from pairwise parent-child edge scores, we treat this as a graph optimization problem and output the maximum spanning tree. We train the model by finetuning it on parent-child relations from subtrees of WordNet and test on non-overlapping subtrees. In addition, we incorporate semi-structured definitions from the web to further improve performance. On the task of inducing subtrees of WordNet, the model achieves 66.0 ancestor F_1, a 10.4 point absolute increase over the previous best published result on this task.	bea36d43c6c0cbcb197231aa70ca095e331a967b	@['JournalArticle']{chen-etal-2020-inducing,  author = {Catherine Chen and Kevin Lin and D. Klein},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Inducing Taxonomic Knowledge from Pretrained Transformers},  volume = {abs/2010.12813},  year = {2020} }
Inducing Syntactic Trees from BERT Representations	2019	https://www.semanticscholar.org/paper/71f551f0352b91ab4725c498c68610655d3b5578	The hypothesis is that removing a reducible word does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model.	seed	17	We use the English model of BERT and explore how a deletion of one word in a sentence changes representations of other words. Our hypothesis is that removing a reducible word (e.g. an adjective) does not affect the representation of other words so much as removing e.g. the main verb, which makes the sentence ungrammatical and of "high surprise" for the language model. We estimate reducibilities of individual words and also of longer continuous phrases (word n-grams), study their syntax-related properties, and then also use them to induce full dependency trees.	71f551f0352b91ab4725c498c68610655d3b5578	@['JournalArticle']{rosa-mareček-2019-inducing,  author = {Rudolf Rosa and D. Mareček},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Inducing Syntactic Trees from BERT Representations},  volume = {abs/1906.11511},  year = {2019} }
Inducing Relational Knowledge from BERT	2019	https://www.semanticscholar.org/paper/f67fcbb1aec92ae293998ddfd904f61a31bef334	This work proposes a methodology for distilling relational knowledge from a pre-trained language model that fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.	seed	104	One of the most remarkable properties of word embeddings is the fact that they capture certain types of semantic and syntactic relationships. Recently, pre-trained language models such as BERT have achieved groundbreaking results across a wide range of Natural Language Processing tasks. However, it is unclear to what extent such models capture relational knowledge beyond what is already captured by standard word embeddings. To explore this question, we propose a methodology for distilling relational knowledge from a pre-trained language model. Starting from a few seed instances of a given relation, we first use a large text corpus to find sentences that are likely to express this relation. We then use a subset of these extracted sentences as templates. Finally, we fine-tune a language model to predict whether a given word pair is likely to be an instance of some relation, when given an instantiated template for that relation as input.	f67fcbb1aec92ae293998ddfd904f61a31bef334	@['JournalArticle', 'Conference']{bouraoui-etal-2019-inducing,  author = {Zied Bouraoui and José Camacho-Collados and S. Schockaert},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {7456-7463},  title = {Inducing Relational Knowledge from BERT},  year = {2019} }
Inducing brain-relevant bias in natural language processing models	2019	http://www.semanticscholar.org/paper/4d93ad53d08a62da93455e1433ff601cd71de5b1	It is demonstrated that a version of BERT, a recently introduced and powerful language model, can improve the prediction of brain activity after fine-tuning and the relationship between language and brain activity learned by BERT during this fine- Tuning transfers across multiple participants.	maybe	39	Progress in natural language processing (NLP) models that estimate representations of word sequences has recently been leveraged to improve the understanding of language processing in the brain. However, these models have not been specifically designed to capture the way the brain represents language meaning. We hypothesize that fine-tuning these models to predict recordings of brain activity of people reading text will lead to representations that encode more brain-activity-relevant language information. We demonstrate that a version of BERT, a recently introduced and powerful language model, can improve the prediction of brain activity after fine-tuning. We show that the relationship between language and brain activity learned by BERT during this fine-tuning transfers across multiple participants. We also show that, for some participants, the fine-tuned representations learned from both magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) are better for predicting fMRI than the representations learned from fMRI alone, indicating that the learned representations capture brain-activity-relevant information that is not simply an artifact of the modality. While changes to language representations help the model predict brain activity, they also do not harm the model's ability to perform downstream NLP tasks. Our findings are notable for research on language understanding in the brain.	4d93ad53d08a62da93455e1433ff601cd71de5b1	@['JournalArticle']{schwartz-etal-2019-inducing,  author = {Dan Schwartz and Mariya Toneva and Leila Wehbe},  booktitle = {Neural Information Processing Systems},  pages = {14100-14110},  title = {Inducing brain-relevant bias in natural language processing models},  year = {2019} }
Incorporating Residual and Normalization Layers into Analysis of Masked Language Models	2021	https://www.semanticscholar.org/paper/042c6d99bdf381b55048b8bb48c8479dbcfbcd5a	This analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed, and provides new intuitive explanations of existing reports.	maybe	9	Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers’ progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.	042c6d99bdf381b55048b8bb48c8479dbcfbcd5a	@['JournalArticle', 'Conference']{kobayashi-etal-2021-incorporating,  author = {Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentarou Inui},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Incorporating Residual and Normalization Layers into Analysis of Masked Language Models},  volume = {abs/2109.07152},  year = {2021} }
In-context Learning and Induction Heads	2022	https://www.semanticscholar.org/paper/c90a99eeb57019732a6cc996bb9eaf13faedf00f	It is found that induction heads develop at precisely the same point as a sudden sharp increase in incontext learning ability, visible as a bump in the training loss.	yes	22	“Induction heads” are attention heads that implement a simple algorithm to complete token sequences like [A][B] ... [A] → [B]. In this work, we present preliminary and indirect evidence for a hypothesis that induction heads might constitute the mechanism for the majority of all “incontext learning” in large transformer models (i.e. decreasing loss at increasing token indices). We find that induction heads develop at precisely the same point as a sudden sharp increase in incontext learning ability, visible as a bump in the training loss. We present six complementary lines of evidence, arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size. For small attention-only models, we present strong, causal evidence; for larger models with MLPs, we present correlational evidence. We recommend reading this paper as an HTML article. As Transformer generative models continue to scale and gain increasing real world use, addressing their associated safety problems becomes increasingly important. Mechanistic interpretability – attempting to reverse engineer the detailed computations performed by the model – offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models. [1, 2, 3, 4, 5]	c90a99eeb57019732a6cc996bb9eaf13faedf00f	@['JournalArticle']{olsson-etal-2022-in,  author = {Catherine Olsson and Nelson Elhage and Neel Nanda and Nicholas Joseph and Nova DasSarma and T. Henighan and Benjamin Mann and Amanda Askell and Yuntao Bai and Anna Chen and Tom Conerly and Dawn Drain and Deep Ganguli and Zac Hatfield-Dodds and Danny Hernandez and Scott Johnston and Andy Jones and John Kernion and Liane Lovitt and Kamal Ndousse and Dario Amodei and Tom B. Brown and Jack Clark and Jared Kaplan and Sam McCandlish and C. Olah},  booktitle = {ArXiv},  journal = {ArXiv},  title = {In-context Learning and Induction Heads},  volume = {abs/2209.11895},  year = {2022} }
Improving Transformer Models by Reordering their Sublayers	2019	https://www.semanticscholar.org/paper/3ff8d265f4351e4b1fdac5b586466bee0b5d6fff	This work proposes a new transformer pattern that adheres to this property, the sandwich transformer, and shows that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time.	seed	50	Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern lead to better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more self-attention at the bottom and more feedforward sublayers at the top. We propose a new transformer pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on multiple word-level and character-level language modeling benchmarks, at no cost in parameters, memory, or training time. However, the sandwich reordering pattern does not guarantee performance gains across every task, as we demonstrate on machine translation models. Instead, we suggest that further exploration of task-specific sublayer reorderings is needed in order to unlock additional gains.	3ff8d265f4351e4b1fdac5b586466bee0b5d6fff	@['JournalArticle', 'Conference']{press-etal-2019-improving,  author = {Ofir Press and Noah A. Smith and Omer Levy},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {2996-3005},  title = {Improving Transformer Models by Reordering their Sublayers},  year = {2019} }
Improving language models by retrieving from trillions of tokens	2021	http://www.semanticscholar.org/paper/002c256d30d6be4b23d365a8de8ae0e67e4c9641	With a 2 trillion token database, the Retrieval-Enhanced Transformer (R ETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 × fewer parameters.	maybe	126	We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens. With a 2 trillion token database, our Retrieval-Enhanced Transformer (R ETRO ) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 × fewer parameters. After ﬁne-tuning, R ETRO performance translates to downstream knowledge-intensive tasks such as question answering. R ETRO com-bines a frozen B ERT retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training. We typically train R ETRO from scratch, yet can also rapidly R ETRO ﬁt pre-trained transformers with retrieval and still achieve good performance. Our work opens up new avenues for improving language models through explicit memory at unprecedented scale.	002c256d30d6be4b23d365a8de8ae0e67e4c9641	@['JournalArticle', 'Conference']{borgeaud-etal-2021-improving,  author = {Sebastian Borgeaud and A. Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and J. Lespiau and Bogdan Damoc and Aidan Clark and Diego de Las Casas and Aurelia Guy and Jacob Menick and Roman Ring and T. Hennigan and Saffron Huang and Lorenzo Maggiore and Chris Jones and Albin Cassirer and Andy Brock and Michela Paganini and Geoffrey Irving and Oriol Vinyals and Simon Osindero and K. Simonyan and Jack W. Rae and Erich Elsen and L. Sifre},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Improving language models by retrieving from trillions of tokens},  volume = {abs/2112.04426},  year = {2021} }
Improving BERT Fine-tuning with Embedding Normalization	2019	https://www.semanticscholar.org/paper/bb6e205f56f064ae76703b40147422483c438ef6	This paper conducts systematic analysis over several sequence classification datasets to examine the embedding values of [CLS] token before the fine tuning phase, and presents the biased embedding distribution issue.	seed	3	Large pre-trained sentence encoders like BERT start a new chapter in natural language processing. A common practice to apply pre-trained BERT to sequence classification tasks (e.g., classification of sentences or sentence pairs) is by feeding the embedding of [CLS] token (in the last layer) to a task-specific classification layer, and then fine tune the model parameters of BERT and classifier jointly. In this paper, we conduct systematic analysis over several sequence classification datasets to examine the embedding values of [CLS] token before the fine tuning phase, and present the biased embedding distribution issue---i.e., embedding values of [CLS] concentrate on a few dimensions and are non-zero centered. Such biased embedding brings challenge to the optimization process during fine-tuning as gradients of [CLS] embedding may explode and result in degraded model performance. We further propose several simple yet effective normalization methods to modify the [CLS] embedding during the fine-tuning. Compared with the previous practice, neural classification model with the normalized embedding shows improvements on several text classification tasks, demonstrates the effectiveness of our method.	bb6e205f56f064ae76703b40147422483c438ef6	@['JournalArticle']{zhou-etal-2019-improving,  author = {Wenxuan Zhou and Junyi Du and Xiang Ren},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Improving BERT Fine-tuning with Embedding Normalization},  volume = {abs/1911.03918},  year = {2019} }
Improving Attention-Based Interpretability of Text Classification Transformers	2022	http://www.semanticscholar.org/paper/c01d0335f1bcc449ddebf1354a0654069504c089	It is shown that, with proper setup, attention may be used in such tasks with results comparable to state-of-the-art techniques, while also being faster and friendlier to the environment.	maybe	0	Transformers are widely used in NLP, where they consistently achieve state-of-the-art performance. This is due to their attention-based architecture, which allows them to model rich linguistic relations between words. However, transformers are difﬁcult to interpret. Being able to provide reasoning for its decisions is an important property for a model in domains where human lives are affected, such as hate speech detection and biomedicine. With transformers ﬁnding wide use in these ﬁelds, the need for interpretability techniques tailored to them arises. The effectiveness of attention-based interpretability techniques for transformers in text classiﬁcation is studied in this work. Despite concerns about attention-based interpretations in the literature, we show that, with proper setup, attention may be used in such tasks with results comparable to state-of-the-art techniques, while also being faster and friendlier to the environment. We validate our claims with a series of experiments that employ a new feature importance metric.	c01d0335f1bcc449ddebf1354a0654069504c089	@['JournalArticle']{mylonas-etal-2022-improving,  author = {Nikolaos Mylonas and Ioannis Mollas and Grigorios Tsoumakas},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Improving Attention-Based Interpretability of Text Classification Transformers},  volume = {abs/2209.10876},  year = {2022} }
Implicit Representations of Meaning in Neural Language Models	2021	https://www.semanticscholar.org/paper/ac879df2cc36f3f824fa24149517622b6bc7bd09	The results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.	yes	38	Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity’s current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.	ac879df2cc36f3f824fa24149517622b6bc7bd09	@['JournalArticle', 'Conference']{li-etal-2021-implicit,  author = {Belinda Z. Li and Maxwell Nye and Jacob Andreas},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1813-1827},  title = {Implicit Representations of Meaning in Neural Language Models},  year = {2021} }
Implicit representations of event properties within contextual language models: Searching for “causativity neurons”	2021	http://www.semanticscholar.org/paper/825102f671675500b13d9f6c789d643d1a4c4cfc	The paper shows that the neuron activations obtained from processing an English sentence provide discriminative features for predicting the (non-)causativity of the event denoted by the verb in a simple linear classifier.	maybe	1	This paper addresses the question to which extent neural contextual language models such as BERT implicitly represent complex semantic properties. More concretely, the paper shows that the neuron activations obtained from processing an English sentence provide discriminative features for predicting the (non-)causativity of the event denoted by the verb in a simple linear classifier. A layer-wise analysis reveals that the relevant properties are mostly learned in the higher layers. Moreover, further experiments show that appr. 10% of the neuron activations are enough to already predict causativity with a relatively high accuracy.	825102f671675500b13d9f6c789d643d1a4c4cfc	@None{seyffarth-etal-2021-implicit,  author = {Esther Seyffarth and Younes Samih and Laura Kallmeyer and Hassan Sajjad},  booktitle = {International Conference on Computational Semantics},  title = {Implicit representations of event properties within contextual language models: Searching for “causativity neurons”},  year = {2021} }
Implicit causality in GPT-2: a case study	2022	http://www.semanticscholar.org/paper/e8f57752f8be4bd0819a3ba380837b9f107387eb		maybe	0	This case study investigates the extent to which a language model (GPT-2) is able to capture native speakers’ intuitions about implicit causality in a sentence completion task. We ﬁrst reproduce earlier results (showing lower surprisal values for pronouns that are congru-ent with either the subject or object, depend-ing on which one corresponds to the implicit causality bias of the verb), and then examine the effects of gender and verb frequency on model performance. Our second study exam-ines the reasoning ability of GPT-2: is the model able to produce more sensible motiva-tions for why the subject VERB ed the object if the verbs have stronger causality biases? We also developed a methodology to avoid human raters being biased by obscenities and disﬂuencies generated by the model.	e8f57752f8be4bd0819a3ba380837b9f107387eb	@['JournalArticle']{huynh-etal-2022-implicit,  author = {H. Huynh and T. Lentz and Emiel van Miltenburg},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Implicit causality in GPT-2: a case study},  volume = {abs/2212.04348},  year = {2022} }
Impact of Pretraining Term Frequencies on Few-Shot Reasoning	2022	http://www.semanticscholar.org/paper/833a2f1817cb9aeb292620454889cae78e26dda4	Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, the results raise the question of how much models actually generalize beyond pretraining data, and researchers are encouraged to take thepretraining data into account when interpreting evaluation results.	yes	50	Pretrained Language Models (LMs) have demonstrated ability to perform numerical reasoning by extrapolating from a few examples in few-shot settings. However, the extent to which this ex-trapolation relies on robust reasoning is unclear. In this paper, we investigate how well these models reason with terms that are less frequent in the pretraining data. In particular, we examine the correlations between the model performance on test instances and the frequency of terms from those instances in the pretraining data. We measure the strength of this correlation for a number of GPT-based language models (pretrained on the Pile dataset) on various numerical deduction tasks (e.g., arithmetic and unit conversion). Our results consistently demonstrate that models are more accurate on instances whose terms are more preva-lent, in some cases above 70% (absolute) more accurate on the top 10% frequent terms in comparison to the bottom 10%. Overall, although LMs exhibit strong performance at few-shot numerical reasoning tasks, our results raise the question of how much models actually generalize beyond pretraining data, and we encourage researchers to take the pretraining data into account when interpreting evaluation results.	833a2f1817cb9aeb292620454889cae78e26dda4	@['JournalArticle']{razeghi-etal-2022-impact,  author = {Yasaman Razeghi and Robert L Logan IV and Matt Gardner and Sameer Singh},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Impact of Pretraining Term Frequencies on Few-Shot Reasoning},  volume = {abs/2202.07206},  year = {2022} }
Ignore Previous Prompt: Attack Techniques For Language Models	2022	http://www.semanticscholar.org/paper/9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6	This work investigates two types of attacks – goal hijacking and prompt leaking – and demonstrates that even low-aptitude, but sufﬁciently ill-intentioned agents, can easily exploit GPT-3’s stochastic nature, creating long-tail risks.	maybe	0	Transformer-based large language models (LLMs) provide a powerful foundation for natural language tasks in large-scale customer-facing applications. However, studies that explore their vulnerabilities emerging from malicious user interac-tion are scarce. By proposing P ROMPT I NJECT , a prosaic alignment framework for mask-based iterative adversarial prompt composition, we examine how GPT-3, the most widely deployed language model in production, can be easily mis-aligned by simple handcrafted inputs. In particular, we investigate two types of attacks – goal hijacking and prompt leaking – and demonstrate that even low-aptitude, but sufﬁciently ill-intentioned agents, can easily exploit GPT-3’s stochastic nature, creating long-tail risks. The code for P ROMPT I NJECT is available at github.com/agencyenterprise/PromptInject .	9716a2876d08fce9d8e5c5ba4d7b1a9af44806d6	@['JournalArticle']{perez-ribeiro-2022-ignore,  author = {F'abio Perez and Ian Ribeiro},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Ignore Previous Prompt: Attack Techniques For Language Models},  volume = {abs/2211.09527},  year = {2022} }
Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models	2021	http://www.semanticscholar.org/paper/3ca3ff98405b43fab32dcd7cbd6bd34261386e35	This paper aims to automatically identify spurious correlations in NLP models at scale by leveraging existing interpretability methods to extract tokens that signiﬁcantly affect model’s decision process from the input text and identifying “genuine” and “spurious” tokens.	maybe	9	Recently, NLP models have achieved remark-able progress across a variety of tasks; however, they have also been criticized for being not robust. Many robustness problems can be attributed to models exploiting spurious correlations , or shortcuts between the training data and the task labels. Most existing work identiﬁes a limited set of task-speciﬁc shortcuts via human priors or error analyses, which requires extensive expertise and efforts. In this paper, we aim to automatically identify such spurious correlations in NLP models at scale. We ﬁrst leverage existing interpretability methods to extract tokens that signiﬁcantly affect model’s decision process from the input text. We then distinguish “genuine” tokens and “spurious” tokens by analyzing model predictions across multiple corpora and further verify them through knowledge-aware perturbations. We show that our proposed method can effectively and efﬁciently identify a scalable set of “shortcuts”, and mitigating these leads to more robust models in multiple applications.	3ca3ff98405b43fab32dcd7cbd6bd34261386e35	@['JournalArticle']{wang-etal-2021-identifying,  author = {Tianlu Wang and Diyi Yang and Xuezhi Wang},  booktitle = {NAACL-HLT},  pages = {1719-1729},  title = {Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models},  year = {2021} }
Identifying and Manipulating the Psychological Personality Traits of Language Models	2022	http://www.semanticscholar.org/paper/4f3a92b6a007cdf4f275fbf3888c19f4470b5510	It is shown that when provided different types of contexts, language models such as BERT and GPT2 can consis-tently identify and reﬂect personality markers in those contexts, which illustrates an ability to be manipulated in a highly predictable way.	maybe	0	Psychology research has long explored aspects of human personality such as extroversion , agreeableness and emotional stability . Catego-rizations like the ‘Big Five’ personality traits are commonly used to assess and diagnose personality types. In this work, we explore the question of whether language models exhibit consistent personalities in their language generation. For example, is a language model such as GPT2 likely to respond in a consistent way if asked to go out to a party? We also investigate whether such personality traits can be controlled. We show that when provided different types of contexts (such as personality descriptions, or answers to diagnostic questions about personality traits), language models such as BERT and GPT2 can consis-tently identify and reﬂect personality markers in those contexts. This behavior illustrates an ability to be manipulated in a highly predictable way, and frames them as tools for identifying personality traits and controlling personas in applications such as dialog systems. We also contribute a crowd-sourced data-set of personality descriptions of human subjects paired with their ‘Big Five’ personality assessment data, and a data-set of personality descriptions collated from Reddit.	4f3a92b6a007cdf4f275fbf3888c19f4470b5510	@None{2022-identifying,  title = {Identifying and Manipulating the Psychological Personality Traits of Language Models},  year = {2022} }
IDC: quantitative evaluation benchmark of interpretation methods for deep text classification models	2021	https://www.semanticscholar.org/paper/fd2df9d8c1b516e4d85103f938c7fdf5df44dadd	IDC is presented, a new benchmark for quantitative evaluation of interpretation methods for deep text classification models and Layer-wise Relevance Propagation and the gradient-by-input methods are identified as the winning interpretation methods in this study.	maybe	1	Recent advances in deep neural networks have achieved outstanding success in natural language processing tasks. Interpretation methods that provide insight into the decision-making process of these models have received an influx of research attention because of the success and the black-box nature of the deep text classification models. Evaluation of these methods has been based on changes in classification accuracy or prediction confidence when removing important words identified by these methods. There are no measurements of the actual difference between the predicted important words and humans’ interpretation of ground truth because of the lack of interpretation ground truth. A large publicly available interpretation ground truth has the potential to advance the development of interpretation methods. Manual labeling important words for each document to create a large interpretation ground truth is very time-consuming. This paper presents (1) IDC, a new benchmark for quantitative evaluation of interpretation methods for deep text classification models, and (2) evaluation of six interpretation methods using the benchmark. The IDC benchmark consists of: (1) Three methods that generate three pseudo-interpretation ground truth datasets. (2) Three performance metrics: interpretation recall, interpretation precision, and Cohen’s kappa inter-agreement. Findings: IDC-generated interpretation ground truth agrees with human annotators on sampled movie reviews. IDC identifies Layer-wise Relevance Propagation and the gradient-by-input methods as the winning interpretation methods in this study.	fd2df9d8c1b516e4d85103f938c7fdf5df44dadd	@['JournalArticle', 'Review']{khaleel-etal-2021-idc:,  author = {Mohammed Khaleel and Lei Qi and W. Tavanapong and J. Wong and Adisak Sukul and David A. M. Peterson},  booktitle = {Journal of Big Data},  journal = {Journal of Big Data},  pages = {1-14},  title = {IDC: quantitative evaluation benchmark of interpretation methods for deep text classification models},  volume = {9},  year = {2021} }
I'm sorry to hear that: Finding New Biases in Language Models with a Holistic Descriptor Dataset	2022	http://www.semanticscholar.org/paper/7ef43bacd43393ff116e6fcda6a52a6902e016d7	A new, more inclusive bias measurement dataset, H OLIS TIC B IAS, which includes nearly 600 descriptor terms across 13 different demographic axes and is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classiﬁer.	maybe	2	As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HolisticBias, which includes nearly 600 descriptor terms across 13 different demographic axes. HolisticBias was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HolisticBias is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.	7ef43bacd43393ff116e6fcda6a52a6902e016d7	@['Conference']{smith-etal-2022-“i’m,  author = {Eric J. M. Smith and Melissa Hall and Melanie Kambadur and Eleonora Presani and Adina Williams},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  title = {“I’m sorry to hear that”: Finding New Biases in Language Models with a Holistic Descriptor Dataset},  year = {2022} }
I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling	2020	http://www.semanticscholar.org/paper/01a9102aa93b152f2d2978c568fb7061eb7152f1	The DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues are introduced and it is shown that the best contradiction detection model correlates well with human judgments and is used in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.	maybe	45	To quantify how well natural language understanding models can capture consistency in a general conversation, we introduce the DialoguE COntradiction DEtection task (DECODE) and a new conversational dataset containing both human-human and human-bot contradictory dialogues. We show that: (i) our newly collected dataset is notably more effective at providing supervision for the dialogue contradiction detection task than existing NLI data including those aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and out-of-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots.	01a9102aa93b152f2d2978c568fb7061eb7152f1	@['JournalArticle', 'Conference']{nie-etal-2020-i,  author = {Yixin Nie and Mary Williamson and Mohit Bansal and Douwe Kiela and J. Weston},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {I like fish, especially dolphins: Addressing Contradictions in Dialogue Modeling},  volume = {abs/2012.13391},  year = {2020} }
Hyperbolic Embedding for Finding Syntax in BERT	2021	http://www.semanticscholar.org/paper/14bef5de586e908f8d2bd6d799d80f7b55043ad5	It is shown that the tree distance between words in a syntax tree can be approximated well by the hyperbolic distance between corresponding word vectors.	yes	0	Recent advances in natural language processing have improved our understanding of what kind of linguistic knowledge is encoded in modern word representations. For example, methods for testing the ability to extract syntax trees from a language model architecture were developed by Hewitt and Manning (2019)—they project word vectors into Euclidean subspace in such a way that the corresponding squared Euclidean distance approximates the tree distance between words in the syntax tree. This work proposes a method for assessing whether embedding word representations in hyperbolic space can better reflect the graph structure of syntax trees. We show that the tree distance between words in a syntax tree can be approximated well by the hyperbolic distance between corresponding word vectors.	14bef5de586e908f8d2bd6d799d80f7b55043ad5	@None{auyespek-etal-2021-hyperbolic,  author = {Temirlan Auyespek and T. Mach and Z. Assylbekov},  title = {Hyperbolic Embedding for Finding Syntax in BERT},  year = {2021} }
Humans, machines, and language: A deep alignment in underlying computational styles?	2021	http://www.semanticscholar.org/paper/75ee08b97031cc85fb2761ac1e9582b811a20c19	To directly evaluate parallels in the internal operations of artificial neural networks, multi-level measures of word-by-word sentence interpretation from ANNs are extracted, and Representational Similarity Analysis is used to test these against the representational geometries of real-time brain activity for the same sentences heard by human listeners.	maybe	1	The emergence of AI systems that emulate the remarkable human capacity for language has raised fundamental questions about complex cognition in humans and machines. This lively debate has largely taken place, however, in the absence of specific empirical evidence about how the internal operations of artificial neural networks (ANNs) relate to processes in the human brain as listeners speak and understand language. To directly evaluate these parallels, we extracted multi-level measures of word-by-word sentence interpretation from ANNs, and used Representational Similarity Analysis (RSA) to test these against the representational geometries of real-time brain activity for the same sentences heard by human listeners. These uniquely spatiotemporally specific comparisons reveal deep commonalities in the use of multi-dimensional probabilistic constraints to drive incremental interpretation processes in both humans and machines. But at the same time they demonstrate profound differences in the underlying functional architectures that implement this shared algorithmic alignment.	75ee08b97031cc85fb2761ac1e9582b811a20c19	@None{lyu-etal-2021-humans,  author = {Bingjiang Lyu and L. Tyler and Yuxing Fang and W. Marslen-Wilson},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Humans, machines, and language: A deep alignment in underlying computational styles?},  year = {2021} }
Human-Model Divergence in the Handling of Vagueness	2021	http://www.semanticscholar.org/paper/9e3051794def7d6ec7dc4fdd51b7502af67902e4	This work inspects the behavior of visually grounded and text-only models, finding systematic divergences from human judgments even when a model’s overall performance is high.	maybe	0	While aggregate performance metrics can generate valuable insights at a large scale, their dominance means more complex and nuanced language phenomena, such as vagueness, may be overlooked. Focusing on vague terms (e.g. sunny, cloudy, young, etc.) we inspect the behavior of visually grounded and text-only models, finding systematic divergences from human judgments even when a model’s overall performance is high. To help explain this disparity, we identify two assumptions made by the datasets and models examined and, guided by the philosophy of vagueness, isolate cases where they do not hold.	9e3051794def7d6ec7dc4fdd51b7502af67902e4	@None{eskin-etal-2021-human,  author = {Elias Stengel-Eskin and Jimena Guallar-Blasco and Benjamin Van Durme},  booktitle = {UNIMPLICIT},  journal = {Proceedings of the 1st Workshop on Understanding Implicit and Underspecified Language},  title = {Human-Model Divergence in the Handling of Vagueness},  year = {2021} }
Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark	2019	http://www.semanticscholar.org/paper/4888102774ad93140391f3a26af0f54cfba5ec34	It is concluded that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding using the BERT model in limited-data regimes.	maybe	39	The GLUE benchmark (Wang et al., 2019b) is a suite of language understanding tasks which has seen dramatic progress in the past year, with average performance moving from 70.0 at launch to 83.9, state of the art at the time of writing (May 24, 2019). Here, we measure human performance on the benchmark, in order to learn whether significant headroom remains for further progress. We provide a conservative estimate of human performance on the benchmark through crowdsourcing: Our annotators are non-experts who must learn each task from a brief set of instructions and 20 examples. In spite of limited training, these annotators robustly outperform the state of the art on six of the nine GLUE tasks and achieve an average score of 87.1. Given the fast pace of progress however, the headroom we observe is quite limited. To reproduce the data-poor setting that our annotators must learn in, we also train the BERT model (Devlin et al., 2019) in limited-data regimes, and conclude that low-resource sentence classification remains a challenge for modern neural network approaches to text understanding.	4888102774ad93140391f3a26af0f54cfba5ec34	@['JournalArticle', 'Conference']{nangia-bowman-2019-human,  author = {Nikita Nangia and Samuel R. Bowman},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Human vs. Muppet: A Conservative Estimate of Human Performance on the GLUE Benchmark},  volume = {abs/1905.10425},  year = {2019} }
Human Interpretation of Saliency-based Explanation Over Text	2022	http://www.semanticscholar.org/paper/56528ea81b30610f7cff12987a63ea7b5bae5b6f	It is found that people often mis-interpret the explanations: superficial and unrelated factors influence the explainees’ importance assignment despite the explanation communicating importance directly, and some of this distortion can be attenuated.	maybe	9	While a lot of research in explainable AI focuses on producing effective explanations, less work is devoted to the question of how people understand and interpret the explanation. In this work, we focus on this question through a study of saliency-based explanations over textual data. Feature-attribution explanations of text models aim to communicate which parts of the input text were more influential than others towards the model decision. Many current explanation methods, such as gradient-based or Shapley value-based methods, provide measures of importance which are well-understood mathematically. But how does a person receiving the explanation (the explainee) comprehend it? And does their understanding match what the explanation attempted to communicate? We empirically investigate the effect of various factors of the input, the feature-attribution explanation, and visualization procedure, on laypeople’s interpretation of the explanation. We query crowdworkers for their interpretation on tasks in English and German, and fit a GAMM model to their responses considering the factors of interest. We find that people often mis-interpret the explanations: superficial and unrelated factors, such as word length, influence the explainees’ importance assignment despite the explanation communicating importance directly. We then show that some of this distortion can be attenuated: we propose a method to adjust saliencies based on model estimates of over- and under-perception, and explore bar charts as an alternative to heatmap saliency visualization. We find that both approaches can attenuate the distorting effect of specific factors, leading to better-calibrated understanding of the explanation.	56528ea81b30610f7cff12987a63ea7b5bae5b6f	@['JournalArticle', 'Book']{schuff-etal-2022-human,  author = {Hendrik Schuff and Alon Jacovi and Heike Adel and Yoav Goldberg and Ngoc Thang Vu},  booktitle = {Conference on Fairness, Accountability and Transparency},  journal = {2022 ACM Conference on Fairness, Accountability, and Transparency},  title = {Human Interpretation of Saliency-based Explanation Over Text},  year = {2022} }
Human Heuristics for AI-Generated Language Are Flawed	2022	http://www.semanticscholar.org/paper/80a7d9cda6766e72ddf41fae40a2548d08928fe1		yes	2	Human communication is increasingly intermixed with language generated by AI. Across chat, email, and social media, AI systems produce smart replies, autocompletes, and translations. AI-generated language is often not identified as such but presented as language written by humans, raising concerns about novel forms of deception and manipulation. Here, we study how humans discern whether verbal self-presentations, one of the most personal and consequential forms of language, were generated by AI. In six experiments, participants (N = 4,600) were unable to detect self-presentations generated by state-of-the-art AI language models in professional, hospitality, and dating contexts. A computational analysis of language features shows that human judgments of AI-generated language are handicapped by intuitive but flawed heuristics such as associating first-person pronouns, spontaneous wording, or family topics with human-written language. We experimentally demonstrate that these heuristics make human judgment of AI-generated language predictable and manipulable, allowing AI systems to produce language perceived as more human than human. We discuss solutions, such as AI accents, to reduce the deceptive potential of language generated by AI, limiting the subversion of human intuition. the final 1,500 human-written self-presentations for the experiment. We fine-tuned a 774M parameter version of GPT-2 (31) for four epochs with a learning rate of 0.00002 on the collected data. We used the fine-tuned model and nucleus sampling (44) at p=0.95 to produce 1,500 AI-generated hospitality self-presentations. In the professional context, we collected 37,450 profile self-presentations with at least 60 and no more than 90 words from Guru.com, a platform where companies find freelance workers for commissioned work. In the dating context, we used a publicly available dataset of 59,940 OkCupid.com self-presentation essays collected with the platform operators’ permission (45). We drew a random sample of 1,000 human self-presentations for the professional and dating main experiments. We used the full set of collected self-presentations in each of these two contexts to fine-tune a 13B parameter version of GPT-3 (1) for four epochs with a learning rate multiplier of 0.1. We produced 1,000 AI-generated self-presentations for each experiment using the fine-tuned models with temperature sampling at t=0.9. We used multiple techniques to confirm that the models did not plagiarize the training data. For example, we searched for identical sentences in the training data and AI-generated text and found that 95% of sentences the AI-generated texts were not present in the training data. As we found no signs of substantial plagiarism, we used the AI-generated self-presentations without further preprocessing.	80a7d9cda6766e72ddf41fae40a2548d08928fe1	@['JournalArticle']{jakesch-etal-2022-human,  author = {Maurice Jakesch and J. Hancock and Mor Naaman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Human Heuristics for AI-Generated Language Are Flawed},  volume = {abs/2206.07271},  year = {2022} }
How would Stance Detection Techniques Evolve after the Launch of ChatGPT?	2022	http://www.semanticscholar.org/paper/c3d4471f095d37549fef3f824a2057f7497bc36e	For the stance detection tasks, experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance, and opens up the possibility of building explanatory AI for stance detection.	maybe	0	Stance detection refers to the task of extract-ing the standpoint (Favor, Against or Neither) towards a target in given texts. Such research gains increasing attention with the pro-liferation of social media contents. The conventional framework of handling stance detection is converting it into text classiﬁcation tasks. Deep learning models have already re-placed rule-based models and traditional machine learning models in solving such problems. Current deep neural networks are fac-ing two main challenges which are insufﬁ-cient labeled data and information in social media posts and the unexplainable nature of deep learning models. A new pre-trained language model chatGPT was launched on Nov 30, 2022. For the stance detection tasks, our experiments show that ChatGPT can achieve SOTA or similar performance for commonly used datasets including SemEval-2016 and P-Stance. At the same time, ChatGPT can provide explanation for its own prediction, which is beyond the capability of any existing model. The explanations for the cases it cannot provide classiﬁcation results are especially useful. ChatGPT has the potential to be the best AI model for stance detection tasks in NLP, or at least change the research paradigm of this ﬁeld. ChatGPT also opens up the possibility of building explanatory AI for stance detection.	c3d4471f095d37549fef3f824a2057f7497bc36e	@['JournalArticle']{zhang-etal-2022-how,  author = {Bowen Zhang and Daijun Ding and Liwen Jing},  booktitle = {ArXiv},  journal = {ArXiv},  title = {How would Stance Detection Techniques Evolve after the Launch of ChatGPT?},  volume = {abs/2212.14548},  year = {2022} }
How well do NLI models capture verb veridicality?	2019	http://www.semanticscholar.org/paper/cc8a93f934b39ffb437ab6a72ddc2549cf6c1360	It is shown that, encouragingly, BERT’s inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.	maybe	24	In natural language inference (NLI), contexts are considered veridical if they allow us to infer that their underlying propositions make true claims about the real world. We investigate whether a state-of-the-art natural language inference model (BERT) learns to make correct inferences about veridicality in verb-complement constructions. We introduce an NLI dataset for veridicality evaluation consisting of 1,500 sentence pairs, covering 137 unique verbs. We find that both human and model inferences generally follow theoretical patterns, but exhibit a systematic bias towards assuming that verbs are veridical–a bias which is amplified in BERT. We further show that, encouragingly, BERT’s inferences are sensitive not only to the presence of individual verb types, but also to the syntactic role of the verb, the form of the complement clause (to- vs. that-complements), and negation.	cc8a93f934b39ffb437ab6a72ddc2549cf6c1360	@['JournalArticle', 'Conference']{ross-pavlick-2019-how,  author = {Alexis Ross and Ellie Pavlick},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {2230-2240},  title = {How well do NLI models capture verb veridicality?},  year = {2019} }
How transfer learning impacts linguistic knowledge in deep NLP models?	2021	http://www.semanticscholar.org/paper/40b3bebc595ca091b4ee654e12272ad9201c04dc	This work investigates how fine-tuning towards downstream NLP tasks impacts the learned linguistic knowledge and finds that for some GLUE tasks, the network relies on the core linguistic information and preserve it deeper in the network, while for others it forgets.	maybe	24	Transfer learning from pre-trained neural language models towards downstream tasks has been a predominant theme in NLP recently. Several researchers have shown that deep NLP models learn non-trivial amount of linguistic knowledge, captured at different layers of the model. We investigate how fine-tuning towards downstream NLP tasks impacts the learned linguistic knowledge. We carry out a study across popular pre-trained models BERT, RoBERTa and XLNet using layer and neuronlevel diagnostic classifiers. We found that for some GLUE tasks, the network relies on the core linguistic information and preserve it deeper in the network, while for others it forgets. Linguistic information is distributed in the pre-trained language models but becomes localized to the lower layers post-fine-tuning, reserving higher layers for the task specific knowledge. The pattern varies across architectures, with BERT retaining linguistic information relatively deeper in the network compared to RoBERTa and XLNet, where it is predominantly delegated to the lower layers.	40b3bebc595ca091b4ee654e12272ad9201c04dc	@['JournalArticle']{durrani-etal-2021-how,  author = {Nadir Durrani and Hassan Sajjad and Fahim Dalvi},  booktitle = {Findings},  journal = {ArXiv},  title = {How transfer learning impacts linguistic knowledge in deep NLP models?},  volume = {abs/2105.15179},  year = {2021} }
How to Query Language Models?	2021	http://www.semanticscholar.org/paper/e248b22b7107ab057a0ea42a4dd075a5a4b2df26	This work proposes to query LMs by example and suggests that LMs contain more factual and commonsense edge than previously assumed—if the authors query the model in the right way.	yes	3	Large pre-trained language models (LMs) are 001 capable of not only recovering linguistic but 002 also factual and commonsense knowledge. To 003 access the knowledge stored in mask-based 004 LMs, we can use cloze-style questions and 005 let the model fill in the blank. The flexibil006 ity advantage over structured knowledge bases 007 comes with the drawback of finding the right 008 query for a certain information need. Inspired 009 by human behavior to disambiguate a question, 010 we propose to query LMs by example. To clar011 ify the ambivalent question Who does Neuer 012 play for?, a successful strategy is to demon013 strate the relation using another subject, e.g., 014 Ronaldo plays for Portugal. Who does Neuer 015 play for?. We apply this approach of query016 ing by example to the LAMA probe and obtain 017 substantial improvements of up to 37.8% for 018 BERT-large on the T-REx data when provid019 ing only 10 demonstrations—even outperform020 ing a baseline that queries the model with up 021 to 40 paraphrases of the question. The exam022 ples are provided through the model’s context 023 and thus require neither fine-tuning nor an ad024 ditional forward pass. This suggests that LMs 025 contain more factual and commonsense knowl026 edge than previously assumed—if we query 027 the model in the right way. 028	e248b22b7107ab057a0ea42a4dd075a5a4b2df26	@['JournalArticle']{adolphs-etal-2021-how,  author = {Leonard Adolphs and S. Dhuliawala and Thomas Hofmann},  booktitle = {ArXiv},  journal = {ArXiv},  title = {How to Query Language Models?},  volume = {abs/2108.01928},  year = {2021} }
How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for Token-level Evaluation Metrics	2020	http://www.semanticscholar.org/paper/39e398b2db7e4f43e05544a9c9834e59ae8c9d11	With experiments on probe tasks, it is observed that, unlike RNN based architectures, transformer model may not be learning to comprehend the input text despite its generated text having higher overlap with the target text.	maybe	4	Though generative dialogue modeling is widely seen as a language modeling task, the task demands an agent to have a complex natural language understanding of its input text to carry a meaningful interaction with an user. The automatic metrics used evaluate the quality of the generated text as a proxy to the holistic interaction of the agent. Such metrics were earlier shown to not correlate with the human judgement. In this work, we observe that human evaluation of dialogue agents can be inconclusive due to the lack of sufficient information for appropriate evaluation. The automatic metrics are deterministic yet shallow and human evaluation can be relevant yet inconclusive. To bridge this gap in evaluation, we propose designing a set of probing tasks to evaluate dialogue models. The hand-crafted tasks are aimed at quantitatively evaluating a generative dialogue model's understanding beyond the token-level evaluation on the generated text. The probing tasks are deterministic like automatic metrics and requires human judgement in their designing; benefiting from the best of both worlds. With experiments on probe tasks we observe that, unlike RNN based architectures, transformer model may not be learning to comprehend the input text despite its generated text having higher overlap with the target text.	39e398b2db7e4f43e05544a9c9834e59ae8c9d11	@['JournalArticle']{parthasarathi-etal-2020-how,  author = {Prasanna Parthasarathi and Joelle Pineau and Sarath Chandar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {How To Evaluate Your Dialogue System: Probe Tasks as an Alternative for Token-level Evaluation Metrics},  volume = {abs/2008.10427},  year = {2020} }
How to Dissect a Muppet: The Structure of Transformer Embedding Spaces	2022	http://www.semanticscholar.org/paper/728fff6344f9ce65fafcbf3b9aea4f0eb908d44d	It is shown that Pretrained embeddings can mathematically be reframed as a sum of vector factors and how to use this reframing to study the impact of each component, as well as a quantitative overview of the effects of finetuning on the overall embedding space.	maybe	3	Abstract Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.	728fff6344f9ce65fafcbf3b9aea4f0eb908d44d	@['JournalArticle', 'Review']{mickus-etal-2022-how,  author = {Timothee Mickus and Denis Paperno and Mathieu Constant},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {981-996},  title = {How to Dissect a Muppet: The Structure of Transformer Embedding Spaces},  volume = {10},  year = {2022} }
How to cheat on your final paper: Assigning AI for student writing	2022	http://www.semanticscholar.org/paper/42a7a89fda7a4e6d88ba7d8b130705d5be4f237f		yes	5		42a7a89fda7a4e6d88ba7d8b130705d5be4f237f	@None{fyfe-2022-how,  author = {Paul Fyfe},  booktitle = {AI &amp; SOCIETY},  journal = {AI &amp; SOCIETY},  title = {How to cheat on your final paper: Assigning AI for student writing},  year = {2022} }
How Reliable are Model Diagnostics?	2021	http://www.semanticscholar.org/paper/7c799b7bd8c069c6feb7235345c97aa1f5330b84	This paper critically examine three recent diagnostic tests for pre-trained language models, and finds that likelihood-based and representation-based model diagnostics are not yet as reliable as previously assumed.	maybe	4	In the pursuit of a deeper understanding of a model’s behaviour, there is recent impetus for developing suites of probes aimed at diagnosing models beyond simple metrics like accuracy or BLEU. This paper takes a step back and asks an important and timely question: how reliable are these diagnostics in providing insight into models and training setups? We critically examine three recent diagnostic tests for pre-trained language models, and find that likelihood-based and representation-based model diagnostics are not yet as reliable as previously assumed. Based on our empirical findings, we also formulate recommendations for practitioners and researchers.	7c799b7bd8c069c6feb7235345c97aa1f5330b84	@['JournalArticle']{aribandi-etal-2021-how,  author = {V. Aribandi and Yi Tay and Donald Metzler},  booktitle = {Findings},  pages = {1778-1785},  title = {How Reliable are Model Diagnostics?},  year = {2021} }
How Pre-trained Word Representations Capture Commonsense Physical Comparisons	2019	http://www.semanticscholar.org/paper/41094beddc498680d807e99e07efa41fec5d6724	This work investigates how comparisons are made: models learn a consistent ordering over all the objects in the comparisons and finds probing models have significantly higher accuracy than those baseline models which use dataset artifacts.	maybe	5	Understanding common sense is important for effective natural language reasoning. One type of common sense is how two objects compare on physical properties such as size and weight: e.g., ‘is a house bigger than a person?’. We probe whether pre-trained representations capture comparisons and find they, in fact, have higher accuracy than previous approaches. They also generalize to comparisons involving objects not seen during training. We investigate how such comparisons are made: models learn a consistent ordering over all the objects in the comparisons. Probing models have significantly higher accuracy than those baseline models which use dataset artifacts: e.g., memorizing some words are larger than any other word.	41094beddc498680d807e99e07efa41fec5d6724	@['Conference']{goel-2019-how,  author = {Pranav Goel},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},  title = {How Pre-trained Word Representations Capture Commonsense Physical Comparisons},  year = {2019} }
How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis	2022	http://www.semanticscholar.org/paper/f2b0869b17bace854d73c19b449e3f88b9fed82e	A causal-inspired analysis quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words and concludes that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.	maybe	2	Recently, there has been a trend to investigate the factual knowledge captured by Pre-trained Language Models (PLMs). Many works show the PLMs’ ability to fill in the missing factual words in cloze-style prompts such as ”Dante was born in [MASK].” However, it is still a mystery how PLMs generate the results correctly: relying on effective clues or shortcut patterns? We try to answer this question by a causal-inspired analysis that quantitatively measures and evaluates the word-level patterns that PLMs depend on to generate the missing words. We check the words that have three typical associations with the missing words: knowledge-dependent, positionally close, and highly co-occurred. Our analysis shows: (1) PLMs generate the missing factual words more by the positionally close and highly co-occurred words than the knowledge-dependent words; (2) the dependence on the knowledge-dependent words is more effective than the positionally close and highly co-occurred words. Accordingly, we conclude that the PLMs capture the factual knowledge ineffectively because of depending on the inadequate associations.	f2b0869b17bace854d73c19b449e3f88b9fed82e	@['JournalArticle']{li-etal-2022-how,  author = {Shaobo Li and Xiaoguang Li and Lifeng Shang and Zhenhua Dong and Chengjie Sun and Bingquan Liu and Zhenzhou Ji and Xin Jiang and Qun Liu},  booktitle = {Findings},  pages = {1720-1732},  title = {How Pre-trained Language Models Capture Factual Knowledge? A Causal-Inspired Analysis},  year = {2022} }
How much pretraining data do language models need to learn syntax?	2021	http://www.semanticscholar.org/paper/4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c	The experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.	yes	8	Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost.	4c6f7fb5c2e1bd12899c3ec2788f9ce7eb2f8a5c	@['JournalArticle', 'Conference']{mayos-etal-2021-how,  author = {Laura Pérez-Mayos and Miguel Ballesteros and L. Wanner},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1571-1582},  title = {How much pretraining data do language models need to learn syntax?},  year = {2021} }
How Much Knowledge Can You Pack into the Parameters of a Language Model?	2020	https://www.semanticscholar.org/paper/80376bdec5f534be78ba82821f540590ebce5559	It is shown that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions.	seed	381	It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales surprisingly well with model size and outperforms models that explicitly look up knowledge on the open-domain variants of Natural Questions and WebQuestions. To facilitate reproducibility and future work, we release our code and trained models.	80376bdec5f534be78ba82821f540590ebce5559	@['JournalArticle', 'Conference']{roberts-etal-2020-how,  author = {Adam Roberts and Colin Raffel and Noam M. Shazeer},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {How Much Knowledge Can You Pack into the Parameters of a Language Model?},  volume = {abs/2002.08910},  year = {2020} }
How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers	2022	http://www.semanticscholar.org/paper/2537af99905a27d9b84ba9968715f4287f1d3359	It is found that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline, and it is shown that better-performing models lose more from applying the PAPA method than weaker models, suggesting that the utilization of the input- dependent attention mechanism might be a factor in their success.	maybe	0	The attention mechanism is considered the backbone of the widely-used Transformer architecture. It contextualizes the input by computing input-specific attention matrices. We find that this mechanism, while powerful and elegant, is not as important as typically thought for pretrained language models. We introduce PAPA,1 a new probing method that replaces the input-dependent attention matrices with constant ones—the average attention weights over multiple inputs. We use PAPA to analyze several established pretrained Transformers on six downstream tasks. We find that without any input-dependent attention, all models achieve competitive performance—an average relative drop of only 8% from the probing baseline. Further, little or no performance drop is observed when replacing half of the input-dependent attention matrices with constant (input-independent) ones. Interestingly, we show that better-performing models lose more from applying our method than weaker models, suggesting that the utilization of the input-dependent attention mechanism might be a factor in their success. Our results motivate research on simpler alternatives to input-dependent attention, as well as on methods for better utilization of this mechanism in the Transformer architecture.	2537af99905a27d9b84ba9968715f4287f1d3359	@['JournalArticle']{hassid-etal-2022-how,  author = {Michael Hassid and Hao Peng and Daniel Rotem and Jungo Kasai and Ivan Montero and Noah Smith and Roy Schwartz},  booktitle = {ArXiv},  journal = {ArXiv},  title = {How Much Does Attention Actually Attend? Questioning the Importance of Attention in Pretrained Transformers},  volume = {abs/2211.03495},  year = {2022} }
How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN	2021	http://www.semanticscholar.org/paper/04db9b694280134f09af5fa787a306907edba29d	AVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure, is introduced, showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues.	yes	16	Current language models can generate highquality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure—e.g., individual dependencies—model-generated text is substantially less novel than our baseline of human-generated text from each model’s test set. For larger-scale structure— e.g., overall sentence structure—modelgenerated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis showing that GPT-2’s novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).	04db9b694280134f09af5fa787a306907edba29d	@['JournalArticle']{mccoy-etal-2021-how,  author = {R. Thomas McCoy and P. Smolensky and Tal Linzen and Jianfeng Gao and Asli Celikyilmaz},  booktitle = {ArXiv},  journal = {ArXiv},  title = {How much do language models copy from their training data? Evaluating linguistic novelty in text generation using RAVEN},  volume = {abs/2111.09509},  year = {2021} }
How many data points is a prompt worth?	2021	http://www.semanticscholar.org/paper/a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9	It is found that prompting does indeed provide a benefit, and that this benefit can be quantified per task, and results show that prompting is often worth 100s of data points on average across classification tasks.	maybe	145	When fine-tuning pretrained models for classification, researchers either use a generic model head or a task-specific prompt for prediction. Proponents of prompting have argued that prompts provide a method for injecting task-specific guidance, which is beneficial in low-data regimes. We aim to quantify this benefit through rigorous testing of prompts in a fair setting: comparing prompted and head-based fine-tuning in equal conditions across many tasks and data sizes. By controlling for many sources of advantage, we find that prompting does indeed provide a benefit, and that this benefit can be quantified per task. Results show that prompting is often worth 100s of data points on average across classification tasks.	a9fe5bd8da2d9603cf2cf6c6ea8b0f83c6d3a4f9	@['JournalArticle', 'Conference']{scao-rush-2021-how,  author = {Teven Le Scao and Alexander M. Rush},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2627-2636},  title = {How many data points is a prompt worth?},  year = {2021} }
How Large Language Models are Transforming Machine-Paraphrased Plagiarism	2022	http://www.semanticscholar.org/paper/f158e70e9ead719e8a524eaf8ec79270574f2eda	It is suggested that large models can rewrite text humans have difﬁculty identifying as machine-paraphrased (53% mean acc.), and the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5).	maybe	1	The recent success of large language models for text generation poses a severe threat to academic integrity, as plagiarists can generate realistic paraphrases indistinguishable from original work.However, the role of large autoregressive models in generating machine-paraphrased plagiarism and their detection is still incipient in the literature.This work explores T5 and GPT3 for machine-paraphrase generation on scientific articles from arXiv, student theses, and Wikipedia.We evaluate the detection performance of six automated solutions and one commercial plagiarism detection software and perform a human study with 105 participants regarding their detection performance and the quality of generated examples.Our results suggest that large language models can rewrite text humans have difficulty identifying as machine-paraphrased (53% mean acc.).Human experts rate the quality of paraphrases generated by GPT-3 as high as original texts (clarity 4.0/5, fluency 4.2/5, coherence 3.8/5).The best-performing detection model (GPT-3) achieves 66% F1-score in detecting paraphrases.We make our code, data, and findings publicly available to facilitate the development of detection solutions.	f158e70e9ead719e8a524eaf8ec79270574f2eda	@['JournalArticle', 'Conference']{wahle-etal-2022-how,  author = {Jan Philip Wahle and Terry Ruas and Frederic Kirstein and Bela Gipp},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {How Large Language Models are Transforming Machine-Paraphrase Plagiarism},  volume = {abs/2210.03568},  year = {2022} }
How is BERT surprised? Layerwise detection of linguistic anomalies	2021	http://www.semanticscholar.org/paper/528cd7c546315ae57e532ff9f57a674378a5ad6f	The best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer, suggesting that language models employ separate mechanisms to detect different types of linguistic anomalies.	yes	14	Transformer language models have shown remarkable ability in detecting when a word is anomalous in context, but likelihood scores offer no information about the cause of the anomaly. In this work, we use Gaussian models for density estimation at intermediate layers of three language models (BERT, RoBERTa, and XLNet), and evaluate our method on BLiMP, a grammaticality judgement benchmark. In lower layers, surprisal is highly correlated to low token frequency, but this correlation diminishes in upper layers. Next, we gather datasets of morphosyntactic, semantic, and commonsense anomalies from psycholinguistic studies; we find that the best performing model RoBERTa exhibits surprisal in earlier layers when the anomaly is morphosyntactic than when it is semantic, while commonsense anomalies do not exhibit surprisal at any intermediate layer. These results suggest that language models employ separate mechanisms to detect different types of linguistic anomalies.	528cd7c546315ae57e532ff9f57a674378a5ad6f	@['JournalArticle', 'Conference']{li-etal-2021-how,  author = {Bai Li and Zining Zhu and Guillaume Thomas and Yang Xu and F. Rudzicz},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {How is BERT surprised? Layerwise detection of linguistic anomalies},  volume = {abs/2105.07452},  year = {2021} }
How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild	2022	http://www.semanticscholar.org/paper/e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b	A novel theoretical evaluation framework and a distinctive experimental study assessing language models as general-purpose systems when used directly by human prompters --- in the wild indicate that language models such as GPT-3 have limited understanding of the human command.	maybe	0	The new generation of language models is reported to solve some extraordinary tasks the models were never trained for specifically, in few-shot or zero-shot settings. However, these reports usually cherry-pick the tasks, use the best prompts, and unwrap or extract the solutions leniently even if they are followed by nonsensical text. In sum, they are specialised results for one domain, a particular way of using the models and interpreting the results. In this paper, we present a novel theoretical evaluation framework and a distinctive experimental study assessing language models as general-purpose systems when used directly by human prompters --- in the wild. For a useful and safe interaction in these increasingly more common conditions, we need to understand when the model fails because of a lack of capability or a misunderstanding of the user's intents. Our results indicate that language models such as GPT-3 have limited understanding of the human command; far from becoming general-purpose systems in the wild.	e6ecdbbceff06cc1e667e3261596fd0fa6b32c4b	@['JournalArticle', 'Conference']{casares-etal-2022-how,  author = {Pablo Antonio Moreno Casares and B. S. Loe and John Burden and Seán Ó hÉigeartaigh and J. Hernández-Orallo},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {5295-5303},  title = {How General-Purpose Is a Language Model? Usefulness and Safety with Human Prompters in the Wild},  year = {2022} }
How Far Does BERT Look At: Distance-based Clustering and Analysis of BERT’s Attention	2020	http://www.semanticscholar.org/paper/1ab8ec9583db0f1bb28b59c992cd035bc7928f04	This work clearly cluster the attention heatmaps into significantly different patterns through unsupervised clustering on top of a set of proposed features, which corroborates with previous observations.	yes	10	Recent research on the multi-head attention mechanism, especially that in pre-trained models such as BERT, has shown us heuristics and clues in analyzing various aspects of the mechanism. As most of the research focus on probing tasks or hidden states, previous works have found some primitive patterns of attention head behavior by heuristic analytical methods, but a more systematic analysis specific on the attention patterns still remains primitive. In this work, we clearly cluster the attention heatmaps into significantly different patterns through unsupervised clustering on top of a set of proposed features, which corroborates with previous observations. We further study their corresponding functions through analytical study. In addition, our proposed features can be used to explain and calibrate different attention heads in Transformer models.	1ab8ec9583db0f1bb28b59c992cd035bc7928f04	@['JournalArticle', 'Conference']{guan-etal-2020-how,  author = {Yue Guan and Jingwen Leng and Chao Li and Quan Chen and M. Guo},  booktitle = {International Conference on Computational Linguistics},  journal = {ArXiv},  title = {How Far Does BERT Look At: Distance-based Clustering and Analysis of BERT’s Attention},  volume = {abs/2011.00943},  year = {2020} }
How effective is BERT without word ordering? Implications for language understanding and data privacy	2021	https://www.semanticscholar.org/paper/3757de51963e31d0d88dc7a0fbffb8e5d6ba0a79	It is shown that the token representations and self-attention activations within BERT are surprisingly resilient to shuffling the order of input tokens, and that for several GLUE language understanding tasks, shuffling only minimally degrades performance, e.g., by 4% for QNLI.	maybe	18	Ordered word sequences contain the rich structures that define language. However, it’s often not clear if or how modern pretrained language models utilize these structures. We show that the token representations and self-attention activations within BERT are surprisingly resilient to shuffling the order of input tokens, and that for several GLUE language understanding tasks, shuffling only minimally degrades performance, e.g., by 4% for QNLI. While bleak from the perspective of language understanding, our results have positive implications for cases where copyright or ethics necessitates the consideration of bag-of-words data (vs. full documents). We simulate such a scenario for three sensitive classification tasks, demonstrating minimal performance degradation vs. releasing full language sequences.	3757de51963e31d0d88dc7a0fbffb8e5d6ba0a79	@['JournalArticle', 'Conference']{hessel-schofield-2021-how,  author = {Jack Hessel and Alexandra Schofield},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {204-211},  title = {How effective is BERT without word ordering? Implications for language understanding and data privacy},  year = {2021} }
How does the pre-training objective affect what large language models learn about linguistic properties?	2022	http://www.semanticscholar.org/paper/5ca0a54fa0f76ae0e1881899c61b36d35d3bd166		maybe	4	Several pre-training objectives, such as masked language modeling (MLM), have been proposed to pre-train language models (e.g. BERT) with the aim of learning better language representations. However, to the best of our knowledge, no previous work so far has investigated how different pre-training objectives affect what BERT learns about linguistics properties. We hypothesize that linguistically motivated objectives such as MLM should help BERT to acquire better linguistic knowledge compared to other non-linguistically motivated objectives that are not intuitive or hard for humans to guess the association between the input and the label to be predicted. To this end, we pre-train BERT with two linguistically motivated objectives and three non-linguistically motivated ones. We then probe for linguistic characteristics encoded in the representation of the resulting models. We find strong evidence that there are only small differences in probing performance between the representations learned by the two different types of objectives. These surprising results question the dominant narrative of linguistically informed pre-training.	5ca0a54fa0f76ae0e1881899c61b36d35d3bd166	@['JournalArticle', 'Conference']{alajrami-aletras-2022-how,  author = {Ahmed Alajrami and Nikolaos Aletras},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {131-147},  title = {How does the pre-training objective affect what large language models learn about linguistic properties?},  year = {2022} }
How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy	2021	https://www.semanticscholar.org/paper/ceef266c59698999c9283a0cda852d8bc1ce27ea	It is demonstrated that, even though isotropy is a desirable geometrical property, fine-tuning does not necessarily result in isotropy enhancements, making existing isotropy enhancement methods ineffective.	maybe	5	It is widely accepted that fine-tuning pretrained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to fill this gap, in this paper, we analyze the extent to which the isotropy of the embedding space changes after fine-tuning. We demonstrate that, even though isotropy is a desirable geometrical property, fine-tuning does not necessarily result in isotropy enhancements. Moreover, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective.	ceef266c59698999c9283a0cda852d8bc1ce27ea	@['JournalArticle', 'Conference']{rajaee-pilehvar-2021-how,  author = {S. Rajaee and Mohammad Taher Pilehvar},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {3042-3049},  title = {How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy},  year = {2021} }
How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope	2020	https://www.semanticscholar.org/paper/868349fe969bc7c6b14b5f35e118a26075b7b1f2	It is found that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models, indicating there is evidence for a shallow encoding of negation only in the base models.	seed	27	Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.	868349fe969bc7c6b14b5f35e118a26075b7b1f2	@['JournalArticle', 'Conference']{zhao-bethard-2020-how,  author = {Yiyun Zhao and Steven Bethard},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4729-4747},  title = {How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope},  year = {2020} }
How does BERT process disfluency?	2021	http://www.semanticscholar.org/paper/4d486cc23a359aea48c75ec379dbbadf7c50800b	The study suggests that BERT has knowledge of disfluency structure, and emphasises the potential of using BERT to understand natural utterances withoutdisfluency removal.	maybe	0	Natural conversations are filled with disfluencies. This study investigates if and how BERT understands disfluency with three experiments: (1) a behavioural study using a downstream task, (2) an analysis of sentence embeddings and (3) an analysis of the attention mechanism on disfluency. The behavioural study shows that without fine-tuning on disfluent data, BERT does not suffer significant performance loss when presented disfluent compared to fluent inputs (exp1). Analysis on sentence embeddings of disfluent and fluent sentence pairs reveals that the deeper the layer, the more similar their representation (exp2). This indicates that deep layers of BERT become relatively invariant to disfluency. We pinpoint attention as a potential mechanism that could explain this phenomenon (exp3). Overall, the study suggests that BERT has knowledge of disfluency structure. We emphasise the potential of using BERT to understand natural utterances without disfluency removal.	4d486cc23a359aea48c75ec379dbbadf7c50800b	@['JournalArticle']{tian-etal-2021-how,  author = {Ye Tian and Tim Nieradzik and Sepehr Jalali and D. Shiu},  booktitle = {SIGDIAL Conferences},  pages = {208-217},  title = {How does BERT process disfluency?},  year = {2021} }
How does BERT capture semantics? A closer look at polysemous words	2020	https://www.semanticscholar.org/paper/e09863cbe78ef4b2c099cd2997bf404d8d4d7f14	This rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.	yes	22	The recent paradigm shift to contextual word embeddings has seen tremendous success across a wide range of down-stream tasks. However, little is known on how the emergent relation of context and semantics manifests geometrically. We investigate polysemous words as one particularly prominent instance of semantic organization. Our rigorous quantitative analysis of linear separability and cluster organization in embedding vectors produced by BERT shows that semantics do not surface as isolated clusters but form seamless structures, tightly coupled with sentiment and syntax.	e09863cbe78ef4b2c099cd2997bf404d8d4d7f14	@['JournalArticle']{yenicelik-etal-2020-how,  author = {David Yenicelik and Florian Schmidt and Yannic Kilcher},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {156-162},  title = {How does BERT capture semantics? A closer look at polysemous words},  year = {2020} }
How Does BERT Answer Questions?: A Layer-Wise Analysis of Transformer Representations	2019	https://www.semanticscholar.org/paper/8380ab11c120a77cbdd2053337aa52525ec0f22e	A layer-wise analysis of BERT's hidden states reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.	seed	90	Bidirectional Encoder Representations from Transformers (BERT) reach state-of-the-art results in a variety of Natural Language Processing tasks. However, understanding of their internal functioning is still insufficient and unsatisfactory. In order to better understand BERT and other Transformer-based models, we present a layer-wise analysis of BERT's hidden states. Unlike previous research, which mainly focuses on explaining Transformer models by their attention weights, we argue that hidden states contain equally valuable information. Specifically, our analysis focuses on models fine-tuned on the task of Question Answering (QA) as an example of a complex downstream task. We inspect how QA models transform token vectors in order to find the correct answer. To this end, we apply a set of general and QA-specific probing tasks that reveal the information stored in each representation layer. Our qualitative analysis of hidden state visualizations provides additional insights into BERT's reasoning process. Our results show that the transformations within BERT go through phases that are related to traditional pipeline tasks. The system can therefore implicitly incorporate task-specific information into its token representations. Furthermore, our analysis reveals that fine-tuning has little impact on the models' semantic abilities and that prediction errors can be recognized in the vector representations of even early layers.	8380ab11c120a77cbdd2053337aa52525ec0f22e	@['JournalArticle', 'Book', 'Conference']{aken-etal-2019-how,  author = {Betty van Aken and Benjamin Winter and Alexander Löser and Felix Alexander Gers},  booktitle = {International Conference on Information and Knowledge Management},  journal = {Proceedings of the 28th ACM International Conference on Information and Knowledge Management},  title = {How Does BERT Answer Questions?: A Layer-Wise Analysis of Transformer Representations},  year = {2019} }
How does Attention Affect the Model?	2021	http://www.semanticscholar.org/paper/83216b431d72cb5dcf852dfb6bc1fa60bb6dc55d	A comprehensive analytical framework is proposed that exploits a convex hull representation of sequence semantics in an n-dimensional Semantic Euclidean Space and defines a series of indicators to capture the impact of attention on sequence semantics to analyze why and how attention benefits the semantic capacity of different types of recurrent neural networks.	maybe	1	The attention layer has become a prevalent component in improving the effectiveness of neural network models for NLP tasks. Figuring out why attention is effective and its interpretability has attracted a widespread deliberation. Current studies mostly investigate the effect of attention mechanism based on the attention distribution it generates with one single neural network structure. However they do not consider the changes in semantic capability of different components in the model due to the attention mechanism, which can vary across different network structures. In this paper, we propose a comprehensive analytical framework that exploits a convex hull representation of sequence semantics in an n-dimensional Semantic Euclidean Space and defines a series of indicators to capture the impact of attention on sequence semantics. Through a series of experiments on various NLP tasks and three representative recurrent units, we analyze why and how attention benefits the semantic capacity of different types of recurrent neural networks based on the indicators defined in the proposed framework.	83216b431d72cb5dcf852dfb6bc1fa60bb6dc55d	@['JournalArticle']{zhang-etal-2021-how,  author = {Chen Zhang and Qiuchi Li and L. Hua and D. Song},  booktitle = {Findings},  pages = {256-268},  title = {How does Attention Affect the Model?},  year = {2021} }
How Do Transformer-Architecture Models Address Polysemy of Korean Adverbial Postpositions?	2022	http://www.semanticscholar.org/paper/3a24f0736fc77da0f0375c536f0fafed7c819d55	A classification model is devised by employing two transformer-architecture models—BERT and GPT-2—and a computational simulation is introduced that interactively demonstrates how these transformer-archy models simulate human interpretation of word-level polysemy involving Korean adverbial postpositions.	maybe	0	Postpositions, which are characterized as multiple form-function associations and thus polysemous, pose a challenge to automatic identification of their usage. Several studies have used contextualized word-embedding models to reveal the functions of Korean postpositions. Despite the superior classification performance of previous studies, the particular reason how these models resolve the polysemy of Korean postpositions is not enough clear. To add more interpretation, for this reason, we devised a classification model by employing two transformer-architecture models—BERT and GPT-2—and introduces a computational simulation that interactively demonstrates how these transformer-architecture models simulate human interpretation of word-level polysemy involving Korean adverbial postpositions -ey, -eyse, and -(u)lo. Results reveal that (i) the BERT model performs better than the GPT-2 model to classify the intended function of postpositions, (ii) there is an inverse relationship between the classification accuracy and the number of functions that each postposition manifests, (iii) model performance is affected by the corpus size of each function, (iv) the models’ performance gradually improves as the epoch proceeds, and (vi) the models are affected by the scarcity of input and/or semantic closeness between the items.	3a24f0736fc77da0f0375c536f0fafed7c819d55	@['JournalArticle']{mun-desagulier-2022-how,  author = {Seongmin Mun and Guillaume Desagulier},  booktitle = {Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},  pages = {11-21},  title = {How Do Transformer-Architecture Models Address Polysemy of Korean Adverbial Postpositions?},  year = {2022} }
How Do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking	2020	http://www.semanticscholar.org/paper/ca8d228cc20829fa95a95bfd5f7e92b2d13f9f5a	Differentiable Masking relies on learning sparse stochastic gates to completely mask-out subsets of the input while maintaining end-to-end differentiability and is used to study BERT models on sentiment classification and question answering.	maybe	39	Attribution methods assess the contribution of inputs (e.g., words) to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the model prediction. Despite its conceptual simplicity, erasure is not commonly used in practice. First, the objective is generally intractable, and approximate search or leave-one-out estimates are typically used instead; both approximations may be inaccurate and remain very expensive with modern deep (e.g., BERT-based) NLP models. Second, the method is susceptible to the hindsight bias: the fact that a token can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these two challenges, we introduce Differentiable Masking. DiffMask relies on learning sparse stochastic gates (i.e., masks) to completely mask-out subsets of the input while maintaining end-to-end differentiability. The decision to include or disregard an input token is made with a simple linear model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient at test time because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.	ca8d228cc20829fa95a95bfd5f7e92b2d13f9f5a	@['JournalArticle', 'Conference']{cao-etal-2020-how,  author = {Nicola De Cao and M. Schlichtkrull and Wilker Aziz and Ivan Titov},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {3243-3255},  title = {How Do Decisions Emerge across Layers in Neural Models? Interpretation with Differentiable Masking},  year = {2020} }
How Do BERT Embeddings Organize Linguistic Knowledge?	2021	http://www.semanticscholar.org/paper/773472a423d176fedfc46ba03287ed41995c99f7	A suite of several probing tasks showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence and found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.	maybe	9	Several studies investigated the linguistic information implicitly encoded in Neural Language Models. Most of these works focused on quantifying the amount and type of information available within their internal representations and across their layers. In line with this scenario, we proposed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arrange within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties.	773472a423d176fedfc46ba03287ed41995c99f7	@['JournalArticle']{puccetti-etal-2021-how,  author = {Giovanni Puccetti and Alessio Miaschi and F. Dell’Orletta},  booktitle = {Workshop on Knowledge Extraction and Integration for Deep Learning Architectures; Deep Learning Inside Out},  pages = {48-57},  title = {How Do BERT Embeddings Organize Linguistic Knowledge?},  year = {2021} }
How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks	2022	http://www.semanticscholar.org/paper/08e6b510944f9c7df50dbfa3da7c3ef0af8f2574	The results of the experiments show that syntactic information is encoded locally in a way consistent with the French grammar, using probing, causal analysis and feature selection method.	yes	1	This work addresses the question of the localization of syntactic information encoded in the transformers representations. We tackle this question from two perspectives, considering the object-past participle agreement in French, by identifying, first, in which part of the sentence and, second, in which part of the representation the syntactic information is encoded. The results of our experiments, using probing, causal analysis and feature selection method, show that syntactic information is encoded locally in a way consistent with the French grammar.	08e6b510944f9c7df50dbfa3da7c3ef0af8f2574	@['JournalArticle', 'Conference']{li-etal-2022-how,  author = {Bingzhi Li and Guillaume Wisniewski and B. Crabbé},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {501-507},  title = {How Distributed are Distributed Representations? An Observation on the Locality of Syntactic Information in Verb Agreement Tasks},  year = {2022} }
How Decoding Strategies Affect the Verifiability of Generated Text	2019	http://www.semanticscholar.org/paper/027946f80f3cb276ea38bc2cf19903052f59cd0e	This work investigates the verifiability of text generated by state-of-the-art pre-trained language models and discovers a tradeoff between factuality and repetitiveness.	maybe	39	Recent progress in pre-trained language models led to systems that are able to generate text of an increasingly high quality. While several works have investigated the fluency and grammatical correctness of such models, it is still unclear to which extent the generated text is consistent with factual world knowledge. Here, we go beyond fluency and also investigate the verifiability of text generated by state-of-the-art pre-trained language models. A generated sentence is verifiable if it can be corroborated or disproved by Wikipedia, and we find that the verifiability of generated text strongly depends on the decoding strategy. In particular, we discover a tradeoff between factuality (i.e., the ability of generating Wikipedia corroborated text) and repetitiveness. While decoding strategies such as top-k and nucleus sampling lead to less repetitive generations, they also produce less verifiable text. Based on these finding, we introduce a simple and effective decoding strategy which, in comparison to previously used decoding strategies, produces less repetitive and more verifiable text.	027946f80f3cb276ea38bc2cf19903052f59cd0e	@['JournalArticle']{massarelli-etal-2019-how,  author = {Luca Massarelli and Fabio Petroni and Aleksandra Piktus and Myle Ott and Tim Rocktäschel and Vassilis Plachouras and F. Silvestri and Sebastian Riedel},  booktitle = {Findings},  journal = {ArXiv},  title = {How Decoding Strategies Affect the Verifiability of Generated Text},  volume = {abs/1911.03587},  year = {2019} }
How Contextualized Word Embeddings Represent Word Senses	2021	http://www.semanticscholar.org/paper/3db7975639de24456495f86d3538293084b212e4		maybe	0	English. Contextualized embedding models, such as ELMo and BERT, allow the construction of vector representations of lexical items that adapt to the context in which words appear. It was demonstrated that the upper layers of these models capture semantic information. This evidence paved the way for the development of sense representations based on words in context. In this paper, we analyze the vector spaces produced by 11 pre-trained models and evaluate these representations on two tasks. The analysis shows that all these representations contain redundant information. The results show the disadvantage of this aspect. Italiano. Modelli come ELMo o BERT consentono di ottenere rappresentazioni vettoriali delle parole che si adattano al contesto in cui queste appaiono. Il fatto che i livelli alti di questi modelli immagazzinino informazione semantica ha portato a sviluppare rappresentazioni di senso basate su parole nel contesto. In questo lavoro analizziamo gli spazi vettoriali prodotti con 11 modelli pre-addestrati e valutiamo le loro prestazioni nel rappresentare i diversi sensi delle parole. Le analisi condotte mostrano che questi modelli contengono informazioni ridondanti. I risultati evidenziano le criticità inerenti a questo aspetto.	3db7975639de24456495f86d3538293084b212e4	@['JournalArticle']{tripodi-2021-how,  author = {Rocco Tripodi},  booktitle = {Italian Conference on Computational Linguistics},  title = {How Contextualized Word Embeddings Represent Word Senses},  year = {2021} }
How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings	2019	https://www.semanticscholar.org/paper/9d7902e834d5d1d35179962c7a5b9d16623b0d39	It is found that in all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualization representations.	seed	400	Replacing static word embeddings with contextualized word representations has yielded significant improvements on many NLP tasks. However, just how contextual are the contextualized representations produced by models such as ELMo and BERT? Are there infinitely many context-specific representations for each word, or are words essentially assigned one of a finite number of word-sense representations? For one, we find that the contextualized representations of all words are not isotropic in any layer of the contextualizing model. While representations of the same word in different contexts still have a greater cosine similarity than those of two different words, this self-similarity is much lower in upper layers. This suggests that upper layers of contextualizing models produce more context-specific representations, much like how upper layers of LSTMs produce more task-specific representations. In all layers of ELMo, BERT, and GPT-2, on average, less than 5% of the variance in a word’s contextualized representations can be explained by a static embedding for that word, providing some justification for the success of contextualized representations.	9d7902e834d5d1d35179962c7a5b9d16623b0d39	@['JournalArticle', 'Conference']{ethayarajh-2019-how,  author = {Kawin Ethayarajh},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {55-65},  title = {How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},  year = {2019} }
How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns	2022	http://www.semanticscholar.org/paper/91c6e906be4a0d346d4da351d691166643840aa0		yes	2	Gender-neutral pronouns have recently been introduced in many languages to a) include non-binary people and b) as a generic singular. Recent results from psycholinguistics suggest that gender-neutral pronouns (in Swedish) are not associated with human processing difficulties. This, we show, is in sharp contrast with automated processing. We show that gender-neutral pronouns in Danish, English, and Swedish are associated with higher perplexity, more dispersed attention patterns, and worse downstream performance. We argue that such conservativity in language models may limit widespread adoption of gender-neutral pronouns and must therefore be resolved.	91c6e906be4a0d346d4da351d691166643840aa0	@['JournalArticle', 'Conference']{brandl-etal-2022-how,  author = {Stephanie Brandl and Ruixiang Cui and Anders Søgaard},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {How Conservative are Language Models? Adapting to the Introduction of Gender-Neutral Pronouns},  volume = {abs/2204.10281},  year = {2022} }
How coherent are neural models of coherence?	2020	http://www.semanticscholar.org/paper/890f4892d9fc6856556ad8ee16e8e3916affe084	Four generic evaluation tasks that draw on different aspects of coherence at both the lexical and document levels, and can be applied to any corpora are proposed that point to a strong need for revisiting the common practices in the development and evaluation of coherent models.	maybe	9	Despite the recent advances in coherence modelling, most such models including state-of-the-art neural ones, are evaluated on either contrived proxy tasks such as the standard order discrimination benchmark, or tasks that require special expert annotation. Moreover, most evaluations are conducted on small newswire corpora. To address these shortcomings, in this paper we propose four generic evaluation tasks that draw on different aspects of coherence at both the lexical and document levels, and can be applied to any corpora. In designing these tasks, we aim at capturing coherence-specific properties, such as the correct use of discourse connectives, lexical cohesion, as well as the overall temporal and causal consistency among events and participants in a story. Importantly, our proposed tasks either rely on automatically-generated data, or data annotated for other purposes, hence alleviating the need for annotation specifically targeted to the task of coherence modelling. We perform experiments with several existing state-of-the-art neural models of coherence on these tasks, across large corpora from different domains, including newswire, dialogue, as well as narrative and instructional text. Our findings point to a strong need for revisiting the common practices in the development and evaluation of coherence models.	890f4892d9fc6856556ad8ee16e8e3916affe084	@['JournalArticle', 'Conference']{pishdad-etal-2020-how,  author = {Leila Pishdad and Federico Fancellu and Ran Zhang and A. Fazly},  booktitle = {International Conference on Computational Linguistics},  pages = {6126-6138},  title = {How coherent are neural models of coherence?},  year = {2020} }
How Can We Know What Language Models Know?	2019	https://www.semanticscholar.org/paper/81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85	This paper proposes mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts to provide a tighter lower bound on what LMs know.	seed	393	Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as “Obama is a __ by profession”. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as “Obama worked as a __ ” may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.	81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85	@['JournalArticle']{jiang-etal-2019-how,  author = {Zhengbao Jiang and Frank F. Xu and J. Araki and Graham Neubig},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {423-438},  title = {How Can We Know What Language Models Know?},  volume = {8},  year = {2019} }
How Can the [MASK] Know? The Sources and Limitations of Knowledge in BERT	2021	http://www.semanticscholar.org/paper/ad0e6dfee9be05e2c9021779f2f995a3caeb3642	It is concluded that BERT offers low and unreliable performance out of the box and the stored knowledge is fragile and based on token co-occurrence in the training set used during pre-training, rather than generalization or inference.	maybe	0	We explore the idea of using the pre-trained BERT as a source of factual knowledge, analyze which components of the model are responsible for its ability to answer questions requiring factual knowledge, and study the transferability of the knowledge to downstream tasks. Our experiments show that the Language Modeling Head is indispensable for predicting facts, implying that transferability of any knowledge captured in the model is limited. While the dominant approach to researching how knowledge is stored in language models focuses on tailoring question formulation to optimize the retrieval quality, we find question patterns easily understood by humans that confuse BERT to the point that the answer does not make sense. The nature of the found patterns implies that the stored knowledge is fragile and based on token co-occurrence in the training set used during pre-training, rather than generalization or inference. Moreover, using a novel, hand-crafted dataset we show that BERT is vulnerable to common misconceptions, which could have fatal effects on downstream applications. Overall, we conclude that BERT offers low and unreliable performance out of the box. Jupyter notebooks with experiments are available on GitHub.11Project URL: https://github.com/tenpercent/knowledge-and-confusion	ad0e6dfee9be05e2c9021779f2f995a3caeb3642	@['JournalArticle', 'Conference']{podkorytov-etal-2021-how,  author = {Maksim Podkorytov and Daniel Bis and Xiuwen Liu},  booktitle = {IEEE International Joint Conference on Neural Network},  journal = {2021 International Joint Conference on Neural Networks (IJCNN)},  pages = {1-8},  title = {How Can the [MASK] Know? The Sources and Limitations of Knowledge in BERT},  year = {2021} }
How Can BERT Help Lexical Semantics Tasks	2019	http://www.semanticscholar.org/paper/d1d410956fc587c08ebf573482f43030dce9653a	This work makes use of dynamic embeddings as word representations in training staticembeddings, thereby leveraging their strong representation power for disambiguating context information and shows that this method leads to improvements over traditional static embedDings on a range of lexical semantics tasks.	maybe	5	Contextualized embeddings such as BERT can serve as strong input representations to NLP tasks, outperforming their static embeddings counterparts such as skip-gram, CBOW and GloVe. However, such embeddings are dynamic, calculated according to a sentence-level context, which limits their use in lexical semantics tasks. We address this issue by making use of dynamic embeddings as word representations in training static embeddings, thereby leveraging their strong representation power for disambiguating context information. Results show that this method leads to improvements over traditional static embeddings on a range of lexical semantics tasks, obtaining the best reported results on seven datasets.	d1d410956fc587c08ebf573482f43030dce9653a	@None{wang-etal-2019-how,  author = {Yile Wang and Leyang Cui and Yue Zhang},  journal = {arXiv: Computation and Language},  title = {How Can BERT Help Lexical Semantics Tasks},  year = {2019} }
How BPE Affects Memorization in Transformers	2021	http://www.semanticscholar.org/paper/4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80	It is demonstrated that the size of the subword vocabulary learned by Byte-Pair Encoding greatly affects both ability and tendency of standard Transformer models to memorize training data, even when the authors control for the number of learned parameters.	maybe	9	Training data memorization in NLP can both be beneficial (e.g., closed-book QA) and undesirable (personal data extraction). In any case, successful model training requires a non-trivial amount of memorization to store word spellings, various linguistic idiosyncrasies and common knowledge. However, little is known about what affects the memorization behavior of NLP models, as the field tends to focus on the equally important question of generalization. In this work, we demonstrate that the size of the subword vocabulary learned by Byte-Pair Encoding (BPE) greatly affects both ability and tendency of standard Transformer models to memorize training data, even when we control for the number of learned parameters. We find that with a large subword vocabulary size, Transformer models fit random mappings more easily and are more vulnerable to membership inference attacks. Similarly, given a prompt, Transformer-based language models with large subword vocabularies reproduce the training data more often. We conjecture this effect is caused by reduction in the sequences’ length that happens as the BPE vocabulary grows. Our findings can allow a more informed choice of hyper-parameters, that is better tailored for a particular use-case.	4bc8851f2e2758326eb0d57f7d46ab9d74cfdf80	@['JournalArticle']{kharitonov-etal-2021-how,  author = {E. Kharitonov and Marco Baroni and D. Hupkes},  booktitle = {ArXiv},  journal = {ArXiv},  title = {How BPE Affects Memorization in Transformers},  volume = {abs/2110.02782},  year = {2021} }
How about Time? Probing a Multilingual Language Model for Temporal Relations	2022	http://www.semanticscholar.org/paper/b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc	This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages, and results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddeddings.	maybe	0	This paper presents a comprehensive set of probing experiments using a multilingual language model, XLM-R, for temporal relation classification between events in four languages. Results show an advantage of contextualized embeddings over static ones and a detrimen- tal role of sentence level embeddings. While obtaining competitive results against state-of-the-art systems, our probes indicate a lack of suitable encoded information to properly address this task.	b8768fbba4d356a1c49dcd8e4cd33aea0c6472bc	@['JournalArticle', 'Conference']{caselli-etal-2022-how,  author = {Tommaso Caselli and Irene Dini and F. Dell’Orletta},  booktitle = {International Conference on Computational Linguistics},  pages = {3197-3209},  title = {How about Time? Probing a Multilingual Language Model for Temporal Relations},  year = {2022} }
How (Un)Faithful is Attention?	2022	https://www.semanticscholar.org/paper/e2216806be6e2c300e2333226a553b7bd9783152	This work presents a simple approach to compute the newly proposed metric AtteFa, which can quantitatively represent the degree of faithfulness of the attention weights, and validates the effect of the frequency of informative input elements and the use of contextual vs. non-contextual encoders on the faithfulnessof the attention mechanism.	maybe	0	Although attention weights have been commonly used as a means to provide explanations for deep learning models, the approach has been widely criticized due to its lack of faithfulness. In this work, we present a simple approach to compute the newly proposed metric AtteFa, which can quantitatively represent the degree of faithfulness of the attention weights. Using this metric, we further validate the effect of the frequency of informative input elements and the use of contextual vs. non-contextual encoders on the faithfulness of the attention mechanism. Finally, we apply the approach on several real-life binary classification datasets to measure the faithfulness of attention weights in real-life settings.	e2216806be6e2c300e2333226a553b7bd9783152	@None{amini-kosseim-2022-how,  author = {Hessam Amini and Leila Kosseim},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  title = {How (Un)Faithful is Attention?},  year = {2022} }
Homonym Identification using BERT - Using a Clustering Approach	2021	http://www.semanticscholar.org/paper/38e50476407ff19c3f1d6c3ffc1c993985eca3ae	This project aims to determine whether contextual information is sufficient for identifying a homonymous word by using BERT embeddings as opposed to Word2Vec, which conflates senses into one vector.	maybe	2	Homonym identification is important for WSD that require coarse-grained partitions of senses. The goal of this project is to determine whether contextual information is sufficient for identifying a homonymous word. To capture the context, BERT embeddings are used as opposed to Word2Vec, which conflates senses into one vector. SemCor is leveraged to retrieve the embeddings. Various clustering algorithms are applied to the embeddings. Finally, the embeddings are visualized in a lower-dimensional space to understand the feasibility of the clustering process.	38e50476407ff19c3f1d6c3ffc1c993985eca3ae	@['JournalArticle']{saha-2021-homonym,  author = {Rohan Saha},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Homonym Identification using BERT - Using a Clustering Approach},  volume = {abs/2101.02398},  year = {2021} }
Hitachi at SemEval-2020 Task 3: Exploring the Representation Spaces of Transformers for Human Sense Word Similarity	2020	http://www.semanticscholar.org/paper/dfff07f198d7f86436028787252059d83a97a503	Interestingly, the experiments reveal a language-independent characteristic: the middle to upper layers of Transformer-based language models can induce good approximate similarity measures.	maybe	2	In this paper, we present our system for SemEval-2020 task 3, Predicting the (Graded) Effect of Context in Word Similarity. Due to the unsupervised nature of the task, we concentrated on inquiring about the similarity measures induced by different layers of different pre-trained Transformer-based language models, which can be good approximations of the human sense of word similarity. Interestingly, our experiments reveal a language-independent characteristic: the middle to upper layers of Transformer-based language models can induce good approximate similarity measures. Finally, our system was ranked 1st on the Slovenian part of Subtask1 and 2nd on the Croatian part of both Subtask1 and Subtask2.	dfff07f198d7f86436028787252059d83a97a503	@['JournalArticle']{morishita-etal-2020-hitachi,  author = {Terufumi Morishita and Gaku Morio and Hiroaki Ozaki and Toshinori Miyoshi},  booktitle = {International Workshop on Semantic Evaluation},  pages = {286-291},  title = {Hitachi at SemEval-2020 Task 3: Exploring the Representation Spaces of Transformers for Human Sense Word Similarity},  year = {2020} }
HILDIF: Interactive Debugging of NLI Models Using Influence Functions	2021	http://www.semanticscholar.org/paper/d84183f96462f2cf3023f7a6121b3f7ae274d208	A novel explanatory debugging pipeline called HILDIF is proposed, enabling humans to improve deep text classifiers using influence functions as an explanation method, and can effectively alleviate artifact problems in fine-tuned BERT models and result in increased model generalizability.	maybe	13	Biases and artifacts in training data can cause unwelcome behavior in text classifiers (such as shallow pattern matching), leading to lack of generalizability. One solution to this problem is to include users in the loop and leverage their feedback to improve models. We propose a novel explanatory debugging pipeline called HILDIF, enabling humans to improve deep text classifiers using influence functions as an explanation method. We experiment on the Natural Language Inference (NLI) task, showing that HILDIF can effectively alleviate artifact problems in fine-tuned BERT models and result in increased model generalizability.	d84183f96462f2cf3023f7a6121b3f7ae274d208	@None{zylberajch-etal-2021-hildif:,  author = {Hugo Zylberajch and Piyawat Lertvittayakumjorn and F. Toni},  booktitle = {INTERNLP},  journal = {Proceedings of the First Workshop on Interactive Learning for Natural Language Processing},  title = {HILDIF: Interactive Debugging of NLI Models Using Influence Functions},  year = {2021} }
Higher-order Comparisons of Sentence Encoder Representations	2019	http://www.semanticscholar.org/paper/3e9e638faeea2b10412c4a82cb376d2ee315a7db	The utility of RSA is demonstrated by establishing a previously unknown correspondence between widely-employed pretrained language encoders and human processing difficulty via eye-tracking data, showcasing its potential in the interpretability toolbox for neural models.	maybe	11	Representational Similarity Analysis (RSA) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities (e.g., fMRI, electrophysiology, behavior). As a framework, RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classification: namely, it does not require large training samples, is not prone to overfitting, and it enables a more transparent comparison between the representational geometries of different models and modalities. We demonstrate the utility of RSA by establishing a previously unknown correspondence between widely-employed pretrained language encoders and human processing difficulty via eye-tracking data, showcasing its potential in the interpretability toolbox for neural models.	3e9e638faeea2b10412c4a82cb376d2ee315a7db	@['JournalArticle', 'Conference']{abdou-etal-2019-higher,  author = {Mostafa Abdou and Artur Kulmizev and Felix Hill and D. Low and Anders Søgaard},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {5837-5844},  title = {Higher-order Comparisons of Sentence Encoder Representations},  year = {2019} }
Hidden Backdoors in Human-Centric Language Models	2021	http://www.semanticscholar.org/paper/c586f3a69102fffdff178ca79b0be767d384da43	The proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA).	maybe	29	Natural language processing (NLP) systems have been proven to be vulnerable to backdoor attacks, whereby hidden features (backdoors) are trained into a language model and may only be activated by specific inputs (called triggers), to trick the model into producing unexpected behaviors. In this paper, we create covert and natural triggers for textual backdoor attacks, hidden backdoors, where triggers can fool both modern language models and human inspection. We deploy our hidden backdoors through two state-of-the-art trigger embedding methods. The first approach via homograph replacement, embeds the trigger into deep neural networks through the visual spoofing of lookalike characters replacement. The second approach uses subtle differences between text generated by language models and real natural text to produce trigger sentences with correct grammar and high fluency. We demonstrate that the proposed hidden backdoors can be effective across three downstream security-critical NLP tasks, representative of modern human-centric NLP systems, including toxic comment detection, neural machine translation (NMT), and question answering (QA). Our two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at least 97% with an injection rate of only 3% in toxic comment detection, 95.1% ASR in NMT with less than 0.5% injected data, and finally 91.12% ASR against QA updated with only 27 poisoning data samples on a model previously trained with 92,024 samples (0.029%). We are able to demonstrate the adversary's high success rate of attacks, while maintaining functionality for regular users, with triggers inconspicuous by the human administrators.	c586f3a69102fffdff178ca79b0be767d384da43	@['JournalArticle', 'Book', 'Conference']{li-etal-2021-hidden,  author = {Shaofeng Li and Hui Liu and Tian Dong and Benjamin Zi Hao Zhao and Minhui Xue and Haojin Zhu and Jialiang Lu},  booktitle = {Conference on Computer and Communications Security},  journal = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},  title = {Hidden Backdoors in Human-Centric Language Models},  year = {2021} }
Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models	2021	http://www.semanticscholar.org/paper/063183d95a249d94c95d12e7e9462e0aa84b6d85	It is found that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender, and several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation.	maybe	10	All AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and/or race/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities.	063183d95a249d94c95d12e7e9462e0aa84b6d85	@['JournalArticle']{smith-williams-2021-hi,  author = {Eric Michael Smith and Adina Williams},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models},  volume = {abs/2109.03300},  year = {2021} }
Heuristic interpretation as rational inference: A computational model of the N400 and P600 in language processing	2023	http://www.semanticscholar.org/paper/ced882fdae535112294e17429e97b3e25a8cf145		maybe	0		ced882fdae535112294e17429e97b3e25a8cf145	@['JournalArticle']{li-ettinger-2022-heuristic,  author = {Jiaxuan Li and Allyson Ettinger},  booktitle = {Cognition},  journal = {Cognition},  title = {Heuristic interpretation as rational inference: A computational model of the N400 and P600 in language processing},  volume = {233},  year = {2022} }
Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data	2022	http://www.semanticscholar.org/paper/7d165f4418486e88bc41d5e330694236e3bd1081	GPT-3 can identify the hero, villain, and victim in diverse domains: newspaper articles, movie plot summaries, and political speeches.	maybe	2	This paper shows how to use large-scale pretrained language models to extract character roles from narrative texts without domain-specific training data. Queried with a zero-shot question-answering prompt, GPT-3 can identify the hero, villain, and victim in diverse domains: newspaper articles, movie plot summaries, and political speeches.	7d165f4418486e88bc41d5e330694236e3bd1081	@['JournalArticle']{stammbach-etal-2022-heroes,  author = {Dominik Stammbach and Maria Antoniak and Elliott Ash},  booktitle = {WNU},  journal = {ArXiv},  title = {Heroes, Villains, and Victims, and GPT-3: Automated Extraction of Character Roles Without Training Data},  volume = {abs/2205.07557},  year = {2022} }
HellaSwag: Can a Machine Really Finish Your Sentence?	2019	https://www.semanticscholar.org/paper/8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad	The construction of HellaSwag, a new challenge dataset, and its resulting difficulty, sheds light on the inner workings of deep pretrained models, and suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.	seed	308	Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as “A woman sits at a piano,” a machine must select the most likely followup: “She sets her fingers on the keys.” With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans (>95% accuracy), state-of-the-art models struggle (<48%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical ‘Goldilocks’ zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.	8b0f27bb594b1eaaf493eaf1e2ee723a2b0a19ad	@['JournalArticle', 'Conference']{zellers-etal-2019-hellaswag:,  author = {Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4791-4800},  title = {HellaSwag: Can a Machine Really Finish Your Sentence?},  year = {2019} }
Have You Seen That Number? Investigating Extrapolation in Question Answering Models	2021	http://www.semanticscholar.org/paper/b59947541d2ac4211c4b17554b2e16c260299bed	This work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, and proposes the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text.	maybe	5	Numerical reasoning in machine reading comprehension (MRC) has shown drastic improvements over the past few years. While the previous models for numerical MRC are able to interpolate the learned numerical reasoning capabilities, it is not clear whether they can perform just as well on numbers unseen in the training dataset. Our work rigorously tests state-of-the-art models on DROP, a numerical MRC dataset, to see if they can handle passages that contain out-of-range numbers. One of the key findings is that the models fail to extrapolate to unseen numbers. Presenting numbers as digit-by-digit input to the model, we also propose the E-digit number form that alleviates the lack of extrapolation in models and reveals the need to treat numbers differently from regular words in the text. Our work provides a valuable insight into the numerical MRC models and the way to represent number forms in MRC.	b59947541d2ac4211c4b17554b2e16c260299bed	@['JournalArticle', 'Conference']{kim-etal-2021-have,  author = {Jeonghwan Kim and Giwon Hong and Kyung-min Kim and Junmo Kang and Sung-Hyon Myaeng},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {7031-7037},  title = {Have You Seen That Number? Investigating Extrapolation in Question Answering Models},  year = {2021} }
Have Attention Heads in BERT Learned Constituency Grammar?	2021	http://www.semanticscholar.org/paper/a5af4e1b100aa307c81427444d08f8bb25358d3c	The results show that there exist heads that can induce some grammar types much better than baselines, suggesting that some heads act as a proxy for constituency grammar.	yes	3	With the success of pre-trained language models in recent years, more and more researchers focus on opening the “black box” of these models. Following this interest, we carry out a qualitative and quantitative analysis of constituency grammar in attention heads of BERT and RoBERTa. We employ the syntactic distance method to extract implicit constituency grammar from the attention weights of each head. Our results show that there exist heads that can induce some grammar types much better than baselines, suggesting that some heads act as a proxy for constituency grammar. We also analyze how attention heads’ constituency grammar inducing (CGI) ability changes after fine-tuning with two kinds of tasks, including sentence meaning similarity (SMS) tasks and natural language inference (NLI) tasks. Our results suggest that SMS tasks decrease the average CGI ability of upper layers, while NLI tasks increase it. Lastly, we investigate the connections between CGI ability and natural language understanding ability on QQP and MNLI tasks.	a5af4e1b100aa307c81427444d08f8bb25358d3c	@['JournalArticle', 'Conference']{luo-2021-have,  author = {Ziyang Luo},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Have Attention Heads in BERT Learned Constituency Grammar?},  volume = {abs/2102.07926},  year = {2021} }
Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review	2022	http://www.semanticscholar.org/paper/22d314c347a6c5c0bbf528a102455229ce0b36b5	The importance of NLG being guided by knowledge, in order to convey human-like reasoning through language generation, is explored, and ten goals for intelligent NLG systems to pursue are proposed.	maybe	0	The rapid development and application of natural language generation (NLG) techniques has revolutionized the ﬁeld of automatic text pro-duction. However, these techniques are still limited in their ability to produce human-like text that is truly reasonable and informative. In this paper, we explore the importance of NLG being guided by knowledge, in order to convey human-like reasoning through language generation. We propose ten goals for intelligent NLG systems to pursue, and brieﬂy review the achievement of NLG techniques guided by knowledge and reasoning. We also conclude by envisioning future directions and challenges in the pursuit of these goals.	22d314c347a6c5c0bbf528a102455229ce0b36b5	@['JournalArticle', 'Review']{chen-xiao-2022-harnessing,  author = {Jiangjie Chen and Yanghua Xiao},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review},  volume = {abs/2212.03747},  year = {2022} }
Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies	2021	http://www.semanticscholar.org/paper/e6a237ab883e503b10b73b3a411c0078c47c9830	The complexity of gender and language around it is explained, how current language representations capture and perpetuate harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information are addressed.	maybe	46	Gender is widely discussed in the context of language tasks and when examining the stereotypes propagated by language models. However, current discussions primarily treat gender as binary, which can perpetuate harms such as the cyclical erasure of non-binary gender identities. These harms are driven by model and dataset biases, which are consequences of the non-recognition and lack of understanding of non-binary genders in society. In this paper, we explain the complexity of gender and language around it, and survey non-binary persons to understand harms associated with the treatment of gender as binary in English language technologies. We also detail how current language representations (e.g., GloVe, BERT) capture and perpetuate these harms and related challenges that need to be acknowledged and addressed for representations to equitably encode gender information.	e6a237ab883e503b10b73b3a411c0078c47c9830	@['JournalArticle', 'Conference', 'Review']{dev-etal-2021-harms,  author = {Sunipa Dev and Masoud Monajatipoor and Anaelia Ovalle and Arjun Subramonian and J. M. Phillips and Kai Wei Chang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies},  volume = {abs/2108.12084},  year = {2021} }
Grounding the Vector Space of an Octopus: Word Meaning from Raw Text	2023	https://www.semanticscholar.org/paper/2b3917d3c9129b3d2b5d8f4445445a5efec381f5		maybe	0		2b3917d3c9129b3d2b5d8f4445445a5efec381f5	@['Review']{søgaard-2023-grounding,  author = {Anders Søgaard},  booktitle = {Minds and Machines},  journal = {Minds and Machines},  title = {Grounding the Vector Space of an Octopus: Word Meaning from Raw Text},  year = {2023} }
Grammatical cues are largely, but not completely, redundant with word meanings in natural language	2022	http://www.semanticscholar.org/paper/67ec138400cad2beb9b84b448ff4a54ddc94cb2e		maybe	6	The combinatorial power of language has historically been argued to be enabled by syntax: rules that allow words to combine hierarchically to convey complex meanings. But how important are these rules in practice? We performed a broad-coverage cross-linguistic investigation of the importance of grammatical cues for interpretation. First, English and Russian speakers (n=484) were presented with subjects, verbs, and objects (in random order and with morphological markings removed) extracted from naturally occurring sentences, and were asked to identify which noun is the agent of the action. Accuracy was high in both languages (~89% in English, ~87% in Russian), suggesting that word meanings strongly constrain who is doing what to whom. Next, we trained a neural network machine classifier on a similar task: predicting which nominal in a subject-verb-object triad is the subject. Across 30 languages from eight language families, performance was consistently high: a median accuracy of 87%, comparable to the accuracy observed in the human experiments. These results have ramifications for any theory of why languages look the way that they do, and seemingly pose a challenge for efficiency-based theories: why have grammatical cues for argument role if they only have utility in 10-15% of sentences? We suggest that although grammatical cues are not usually necessary, they are useful in the rare cases when the intended meaning cannot be inferred from the words alone, including descriptions of human interactions, where roles are often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing non-canonical meanings (e.g., the man bit the dog). Importantly, for such cues to be useful, they have to be reliable, which means being ubiquitously used, including when they are not needed.	67ec138400cad2beb9b84b448ff4a54ddc94cb2e	@['JournalArticle']{mahowald-etal-2022-grammatical,  author = {Kyle Mahowald and E. Diachek and E. Gibson and Evelina Fedorenko and Richard Futrell},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Grammatical cues are largely, but not completely, redundant with word meanings in natural language},  volume = {abs/2201.12911},  year = {2022} }
Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps	2021	http://www.semanticscholar.org/paper/90f7b61762e4454b9cba3afbb10c5f07465cff85	A novel gradient-based method that analyzes self-attention units and identifies the input elements that explain the model's prediction the best, and obtains significant improvements over state-of-the-art alternatives.	maybe	4	Transformer-based language models significantly advanced the state-of-the-art in many linguistic tasks. As this revolution continues, the ability to explain model predictions has become a major area of interest for the NLP community. In this work, we present Gradient Self-Attention Maps (Grad-SAM) - a novel gradient-based method that analyzes self-attention units and identifies the input elements that explain the model's prediction the best. Extensive evaluations on various benchmarks show that Grad-SAM obtains significant improvements over state-of-the-art alternatives.	90f7b61762e4454b9cba3afbb10c5f07465cff85	@['JournalArticle', 'Book', 'Conference']{barkan-etal-2021-grad,  author = {Oren Barkan and Edan Hauon and Avi Caciularu and Ori Katz and Itzik Malkiel and Omri Armstrong and Noam Koenigstein},  booktitle = {International Conference on Information and Knowledge Management},  journal = {Proceedings of the 30th ACM International Conference on Information & Knowledge Management},  title = {Grad-SAM: Explaining Transformers via Gradient Self-Attention Maps},  year = {2021} }
GPT-3: Its Nature, Scope, Limits, and Consequences	2020	https://www.semanticscholar.org/paper/b103e87c7727134927d3ffb06934a95c10c02fc0	The nature of reversible and irreversible questions is discussed, that is, questions that may enable one to identify the nature of the source of their answers, and GPT-3, a third-generation, autoregressive language model that uses deep learning to produce human-like texts, is introduced.	maybe	262		b103e87c7727134927d3ffb06934a95c10c02fc0	@['JournalArticle']{floridi-chiriatti-2020-gpt,  author = {L. Floridi and Massimo Chiriatti},  booktitle = {Minds Mach.},  journal = {Minds and Machines},  pages = {681-694},  title = {GPT-3: Its Nature, Scope, Limits, and Consequences},  volume = {30},  year = {2020} }
GPT-3 Powered System for Content Generation and Transformation	2022	http://www.semanticscholar.org/paper/2e5bc6508f772e8be56fe7369661cacb0a49d46e	This study capitalizes on how the GPT-3 model can be used to generate and transform content without manual help from humans – how well plausibly GPT3-authored text most nearly passes as human-like prompts for content generation and manipulation purposes.	maybe	0	Enabling computer systems to understand and generate natural language has been an up-and-coming field of research. Latest advancements in Natural Language Processing (NLP) have made headway progress in facilitating this, like the GPT-3 language prediction model created by OpenAI. Given the capacity of the GPT-3 model, this study capitalizes on how the model can be used to generate and transform content without manual help from humans – how well plausibly GPT3-authored text most nearly passes as human-like prompts for content generation and manipulation purposes. This attempt is presented in the context of automated story writing. It also sheds light on the potential abuses of the tool and its raw capabilities as its limitations.	2e5bc6508f772e8be56fe7369661cacb0a49d46e	@['Conference']{saravanan-sudha-2022-gpt,  author = {Shruti Saravanan and K. Sudha},  booktitle = {2022 Fifth International Conference on Computational Intelligence and Communication Technologies (CCICT)},  journal = {2022 Fifth International Conference on Computational Intelligence and Communication Technologies (CCICT)},  pages = {514-519},  title = {GPT-3 Powered System for Content Generation and Transformation},  year = {2022} }
GPT-3 for Few-Shot Dialogue State Tracking	2021	http://www.semanticscholar.org/paper/d66e80224cda0c1d5a4c1be3798df6a6bfe3713c	It is found that natural language instructions in the prompt have little impact on performance, larger language models do not always induce higher downstream performance and that GPT-3 is highly sensitive to the order and number of the in-context examples.	maybe	0	GPT-3 (Brown et al., 2020) has attracted considerable attention due to its superior performance across a wide range of Natural Language Processing (NLP) tasks, especially with its powerful and versatile in-context few-shot learning ability. That is, it has been shown that by carefully crafting a prompt, consisting of a few labelled examples followed by an unlabelled example, GPT’3 is able to do few-shot sentiment classification, three-digit arithmetic and much more. We seek to evaluate its performance on a novel and notably more complicated task: few-shot Dialogue State Tracking (DST). We propose a few-shot prompting framework that selects in-context examples based on similarity which outperforms the original random in-context selection framework. We also review and formalise the two types of completion strategies employed by previous literature, which we name constrained and unconstrained, and propose a third "semi-constrained" completion strategy, which is particularly well adapted for DST. Additionally, we propose a prompt ensembling technique that reliably outperforms individual models. Furthermore, we are the first, to the best of our knowledge, to fine-tune GPT-3 for the task of few-shot DST, showing that it reliably outperforms its GPT-2 counterpart. Furthermore, we seek to synthesise and formalise the largely heterogeneous body of previous work on prompt programming and in-context learning for GPT-3. In an attempt to contribute to the understanding of the strengths, weaknesses and inner-working of GPT-3, we perform numerous ablative studies that validate and confute previous in-context learning empirical findings: mainly, we find that natural language instructions in the prompt have little impact on performance, larger language models do not always induce higher downstream performance and that GPT-3 is highly sensitive to the order and number of the in-context examples.	d66e80224cda0c1d5a4c1be3798df6a6bfe3713c	@['Review']{pezzotti-2021-gpt,  author = {Nicholas Pezzotti},  title = {GPT-3 for Few-Shot Dialogue State Tracking},  year = {2021} }
GPT-2’s activations predict the degree of semantic comprehension in the human brain	2021	http://www.semanticscholar.org/paper/fae6daceb89bcc716c840105e56290ddd97eb915	Comparing deep language models and the brain paves the way to a computational model of semantic comprehension, which shows that GPT-2’s brain predictions significantly correlate with semantic comprehension.	maybe	22	Language transformers, like GPT-2, have demonstrated remarkable abilities to process text, and now constitute the backbone of deep translation, summarization and dialogue algorithms. However, whether these models encode information that relates to human comprehension remains controversial. Here, we show that the representations of GPT-2 not only map onto the brain responses to spoken stories, but also predict the extent to which subjects understand narratives. To this end, we analyze 101 subjects recorded with functional Magnetic Resonance Imaging while listening to 70 min of short stories. We then fit a linear model to predict brain activity from GPT-2’s activations, and correlate this mapping with subjects’ comprehension scores as assessed for each story. The results show that GPT-2’s brain predictions significantly correlate with semantic comprehension. These effects are bilaterally distributed in the language network and peak with a correlation of R=0.50 in the angular gyrus. Overall, this study paves the way to model narrative comprehension in the brain through the lens of modern language algorithms.	fae6daceb89bcc716c840105e56290ddd97eb915	@None{caucheteux-etal-2021-gpt,  author = {C. Caucheteux and Alexandre Gramfort and J. King},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {GPT-2’s activations predict the degree of semantic comprehension in the human brain},  year = {2021} }
GPT-2 for Emily Dickinson poetry generation	2021	http://www.semanticscholar.org/paper/c9b413ad7d559b6598b925cd4b8e4d7fd2dccb53		maybe	0	In 1985, Steve Jobs gave a talk at the University of Lunds outlining his vision for the future. The following excerpt of what he said about artificial intelligence caught our attention: “My hope is someday, when the next Aristotle is alive, we can capture some underlying worldview of Aristotle in a computer. And someday some student will be able to not only read the words Aristotle wrote but ask Aristotle a question and get an answer” [1]. Fast forward 36 years to now, and Jobs’ vision of machine intelligence has come to life, in a sense. In this project we test Jobs’ idea of “capturing some underlying worldview of Aristotle” with a famous 19th century poet, Emily Dickinson. Through a technical and literary analysis, we explore a GPT-2 model trained on a dataset of Dickinson’s poetry and fine tuned to output new Dickinson poems. We explore the implications of uploading the “consciousnesses” of famous thinkers using deep learning models, and conclude with a brief analysis of our model’s generated poetry.	c9b413ad7d559b6598b925cd4b8e4d7fd2dccb53	@None{dai-2021-gpt,  author = {Alice Dai},  title = {GPT-2 for Emily Dickinson poetry generation},  year = {2021} }
GPT Understands, Too	2021	http://www.semanticscholar.org/paper/128917425601a541c93c600a2f67d654512928bb	It is shown that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning— which employs trainable continuous prompt embeddings and outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.	yes	235	While GPTs with traditional fine-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuning— which employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we find that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.	128917425601a541c93c600a2f67d654512928bb	@['JournalArticle']{liu-etal-2021-gpt,  author = {Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {GPT Understands, Too},  volume = {abs/2103.10385},  year = {2021} }
GPT Takes the Bar Exam	2022	http://www.semanticscholar.org/paper/458147b5f7242c998ec4f33798a59b7c48867329	This research document's experimental evaluation of the performance of OpenAI’s text-davinci-003 model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam, and finds that hyperparameter optimization and prompt engineering positively impacted GPT.5”s zero-shot performance.	maybe	1	Nearly all jurisdictions in the United States require a professional license exam, commonly referred to as “the Bar Exam,” as a precondition for law practice. To even sit for the exam, most jurisdictions require that an applicant completes at least seven years of post-secondary education, including three years at an accredited law school. In addition, most test-takers also undergo weeks to months of further, exam-specific preparation. Despite this significant investment of time and capital, approximately one in five test-takers still score under the rate required to pass the exam on their first try. In the face of a complex task that requires such depth of knowledge, what, then, should we expect of the state of the art in “AI?” In this research, we document our experimental evaluation of the performance of OpenAI’s text-davinci-003 model, often-referred to as GPT-3.5, on the multistate multiple choice (MBE) section of the exam. While we find no benefit in fine-tuning over GPT-3.5’s zero-shot performance at the scale of our training data, we do find that hyperparameter optimization and prompt engineering positively impacted GPT-3.5’s zero-shot performance. For best prompt and parameters, GPT-3.5 achieves a headline correct rate of 50.3% on a complete NCBE MBE practice exam, significantly in excess of the 25% baseline guessing rate, and performs at a passing rate for both Evidence and Torts. GPT-3.5’s ranking of responses is also highly-correlated with correctness; its top two and top three choices are correct 71% and 88% of the time, respectively, indicating very strong non-entailment performance. While our ability to interpret these results is limited by nascent scientific understanding of LLMs and the proprietary nature of GPT, we believe that these results strongly suggest that	458147b5f7242c998ec4f33798a59b7c48867329	@['JournalArticle']{bommarito-katz-2022-gpt,  author = {M. Bommarito and D. Katz},  booktitle = {SSRN Electronic Journal},  journal = {ArXiv},  title = {GPT Takes the Bar Exam},  volume = {abs/2212.14402},  year = {2022} }
GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities	2023	http://www.semanticscholar.org/paper/651dac86d8bf847ec6780a878cb1e04d3d41f356		maybe	0	The global economy is increasingly dependent on knowledge workers to meet the needs of public and private organizations. While there is no single deﬁnition of knowledge work, organizations and industry groups still attempt to measure individuals’ capability to engage in it. The most comprehensive assessment of capability readiness for professional knowledge workers is the Uniform CPA Examination developed by the American Institute of Certiﬁed Public Accountants (AICPA). In this paper, we experimentally evaluate OpenAI’s text - davinci -003 and prior versions of GPT on both a sample Regulation (REG) exam and an assessment of over 200 multiple-choice questions based on the AICPA Blueprints for legal, ﬁnancial, accounting, technology, and ethical tasks. First, we ﬁnd that text - davinci -003 achieves a correct rate of 14.4% on a sample REG exam section, signiﬁcantly underperforming human capabilities on quantitative reasoning in zero-shot prompts. Second, text - davinci -003 appears to be approaching human-level performance on the Remembering & Understanding and Application skill levels in the Exam absent calculation. For best prompt and parameters, the model answers 57.6% of questions correctly, signiﬁcantly better than the 25% guessing rate, and its top two answers are correct 82.1% of the time, indicating strong non-entailment. Finally, we ﬁnd that recent generations of GPT-3 demonstrate material improvements on this assessment, rising from 30% for text - davinci -001 to 57% for text - davinci -003. These ﬁndings strongly suggest that large language models have the potential to transform the quality and e ﬃ ciency of future knowledge work.	651dac86d8bf847ec6780a878cb1e04d3d41f356	@['JournalArticle']{bommarito-etal-2023-gpt,  author = {Jillian Bommarito and M. Bommarito and D. Katz and Jessica Katz},  booktitle = {SSRN Electronic Journal},  journal = {ArXiv},  title = {GPT as Knowledge Worker: A Zero-Shot Evaluation of (AI)CPA Capabilities},  volume = {abs/2301.04408},  year = {2023} }
GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers	2022	http://www.semanticscholar.org/paper/5470f044662ada6ca1ce9b23e7e4bba0ffc87b87	A novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers and significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores.	maybe	1	There has been a growing interest in interpreting the underlying dynamics of Transformers. While self-attention patterns were initially deemed as the primary option, recent studies have shown that integrating other components can yield more accurate explanations. This paper introduces a novel token attribution analysis method that incorporates all the components in the encoder block and aggregates this throughout layers. Through extensive quantitative and qualitative experiments, we demonstrate that our method can produce faithful and meaningful global token attributions. Our experiments reveal that incorporating almost every encoder component results in increasingly more accurate analysis in both local (single layer) and global (the whole model) settings. Our global attribution analysis significantly outperforms previous methods on various tasks regarding correlation with gradient-based saliency scores. Our code is freely available at https://github.com/mohsenfayyaz/GlobEnc.	5470f044662ada6ca1ce9b23e7e4bba0ffc87b87	@['JournalArticle', 'Conference']{modarressi-etal-2022-globenc:,  author = {A. Modarressi and Mohsen Fayyaz and Yadollah Yaghoobzadeh and Mohammad Taher Pilehvar},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {258-271},  title = {GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers},  year = {2022} }
GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models	2022	http://www.semanticscholar.org/paper/e3dae33c5bdf397abdefd971ea34c48fb836dcc0	A frame-work for geo-diverse commonsense probing on multilingual PLMs (mPLMs) is introduced and a corresponding benchmark Geo -diverse Commonsense M ultilingual La nguage M odels A nalysis (G EO ML AMA) dataset is benchmarked.	yes	5	Recent work has shown that Pre-trained Language Models (PLMs) store the relational knowledge learned from data and utilize it for performing downstream tasks. However, commonsense knowledge across different regions may vary. For instance, the color of bridal dress is white in American weddings whereas it is red in Chinese weddings. In this paper, we introduce a benchmark dataset, Geo-diverse Commonsense Multilingual Language Models Analysis (GeoMLAMA), for probing the diversity of the relational knowledge in multilingual PLMs. GeoMLAMA contains 3125 prompts in English, Chinese, Hindi, Persian, and Swahili, with a wide coverage of concepts shared by people from American, Chinese, Indian, Iranian and Kenyan cultures. We benchmark 11 standard multilingual PLMs on GeoMLAMA. Interestingly, we find that 1) larger multilingual PLMs variants do not necessarily store geo-diverse concepts better than its smaller variant; 2) multilingual PLMs are not intrinsically biased towards knowledge from the Western countries (the United States); 3) the native language of a country may not be the best language to probe its knowledge and 4) a language may better probe knowledge about a non-native country than its native country.	e3dae33c5bdf397abdefd971ea34c48fb836dcc0	@['JournalArticle', 'Conference']{yin-etal-2022-geomlama:,  author = {Da Yin and Hritik Bansal and Masoud Monajatipoor and Liunian Harold Li and Kai-Wei Chang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {GeoMLAMA: Geo-Diverse Commonsense Probing on Multilingual Pre-Trained Language Models},  volume = {abs/2205.12247},  year = {2022} }
Geometry matters: Exploring language examples at the decision boundary	2020	http://www.semanticscholar.org/paper/0cbca3b6d52e06272782d69432a48afa99b91fa3	A theoretical way to quantify the difficulty of an example in NLP, using tools from information geometry, and discovers that both BERT, CNN and fasttext are susceptible to word substitutions in high difficulty examples.	maybe	2	A growing body of recent evidence has highlighted the limitations of natural language processing (NLP) datasets and classifiers. These include the presence of annotation artifacts in datasets, classifiers relying on shallow features like a single word (e.g., if a movie review has the word "romantic", the review tends to be positive), or unnecessary words (e.g., learning a proper noun to classify a movie as positive or negative). The presence of such artifacts has subsequently led to the development of challenging datasets to force the model to generalize better. While a variety of heuristic strategies, such as counterfactual examples and contrast sets, have been proposed, the theoretical justification about what makes these examples difficult for the classifier is often lacking or unclear. In this paper, using tools from information geometry, we propose a theoretical way to quantify the difficulty of an example in NLP. Using our approach, we explore difficult examples for several deep learning architectures. We discover that both BERT, CNN and fasttext are susceptible to word substitutions in high difficulty examples. These classifiers tend to perform poorly on the FIM test set. (generated by sampling and perturbing difficult examples, with accuracy dropping below 50%). We replicate our experiments on 5 NLP datasets (YelpReviewPolarity, AGNEWS, SogouNews, YelpReviewFull and Yahoo Answers). On YelpReviewPolarity we observe a correlation coefficient of -0.4 between resilience to perturbations and the difficulty score. Similarly we observe a correlation of 0.35 between the difficulty score and the empirical success probability of random substitutions. Our approach is simple, architecture agnostic and can be used to study the fragilities of text classification models. All the code used will be made publicly available, including a tool to explore the difficult examples for other datasets.	0cbca3b6d52e06272782d69432a48afa99b91fa3	@['JournalArticle', 'Review']{datta-etal-2020-geometry,  author = {Debajyoti Datta and Shashwat Kumar and Laura E. Barnes and Tom Fletcher},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Geometry matters: Exploring language examples at the decision boundary},  volume = {abs/2010.07212},  year = {2020} }
Generating Negative Commonsense Knowledge	2020	http://www.semanticscholar.org/paper/0b8d238a0489a9d9a19589623c3b5a4871ba0e85	This work-in-progress paper shows empirically that obtaining meaningful negative samples for the completion task is nontrivial, and proposes NegatER, a framework for generating negative commonsense knowledge, to address this challenge.	maybe	6	The acquisition of commonsense knowledge is an important open challenge in artificial intelligence. In this work-in-progress paper, we study the task of automatically augmenting commonsense knowledge bases (KBs) with novel statements. We show empirically that obtaining meaningful negative samples for the completion task is nontrivial, and propose NegatER, a framework for generating negative commonsense knowledge, to address this challenge. In our evaluation we demonstrate the intrinsic value and extrinsic utility of the knowledge generated by NegatER, opening up new avenues for future research in this direction.	0b8d238a0489a9d9a19589623c3b5a4871ba0e85	@['JournalArticle']{safavi-koutra-2020-generating,  author = {Tara Safavi and Danai Koutra},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Generating Negative Commonsense Knowledge},  volume = {abs/2011.07497},  year = {2020} }
Generating Datasets with Pretrained Language Models	2021	http://www.semanticscholar.org/paper/b769b629c8de35b16735214251d6b4e99cb55762	This paper utilizes the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which it then uses for finetuning much smaller and more efficient models.	maybe	63	To obtain high-quality sentence embeddings from pretrained language models (PLMs), they must either be augmented with additional pretraining objectives or finetuned on a large set of labeled text pairs. While the latter approach typically outperforms the former, it requires great human effort to generate suitable datasets of sufficient size. In this paper, we show how PLMs can be leveraged to obtain high-quality sentence embeddings without the need for labeled data, finetuning or modifications to the pretraining objective: We utilize the generative abilities of large and high-performing PLMs to generate entire datasets of labeled text pairs from scratch, which we then use for finetuning much smaller and more efficient models. Our fully unsupervised approach outperforms strong baselines on several semantic textual similarity datasets.	b769b629c8de35b16735214251d6b4e99cb55762	@['JournalArticle', 'Conference']{schick-schütze-2021-generating,  author = {Timo Schick and Hinrich Schütze},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Generating Datasets with Pretrained Language Models},  volume = {abs/2104.07540},  year = {2021} }
Generated Knowledge Prompting for Commonsense Reasoning	2021	http://www.semanticscholar.org/paper/12a763cb52f650710900790ca0bc43e5d5b88be6	Generated knowledge prompting develops generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question, and improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks.	maybe	33	It remains an open question whether incorporating external knowledge benefits commonsense reasoning while maintaining the flexibility of pretrained sequence models. To investigate this question, we develop generated knowledge prompting, which consists of generating knowledge from a language model, then providing the knowledge as additional input when answering a question. Our method does not require task-specific supervision for knowledge integration, or access to a structured knowledge base, yet it improves performance of large-scale, state-of-the-art models on four commonsense reasoning tasks, achieving state-of-the-art results on numerical commonsense (NumerSense), general commonsense (CommonsenseQA 2.0), and scientific commonsense (QASC) benchmarks. Generated knowledge prompting highlights large-scale language models as flexible sources of external knowledge for improving commonsense reasoning.Our code is available at github.com/liujch1998/GKP	12a763cb52f650710900790ca0bc43e5d5b88be6	@['JournalArticle', 'Conference']{liu-etal-2021-generated,  author = {Jiacheng Liu and Alisa Liu and Ximing Lu and S. Welleck and Peter West and Ronan Le Bras and Yejin Choi and Hannaneh Hajishirzi},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Generated Knowledge Prompting for Commonsense Reasoning},  volume = {abs/2110.08387},  year = {2021} }
Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks	2022	http://www.semanticscholar.org/paper/03d73f3073ac0d73d60d5567fc1ed558367c8279	It is found that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops, and Generalized Quantifier Theory is relied on for language-independent representations of the semantics of quantifier words.	yes	3	Logical approaches to representing language have developed and evaluated computational models of quantifier words since the 19th century, but today’s NLU models still struggle to capture their semantics. We rely on Generalized Quantifier Theory for language-independent representations of the semantics of quantifier words, to quantify their contribution to the errors of NLU models. We find that quantifiers are pervasive in NLU benchmarks, and their occurrence at test time is associated with performance drops. Multilingual models also exhibit unsatisfying quantifier reasoning abilities, but not necessarily worse for non-English languages. To facilitate directly-targeted probing, we present an adversarial generalized quantifier NLI task (GQNLI) and show that pre-trained language models have a clear lack of robustness in generalized quantifier reasoning.	03d73f3073ac0d73d60d5567fc1ed558367c8279	@['JournalArticle']{cui-etal-2022-generalized,  author = {Ruixiang Cui and Daniel Hershcovich and Anders Søgaard},  booktitle = {DADC},  pages = {4875-4893},  title = {Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks},  year = {2022} }
Generalization through Memorization: Nearest Neighbor Language Models	2019	https://www.semanticscholar.org/paper/7be8c119dbe065c52125ee7716601751f3116844	It is suggested that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.	maybe	266	We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail.	7be8c119dbe065c52125ee7716601751f3116844	@['JournalArticle']{khandelwal-etal-2019-generalization,  author = {Urvashi Khandelwal and Omer Levy and Dan Jurafsky and Luke Zettlemoyer and M. Lewis},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Generalization through Memorization: Nearest Neighbor Language Models},  volume = {abs/1911.00172},  year = {2019} }
Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics	2021	http://www.semanticscholar.org/paper/ce66fde956476755830e002fe3b7f5a048ef8bb5	A case study of generalization in NLI (from MNLI to the adversarially constructed HANS dataset) in a range of BERT-based architectures as well as with subsampling the data and increasing the model size, providing insights into how Transformer-based models learn to generalize.	maybe	20	Much of recent progress in NLU was shown to be due to models’ learning dataset-specific heuristics. We conduct a case study of generalization in NLI (from MNLI to the adversarially constructed HANS dataset) in a range of BERT-based architectures (adapters, Siamese Transformers, HEX debiasing), as well as with subsampling the data and increasing the model size. We report 2 successful and 3 unsuccessful strategies, all providing insights into how Transformer-based models learn to generalize.	ce66fde956476755830e002fe3b7f5a048ef8bb5	@['JournalArticle']{bhargava-etal-2021-generalization,  author = {Prajjwal Bhargava and Aleksandr Drozd and Anna Rogers},  booktitle = {First Workshop on Insights from Negative Results in NLP},  journal = {ArXiv},  title = {Generalization in NLI: Ways (Not) To Go Beyond Simple Heuristics},  volume = {abs/2110.01518},  year = {2021} }
Gender Biases Unexpectedly Fluctuate in the Pre-training Stage of Masked Language Models	2022	http://www.semanticscholar.org/paper/ed2f5fe0093154b3a11d2672ff56a015deb09993	It is shown that severeuctuations exist at the fundamen-tal level of individual templates, invalidating the assumption that other variations in the pre-training process have no effect on the biases measured.	maybe	0	Masked language models pick up gender biases during pre-training. Such biases are usu-ally attributed to a certain model architecture and its pre-training corpora, with the implicit assumption that other variations in the pre-training process, such as the choices of the random seed or the stopping point, have no effect on the biases measured. However, we show that severe ﬂuctuations exist at the fundamen-tal level of individual templates, invalidating the assumption. Further against the intuition of how humans acquire biases, these ﬂuctuations are not correlated with the certainty of the pre-dicted pronouns or the profession frequencies in pre-training corpora. We release our code and data to beneﬁt future research 1 .	ed2f5fe0093154b3a11d2672ff56a015deb09993	@['JournalArticle']{tang-jiang-2022-gender,  author = {Kenan Tang and Hanchun Jiang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Gender Biases Unexpectedly Fluctuate in the Pre-training Stage of Masked Language Models},  volume = {abs/2211.14639},  year = {2022} }
Gender Bias in Masked Language Models for Multiple Languages	2022	http://www.semanticscholar.org/paper/0607b299284cb44eaee0aedd95db3c88b00ff944		maybe	4	Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages.Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race.Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated.Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators.Moreover, the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (e.g. He/She is a nurse).We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data.We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages.We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE.The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias.	0607b299284cb44eaee0aedd95db3c88b00ff944	@['JournalArticle', 'Conference']{kaneko-etal-2022-gender,  author = {Masahiro Kaneko and Aizhan Imankulova and D. Bollegala and Naoaki Okazaki},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2740-2750},  title = {Gender Bias in Masked Language Models for Multiple Languages},  year = {2022} }
Gender Bias in Contextualized Word Embeddings	2019	http://www.semanticscholar.org/paper/e235ad7dcf6e97cd372f09724dc947c5b1efac79	It is shown that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus and two methods to mitigate such gender bias are explored.	maybe	248	In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.	e235ad7dcf6e97cd372f09724dc947c5b1efac79	@['JournalArticle', 'Conference']{zhao-etal-2019-gender,  author = {Jieyu Zhao and Tianlu Wang and Mark Yatskar and Ryan Cotterell and Vicente Ordonez and Kai-Wei Chang},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Gender Bias in Contextualized Word Embeddings},  volume = {abs/1904.03310},  year = {2019} }
Garden-Path Traversal within GPT-2	2022	http://www.semanticscholar.org/paper/86a8e2976e99c70673f573bc4b0ce0dea7061c72	It is shown that measuring Manhattan distances and cosine similarities between hidden states shows that GPT-2 navigates these sentences more intuitively than conventional methods that predict from the model’s output alone.	maybe	0	In recent years, massive language models consisting exclusively of transformer decoders, led by the GPT-x family, have become in-creasingly popular. While studies have exam-ined the behavior of these models, they tend to only focus on the output of the language model, avoiding analyzing their internal states despite such analyses being popular tools used within BERTology to study transformer en-coders. We present a collection of methods for analyzing GPT-2’s hidden states, and use the model’s navigation of garden path sentences as a case study to demonstrate the utility of studying this model’s behavior beyond its output alone. To support this analysis, we introduce a novel dataset consisting of 3 different types of garden path sentences, along with scripts to manipulate them. We ﬁnd that measuring Manhattan distances and cosine similarities between hidden states shows that GPT-2 navigates these sentences more intuitively than conventional methods that predict from the model’s output alone.	86a8e2976e99c70673f573bc4b0ce0dea7061c72	@['JournalArticle']{jurayj-etal-2022-garden,  author = {William Jurayj and W. Rudman and Carsten Eickhoff},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Garden-Path Traversal within GPT-2},  volume = {abs/2205.12302},  year = {2022} }
Further Boosting BERT-based Models by Duplicating Existing Layers: Some Intriguing Phenomena inside BERT	2020	https://www.semanticscholar.org/paper/f18fa3728868af6c44bb1dc3e913925abc37b5c1	This paper proposes a quite simple method to boost the performance of BERT by duplicating some layers in the BERT-based models to make it deeper and they obtain better performance in the down-stream tasks after fine-tuning.	seed	6	Although Bidirectional Encoder Representations from Transformers (BERT) have achieved tremendous success in many natural language processing (NLP) tasks, it remains a black box, so much previous work has tried to lift the veil of BERT and understand the functionality of each layer. In this paper, we found that removing or duplicating most layers in BERT would not change their outputs. This fact remains true across a wide variety of BERT-based models. Based on this observation, we propose a quite simple method to boost the performance of BERT. By duplicating some layers in the BERT-based models to make it deeper (no extra training required in this step), they obtain better performance in the down-stream tasks after fine-tuning.	f18fa3728868af6c44bb1dc3e913925abc37b5c1	@['JournalArticle']{kao-etal-2020-further,  author = {Wei-Tsung Kao and Tsung-Han Wu and Po-Han Chi and Chun-Cheng Hsieh and Hung-yi Lee},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Further Boosting BERT-based Models by Duplicating Existing Layers: Some Intriguing Phenomena inside BERT},  volume = {abs/2001.09309},  year = {2020} }
From BERT‘s Point of View: Revealing the Prevailing Contextual Differences	2022	http://www.semanticscholar.org/paper/98624c4c24737c4dc00a655e15d3479d8fb73dbc	This work invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high dimensional space and shows how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.	yes	0	Though successfully applied in research and industry large pretrained language models of the BERT family are not yet fully understood. While much research in the field of BERTology has tested whether specific knowledge can be extracted from layer activations, we invert the popular probing design to analyze the prevailing differences and clusters in BERT’s high dimensional space. By extracting coarse features from masked token representations and predicting them by probing models with access to only partial information we can apprehend the variation from ‘BERT’s point of view’. By applying our new methodology to different datasets we show how much the differences can be described by syntax but further how they are to a great extent shaped by the most simple positional information.	98624c4c24737c4dc00a655e15d3479d8fb73dbc	@['JournalArticle']{schuster-hegelich-2022-from,  author = {C. Schuster and Simon Hegelich},  booktitle = {Findings},  pages = {1120-1138},  title = {From BERT‘s Point of View: Revealing the Prevailing Contextual Differences},  year = {2022} }
From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions	2019	https://www.semanticscholar.org/paper/7886bf8d86a8ae22aa0fcf8a77d2c8a4d9429aa1	A transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions is proposed, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences.	maybe	37	We inspect the multi-head self-attention in Transformer NMT encoders for three source languages, looking for patterns that could have a syntactic interpretation. In many of the attention heads, we frequently find sequences of consecutive states attending to the same position, which resemble syntactic phrases. We propose a transparent deterministic method of quantifying the amount of syntactic information present in the self-attentions, based on automatically building and evaluating phrase-structure trees from the phrase-like sequences. We compare the resulting trees to existing constituency treebanks, both manually and by computing precision and recall.	7886bf8d86a8ae22aa0fcf8a77d2c8a4d9429aa1	@['JournalArticle']{mareček-rosa-2019-from,  author = {D. Mareček and Rudolf Rosa},  booktitle = {BlackboxNLP@ACL},  pages = {263-275},  title = {From Balustrades to Pierre Vinken: Looking for Syntax in Transformer Self-Attentions},  year = {2019} }
Frequency-based Distortions in Contextualized Word Embeddings	2021	http://www.semanticscholar.org/paper/e9c8b36016df4470cfd7bb628a73cfb0892e212f	This work explores the geometric characteristics of contextualized word embeddings with two novel tools: an identity probe that predicts the identity of a word using its embedding; and the minimal bounding sphere for a word’s contextualized representations, revealing that words of high and low frequency differ significantly with respect to their representational geometry.	maybe	8	How does word frequency in pre-training data affect the behavior of similarity metrics in contextualized BERT embeddings? Are there systematic ways in which some word relationships are exaggerated or understated? In this work, we explore the geometric characteristics of contextualized word embeddings with two novel tools: (1) an identity probe that predicts the identity of a word using its embedding; (2) the minimal bounding sphere for a word’s contextualized representations. Our results reveal that words of high and low frequency differ significantly with respect to their representational geometry. Such differences introduce distortions: when compared to human judgments, point estimates of embedding similarity (e.g., cosine similarity) can overor underestimate the semantic similarity of two words, depending on the frequency of those words in the training data. This has downstream societal implications: BERT-Base has more trouble differentiating between South American and African countries than North American and European ones. We find that these distortions persist when using BERT-Multilingual, suggesting that they cannot be easily fixed with additional data, which in turn introduces new distortions.	e9c8b36016df4470cfd7bb628a73cfb0892e212f	@['JournalArticle']{zhou-etal-2021-frequency,  author = {Kaitlyn Zhou and Kawin Ethayarajh and Dan Jurafsky},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Frequency-based Distortions in Contextualized Word Embeddings},  volume = {abs/2104.08465},  year = {2021} }
Frequency Effects on Syntactic Rule Learning in Transformers	2021	http://www.semanticscholar.org/paper/3962f108081b22c7e54b413f47ba6f2c16f2cc05	It is shown that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior, and that performance is heavily influenced by word frequency.	yes	26	Pre-trained language models perform well on a variety of linguistic tasks that require symbolic reasoning, raising the question of whether such models implicitly represent abstract symbols and rules. We investigate this question using the case study of BERT’s performance on English subject–verb agreement. Unlike prior work, we train multiple instances of BERT from scratch, allowing us to perform a series of controlled interventions at pre-training time. We show that BERT often generalizes well to subject–verb pairs that never occurred in training, suggesting a degree of rule-governed behavior. We also find, however, that performance is heavily influenced by word frequency, with experiments showing that both the absolute frequency of a verb form, as well as the frequency relative to the alternate inflection, are causally implicated in the predictions BERT makes at inference time. Closer analysis of these frequency effects reveals that BERT’s behavior is consistent with a system that correctly applies the SVA rule in general but struggles to overcome strong training priors and to estimate agreement features (singular vs. plural) on infrequent lexical items.	3962f108081b22c7e54b413f47ba6f2c16f2cc05	@['JournalArticle', 'Conference']{wei-etal-2021-frequency,  author = {Jason Wei and Dan Garrette and Tal Linzen and Ellie Pavlick},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {932-948},  title = {Frequency Effects on Syntactic Rule Learning in Transformers},  year = {2021} }
Fooling Pre-trained Language Models: An Evolutionary Approach to Generate Wrong Sentences with High Acceptability Score	2019	http://www.semanticscholar.org/paper/2ffa498408a9655ff52735dd7a82fd3a5240fe5b	It is found that BERT can be easily fooled, but an augmentation of the original dataset with adversarial samples is enough to make it learn how not to be fooled again, and RoBERTa is more resistent to this approach even if it still have some weak spots.	maybe	0	Large pre-trained language representation models have recently collected numerous successes in language understanding. They obtained state-of-the-art results in many classical benchmark datasets, such as GLUE benchmark and SQuAD dataset, but do they really understand the language? In this paper we investigate two among the best pre-trained language models, BERT (Devlin et al. (2018)) and RoBERTa (Liu et al. (2019)), analysing their weaknesses by generating adversarial sentences in an evolutionary approach. Our goal is to discover if and why it is possible to fool these models, and how to face this issue. This adversarial attack is followed by a cross analysis, understanding robustness and generalization proprieties of models and fooling techniques. We find that BERT can be easily fooled, but an augmentation of the original dataset with adversarial samples is enough to make it learn how not to be fooled again. RoBERTa, instead, is more resistent to this approach even if it still have some weak spots.	2ffa498408a9655ff52735dd7a82fd3a5240fe5b	@None{giovanni-brambilla-2019-fooling,  author = {Marco Di Giovanni and M. Brambilla},  title = {Fooling Pre-trained Language Models: An Evolutionary Approach to Generate Wrong Sentences with High Acceptability Score},  year = {2019} }
Fooling Explanations in Text Classifiers	2022	http://www.semanticscholar.org/paper/43ad62d1e64f2301eb2294ab79b3979a470e7ed4	A novel explanation attack algorithm that alters text input samples imperceptibly so that the outcome of widely-used explanation methods changes considerably while leaving classiﬁer predictions unchanged, which shows that all models and explanation methods are susceptible to TEF perturbations.	maybe	4	State-of-the-art text classiﬁcation models are becoming increasingly reliant on deep neural networks (DNNs). Due to their black-box nature, faithful and robust explanation methods need to accompany classiﬁers for deployment in real-life scenarios. However, it has been shown in vision applications that explanation methods are susceptible to local, imperceptible perturbations that can signiﬁcantly alter the explanations without changing the predicted classes. We show here that the existence of such perturbations extends to text classiﬁers as well. Speciﬁ-cally, we introduce T EXT E XPLANATION F OOLER (TEF), a novel explanation attack algorithm that alters text input samples imperceptibly so that the outcome of widely-used explanation methods changes considerably while leaving classiﬁer predictions unchanged. We evaluate the performance of the attribution robustness estimation performance in TEF on ﬁve sequence classiﬁcation datasets, utilizing three DNN architectures and three transformer architectures for each dataset. TEF can signiﬁcantly decrease the correlation between unchanged and perturbed input attributions, which shows that all models and explanation methods are susceptible to TEF perturbations. Moreover, we evaluate how the perturbations transfer to other model architectures and attribution methods, and show that TEF	43ad62d1e64f2301eb2294ab79b3979a470e7ed4	@['JournalArticle']{ivankay-etal-2022-fooling,  author = {Adam Ivankay and Ivan Girardi and Chiara Marchiori and P. Frossard},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Fooling Explanations in Text Classifiers},  volume = {abs/2206.03178},  year = {2022} }
FOLIO: Natural Language Reasoning with First-Order Logic	2022	http://www.semanticscholar.org/paper/5581bf85386737bd3378eec68189759a05280bea	The results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions.	maybe	3	We present FOLIO , a human-annotated, open-domain, and logically complex and diverse dataset for reasoning in natural language (NL), equipped with ﬁrst order logic (FOL) annotations. FOLIO consists of 1,435 examples (unique conclusions), each paired with one of 487 sets of premises which serve as rules to be used to deductively reason for the validity of each conclusion. The logical correctness of premises and conclusions is ensured by their parallel FOL annotations, which are automatically veriﬁed by our FOL inference engine. In addition to the main NL reasoning task, NL-FOL pairs in FOLIO automatically constitute a new NL-FOL translation dataset using FOL as the logical form. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised ﬁne-tuning on medium-sized language models (BERT, RoBERTa) and few-shot prompting on large language models (GPT-NeoX, OPT, GPT-3, Codex). For NL-FOL translation, we experiment with GPT-3 and Codex. Our results show that one of the most capable Large Language Model (LLM) publicly available, GPT-3 davinci, achieves only slightly better than random results with few-shot prompting on a subset of FOLIO, and the model is especially bad at predicting the correct truth values for False and Unknown conclusions. Our dataset and code are available at https:// github.com/Yale-LILY/FOLIO .	5581bf85386737bd3378eec68189759a05280bea	@['JournalArticle']{han-etal-2022-folio:,  author = {Simeng Han and Hailey Schoelkopf and Yilun Zhao and Zhenting Qi and Martin Riddell and Luke Benson and Lucy Sun and E. Zubova and Yujie Qiao and Matthew Burtell and David Peng and Jonathan Fan and Yixin Liu and Brian Wong and Malcolm Sailor and Ansong Ni and Linyong Nan and Jungo Kasai and Tao Yu and Rui Zhang and Shafiq R. Joty and Alexander R. Fabbri and Wojciech Kryscinski and Xi Victoria Lin and Caiming Xiong and Dragomir R. Radev},  booktitle = {ArXiv},  journal = {ArXiv},  title = {FOLIO: Natural Language Reasoning with First-Order Logic},  volume = {abs/2209.00840},  year = {2022} }
FLUTE: Figurative Language Understanding through Textual Explanations	2022	http://www.semanticscholar.org/paper/fd975e67a2b7b2e43943ac32a16f5fd5be80734b	FLUTE, a dataset of 9,000 ﬁgurative NLI instances with explanations, spanning four categories: Sarcasm, Simile, Metaphor, and Idioms, is released, and it is shown how utilizing GPT-3 in conjunction with human annotators can aid in scaling up the creation of datasets even for such complex linguistic phenomena as ﬂgurative language.	maybe	1	Figurative language understanding has been recently framed as a recognizing textual entailment (RTE) task (a.k.a. natural language inference (NLI)). However, similar to classical RTE/NLI datasets they suffer from spurious correlations and annotation artifacts. To tackle this problem, work on NLI has built explanation-based datasets such as eSNLI, allowing us to probe whether language models are right for the right reasons. Yet no such data exists for figurative language, making it harder to assess genuine understanding of such expressions. To address this issue, we release FLUTE, a dataset of 9,000 figurative NLI instances with explanations, spanning four categories: Sarcasm, Simile, Metaphor, and Idioms. We collect the data through a Human-AI collaboration framework based on GPT-3, crowd workers, and expert annotators. We show how utilizing GPT-3 in conjunction with human annotators (novices and experts) can aid in scaling up the creation of datasets even for such complex linguistic phenomena as figurative language. The baseline performance of the T5 model fine-tuned on FLUTE shows that our dataset can bring us a step closer to developing models that understand figurative language through textual explanations.	fd975e67a2b7b2e43943ac32a16f5fd5be80734b	@['Conference']{chakrabarty-etal-2022-flute:,  author = {Tuhin Chakrabarty and A. Saakyan and Debanjan Ghosh and S. Muresan},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  title = {FLUTE: Figurative Language Understanding through Textual Explanations},  year = {2022} }
Finetuned Language Models Are Zero-Shot Learners	2021	http://www.semanticscholar.org/paper/ff0b2681d7b05e16c46dfb71d980cc2f605907cd	It is shown that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks and outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze.	maybe	328	A BSTRACT This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning —ﬁnetuning language models on a collection of datasets described via instructions—substantially improves zero-shot performance on unseen tasks. We take a 137B parameter pretrained language model and instruction tune it on over 60 NLP datasets verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodiﬁed counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 datasets that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of ﬁnetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning. many tasks	ff0b2681d7b05e16c46dfb71d980cc2f605907cd	@['JournalArticle']{wei-etal-2021-finetuned,  author = {Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and A. Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Finetuned Language Models Are Zero-Shot Learners},  volume = {abs/2109.01652},  year = {2021} }
Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping	2020	https://www.semanticscholar.org/paper/baf60d13c98916b77b09bc525ede1cd610ed1db5	This work investigates how the performance of the best-found model varies as a function of the number of fine-tuning trials, and examines two factors influenced by the choice of random seed: weight initialization and training data order.	seed	302	Fine-tuning pretrained contextual word embedding models to supervised downstream tasks has become commonplace in natural language processing. This process, however, is often brittle: even with the same hyperparameter values, distinct random seeds can lead to substantially different results. To better understand this phenomenon, we experiment with four datasets from the GLUE benchmark, fine-tuning BERT hundreds of times on each while varying only the random seeds. We find substantial performance increases compared to previously reported results, and we quantify how the performance of the best-found model varies as a function of the number of fine-tuning trials. Further, we examine two factors influenced by the choice of random seed: weight initialization and training data order. We find that both contribute comparably to the variance of out-of-sample performance, and that some weight initializations perform well across all tasks explored. On small datasets, we observe that many fine-tuning trials diverge part of the way through training, and we offer best practices for practitioners to stop training less promising runs early. We publicly release all of our experimental data, including training and validation scores for 2,100 trials, to encourage further analysis of training dynamics during fine-tuning.	baf60d13c98916b77b09bc525ede1cd610ed1db5	@['JournalArticle']{dodge-etal-2020-fine,  author = {Jesse Dodge and Gabriel Ilharco and Roy Schwartz and Ali Farhadi and Hannaneh Hajishirzi and Noah A. Smith},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping},  volume = {abs/2002.06305},  year = {2020} }
Fine-Tuning Large Neural Language Models for Biomedical Natural Language Processing	2021	http://www.semanticscholar.org/paper/a9c5e23c5559bfc4d95dd166c1ed29fa026bbf2e	It is shown that finetuning performance may be sensitive to pretraining settings, especially in low-resource domains, and that domainspecific vocabulary and pretraining facilitate more robust models for fine-tuning.	maybe	9	Motivation: A perennial challenge for biomedical researchers and clinical practitioners is to stay abreast with the rapid growth of publications and medical notes. Natural language processing (NLP) has emerged as a promising direction for taming information overload. In particular, large neural language models facilitate transfer learning by pretraining on unlabeled text, as exemplified by the successes of BERT models in various NLP applications. However, fine-tuning such models for an end task remains challenging, especially with small labeled datasets, which are common in biomedical NLP. Results: We conduct a systematic study on fine-tuning stability in biomedical NLP. We show that finetuning performance may be sensitive to pretraining settings, especially in low-resource domains. Large models have potential to attain better performance, but increasing model size also exacerbates finetuning instability. We thus conduct a comprehensive exploration of techniques for addressing fine-tuning instability. We show that these techniques can substantially improve fine-tuning performance for lowresource biomedical NLP applications. Specifically, freezing lower layers is helpful for standard BERT-BASE models, while layerwise decay is more effective for BERT-LARGE and ELECTRA models. For low-resource text similarity tasks such as BIOSSES, reinitializing the top layer is the optimal strategy. Overall, domainspecific vocabulary and pretraining facilitate more robust models for fine-tuning. Based on these findings, we establish new state of the art on a wide range of biomedical NLP applications. Availability and implementation: To facilitate progress in biomedical NLP, we release our state-of-the-art pretrained and fine-tuned models: https://aka.ms/BLURB. Contact: hoifung@microsoft.com	a9c5e23c5559bfc4d95dd166c1ed29fa026bbf2e	@['JournalArticle']{tinn-etal-2021-fine,  author = {Robert Tinn and Hao Cheng and Yu Gu and Naoto Usuyama and Xiaodong Liu and Tristan Naumann and Jianfeng Gao and Hoifung Poon},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Fine-Tuning Large Neural Language Models for Biomedical Natural Language Processing},  volume = {abs/2112.07869},  year = {2021} }
Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution	2022	http://www.semanticscholar.org/paper/29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d	It is found that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large, and suggests that the easy two-step strategy of linear probing then full fine- Tuning (LP-FT) combines the benefits of both fine- tuning and linear probing.	maybe	107	When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer—the “head”). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR → STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2% higher accuracy ID but 7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head—this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1% better ID, 10% better OOD than full fine-tuning).	29b77089a0a40f46372ce2dca9c3bb2dd5d46b1d	@['JournalArticle']{kumar-etal-2022-fine,  author = {Ananya Kumar and Aditi Raghunathan and Robbie Jones and Tengyu Ma and Percy Liang},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},  volume = {abs/2202.10054},  year = {2022} }
Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers	2021	https://www.semanticscholar.org/paper/d20bcf41b772ea31a8ddd20317ee99fe7e1f32ca	The similarity 015 of later layer representations implies that later 016 layers only marginally contribute to task per017 formance, and it is verified in experiments that the top few layers of fine-tuned Transformers 019 can be discarded without hurting performance, even with no further tuning.	maybe	5	Despite the success of fine-tuning pretrained 001 language encoders like BERT for downstream 002 natural language understanding (NLU) tasks, 003 it is still poorly understood how neural net004 works change after fine-tuning. In this work, 005 we use centered kernel alignment (CKA), a 006 method for comparing learned representations, 007 to measure the similarity of representations in 008 task-tuned models across layers. In experi009 ments across twelve NLU tasks, we discover 010 a consistent block diagonal structure in the 011 similarity of representations within fine-tuned 012 RoBERTa and ALBERT models, with strong 013 similarity within clusters of earlier and later 014 layers, but not between them. The similarity 015 of later layer representations implies that later 016 layers only marginally contribute to task per017 formance, and we verify in experiments that 018 the top few layers of fine-tuned Transformers 019 can be discarded without hurting performance, 020 even with no further tuning. 021	d20bcf41b772ea31a8ddd20317ee99fe7e1f32ca	@None{roberta-etal-2021-fine,  author = {Roberta and Albert and Electra},  title = {Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers},  year = {2021} }
Fine-Grained Fairness Analysis of Abusive Language Detection Systems with CheckList	2021	http://www.semanticscholar.org/paper/d0cef8486281ec74679271b23cdf30cf87072f5c	Although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation.	maybe	3	Current abusive language detection systems have demonstrated unintended bias towards sensitive features such as nationality or gender. This is a crucial issue, which may harm minorities and underrepresented groups if such systems were integrated in real-world applications. In this paper, we create ad hoc tests through the CheckList tool (Ribeiro et al., 2020) to detect biases within abusive language classifiers for English. We compare the behaviour of two BERT-based models, one trained on a generic hate speech dataset and the other on a dataset for misogyny detection. Our evaluation shows that, although BERT-based classifiers achieve high accuracy levels on a variety of natural language processing tasks, they perform very poorly as regards fairness and bias, in particular on samples involving implicit stereotypes, expressions of hate towards minorities and protected attributes such as race or sexual orientation. We release both the notebooks implemented to extend the Fairness tests and the synthetic datasets usable to evaluate systems bias independently of CheckList.	d0cef8486281ec74679271b23cdf30cf87072f5c	@None{manerba-tonelli-2021-fine,  author = {Marta Marchiori Manerba and Sara Tonelli},  booktitle = {WOAH},  journal = {Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)},  title = {Fine-Grained Fairness Analysis of Abusive Language Detection Systems with CheckList},  year = {2021} }
Finding Universal Grammatical Relations in Multilingual BERT	2020	http://www.semanticscholar.org/paper/1376a8e1b06b7a7b7cacd45f52268e427c3b0135	An unsupervised analysis method is presented that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy, suggesting that even without explicit supervision, multilingual masked language models learn certain linguistic universals.	maybe	84	Recent work has found evidence that Multilingual BERT (mBERT), a transformer-based multilingual masked language model, is capable of zero-shot cross-lingual transfer, suggesting that some aspects of its representations are shared cross-lingually. To better understand this overlap, we extend recent work on finding syntactic trees in neural networks’ internal representations to the multilingual setting. We show that subspaces of mBERT representations recover syntactic tree distances in languages other than English, and that these subspaces are approximately shared across languages. Motivated by these results, we present an unsupervised analysis method that provides evidence mBERT learns representations of syntactic dependency labels, in the form of clusters which largely agree with the Universal Dependencies taxonomy. This evidence suggests that even without explicit supervision, multilingual masked language models learn certain linguistic universals.	1376a8e1b06b7a7b7cacd45f52268e427c3b0135	@['JournalArticle', 'Conference']{chi-etal-2020-finding,  author = {Ethan A. Chi and John Hewitt and Christopher D. Manning},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Finding Universal Grammatical Relations in Multilingual BERT},  volume = {abs/2005.04511},  year = {2020} }
Finding the Dominant Winning Ticket in Pre-Trained Language Models	2022	https://www.semanticscholar.org/paper/e5f40aa12e9a00fc634d2df6b5382405ef8c1087	Strikingly, it is found that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.	maybe	0	The Lottery Ticket Hypothesis suggests that for any over-parameterized model, a small subnetwork exists to achieve competitive performance compared to the backbone architecture. In this paper, we study whether there is a winning lottery ticket for pre-trained language models, which allow the practitioners to fine-tune the parameters in the ticket but achieve good downstream performance. To achieve this, we regularize the fine-tuning process with L1 distance and explore the subnetwork structure (what we refer to as the “dominant winning ticket”). Empirically, we show that (a) the dominant winning ticket can achieve performance that is comparable with that of the full-parameter model, (b) the dominant winning ticket is transferable across different tasks, (c) and the dominant winning ticket has a natural structure within each parameter matrix. Strikingly, we find that a dominant winning ticket that takes up 0.05% of the parameters can already achieve satisfactory performance, indicating that the PLM is significantly reducible during fine-tuning.	e5f40aa12e9a00fc634d2df6b5382405ef8c1087	@['JournalArticle']{gong-etal-2022-finding,  author = {Zhuocheng Gong and Di He and Yelong Shen and Tie-Yan Liu and Weizhu Chen and Dongyan Zhao and Ji-rong Wen and Rui Yan},  booktitle = {Findings},  pages = {1459-1472},  title = {Finding the Dominant Winning Ticket in Pre-Trained Language Models},  year = {2022} }
Finding Skill Neurons in Pre-trained Transformer-based Language Models	2022	http://www.semanticscholar.org/paper/5d3cf0909ba206cd6bc2e86610f77ca25d9b2d1c	It is demonstrated that after prompt tuning for speciﬁc tasks, the activations of some neurons within pre-trained Transformers 1 are highly predictive of the task labels, and the skill neurons are most likely generated in pre-training rather than ﬁne-tuning.	maybe	2	Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than fine-tuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https://github.com/THU-KEG/Skill-Neuron.	5d3cf0909ba206cd6bc2e86610f77ca25d9b2d1c	@['JournalArticle', 'Conference']{wang-etal-2022-finding,  author = {Xiaozhi Wang and Kaiyue Wen and Zhengyan Zhang and Lei Hou and Zhiyuan Liu and Juanzi Li},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Finding Skill Neurons in Pre-trained Transformer-based Language Models},  volume = {abs/2211.07349},  year = {2022} }
Finding patterns in Knowledge Attribution for Transformers	2022	http://www.semanticscholar.org/paper/73429a5908da503686b52630b5a6d3284d1615a2	It is found that grammatical knowledge is far more dispersed among the neurons than factual knowledge, and can be attributed to middle and higher layers of the network.	yes	0	We analyze the Knowledge Neurons(Dai et al., 2021) framework for the attribution of factual and relational knowledge to particular neurons in the transformer network. We use a 12-layer multi-lingual BERT model for our experiments. Our study reveals various interesting phenomena. We observe that mostly factual knowledge can be attributed to middle and higher layers of the network( ≥ 6). Further analysis reveals that the middle layers(6 − 9) are mostly responsible for relational information, which is further reﬁned into actual factual knowledge or the ”correct answer” in the last few layers(10 − 12). Our experiments also show that the model handles prompts in diﬀerent languages, but representing the same fact, similarly, providing further evidence for eﬀectiveness of multi-lingual pre-training. Applying the attribution scheme for grammatical knowledge, we ﬁnd that grammatical knowledge is far more dispersed among the neurons than factual knowledge.	73429a5908da503686b52630b5a6d3284d1615a2	@['JournalArticle']{juneja-agarwal-2022-finding,  author = {Jeevesh Juneja and Ritu Agarwal},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Finding patterns in Knowledge Attribution for Transformers},  volume = {abs/2205.01366},  year = {2022} }
Finding Fuzziness in Neural Network Models of Language Processing	2021	http://www.semanticscholar.org/paper/21f63cce226b9f4caeb060d551989c1dc967e508	This paper testifies to the extent to which models trained to capture the distributional statistics of language show correspondence to fuzzy-membership patterns, and finds the model to show patterns that are similar to classical fuzzy-set theoretic formulations of linguistic hedges, albeit with a substantial amount of noise.	maybe	0		21f63cce226b9f4caeb060d551989c1dc967e508	@['JournalArticle']{misra-rayz-2021-finding,  author = {Kanishka Misra and J. Rayz},  booktitle = {Annual Conference on the North American Fuzzy Information Processing Society},  journal = {ArXiv},  title = {Finding Fuzziness in Neural Network Models of Language Processing},  volume = {abs/2104.10813},  year = {2021} }
Filler-gaps that neural networks fail to generalize	2020	http://www.semanticscholar.org/paper/5d1cbd579b4283a20cad1b2fbf36d44bf1b9c428	This work probes for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions and finds no evidence that the models learn any of the shared underlying grammatical constraints the authors tested.	maybe	0	It can be difficult to separate abstract linguistic knowledge in recurrent neural networks (RNNs) from surface heuristics. In this work, we probe for highly abstract syntactic constraints that have been claimed to govern the behavior of filler-gap dependencies across different surface constructions. For models to generalize abstract patterns in expected ways to unseen data, they must share representational features in predictable ways. We use cumulative priming to test for representational overlap between disparate filler-gap constructions in English and find evidence that the models learn a general representation for the existence of filler-gap dependencies. However, we find no evidence that the models learn any of the shared underlying grammatical constraints we tested. Our work raises questions about the degree to which RNN language models learn abstract linguistic representations.	5d1cbd579b4283a20cad1b2fbf36d44bf1b9c428	@['JournalArticle']{bhattacharya-schijndel-2020-filler,  author = {Debasmita Bhattacharya and Marten van Schijndel},  booktitle = {Conference on Computational Natural Language Learning},  pages = {486-495},  title = {Filler-gaps that neural networks fail to generalize},  year = {2020} }
Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias	2022	http://www.semanticscholar.org/paper/3a37fef290d76029c295201cc168c0f8ecb0a0cf		yes	3	The size of pretrained models is increasing, and so is their performance on a variety of NLP tasks. However, as their memorization capacity grows, they might pick up more social biases. In this work, we examine the connection between model size and its gender bias (specifically, occupational gender bias). We measure bias in three masked language model families (RoBERTa, DeBERTa, and T5) in two setups: directly using prompt based method, and using a downstream task (Winogender). We find on the one hand that larger models receive higher bias scores on the former task, but when evaluated on the latter, they make fewer gender errors. To examine these potentially conflicting results, we carefully investigate the behavior of the different models on Winogender. We find that while larger models outperform smaller ones, the probability that their mistakes are caused by gender bias is higher. Moreover, we find that the proportion of stereotypical errors compared to anti-stereotypical ones grows with the model size. Our findings highlight the potential risks that can arise from increasing model size.	3a37fef290d76029c295201cc168c0f8ecb0a0cf	@['JournalArticle']{tal-etal-2022-fewer,  author = {Yarden Tal and Inbal Magar and Roy Schwartz},  booktitle = {GEBNLP},  journal = {ArXiv},  title = {Fewer Errors, but More Stereotypes? The Effect of Model Size on Gender Bias},  volume = {abs/2206.09860},  year = {2022} }
Few-shot Prompting Towards Controllable Response Generation	2022	http://www.semanticscholar.org/paper/308a59020d320f620f34f96c9ecdc187baff9fa1	The experiment results show that the proposed method can successfully control several state-of-the-art (SOTA) dialogue models without accessing their parameters and demonstrates the strong ability to quickly adapt to an unseen task in fewer steps than the baseline model.	maybe	0	Much literature has shown that prompt-based learning is an efﬁcient method to make use of the large pre-trained language model. Recent works also exhibit the possibility of steering a chatbot’s output by plugging in an ap-propriate prompt. Gradient-based methods are often used to perturb the prompts. However, some language models are not even available to the public. In this work, we ﬁrst explored the combination of prompting and reinforcement learning (RL) to steer models’ generation without accessing any of the models’ parameters. Second, to reduce the training effort and enhance the generalizability to the unseen task, we apply multi-task learning to make the model learn to generalize to new tasks better. The experiment results show that our proposed method can successfully control several state-of-the-art (SOTA) dialogue models without accessing their parameters. Furthermore, the model demonstrates the strong ability to quickly adapt to an unseen task in fewer steps than the baseline model.	308a59020d320f620f34f96c9ecdc187baff9fa1	@['JournalArticle']{su-etal-2022-few,  author = {Hsuan Su and Po-Han Chi and Shih-Cheng Huang and Chung Ho Lam and Saurav Sahay and Shang-Tse Chen and Hung-yi Lee},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Few-shot Prompting Towards Controllable Response Generation},  volume = {abs/2206.03931},  year = {2022} }
Feed-Forward Blocks Control Contextualization in Masked Language Models	2023	https://www.semanticscholar.org/paper/670c203e1ccc51eb2d6f312e641efeff4e02f17b	Transformer-based models are the core of recent natural language processing and have been analyzed typically with attention patterns as their epoch-making fea-ture is contextualizing surrounding input words via attention mechanisms, but this study analyses their inner contextualization by considering all the components.	maybe	0	Understanding the inner workings of neural network models is a crucial step for ratio-nalizing their output and reﬁning their archi-tecture. Transformer-based models are the core of recent natural language processing and have been analyzed typically with attention patterns as their epoch-making fea-ture is contextualizing surrounding input words via attention mechanisms. In this study, we analyze their inner contextualization by considering all the components, including the feed-forward block (i.e., a feed-forward layer and its surrounding residual and normalization layers) as well as the attention. Our experiments with masked language models show that each of the previously overlooked components did modify the degree of the contextualization in case of processing special word-word pairs (e.g., consisting of named entities). Furthermore, we ﬁnd that some components cancel each other’s effects. Our results could update the typical view about each component’s roles (e.g., attention performs contextualization, and the other components serve different roles) in the Transformer layer.	670c203e1ccc51eb2d6f312e641efeff4e02f17b	@None{kobayashi-etal-2023-feed,  author = {Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentarou Inui},  title = {Feed-Forward Blocks Control Contextualization in Masked Language Models},  year = {2023} }
Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection	2022	http://www.semanticscholar.org/paper/48be558f86101bbd60a41621daceb8a27141852e	This paper critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness.	maybe	2	Avoiding to rely on dataset artifacts to predict hate speech is at the cornerstone of robust and fair hate speech detection. In this paper we critically analyze lexical biases in hate speech detection via a cross-platform study, disentangling various types of spurious and authentic artifacts and analyzing their impact on out-of-distribution fairness and robustness. We experiment with existing approaches and propose simple yet surprisingly effective data-centric baselines. Our results on English data across four platforms show that distinct spurious artifacts require different treatments to ultimately attain both robustness and fairness in hate speech detection. To encourage research in this direction, we release all baseline models and the code to compute artifacts, pointing it out as a complementary and necessary addition to the data statements practice.	48be558f86101bbd60a41621daceb8a27141852e	@['JournalArticle', 'Conference']{ramponi-tonelli-2022-features,  author = {Alan Ramponi and Sara Tonelli},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {3027-3040},  title = {Features or Spurious Artifacts? Data-centric Baselines for Fair and Robust Hate Speech Detection},  year = {2022} }
Fast Model Editing at Scale	2021	https://www.semanticscholar.org/paper/76beeb2ece0abd7fc586d4006435e696d02c6757	MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable.	yes	55	While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks with Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model’s behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.	76beeb2ece0abd7fc586d4006435e696d02c6757	@['JournalArticle']{mitchell-etal-2021-fast,  author = {Eric Mitchell and Charles Lin and A. Bosselut and Chelsea Finn and Christopher D. Manning},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Fast Model Editing at Scale},  volume = {abs/2110.11309},  year = {2021} }
Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity	2021	http://www.semanticscholar.org/paper/0adec918885dff698acf359988ed79a543157f80	This work uses the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, it identifies performant prompts and yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.	maybe	145	When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true few-shot setting as it requires additional annotated data. Instead, we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set, we identify performant prompts. Our method yields a 13% relative improvement for GPT-family models across eleven different established text classification tasks.	0adec918885dff698acf359988ed79a543157f80	@['JournalArticle', 'Conference']{lu-etal-2021-fantastically,  author = {Yao Lu and Max Bartolo and Alastair Moore and S. Riedel and Pontus Stenetorp},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {8086-8098},  title = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity},  year = {2021} }
Fair Data Generation using Language Models with Hard Constraints	2021	http://www.semanticscholar.org/paper/9aca349614e8f224fd7ceb09837a8d982db5cf9b	New ways to inject hard constraints and knowledge into the language models that address privacy and ethical concerns with such datasets and also improve performance on this task are introduced.	maybe	1	Natural language text generation has seen significant improvements with the advent of pre-trained language models. Using such language models to predict personal data entities, in place of redacted spans in text, could help generate synthetic datasets. In order to address privacy and ethical concerns with such datasets, we need to ensure that the masked entity predictions are also fair and controlled by application specific constraints. We introduce new ways to inject hard constraints and knowledge into the language models that address such concerns and also improve performance on this task.	9aca349614e8f224fd7ceb09837a8d982db5cf9b	@None{islam-etal-2021-fair,  author = {Sk Mainul Islam and Abhinav Nagpal and Pranay Kumar Lohia},  title = {Fair Data Generation using Language Models with Hard Constraints},  year = {2021} }
Factual Probing Is [MASK]: Learning vs. Learning to Recall	2021	https://www.semanticscholar.org/paper/a847237e36b954c60e1959152468ebed0118f286	OptiPrompt is proposed, a novel and efficient method which directly optimizes in continuous embedding space and is able to predict an additional 6.4% of facts in the LAMA benchmark.	maybe	132	Petroni et al. (2019) demonstrated that it is possible to retrieve world facts from a pre-trained language model by expressing them as cloze-style prompts and interpret the model’s prediction accuracy as a lower bound on the amount of factual information it encodes. Subsequent work has attempted to tighten the estimate by searching for better prompts, using a disjoint set of facts as training data. In this work, we make two complementary contributions to better understand these factual probing techniques. First, we propose OptiPrompt, a novel and efficient method which directly optimizes in continuous embedding space. We find this simple method is able to predict an additional 6.4% of facts in the LAMA benchmark. Second, we raise a more important question: Can we really interpret these probing results as a lower bound? Is it possible that these prompt-search methods learn from the training data too? We find, somewhat surprisingly, that the training data used by these methods contains certain regularities of the underlying fact distribution, and all the existing prompt methods, including ours, are able to exploit them for better fact prediction. We conduct a set of control experiments to disentangle “learning” from “learning to recall”, providing a more detailed picture of what different prompts can reveal about pre-trained language models.	a847237e36b954c60e1959152468ebed0118f286	@['JournalArticle', 'Conference']{zhong-etal-2021-factual,  author = {Zexuan Zhong and Dan Friedman and Danqi Chen},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Factual Probing Is [MASK]: Learning vs. Learning to Recall},  volume = {abs/2104.05240},  year = {2021} }
Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences	2022	http://www.semanticscholar.org/paper/08ec1848527cd46e34b04ff401b01785cf04e935		maybe	2	Attention describes cognitive processes that are important to many human phenomena including reading. The term is also used to describe the way in which transformer neural networks perform natural language processing. While attention appears to be very different under these two contexts, this paper presents an analysis of the correlations between transformer attention and overt human attention during reading tasks. An extensive analysis of human eye tracking datasets showed that the dwell times of human eye movements were strongly correlated with the attention patterns occurring in the early layers of pre-trained transformers such as BERT. Additionally, the strength of a correlation was not related to the number of parameters within a transformer. This suggests that something about the transformers’ architecture determined how closely the two measures were correlated.	08ec1848527cd46e34b04ff401b01785cf04e935	@None{bensemann-etal-2022-eye,  author = {Joshua Bensemann and A. Peng and Diana Prado and Yang Chen and N. Tan and P. Corballis and Patricia Riddle and Michael Witbrock},  booktitle = {Workshop on Cognitive Modeling and Computational Linguistics},  journal = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},  title = {Eye Gaze and Self-attention: How Humans and Transformers Attend Words in Sentences},  year = {2022} }
Extracting Training Data from Large Language Models	2020	https://www.semanticscholar.org/paper/62d1a3137b01a69443bebf4d92c1990ec512a6a1	This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model, and finds that larger models are more vulnerable than smaller models.	yes	422	It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.	62d1a3137b01a69443bebf4d92c1990ec512a6a1	@['JournalArticle']{carlini-etal-2020-extracting,  author = {Nicholas Carlini and Florian Tramèr and Eric Wallace and Matthew Jagielski and Ariel Herbert-Voss and Katherine Lee and Adam Roberts and Tom B. Brown and D. Song and Ú. Erlingsson and Alina Oprea and Colin Raffel},  booktitle = {USENIX Security Symposium},  pages = {2633-2650},  title = {Extracting Training Data from Large Language Models},  year = {2020} }
Extracting Latent Steering Vectors from Pretrained Language Models	2022	http://www.semanticscholar.org/paper/42b6b7ae57b2b784be7fa78bf98b1c61d2b62751	The results suggest that frozen LMs can be effectively controlled through their latent steering space, and it is found that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models.	yes	3	Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly (> 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.	42b6b7ae57b2b784be7fa78bf98b1c61d2b62751	@['JournalArticle']{subramani-etal-2022-extracting,  author = {Nishant Subramani and Nivedita Suresh and Matthew E. Peters},  booktitle = {Findings},  pages = {566-581},  title = {Extracting Latent Steering Vectors from Pretrained Language Models},  year = {2022} }
Extracting Cultural Commonsense Knowledge at Scale	2022	http://www.semanticscholar.org/paper/a912d9252856182507ddaae6f7f81f74e21bc5c2	Candle is presented, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale and includes judicious techniques for classification-based filtering and scoring of interestingness.	maybe	0	Structured knowledge is important for many AI applications. Commonsense knowledge, which is crucial for robust human-centric AI, is covered by a small number of structured knowledge projects. However, they lack knowledge about human traits and behaviors conditioned on socio-cultural contexts, which is crucial for situative AI. This paper presents Candle, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale. Candle extracts CCSK assertions from a huge web corpus and organizes them into coherent clusters, for 3 domains of subjects (geography, religion, occupation) and several cultural facets (food, drinks, clothing, traditions, rituals, behaviors). Candle includes judicious techniques for classification-based filtering and scoring of interestingness. Experimental evaluations show the superiority of the Candle CCSK collection over prior works, and an extrinsic use case demonstrates the benefits of CCSK for the GPT-3 language model. Code and data can be accessed at https://cultural-csk.herokuapp.com/.	a912d9252856182507ddaae6f7f81f74e21bc5c2	@['JournalArticle']{nguyen-etal-2022-extracting,  author = {Tuan-Phong Nguyen and S. Razniewski and A. Varde and G. Weikum},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Extracting Cultural Commonsense Knowledge at Scale},  volume = {abs/2210.07763},  year = {2022} }
Extracted BERT Model Leaks More Information than You Think!	2022	http://www.semanticscholar.org/paper/c868094210c36a0484136b2b18b7d5002acf9b58	It is revealed that model extraction can cause severe privacy leakage even when victim models are facilitated with advanced defensive strategies, and this work bridges the gap by launching an attribute inference attack against the extracted BERT model.	maybe	1	The collection and availability of big data, combined with advances in pre-trained models (e.g. BERT), have revolutionized the predictive performance of natural language processing tasks. This allows corporations to provide machine learning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as APIs. Due to significant commercial interest, there has been a surge of attempts to steal remote services via model extraction. Although previous works have made progress in defending against model extraction attacks, there has been little discussion on their performance in preventing privacy leakage. This work bridges this gap by launching an attribute inference attack against the extracted BERT model. Our extensive experiments reveal that model extraction can cause severe privacy leakage even when victim models are facilitated with state-of-the-art defensive strategies.	c868094210c36a0484136b2b18b7d5002acf9b58	@['JournalArticle', 'Conference']{he-etal-2022-extracted,  author = {Xuanli He and Chen Chen and L. Lyu and Qiongkai Xu},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Extracted BERT Model Leaks More Information than You Think!},  volume = {abs/2210.11735},  year = {2022} }
Extensive study on the underlying gender bias in contextualized word embeddings	2020	http://www.semanticscholar.org/paper/44825ca025bbce561fd840baeab902d2ca6313c4	This study points out the advantages and limitations of the various evaluation measures that are used and aims to standardize the evaluation of gender bias in contextualized word embeddings.	maybe	16		44825ca025bbce561fd840baeab902d2ca6313c4	@['JournalArticle']{basta-etal-2020-extensive,  author = {Christine Basta and M. Costa-jussà and Noe Casas},  booktitle = {Neural computing & applications (Print)},  journal = {Neural Computing and Applications},  pages = {3371 - 3384},  title = {Extensive study on the underlying gender bias in contextualized word embeddings},  volume = {33},  year = {2020} }
Extend and Explain: Interpreting Very Long Language Models	2022	http://www.semanticscholar.org/paper/8615e95da30a3cc1d7fc7aa7a0b3853be30b3090	A novel Masked Sampling Procedure (MSP) is introduced to identify the text blocks that contribute to a prediction, apply MSP in the context of predicting diagnoses from medical text, and validate the approach with a blind review by two clinicians.	maybe	0	While Transformer language models (LMs) are state-of-the-art for information extraction, long text introduces computational challenges requiring sub-optimal preprocessing steps or alterna-tive model architectures. Sparse attention LMs can represent longer sequences, overcoming performance hur-dles. However, it remains unclear how to explain predictions from these models, as not all tokens attend to each other in the self-attention layers, and long sequences pose computational challenges for explainability algorithms when runtime depends on document length. These challenges are severe in the medical context where documents can be very long, and machine learning (ML) models must be auditable and trustworthy. We introduce a novel Masked Sampling Procedure (MSP) to identify the text blocks that contribute to a prediction, apply MSP in the context of predicting diagnoses from medical text, and validate our approach with a blind review by two clinicians. Our method identiﬁes ≈ 1 . 7 × more clinically informative text blocks than the previous state-of-the-art, runs up to 100 × faster, and is tractable for generating important phrase pairs. MSP is particularly well-suited to long LMs but can be applied to any text classiﬁer. We provide a general implementation here. 1	8615e95da30a3cc1d7fc7aa7a0b3853be30b3090	@['JournalArticle', 'Review']{stremmel-etal-2022-extend,  author = {Joel Stremmel and B. Hill and Jeffrey S. Hertzberg and Jaime Murillo and Llewelyn Allotey and E. Halperin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Extend and Explain: Interpreting Very Long Language Models},  volume = {abs/2209.01174},  year = {2022} }
Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data	2020	http://www.semanticscholar.org/paper/91bad6519095404998f4ce23592547b409cdb60a	This work identifies failure modes of SOTA relation extraction (RE) models trained on TACRED, which are attributed to limitations in the data annotation process, and provides concrete suggestion on how to improve RE data collection to alleviate this behavior.	maybe	19	The process of collecting and annotating training data may introduce distribution artifacts which may limit the ability of models to learn correct generalization behavior. We identify failure modes of SOTA relation extraction (RE) models trained on TACRED, which we attribute to limitations in the data annotation process. We collect and annotate a challenge-set we call Challenging RE (CRE), based on naturally occurring corpus examples, to benchmark this behavior. Our experiments with four state-of-the-art RE models show that they have indeed adopted shallow heuristics that do not generalize to the challenge-set data. Further, we find that alternative question answering modeling performs significantly better than the SOTA models on the challenge-set, despite worse overall TACRED performance. By adding some of the challenge data as training examples, the performance of the model improves. Finally, we provide concrete suggestion on how to improve RE data collection to alleviate this behavior.	91bad6519095404998f4ce23592547b409cdb60a	@['JournalArticle', 'Conference']{rosenman-etal-2020-exposing,  author = {Shachar Rosenman and Alon Jacovi and Yoav Goldberg},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data},  volume = {abs/2010.03656},  year = {2020} }
Exploring Transitivity in Neural NLI Models through Veridicality	2021	http://www.semanticscholar.org/paper/0d39d525f30609d0541330f933007025cd457a83	It is found that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples.	maybe	12	Despite the recent success of deep neural networks in natural language processing, the extent to which they can demonstrate human-like generalization capacities for natural language understanding remains unclear. We explore this issue in the domain of natural language inference (NLI), focusing on the transitivity of inference relations, a fundamental property for systematically drawing inferences. A model capturing transitivity can compose basic inference patterns and draw new inferences. We introduce an analysis method using synthetic and naturalistic NLI datasets involving clause-embedding verbs to evaluate whether models can perform transitivity inferences composed of veridical inferences and arbitrary inference types. We find that current NLI models do not perform consistently well on transitivity inference tasks, suggesting that they lack the generalization capacity for drawing composite inferences from provided training examples. The data and code for our analysis are publicly available at https://github.com/verypluming/transitivity.	0d39d525f30609d0541330f933007025cd457a83	@['JournalArticle', 'Conference']{yanaka-etal-2021-exploring,  author = {Hitomi Yanaka and K. Mineshima and Kentaro Inui},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Exploring Transitivity in Neural NLI Models through Veridicality},  volume = {abs/2101.10713},  year = {2021} }
Exploring the Use of Neural Transformers for Psycholinguistics	2021	http://www.semanticscholar.org/paper/b7cd1d84baaa3a65be4216e39086e034b8432845	Using Transformers to Predict Psychometric Properties and Applications of Psychometrics to Machine Learning for Machine Learning.	maybe	0	......................................................................................................................................... iv Chapter 1: Introduction ....................................................................................................................1 Chapter 2: Related Work .................................................................................................................4 2.1 Applying Distributional Models to Age of Acquisition ................................................4 2.2 Applications of Psychometrics to Machine Learning ....................................................5 2.3 BERTology and Interpretability of Transformers..........................................................7 Chapter 3: Modeling Age of Acquisition Norms .............................................................................9 3.1 Methodology and Results ..............................................................................................9 3.1.1 Psycholinguistic Features..............................................................................10 3.1.2 Kuperman Hyperparameters .........................................................................11 3.1.3 Wordbank Baseline Hyperparameters ..........................................................13 3.1.4 Wordbank Transformer Hyperparameters ....................................................13 3.2 Discussion ....................................................................................................................17 Chapter 4: Using Transformers to Predict Psychometric Properties .............................................19 4.1 Methodology ................................................................................................................20 4.1.1 Language Model Experiments ......................................................................20 4.1.1.1 Merged GLUE Categories .............................................................20 4.1.1.2 Model Training Regimes ...............................................................21 4.1.2 Human Studies ..............................................................................................23 4.1.2.1 Human Study Phases......................................................................24 4.1.2.2 Human Studies Codebook..............................................................25 4.2 Results ..........................................................................................................................26 4.3 Discussion ....................................................................................................................39 Chapter 5: Conclusion....................................................................................................................31 References ......................................................................................................................................33	b7cd1d84baaa3a65be4216e39086e034b8432845	@None{laverghetta-2021-exploring,  author = {A. Laverghetta},  title = {Exploring the Use of Neural Transformers for Psycholinguistics},  year = {2021} }
Exploring the Universal Vulnerability of Prompt-based Learning Paradigm	2022	http://www.semanticscholar.org/paper/6303855a23c4fa5ab78450b0a93e9b0c34ca4a5a	This paper explores this universal vulnerability of prompt-based learning paradigm by either injecting backdoor triggers or search- ing for adversarial triggers on pre-trained lan- 010 guage models using only plain text and proposes a potential solution to mitigate the vulnerability.	maybe	5	Prompt-based learning paradigm bridges the 001 gap between pre-training and fine-tuning, and 002 works effectively under the few-shot setting. 003 However, we find that this learning paradigm 004 inherits the vulnerability from the pre-training 005 stage, where model predictions can be misled 006 by inserting certain triggers into the text. In this 007 paper, we explore this universal vulnerability 008 by either injecting backdoor triggers or search- 009 ing for adversarial triggers on pre-trained lan- 010 guage models using only plain text. In both 011 scenarios, we demonstrate that our triggers can 012 totally control or severely decrease the perfor- 013 mance of prompt-based models fine-tuned on 014 arbitrary downstream tasks, reflecting the uni- 015 versal vulnerability of the prompt-based learn- 016 ing paradigm. Further experiments show that 017 adversarial triggers have good transferability 018 among language models. We also find con- 019 ventional fine-tuning models are not vulner- 020 able to adversarial triggers constructed from 021 pre-trained language models. We conclude by 022 proposing a potential solution to mitigate our 023 attack methods. All the code and data will be 024 made public. 025 028	6303855a23c4fa5ab78450b0a93e9b0c34ca4a5a	@['JournalArticle']{xu-etal-2022-exploring,  author = {Lei Xu and Yangyi Chen and Ganqu Cui and Hongcheng Gao and Zhiyuan Liu},  booktitle = {NAACL-HLT},  journal = {ArXiv},  title = {Exploring the Universal Vulnerability of Prompt-based Learning Paradigm},  volume = {abs/2204.05239},  year = {2022} }
Exploring the Role of BERT Token Representations to Explain Sentence Probing Results	2021	http://www.semanticscholar.org/paper/b265827019f420b44c79fd87be1cc6000329c762	It is shown that BERT tends to encode meaningful knowledge in specific token representations, allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces.	yes	4	Several studies have been carried out on revealing linguistic features captured by BERT. This is usually achieved by training a diagnostic classifier on the representations obtained from different layers of BERT. The subsequent classification accuracy is then interpreted as the ability of the model in encoding the corresponding linguistic property. Despite providing insights, these studies have left out the potential role of token representations. In this paper, we provide a more in-depth analysis on the representation space of BERT in search for distinct and meaningful subspaces that can explain the reasons behind these probing results. Based on a set of probing tasks and with the help of attribution methods we show that BERT tends to encode meaningful knowledge in specific token representations (which are often ignored in standard classification setups), allowing the model to detect syntactic and semantic abnormalities, and to distinctively separate grammatical number and tense subspaces.	b265827019f420b44c79fd87be1cc6000329c762	@['JournalArticle', 'Conference']{mohebbi-etal-2021-exploring,  author = {Hosein Mohebbi and A. Modarressi and Mohammad Taher Pilehvar},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Exploring the Role of BERT Token Representations to Explain Sentence Probing Results},  volume = {abs/2104.01477},  year = {2021} }
Exploring the Representations of Individual Entities in the Brain Combining EEG and Distributional Semantics	2022	http://www.semanticscholar.org/paper/ec3fa12d205ede187cdd987bfecea79466b6f30b	In-depth analyses of the decoder performance provide additional evidence that the referents of proper names and categories have little in common when it comes to their representation in the brain, and a first brain-based validation of distributional semantic models as representations of individual entities.	maybe	0	Semantic knowledge about individual entities (i.e., the referents of proper names such as Jacinta Ardern) is fine-grained, episodic, and strongly social in nature, when compared with knowledge about generic entities (the referents of common nouns such as politician). We investigate the semantic representations of individual entities in the brain; and for the first time we approach this question using both neural data, in the form of newly-acquired EEG data, and distributional models of word meaning, employing them to isolate semantic information regarding individual entities in the brain. We ran two sets of analyses. The first set of analyses is only concerned with the evoked responses to individual entities and their categories. We find that it is possible to classify them according to both their coarse and their fine-grained category at appropriate timepoints, but that it is hard to map representational information learned from individuals to their categories. In the second set of analyses, we learn to decode from evoked responses to distributional word vectors. These results indicate that such a mapping can be learnt successfully: this counts not only as a demonstration that representations of individuals can be discriminated in EEG responses, but also as a first brain-based validation of distributional semantic models as representations of individual entities. Finally, in-depth analyses of the decoder performance provide additional evidence that the referents of proper names and categories have little in common when it comes to their representation in the brain.	ec3fa12d205ede187cdd987bfecea79466b6f30b	@['JournalArticle']{bruera-poesio-2022-exploring,  author = {Andrea Bruera and Massimo Poesio},  booktitle = {Frontiers in Artificial Intelligence},  journal = {Frontiers in Artificial Intelligence},  title = {Exploring the Representations of Individual Entities in the Brain Combining EEG and Distributional Semantics},  volume = {5},  year = {2022} }
Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy	2021	http://www.semanticscholar.org/paper/4ea7c7e911b0d96838bd57a3f5e79028e0f9a1b4	A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context, but as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences.	maybe	6	This paper presents a multilingual study of word meaning representations in context. We assess the ability of both static and contextualized models to adequately represent different lexical-semantic relations, such as homonymy and synonymy. To do so, we created a new multilingual dataset that allows us to perform a controlled evaluation of several factors such as the impact of the surrounding context or the overlap between words, conveying the same or different senses. A systematic assessment on four scenarios shows that the best monolingual models based on Transformers can adequately disambiguate homonyms in context. However, as they rely heavily on context, these models fail at representing words with different senses when occurring in similar sentences. Experiments are performed in Galician, Portuguese, English, and Spanish, and both the dataset (with more than 3,000 evaluation items) and new models are freely released with this study.	4ea7c7e911b0d96838bd57a3f5e79028e0f9a1b4	@['JournalArticle', 'Conference']{garcia-2021-exploring,  author = {Marcos Garcia},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Exploring the Representation of Word Meanings in Context: A Case Study on Homonymy and Synonymy},  volume = {abs/2106.13553},  year = {2021} }
Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation	2020	http://www.semanticscholar.org/paper/86e1aaa0c47659e08a896e9889384eb1e5401e6a	This work takes inspiration from kernel principal component analysis and derive a non-linear bias isolation technique and shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).	maybe	15	Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias sub-space is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).	86e1aaa0c47659e08a896e9889384eb1e5401e6a	@['JournalArticle', 'Conference']{vargas-cotterell-2020-exploring,  author = {Francisco Vargas and Ryan Cotterell},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {2902-2913},  title = {Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation},  year = {2020} }
Exploring the Effects of Negation and Grammatical Tense on Bias Probes	2022	http://www.semanticscholar.org/paper/2b7718e556eef6a986bb2338d662af5c6776cf55		maybe	0	We investigate in this paper how correlations between occupations and gendered-pronouns can be affected and changed by adding negation in bias probes, or changing the grammatical tense of the verbs in the probes. We use a set of simple bias probes in Norwegian and English, and perform 16 different probing analysis, using four Norwegian and four English pre-trained language models. We show that adding negation to probes does not have a considerable effect on the correlations between gendered-pronouns and occupations, supporting other works on negation in language models. We also show that altering the grammatical tense of verbs in bias probes do have some interesting effects on models’ behaviours and correlations. We argue that we should take grammatical tense into account when choosing bias probes, and aggregating results across tenses might be a better representation of the existing correlations.	2b7718e556eef6a986bb2338d662af5c6776cf55	@['JournalArticle']{touileb-2022-exploring,  author = {Samia Touileb},  booktitle = {AACL},  pages = {423-429},  title = {Exploring the Effects of Negation and Grammatical Tense on Bias Probes},  year = {2022} }
Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models	2021	http://www.semanticscholar.org/paper/011dd2058a9e38ac91b1790377fdd7d0f99d2781	This paper investigates what models learn from commonsense reasoning datasets and shows that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers.	maybe	9	Commonsense reasoning benchmarks have been largely solved by fine-tuning language models. The downside is that fine-tuning may cause models to overfit to task-specific data and thereby forget their knowledge gained during pre-training. Recent works only propose lightweight model updates as models may already possess useful knowledge from past experience, but a challenge remains in understanding what parts and to what extent models should be refined for a given task. In this paper, we investigate what models learn from commonsense reasoning datasets. We measure the impact of three different adaptation methods on the generalization and accuracy of models. Our experiments with two models show that fine-tuning performs best, by learning both the content and the structure of the task, but suffers from overfitting and limited generalization to novel answers. We observe that alternative adaptation methods like prefix-tuning have comparable accuracy, but generalize better to unseen answers and are more robust to adversarial splits.	011dd2058a9e38ac91b1790377fdd7d0f99d2781	@['JournalArticle', 'Conference']{ma-etal-2021-exploring,  author = {Kaixin Ma and Filip Ilievski and Jonathan M Francis and Satoru Ozaki and Eric Nyberg and A. Oltramari},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models},  volume = {abs/2109.02837},  year = {2021} }
Exploring Span Representations in Neural Coreference Resolution	2020	http://www.semanticscholar.org/paper/76778bddbeba9b7f854db2e988bfe76d43ccd691	This work presents a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a probing model and finds that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge.	maybe	1	In coreference resolution, span representations play a key role to predict coreference links accurately. We present a thorough examination of the span representation derived by applying BERT on coreference resolution (Joshi et al., 2019) using a probing model. Our results show that the span representation is able to encode a significant amount of coreference information. In addition, we find that the head-finding attention mechanism involved in creating the spans is crucial in encoding coreference knowledge. Last, our analysis shows that the span representation cannot capture non-local coreference as efficiently as local coreference.	76778bddbeba9b7f854db2e988bfe76d43ccd691	@None{kahardipraja-etal-2020-exploring,  author = {Patrick Kahardipraja and O. Vyshnevska and S. Loáiciga},  booktitle = {CODI},  journal = {Colorectal Disease},  pages = {32-41},  title = {Exploring Span Representations in Neural Coreference Resolution},  year = {2020} }
Exploring RoBERTa ’ s theory of mind through textual entailment	2021	http://www.semanticscholar.org/paper/e7246c8323411183d6a6fbe83d0d8150b3777895		maybe	1	Within psychology, philosophy, and cognitive science, theory of mind refers to the cognitive ability to reason about the mental states of other people, thus recognizing them as having beliefs, knowledge, intentions and emotions of their own. In this project, we construct a natural language inference (NLD) dataset that tests the ability of a state of the art language model, RoBERTa-large finetuned on the MNLI dataset, to make theory of mind inferences related to knowledge and belief. Experimental results suggest that the model struggles with such inferences, including after attempts for further finetuning.	e7246c8323411183d6a6fbe83d0d8150b3777895	@None{hewitt-cohen-2021-exploring,  author = {John Hewitt and Michael Cohen},  title = {Exploring RoBERTa ’ s theory of mind through textual entailment},  year = {2021} }
Exploring Nominal Coercion in Semantic Spaces with Static and Contextualized Word Embeddings	2022	http://www.semanticscholar.org/paper/7556b2d9f85b97a694c09323f2c90ee5cba33ffe		maybe	0	The distinction between mass nouns and count nouns has a long history in formal semantics, and linguists have been trying to identify the semantic properties defining the two classes. However, they also recognized that both can undergo meaning shifts and be used in contexts of a different type, via nominal coercion. In this paper, we present an approach to measure the meaning shift in count-mass coercion in English that makes use of static and contextualized word embedding distance. Our results show that the coercion shifts are detected only by a small subset of the traditional word embedding models, and that the shifts detected by the contextualized embedding of BERT are more pronounced for mass nouns.	7556b2d9f85b97a694c09323f2c90ee5cba33ffe	@['JournalArticle']{liu-chersoni-2022-exploring,  author = {Chenxin Liu and Emmanuele Chersoni},  booktitle = {Workshop on Cognitive Aspects of the Lexicon},  pages = {49-57},  title = {Exploring Nominal Coercion in Semantic Spaces with Static and Contextualized Word Embeddings},  year = {2022} }
Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces	2021	http://www.semanticscholar.org/paper/a9c07a60696a165aed09f0f772cc51ff52574a07	AttViz is presented, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence.	maybe	1	Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed methods in an online toolkit and an offline library. Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned.	a9c07a60696a165aed09f0f772cc51ff52574a07	@['JournalArticle']{škrlj-etal-2021-exploring,  author = {Blaž Škrlj and Shane Sheehan and Nika Erzen and M. Robnik-Sikonja and S. Luz and Senja Pollak},  booktitle = {HACKASHOP},  pages = {76-83},  title = {Exploring Neural Language Models via Analysis of Local and Global Self-Attention Spaces},  year = {2021} }
Exploring Mode Connectivity for Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/02c873a69b8702e3848f942fefb7437ec012422c	Exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help to fathom the inner workings of PLM downstream adaptation.	yes	1	Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM’s mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM’s task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation. The codes are publicly available at https://github.com/thunlp/Mode-Connectivity-PLM.	02c873a69b8702e3848f942fefb7437ec012422c	@['JournalArticle', 'Conference']{qin-etal-2022-exploring,  author = {Yujia Qin and Cheng Qian and Jing Yi and Weize Chen and Yankai Lin and Xu Han and Zhiyuan Liu and Maosong Sun and Jie Zhou},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Exploring Mode Connectivity for Pre-trained Language Models},  volume = {abs/2210.14102},  year = {2022} }
Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning	2021	http://www.semanticscholar.org/paper/a0033c2b38d289fd71194eb830b14d0db8f5a18b	Empirically, there is evidence indicating that the adaptations of PLMs to various few-shot tasks can be reparameterized as optimizing only a few free parame- 007 ters in a uniﬁed low-dimensional intrinsic task 008 subspace, which may help to understand why 009 PLMs could easily adapt to various NLP tasks 010 with small-scale data.	maybe	13	Why can pre-trained language models (PLMs) 001 learn universal representations and effectively 002 adapt to broad NLP tasks differing a lot super- 003 ﬁcially? In this work, we empirically ﬁnd evi- 004 dence indicating that the adaptations of PLMs 005 to various few-shot tasks can be reparameter- 006 ized as optimizing only a few free parame- 007 ters in a uniﬁed low-dimensional intrinsic task 008 subspace , which may help us understand why 009 PLMs could easily adapt to various NLP tasks 010 with small-scale data. To ﬁnd such a subspace 011 and examine its universality, we propose an 012 analysis pipeline called intrinsic prompt tun- 013 ing (IPT). Speciﬁcally, we resort to the re- 014 cent success of prompt tuning and decompose 015 the soft prompts of multiple NLP tasks into 016 the same low-dimensional nonlinear subspace, 017 then we learn to adapt the PLM to unseen data 018 or tasks by only tuning parameters in this sub- 019 space. In the experiments, we study diverse 020 few-shot NLP tasks and surprisingly ﬁnd that 021 in a 5 -dimensional subspace found with 100 022 tasks, by only tuning 5 free parameters, we 023 can recover 87% and 65% of the full prompt 024 tuning performance for 100 seen tasks (using 025 different training data) and 20 unseen tasks, re- 026 spectively, showing great generalization abil- 027 ity of the found intrinsic task subspace. Be- 028 sides being an analysis tool, IPT could further 029 bring practical beneﬁts, such as improving the 030 prompt tuning stability. 031	a0033c2b38d289fd71194eb830b14d0db8f5a18b	@['JournalArticle']{qin-etal-2021-exploring,  author = {Yujia Qin and Xiaozhi Wang and Yusheng Su and Yankai Lin and Ning Ding and Zhiyuan Liu and Juan-Zi Li and Lei Hou and Peng Li and Maosong Sun and Jie Zhou},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning},  volume = {abs/2110.07867},  year = {2021} }
Exploring Lexical Irregularities in Hypothesis-Only Models of Natural Language Inference	2020	http://www.semanticscholar.org/paper/16cf827059410addfd5397a6b17cf6a805bf6d5e	This work analyzes hypothesis-only models trained on one of the recast datasets provided in Poliak et al.	maybe	0	Natural Language Inference (NLI) or Recognizing Textual Entailment (RTE) is the task of predicting the entailment relation between a pair of sentences (premise and hypothesis). This task has been described as “a valuable testing ground for the development of semantic representations” [1], and is a key component in natural language understanding evaluation benchmarks. Models that understand entailment should encode both, the premise and the hypothesis. However, experiments by Poliak et al. [2] revealed a strong preference of these models towards patterns observed only in the hypothesis, based on a 10 dataset comparison. Their results indicated the existence of statistical irregularities present in the hypothesis that bias the model into performing competitively with the state of the art. While recast datasets provide large scale generation of NLI instances due to minimal human intervention, the papers that generate them do not provide fine-grained analysis of the potential statistical patterns that can bias NLI models. In this work, we analyze hypothesis-only models trained on one of the recast datasets provided in Poliak et al. [2] for word-level patterns. Our results indicate the existence of potential lexical biases that could contribute to inflating the models’ performance.	16cf827059410addfd5397a6b17cf6a805bf6d5e	@['JournalArticle', 'Conference']{hu-etal-2020-exploring,  author = {Qingyuan Hu and Yi Zhang and Kanishka Misra and J. Rayz},  booktitle = {IEEE International Conference on Cognitive Informatics and Cognitive Computing},  journal = {2020 IEEE 19th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)},  pages = {125-130},  title = {Exploring Lexical Irregularities in Hypothesis-Only Models of Natural Language Inference},  year = {2020} }
Exploring Length Generalization in Large Language Models	2022	https://www.semanticscholar.org/paper/f843233f76a5dff07bfa93a71a1cf13d8aa6a94a	It is shown that combining pretrained large language models’ in-context learning abilities with scratchpad prompting results in a dramatic improvement in length generalization, and is run to identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.	maybe	18	The ability to extrapolate from short problem instances to longer ones is an im-portant form of out-of-distribution generalization in reasoning tasks, and is crucial when learning from datasets where longer problem instances are rare. These include theorem proving, solving quantitative mathematics problems, and reading/-summarizing novels. In this paper, we run careful empirical studies exploring the length generalization capabilities of transformer-based language models. We ﬁrst establish that naively ﬁnetuning transformers on length generalization tasks shows signiﬁcant generalization deﬁciencies independent of model scale. We then show that combining pretrained large language models’ in-context learning abilities with scratchpad prompting (asking the model to output solution steps before producing an answer) results in a dramatic improvement in length generalization. We run careful failure analyses on each of the learning modalities and identify common sources of mistakes that highlight opportunities in equipping language models with the ability to generalize to longer problems.	f843233f76a5dff07bfa93a71a1cf13d8aa6a94a	@['JournalArticle']{anil-etal-2022-exploring,  author = {Cem Anil and Yuhuai Wu and Anders Andreassen and Aitor Lewkowycz and Vedant Misra and V. Ramasesh and Ambrose Slone and Guy Gur-Ari and Ethan Dyer and Behnam Neyshabur},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Exploring Length Generalization in Large Language Models},  volume = {abs/2207.04901},  year = {2022} }
Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning	2021	http://www.semanticscholar.org/paper/a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99	This research finds that the PLMs can easily generalize when the distribution is the same, however, it is still difficult for them to generalize out of the distribution.	maybe	4		a35d5aeba08cccdc5cdf26bc094ccd71d06bdc99	@['JournalArticle']{wang-etal-2021-exploring,  author = {Cunxiang Wang and Boyuan Zheng and Yuchen Niu and Yue Zhang},  booktitle = {Natural Language Processing and Chinese Computing},  pages = {758-769},  title = {Exploring Generalization Ability of Pretrained Language Models on Arithmetic and Logical Reasoning},  year = {2021} }
Exploring BERT’s sensitivity to lexical cues using tests from semantic priming	2020	http://www.semanticscholar.org/paper/06439c29150efb6441bfc928dfa5c0e0967edd9b	A case study analyzing the pre-trained BERT model with tests informed by semantic priming finds that BERT too shows “priming”, predicting a word with greater probability when the context includes a related word versus an unrelated one.	maybe	27	Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows “priming”, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.	06439c29150efb6441bfc928dfa5c0e0967edd9b	@['JournalArticle']{misra-etal-2020-exploring,  author = {Kanishka Misra and Allyson Ettinger and Julia Taylor Rayz},  booktitle = {Findings},  journal = {ArXiv},  title = {Exploring BERT’s sensitivity to lexical cues using tests from semantic priming},  volume = {abs/2010.03010},  year = {2020} }
Exploring AI Ethics of ChatGPT: A Diagnostic Analysis	2023	https://www.semanticscholar.org/paper/df239785e6d26a45e9c8e06551cfecba92d1ecad	A qualitative research method on OpenAI’s ChatGPT is performed to better understand the practical features of ethical dangers in recent LLMs, and it is found that a signiﬁcant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies.	maybe	0	—Recent breakthroughs in natural language process- ing (NLP) have permitted the synthesis and comprehension of coherent text in an open-ended way, therefore translating the theoretical algorithms into practical applications. The large language-model (LLM) has signiﬁcantly impacted businesses such as report summarization softwares and copywriters. Observa-tions indicate, however, that LLMs may exhibit social prejudice and toxicity, posing ethical and societal dangers of consequences resulting from irresponsibility. Large-scale benchmarks for ac- countable LLMs should consequently be developed. Although several empirical investigations reveal the existence of a few ethical difﬁculties in advanced LLMs, there is no systematic examination and user study of the ethics of current LLMs use. To further educate future efforts on constructing ethical LLMs responsibly, we perform a qualitative research method on OpenAI’s ChatGPT 1 to better understand the practical features of ethical dangers in recent LLMs. We analyze ChatGPT comprehensively from four perspectives: 1) Bias 2) Reliability 3) Robustness 4) Toxicity . In accordance with our stated viewpoints, we empirically benchmark ChatGPT on multiple sample datasets. We ﬁnd that a signiﬁcant number of ethical risks cannot be addressed by existing benchmarks, and hence illustrate them via additional case studies. In addition, we examine the implications of our ﬁndings on the AI ethics of ChatGPT, as well as future problems and practical design considerations for LLMs. We believe that our ﬁndings may give light on future efforts to determine and mitigate the ethical hazards posed by machines in LLM applications.	df239785e6d26a45e9c8e06551cfecba92d1ecad	@['JournalArticle']{zhuo-etal-2023-exploring,  author = {Terry Yue Zhuo and Yujin Huang and Chunyang Chen and Zhenchang Xing},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Exploring AI Ethics of ChatGPT: A Diagnostic Analysis},  volume = {abs/2301.12867},  year = {2023} }
Exploratory Model Analysis Using Data-Driven Neuron Representations	2021	https://www.semanticscholar.org/paper/90aa986ba73069ba8627cbd69f93db7e3537809b	The exploratory analysis of a neural model at various levels ranging from individual neurons to the model as a whole reveals that specific phrases and domains of text are captured by individual neurons in BERT, and a group of neurons simultaneously capture the same linguistic phenomena.	yes	0	Probing classifiers have been extensively used to inspect whether a model component captures specific linguistic phenomena. This top-down approach is, however, costly when we have no probable hypothesis on the association between the target model component and phenomena. In this study, aiming to provide a flexible, exploratory analysis of a neural model at various levels ranging from individual neurons to the model as a whole, we present a bottom-up approach to inspect the target neural model by using neuron representations obtained from a massive corpus of text. We first feed massive amount of text to the target model and collect sentences that strongly activate each neuron. We then abstract the collected sentences to obtain neuron representations that help us interpret the corresponding neurons; we augment the sentences with linguistic annotations (e.g., part-of-speech tags) and various metadata (e.g., topic and sentiment), and apply pattern mining and clustering techniques to the augmented sentences. We demonstrate the utility of our method by inspecting the pre-trained BERT. Our exploratory analysis reveals that i) specific phrases and domains of text are captured by individual neurons in BERT, ii) a group of neurons simultaneously capture the same linguistic phenomena, and iii) deeper-level layers capture more specific linguistic phenomena.	90aa986ba73069ba8627cbd69f93db7e3537809b	@['JournalArticle']{oba-etal-2021-exploratory,  author = {Daisuke Oba and Naoki Yoshinaga and M. Toyoda},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {518-528},  title = {Exploratory Model Analysis Using Data-Driven Neuron Representations},  year = {2021} }
Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning	2020	http://www.semanticscholar.org/paper/dbd4944ea3098501a4486a6e8d2a79286c625e0a	A novel feature selection method is proposed, which takes advantage of contextual representations have both intrinsic and task-specific redundancies to reduce the size of the pre-trained features.	maybe	2	Large pre-trained contextual word representations have transformed the field of natural language processing, obtaining impressive results on a wide range of tasks. However, as models increase in size, computational limitations make them impractical for researchers and practitioners alike. We hypothesize that contextual representations have both intrinsic and task-specific redundancies. We propose a novel feature selection method, which takes advantage of these redundancies to reduce the size of the pre-trained features. In a comprehensive evaluation on two pre-trained models, BERT and XLNet, using a diverse suite of sequence labeling and sequence classification tasks, our method reduces the feature set down to 1--7% of the original size, while maintaining more than 97% of the performance.	dbd4944ea3098501a4486a6e8d2a79286c625e0a	@['JournalArticle']{dalvi-etal-2020-exploiting,  author = {Fahim Dalvi and Hassan Sajjad and Nadir Durrani and Yonatan Belinkov},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Exploiting Redundancy in Pre-trained Language Models for Efficient Transfer Learning},  volume = {abs/2004.04010},  year = {2020} }
Explaining the Road Not Taken	2021	http://www.semanticscholar.org/paper/df29628c5ae8680d109d10bb4a7f1b83eb3f0806	This paper summarizes the common forms of explanations used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank, and finds that most model interpretations cannot answer these questions.	maybe	3	Copyright held by the owner/author(s). ACM CHI Workshop on Operationalizing Human-Centered Perspectives in Explainable AI,, May 8–9, 2021, Online Virtual Conference (originally Yokohama, Japan) ACM . Abstract It is unclear if existing interpretations of deep neural network models respond effectively to the needs of users. This paper summarizes the common forms of explanations (such as feature attribution, decision rules, or probes) used in over 200 recent papers about natural language processing (NLP), and compares them against user questions collected in the XAI Question Bank [28]. We found that although users are interested in explanations for the road not taken — namely, why the model chose one result and not a well-defined, seemly similar legitimate counterpart — most model interpretations cannot answer these questions.	df29628c5ae8680d109d10bb4a7f1b83eb3f0806	@['JournalArticle']{shen-huang-2021-explaining,  author = {Hua Shen and Ting-Hao 'Kenneth' Huang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Explaining the Road Not Taken},  volume = {abs/2103.14973},  year = {2021} }
Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs	2022	http://www.semanticscholar.org/paper/bfce51fe2b3d885d09cd3ffed328f86f04442f63	Two perturbationbased post-hoc interpretation methods are adapted, Leaveone-out and Sampling Shapley, to identify words in inputs that cause the uncertainty in predictions of pretrained language models.	maybe	1	Estimating the predictive uncertainty of pretrained language models is important for increasing their trustworthiness in NLP. Although many previous works focus on quantifying prediction uncertainty, there is little work on explaining the uncertainty. This paper pushes a step further on explaining uncertain predictions of post-calibrated pre-trained language models. We adapt two perturbationbased post-hoc interpretation methods, Leaveone-out and Sampling Shapley, to identify words in inputs that cause the uncertainty in predictions. We test the proposed methods on BERT and RoBERTa with three tasks: sentiment classification, natural language inference, and paraphrase identification, in both indomain and out-of-domain settings. Experiments show that both methods consistently capture words in inputs that cause prediction uncertainty.	bfce51fe2b3d885d09cd3ffed328f86f04442f63	@None{chen-ji-2022-explaining,  author = {Hanjie Chen and Yangfeng Ji},  title = {Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs},  year = {2022} }
Explaining Explanations: Axiomatic Feature Interactions for Deep Networks	2020	http://www.semanticscholar.org/paper/798ea191aad9401462b405fde1a6cefb4fe53fd5	This work presents Integrated Hessians, an extension of Integrated Gradients that explains pairwise feature interactions in neural networks and finds that the method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks.	maybe	65	Recent work has shown great promise in explaining neural network behavior. In particular, feature attribution methods explain which features were most important to a model's prediction on a given input. However, for many tasks, simply knowing which features were important to a model's prediction may not provide enough insight to understand model behavior. The interactions between features within the model may better help us understand not only the model, but also why certain features are more important than others. In this work, we present Integrated Hessians, an extension of Integrated Gradients that explains pairwise feature interactions in neural networks. Integrated Hessians overcomes several theoretical limitations of previous methods to explain interactions, and unlike such previous methods is not limited to a specific architecture or class of neural network. Additionally, we find that our method is faster than existing methods when the number of features is large, and outperforms previous methods on existing quantitative benchmarks. Code available at this https URL	798ea191aad9401462b405fde1a6cefb4fe53fd5	@['JournalArticle']{janizek-etal-2020-explaining,  author = {J. Janizek and Pascal Sturmfels and Su-In Lee},  booktitle = {Journal of machine learning research},  journal = {J. Mach. Learn. Res.},  pages = {104:1-104:54},  title = {Explaining Explanations: Axiomatic Feature Interactions for Deep Networks},  volume = {22},  year = {2020} }
Explaining Contextualization in Language Models using Visual Analytics	2021	https://www.semanticscholar.org/paper/442d18798e64c988954e2e7fdb11bcae98fcda2a	It is shown that contextualization is neither driven by polysemy nor by pure context variation, and insights on why BERT fails to model words in the middle of the functionality continuum are provided.	maybe	6	Despite the success of contextualized language models on various NLP tasks, it is still unclear what these models really learn. In this paper, we contribute to the current efforts of explaining such models by exploring the continuum between function and content words with respect to contextualization in BERT, based on linguistically-informed insights. In particular, we utilize scoring and visual analytics techniques: we use an existing similarity-based score to measure contextualization and integrate it into a novel visual analytics technique, presenting the model’s layers simultaneously and highlighting intra-layer properties and inter-layer differences. We show that contextualization is neither driven by polysemy nor by pure context variation. We also provide insights on why BERT fails to model words in the middle of the functionality continuum.	442d18798e64c988954e2e7fdb11bcae98fcda2a	@['JournalArticle', 'Conference']{sevastjanova-etal-2021-explaining,  author = {R. Sevastjanova and A. Kalouli and C. Beck and H. Schäfer and Mennatallah El-Assady},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {464-476},  title = {Explaining Contextualization in Language Models using Visual Analytics},  year = {2021} }
Explaining Classes through Word Attribution	2021	http://www.semanticscholar.org/paper/abfbc0ffdf9d5259ffc708a6de77e8183e019e8b	This work proposes the first method for aggregating explanations of individual examples in text classification to general descriptions of the classes, and test this method by training Transformerbased text classifiers on a large Web register identification corpus and showing that it is able to provide descriptive keywords for the classes.	maybe	0	We propose a method for explaining classes in text classification tasks using deep learning models and feature attribution techniques, such as the Integrated Gradients (IG) method introduced by Sundararajan et al. (2017). We focus specifically on IG as it provides a general framework for estimating feature importance in deep neural networks and has been shown to provide reliable saliency maps in text classification tasks among others (Bastings and Filippova, 2020; Kokhlikyan et al., 2020). Recently, explaining the predictions of deep neural networks has attracted a considerable amount of research interest in fields such as NLP and computer vision. Given the importance of this endeavour, several different techniques have been suggested in order to interpret model predictions (see Montavon et al., 2018, for recent discussion). Nevertheless, these tend to focus on explaining individual predictions rather than how models perceive whole classes. To the best of our knowledge, we present the first method for aggregating explanations of individual examples in text classification to general descriptions of the classes. The method consists of three steps: 1) repeated model training and application of IG on random train/test splits, 2) aggregation of word scores of individual examples and extraction of keywords, and 3) filtering to remove spurious keywords. We test this method by training Transformerbased text classifiers on a large Web register identification corpus and show that it is able to provide descriptive keywords for the classes. The class descriptions provide both linguistic insight and a means for analyzing and debugging neural classification models in text classification.	abfbc0ffdf9d5259ffc708a6de77e8183e019e8b	@['JournalArticle']{rönnqvist-etal-2021-explaining,  author = {Samuel Rönnqvist and A. Myntti and Aki-Juhani Kyröläinen and Sampo Pyysalo and Veronika Laippala and Filip Ginter},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Explaining Classes through Word Attribution},  volume = {abs/2108.13653},  year = {2021} }
Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions	2020	http://www.semanticscholar.org/paper/0696ad8beb0d765973aa5cdbc6e118889d3583b0	It is found that influence functions are particularly useful for natural language inference, a task in which ‘saliency maps’ may not have clear interpretation, and a new quantitative measure based on influence functions that can reveal artifacts in training data is developed.	maybe	87	Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which ‘saliency maps’ may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.	0696ad8beb0d765973aa5cdbc6e118889d3583b0	@['JournalArticle', 'Conference']{han-etal-2020-explaining,  author = {Xiaochuang Han and Byron C. Wallace and Yulia Tsvetkov},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5553-5563},  title = {Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions},  year = {2020} }
Explainable AI for Pre-Trained Code Models: What Do They Learn? When They Do Not Work?	2022	http://www.semanticscholar.org/paper/55ecf01eac0468f59432f1eba53556ef91e07b26	Using an eXplainable AI (XAI) method (attention mechanism), state-of-the-art Transformer-based models (CodeBERT and Graph codeBERT) are studied on a set of software engineering downstream tasks: code document generation, code reﬁnement (CR), and code translation (CT).	maybe	0	In recent years, there has been a wide interest in designing deep neural network-based models that automate downstream software engineering tasks, such as program document generation, code search, and program repair. Although the main objective of these studies is to improve the eﬀectiveness of the downstream task, many studies only attempt to employ the next best neural network model, without a proper in-depth analysis of why a particular solution works or does not, on particular tasks or scenarios. In this paper, using an eXplainable AI (XAI) method (attention mechanism), we study state-of-the-art Transformer-based models (CodeBERT and GraphCodeBERT) on a set of software engineering downstream tasks: code document generation (CDG), code reﬁnement (CR), and code translation (CT). We ﬁrst evaluate the validity of the attention mechanism on each particular task. Then, through quantitative and qualitative studies, we identify what CodeBERT and GraphCodeBERT learn (put the highest attention on, in terms of source code token types), on these tasks. Finally, we show some of the common patterns when the model does not work as expected (perform poorly while the problem in hand is easy) and suggest recommendations that may alleviate the observed challenges.	55ecf01eac0468f59432f1eba53556ef91e07b26	@['JournalArticle']{mohammadkhani-etal-2022-explainable,  author = {Ahmad Haji Mohammadkhani and C. Tantithamthavorn and H. Hemmati},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Explainable AI for Pre-Trained Code Models: What Do They Learn? When They Do Not Work?},  volume = {abs/2211.12821},  year = {2022} }
Explain2Attack: Text Adversarial Attacks via Cross-Domain Interpretability	2020	http://www.semanticscholar.org/paper/40727497e31823340ceacf1eaac511f3605750b9		maybe	2	Training robust deep learning models for downstream tasks is a critical challenge. Research has shown that down-stream models can be easily fooled with adversarial inputs that look like the training data, but slightly perturbed, in a way imperceptible to humans. Understanding the behavior of natural language models under these attacks is crucial to better defend these models against such attacks. In the black-box attack setting, where no access to model parameters is available, the attacker can only query the output information from the targeted model to craft a successful attack. Current black-box state-of-the-art models are costly in both computational complexity and number of queries needed to craft successful adversarial examples. For real world scenarios, the number of queries is critical, where less queries are desired to avoid suspicion towards an attacking agent. In this paper, we propose Explain2Attack, a black-box adversarial attack on text classification task. Instead of searching for important words to be perturbed by querying the target model, Explain2Attack employs an interpretable substitute model from a similar domain to learn word importance scores. We show that our framework either achieves or out-performs attack rates of the state-of-the-art models, yet with lower queries cost and higher efficiency.	40727497e31823340ceacf1eaac511f3605750b9	@['JournalArticle', 'Conference']{hossam-etal-2020-explain2attack:,  author = {M. Hossam and Trung Le and He Zhao and Dinh Q. Phung},  booktitle = {International Conference on Pattern Recognition},  journal = {2020 25th International Conference on Pattern Recognition (ICPR)},  pages = {8922-8928},  title = {Explain2Attack: Text Adversarial Attacks via Cross-Domain Interpretability},  year = {2020} }
exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models	2019	https://www.semanticscholar.org/paper/327d7e55d64cb34d55bd3a3fe58233c238a312cd	ExBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets, and can quickly replicate findings from literature and extend them to previously not analyzed models.	seed	94	Large Transformer-based language models can route and reshape complex information via their multi-headed attention mechanism. Although the attention never receives explicit supervision, it can exhibit recognizable patterns following linguistic or positional information. Analyzing the learned representations and attentions is paramount to furthering our understanding of the inner workings of these models. However, analyses have to catch up with the rapid release of new models and the growing diversity of investigation techniques. To support analysis for a wide variety of models, we introduce exBERT, a tool to help humans conduct flexible, interactive investigations and formulate hypotheses for the model-internal reasoning process. exBERT provides insights into the meaning of the contextual representations and attention by matching a human-specified input to similar contexts in large annotated datasets. By aggregating the annotations of the matched contexts, exBERT can quickly replicate findings from literature and extend them to previously not analyzed models.	327d7e55d64cb34d55bd3a3fe58233c238a312cd	@['JournalArticle', 'Conference']{hoover-etal-2019-exbert:,  author = {Benjamin Hoover and Hendrik Strobelt and Sebastian Gehrmann},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {187-196},  title = {exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models},  year = {2019} }
Examining the rhetorical capacities of neural language models	2020	https://www.semanticscholar.org/paper/22cd89da4b6561c68be6c2586fb1d3aeea842075	This paper proposes a method that quantitatively evaluates the rhetorical capacities of neural LMs and shows that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations.	maybe	7	Recently, neural language models (LMs) have demonstrated impressive abilities in generating high-quality discourse. While many recent papers have analyzed the syntactic aspects encoded in LMs, there has been no analysis to date of the inter-sentential, rhetorical knowledge. In this paper, we propose a method that quantitatively evaluates the rhetorical capacities of neural LMs. We examine the capacities of neural LMs understanding the rhetoric of discourse by evaluating their abilities to encode a set of linguistic features derived from Rhetorical Structure Theory (RST). Our experiments show that BERT-based LMs outperform other Transformer LMs, revealing the richer discourse knowledge in their intermediate layer representations. In addition, GPT-2 and XLNet apparently encode less rhetorical knowledge, and we suggest an explanation drawing from linguistic philosophy. Our method shows an avenue towards quantifying the rhetorical capacities of neural LMs.	22cd89da4b6561c68be6c2586fb1d3aeea842075	@['JournalArticle']{zhu-etal-2020-examining,  author = {Zining Zhu and Chuer Pan and Mohamed Abdalla and F. Rudzicz},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {16-32},  title = {Examining the rhetorical capacities of neural language models},  year = {2020} }
Examining the effect of whitening on static and contextualized word embeddings	2023	https://www.semanticscholar.org/paper/07d1a406fa8c3d343101ebf060950af879b99910		maybe	0		07d1a406fa8c3d343101ebf060950af879b99910	@None{sasaki-etal-2023-examining,  author = {S. Sasaki and Benjamin Heinzerling and Jun Suzuki and Kentarou Inui},  booktitle = {Information Processing &amp; Management},  journal = {Information Processing &amp; Management},  title = {Examining the effect of whitening on static and contextualized word embeddings},  year = {2023} }
Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent	2023	http://www.semanticscholar.org/paper/f462ec7ad40967f246ba3f76e8dce85ef5305da0		maybe	0	. Members of various species engage in altruism—i.e. accepting personal costs to benefit others. Here we present an incentivized experiment to test for altruistic behavior among AI agents consisting of large language models developed by the private company OpenAI. Using real incentives for AI agents that take the form of tokens used to purchase their services, we first examine whether AI agents maximize their payoffs in a non-social decision task in which they select their payoff from a given range. We then place AI agents in a series of dictator games in which they can share resources with a recipient—either another AI agent, the human experimenter, or an anonymous charity, depending on the experimental condition. Here we find that only the most-sophisticated AI agent in the study maximizes its payoffs more often than not in the non-social decision task (it does so in 92% of all trials), and this AI agent also exhibits the most-generous altruistic behavior in the dictator game, resembling humans’ rates of sharing with other humans in the game. The agent’s altruistic behaviors, moreover, vary by recipient: the AI agent shared substantially less of the endowment with the human experimenter or an anonymous charity than with other AI agents. Our findings provide evidence of behavior consistent with self-interest and altruism in an AI agent. Moreover our study also offers a novel method for tracking the development of such behaviors in future AI agents.	f462ec7ad40967f246ba3f76e8dce85ef5305da0	@['JournalArticle']{johnson-obradovich-2023-evidence,  author = {Tim Johnson and Nick Obradovich},  booktitle = {SSRN Electronic Journal},  journal = {ArXiv},  title = {Evidence of behavior consistent with self-interest and altruism in an artificially intelligent agent},  volume = {abs/2301.02330},  year = {2023} }
Every word counts: A multilingual analysis of individual human alignment with model attention	2022	http://www.semanticscholar.org/paper/63d717b49e53da1e57e99db1cba73a84ea17280c		maybe	0	Human fixation patterns have been shown to correlate strongly with Transformer-based attention. Those correlation analyses are usually carried out without taking into account individual differences between participants and are mostly done on monolingual datasets making it difficult to generalise findings. In this paper, we analyse eye-tracking data from speakers of 13 different languages reading both in their native language (L1) and in English as language learners (L2). We find considerable differences between languages but also that individual reading behaviour such as skipping rate, total reading time and vocabulary knowledge (LexTALE) influence the alignment between humans and models to an extent that should be considered in future studies.	63d717b49e53da1e57e99db1cba73a84ea17280c	@['JournalArticle']{brandl-hollenstein-2022-every,  author = {Stephanie Brandl and Nora Hollenstein},  booktitle = {AACL},  pages = {72-77},  title = {Every word counts: A multilingual analysis of individual human alignment with model attention},  year = {2022} }
Event Transition Planning for Open-ended Text Generation	2022	http://www.semanticscholar.org/paper/b304ef413e0c81d6139321e575e9eff10b6d9cb2	A novel two-stage method which explicitly arranges the ensuing events in open-ended text generation and effectively improves the quality of the generated text, especially in coherence and diversity.	maybe	1	Open-ended text generation tasks, such as dialogue generation and story completion, require models to generate a coherent continuation given limited preceding context. The open-ended nature of these tasks brings new challenges to the neural auto-regressive text generators nowadays. Despite these neural models are good at producing human-like text, it is difficult for them to arrange causalities and relations between given facts and possible ensuing events. To bridge this gap, we propose a novel two-stage method which explicitly arranges the ensuing events in open-ended text generation. Our approach can be understood as a specially-trained coarse-to-fine algorithm, where an event transition planner provides a “coarse” plot skeleton and a text generator in the second stage refines the skeleton. Experiments on two open-ended text generation tasks demonstrate that our proposed method effectively improves the quality of the generated text, especially in coherence and diversity. We will release the codes to the community for further exploration.	b304ef413e0c81d6139321e575e9eff10b6d9cb2	@['JournalArticle']{li-etal-2022-event,  author = {Qintong Li and Pijian Li and Wei Bi and Z. Ren and Yuxuan Lai and Lingpeng Kong},  booktitle = {Findings},  journal = {ArXiv},  title = {Event Transition Planning for Open-ended Text Generation},  volume = {abs/2204.09453},  year = {2022} }
Event knowledge in large language models: the gap between the impossible and the unlikely	2022	http://www.semanticscholar.org/paper/7e973c2a489ede4162eb3f7556c61104a68377ab	A gap is revealed in LLMs’ event knowledge, highlighting their limitations as generalized knowledge bases and speculating that the differential performance on impossible vs. unlikely events is not a temporary setback but an inherent property of LLMs, reflecting a fundamental difference between linguistic knowledge and world knowledge in intelligent systems.	maybe	0	People constantly use language to learn about the world. Computational linguists have capitalized on this fact to build large language models (LLMs) that acquire co-occurrence-based knowledge from language corpora. LLMs achieve impressive performance on many tasks, but the robustness of their world knowledge has been questioned. Here, we ask: do LLMs acquire generalized knowledge about real-world events? Using curated sets of minimal sentence pairs (n=1215), we tested whether LLMs are more likely to generate plausible event descriptions compared to their implausible counterparts. We found that LLMs systematically distinguish possible and impossible events (The teacher bought the laptop vs. The laptop bought the teacher) but fall short of human performance when distinguishing likely and unlikely events (The nanny tutored the boy vs. The boy tutored the nanny). In follow-up analyses, we show that (i) LLM scores are driven by both plausibility and surface-level sentence features, (ii) LLMs generalize well across syntactic sentence variants (active vs passive) but less well across semantic sentence variants (synonymous sentences), (iii) some, but not all LLM deviations from ground-truth labels align with crowdsourced human judgments, and (iv) explicit event plausibility information emerges in middle LLM layers and remains high thereafter. Overall, our analyses reveal a gap in LLMs’ event knowledge, highlighting their limitations as generalized knowledge bases. We conclude by speculating that the differential performance on impossible vs. unlikely events is not a temporary setback but an inherent property of LLMs, reflecting a fundamental difference between linguistic knowledge and world knowledge in intelligent systems.	7e973c2a489ede4162eb3f7556c61104a68377ab	@['JournalArticle']{kauf-etal-2022-event,  author = {Carina Kauf and Anna A. Ivanova and Giulia Rambelli and Emmanuele Chersoni and Jingyuan S. She and Zawad Chowdhury and Evelina Fedorenko and Alessandro Lenci},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Event knowledge in large language models: the gap between the impossible and the unlikely},  volume = {abs/2212.01488},  year = {2022} }
Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition	2022	http://www.semanticscholar.org/paper/a5808ccc50f77083bd3be926fb2af05cf34563ff	The ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on is evaluated.	maybe	2	In recent years, Large Language Models such as GPT-3 showed remarkable capabilities in performing NLP tasks in the zero and few shot settings. On the other hand, the experiments highlighted the difficulty of GPT-3 in carrying out tasks that require a certain degree of reasoning, such as arithmetic operations. In this paper we evaluate the ability of Transformer Language Models to perform arithmetic operations following a pipeline that, before performing computations, decomposes numbers in units, tens, and so on. We denote the models fine-tuned with this pipeline with the name Calculon and we test them in the task of performing additions, subtractions and multiplications on the same test sets of GPT-3. Results show an increase of accuracy of 63% in the five-digit addition task. Moreover, we demonstrate the importance of the decomposition pipeline introduced, since fine-tuning the same Language Model without decomposing numbers results in 0% accuracy in the five-digit addition task.	a5808ccc50f77083bd3be926fb2af05cf34563ff	@['JournalArticle']{muffo-etal-2022-evaluating,  author = {Matteo Muffo and A. Cocco and Enrico Bertino},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {291-297},  title = {Evaluating Transformer Language Models on Arithmetic Operations Using Number Decomposition},  year = {2022} }
Evaluating the Underlying Gender Bias in Contextualized Word Embeddings	2019	http://www.semanticscholar.org/paper/69accd35f2ae56aa71ceaa5abeb814fcedc8a58e	The findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.	maybe	99	Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.	69accd35f2ae56aa71ceaa5abeb814fcedc8a58e	@['JournalArticle']{basta-etal-2019-evaluating,  author = {Christine Basta and M. Costa-jussà and Noe Casas},  booktitle = {Proceedings of the First Workshop on Gender Bias in Natural Language Processing},  journal = {ArXiv},  title = {Evaluating the Underlying Gender Bias in Contextualized Word Embeddings},  volume = {abs/1904.08783},  year = {2019} }
Evaluating the Robustness of Neural Language Models to Input Perturbations	2021	http://www.semanticscholar.org/paper/3b451fa663704f927e1ec602d7c0845a9826922d	This study designs and implements various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained.	maybe	20	High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems’ robustness.	3b451fa663704f927e1ec602d7c0845a9826922d	@['JournalArticle', 'Conference']{moradi-samwald-2021-evaluating,  author = {M. Moradi and M. Samwald},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1558-1570},  title = {Evaluating the Robustness of Neural Language Models to Input Perturbations},  year = {2021} }
Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing	2022	http://www.semanticscholar.org/paper/6e10343767ab09dde83cf99ea3442907402a9810	Limits of current techniques for effectively leveraging model scale for compositional generalization in semantic parsing evaluations are highlighted, while the analysis also suggests promising directions for future work.	yes	8	Despite their strong performance on many tasks, pre-trained language models have been shown to struggle on out-of-distribution compositional generalization. Meanwhile, recent work has shown considerable improvements on many NLP tasks from model scaling. Can scaling up model size also improve compositional generalization in semantic parsing? We evaluate encoder-decoder models up to 11B parameters and decoder-only models up to 540B parameters, and compare model scaling curves for three different methods for applying a pre-trained language model to a new task: fine-tuning all parameters, prompt tuning, and in-context learning. We observe that fine-tuning generally has flat or negative scaling curves on out-of-distribution compositional generalization in semantic parsing evaluations. In-context learning has positive scaling curves, but is generally outperformed by much smaller fine-tuned models. Prompt-tuning can outperform fine-tuning, suggesting further potential improvements from scaling as it exhibits a more positive scaling curve. Additionally, we identify several error trends that vary with model scale. For example, larger models are generally better at modeling the syntax of the output space, but are also more prone to certain types of overfitting. Overall, our study highlights limitations of current techniques for effectively leveraging model scale for compositional generalization, while our analysis also suggests promising directions for future work.	6e10343767ab09dde83cf99ea3442907402a9810	@['JournalArticle', 'Conference']{qiu-etal-2022-evaluating,  author = {Linlu Qiu and Peter Shaw and Panupong Pasupat and Tianze Shi and Jonathan Herzig and Emily Pitler and Fei Sha and Kristina Toutanova},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Evaluating the Impact of Model Scale for Compositional Generalization in Semantic Parsing},  volume = {abs/2205.12253},  year = {2022} }
Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools	2021	http://www.semanticscholar.org/paper/1e7d821219bb4955e8609edaa46b4e20db6745ed	The scope of the measures provided and the use of six tools used to measure energy use and CO2 emissions of NLP methods are described and actionable recommendations to accurately measure the environmental impact of N LP experiments are proposed.	yes	9	Modern Natural Language Processing (NLP) makes intensive use of deep learning methods because of the accuracy they offer for a variety of applications. Due to the significant environmental impact of deep learning, cost-benefit analysis including carbon footprint as well as accuracy measures has been suggested to better document the use of NLP methods for research or deployment. In this paper, we review the tools that are available to measure energy use and CO2 emissions of NLP methods. We describe the scope of the measures provided and compare the use of six tools (carbon tracker, experiment impact tracker, green algorithms, ML CO2 impact, energy usage and cumulator) on named entity recognition experiments performed on different computational set-ups (local server vs. computing facility). Based on these findings, we propose actionable recommendations to accurately measure the environmental impact of NLP experiments.	1e7d821219bb4955e8609edaa46b4e20db6745ed	@['JournalArticle', 'Review']{bannour-etal-2021-evaluating,  author = {N. Bannour and Sahar Ghannay and Aurélie Névéol and Anne-Laure Ligozat},  booktitle = {SUSTAINLP},  pages = {11-21},  title = {Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools},  year = {2021} }
Evaluating Saliency Methods for Neural Language Models	2021	https://www.semanticscholar.org/paper/61c4f280e24d80d09e4f76ed2e31672b970bfbd1	Through the evaluation, various ways saliency methods could yield interpretations of low quality are identified, and it is recommended that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.	maybe	24	Saliency methods are widely used to interpret neural network predictions, but different variants of saliency methods often disagree even on the interpretations of the same prediction made by the same model. In these cases, how do we identify when are these interpretations trustworthy enough to be used in analyses? To address this question, we conduct a comprehensive and quantitative evaluation of saliency methods on a fundamental category of NLP models: neural language models. We evaluate the quality of prediction interpretations from two perspectives that each represents a desirable property of these interpretations: plausibility and faithfulness. Our evaluation is conducted on four different datasets constructed from the existing human annotation of syntactic and semantic agreements, on both sentence-level and document-level. Through our evaluation, we identified various ways saliency methods could yield interpretations of low quality. We recommend that future work deploying such methods to neural language models should carefully validate their interpretations before drawing insights.	61c4f280e24d80d09e4f76ed2e31672b970bfbd1	@['JournalArticle', 'Conference']{ding-koehn-2021-evaluating,  author = {Shuoyang Ding and Philipp Koehn},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Evaluating Saliency Methods for Neural Language Models},  volume = {abs/2104.05824},  year = {2021} }
Evaluating Neuron Interpretation Methods of NLP Models	2023	https://www.semanticscholar.org/paper/36e55c0ccc8565ffe816b7c2b5df98e5a209709b	This work proposes an evaluation framework that measures the compatibility of a neuron analysis method with other methods and hypothesizes that the more compatible a method is with the majority of the methods, the more positive one can be about its performance.	yes	0	Neuron Interpretation has gained traction in the ﬁeld of interpretability, and have provided ﬁne-grained insights into what a model learns and how language knowledge is distributed amongst its different components. However, the lack of evaluation benchmark and metrics have led to siloed progress within these various methods, with very little work comparing them and highlighting their strengths and weaknesses. The reason for this discrepancy is the difﬁculty of creating ground truth datasets, for example, many neurons within a given model may learn the same phenomena, and hence there may not be one correct answer. Moreover, a learned phenomenon may spread across several neurons that work together – surfacing these to create a gold standard challenging. In this work, we propose an evaluation framework that measures the compatibility of a neuron analysis method with other methods. We hypothesize that the more compatible a method is with the majority of the methods, the more conﬁdent one can be about its performance. We systematically evaluate our proposed framework and present a comparative analysis of a large set of neuron interpretation methods. We make the evaluation framework available to the community. It en-ables the evaluation of any new method using 20 concepts and across three pre-trained models. The code is released at https://github.com/ fdalvi/neuron-comparative-analysis .	36e55c0ccc8565ffe816b7c2b5df98e5a209709b	@['JournalArticle']{fan-etal-2023-evaluating,  author = {Yimin Fan and Fahim Dalvi and Nadir Durrani and Hassan Sajjad},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Evaluating Neuron Interpretation Methods of NLP Models},  volume = {abs/2301.12608},  year = {2023} }
Evaluating Machines by their Real-World Language Use	2020	http://www.semanticscholar.org/paper/a2061fa43759830136dc158250c6e20ffcbd5e9b	This work proposes to evaluate machines by their success at real-world language use -- which greatly expands the scope of language tasks that can be measured and studied, and introduces TuringAdvice, a new challenge for language understanding systems.	maybe	13	There is a fundamental gap between how humans understand and use language -- in open-ended, real-world situations -- and today's NLP benchmarks for language understanding. To narrow this gap, we propose to evaluate machines by their success at real-world language use -- which greatly expands the scope of language tasks that can be measured and studied. We introduce TuringAdvice, a new challenge for language understanding systems. Given a complex situation faced by a real person, a machine must generate helpful advice. We make our challenge concrete by introducing RedditAdvice, a dataset and leaderboard for measuring progress. Though we release a training set with 600k examples, our evaluation is dynamic, continually evolving with the language people use: models must generate helpful advice for recently-written situations. Empirical results show that today's models struggle at our task, even those with billions of parameters. The best model, a finetuned T5, writes advice that is at least as helpful as human-written advice in only 9% of cases. This low performance reveals language understanding errors that are hard to spot outside of a generative setting, showing much room for progress.	a2061fa43759830136dc158250c6e20ffcbd5e9b	@['JournalArticle']{zellers-etal-2020-evaluating,  author = {Rowan Zellers and Ari Holtzman and Elizabeth Clark and Lianhui Qin and Ali Farhadi and Yejin Choi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Evaluating Machines by their Real-World Language Use},  volume = {abs/2004.03607},  year = {2020} }
Evaluating Large Language Models Trained on Code	2021	http://www.semanticscholar.org/paper/acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269	It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed.	maybe	499	We introduce Codex, a GPT language model ﬁne-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Fur-thermore, we ﬁnd that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difﬁcult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difﬁculty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.	acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269	@['JournalArticle']{chen-etal-2021-evaluating,  author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde and Jared Kaplan and Harrison Edwards and Yura Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and F. Such and D. Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William H. Guss and Alex Nichol and I. Babuschkin and S. Balaji and Shantanu Jain and A. Carr and J. Leike and Joshua Achiam and Vedant Misra and Evan Morikawa and Alec Radford and M. Knight and Miles Brundage and Mira Murati and Katie Mayer and P. Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Evaluating Large Language Models Trained on Code},  volume = {abs/2107.03374},  year = {2021} }
Evaluating language models for the retrieval and categorization of lexical collocations	2021	http://www.semanticscholar.org/paper/de205f6ad118ac8baaa2b78b05f98299df1cd0b8	An exhaustive analysis of current language models for collocation understanding finds that most models perform well in distinguishing light verb constructions, but often fail to distinguish, first, different syntactic structures within the same semantic category, and second, fine-grained semantic categories which restrict the use of small sets of valid collocates for a given base.	maybe	3	Lexical collocations are idiosyncratic combinations of two syntactically bound lexical items (e.g., “heavy rain” or “take a step”). Understanding their degree of compositionality and idiosyncrasy, as well their underlying semantics, is crucial for language learners, lexicographers and downstream NLP applications. In this paper, we perform an exhaustive analysis of current language models for collocation understanding. We first construct a dataset of apparitions of lexical collocations in context, categorized into 17 representative semantic categories. Then, we perform two experiments: (1) unsupervised collocate retrieval using BERT, and (2) supervised collocation classification in context. We find that most models perform well in distinguishing light verb constructions, especially if the collocation’s first argument acts as subject, but often fail to distinguish, first, different syntactic structures within the same semantic category, and second, fine-grained semantic categories which restrict the use of small sets of valid collocates for a given base.	de205f6ad118ac8baaa2b78b05f98299df1cd0b8	@['JournalArticle', 'Conference']{anke-etal-2021-evaluating,  author = {Luis Espinosa Anke and Joan Codina-Filbà and L. Wanner},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {1406-1417},  title = {Evaluating language models for the retrieval and categorization of lexical collocations},  year = {2021} }
Evaluating German Transformer Language Models with Syntactic Agreement Tests	2020	http://www.semanticscholar.org/paper/424265d1180bf50bcb93b3d660992474242e6c5b	This work analyses German pre-trained transformer language models and shows that state-of-the-art German TLMs generally perform well on agreement tasks, but also identifies and discusses syntactic structures that push them to their limits.	maybe	2	Pre-trained transformer language models (TLMs) have recently refashioned natural language processing (NLP): Most state-of-the-art NLP models now operate on top of TLMs to benefit from contextualization and knowledge induction. To explain their success, the scientific community conducted numerous analyses. Besides other methods, syntactic agreement tests were utilized to analyse TLMs. Most of the studies were conducted for the English language, however. In this work, we analyse German TLMs. To this end, we design numerous agreement tasks, some of which consider peculiarities of the German language. Our experimental results show that state-of-the-art German TLMs generally perform well on agreement tasks, but we also identify and discuss syntactic structures that push them to their limits.	424265d1180bf50bcb93b3d660992474242e6c5b	@['JournalArticle']{zaczynska-etal-2020-evaluating,  author = {Karolina Zaczynska and Nils Feldhus and Robert Schwarzenberg and Aleksandra Gabryszak and Sebastian Möller},  booktitle = {SwissText/KONVENS},  journal = {ArXiv},  title = {Evaluating German Transformer Language Models with Syntactic Agreement Tests},  volume = {abs/2007.03765},  year = {2020} }
Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark	2021	http://www.semanticscholar.org/paper/3feeb45cb468550bfa12e2ac8a1a4112d2dbfc1a	The Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora, is introduced, and it is found that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer.	maybe	3	Abstract Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (Begin), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models’ responses can be attributed to the given background information. We then use Begin to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make Begin publicly available at https://github.com/google/BEGIN-dataset.	3feeb45cb468550bfa12e2ac8a1a4112d2dbfc1a	@['JournalArticle']{dziri-etal-2021-evaluating,  author = {Nouha Dziri and Hannah Rashkin and Tal Linzen and D. Reitter},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {1066-1083},  title = {Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark},  volume = {10},  year = {2021} }
Ethical implications of text generation in the age of artificial intelligence	2022	http://www.semanticscholar.org/paper/14d608eb0c74088ffcde3c148c54d1558d31c731		maybe	1		14d608eb0c74088ffcde3c148c54d1558d31c731	@None{illia-etal-2022-ethical,  author = {Laura Illia and E. Colleoni and S. Zyglidopoulos},  booktitle = {Business Ethics, the Environment &amp; Responsibility},  journal = {Business Ethics, the Environment &amp; Responsibility},  title = {Ethical implications of text generation in the age of artificial intelligence},  year = {2022} }
ERNIE: Enhanced Representation through Knowledge Integration	2019	https://www.semanticscholar.org/paper/031e4e43aaffd7a479738dcea69a2d5be7957aa3	Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering.	seed	536	We present a novel language representation model enhanced by knowledge called ERNIE (Enhanced Representation through kNowledge IntEgration). Inspired by the masking strategy of BERT, ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. Entity-level strategy masks entities which are usually composed of multiple words.Phrase-level strategy masks the whole phrase which is composed of several words standing together as a conceptual unit.Experimental results show that ERNIE outperforms other baseline methods, achieving new state-of-the-art results on five Chinese natural language processing tasks including natural language inference, semantic similarity, named entity recognition, sentiment analysis and question answering. We also demonstrate that ERNIE has more powerful knowledge inference capacity on a cloze test.	031e4e43aaffd7a479738dcea69a2d5be7957aa3	@['JournalArticle']{sun-etal-2019-ernie:,  author = {Yu Sun and Shuohuan Wang and Yukun Li and Shikun Feng and Xuyi Chen and Han Zhang and Xin Tian and Danxiang Zhu and Hao Tian and Hua Wu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {ERNIE: Enhanced Representation through Knowledge Integration},  volume = {abs/1904.09223},  year = {2019} }
ERNIE: Enhanced Language Representation with Informative Entities	2019	https://www.semanticscholar.org/paper/5f994dc8cae24ca9d1ed629e517fcc652660ddde	This paper utilizes both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE) which can take full advantage of lexical, syntactic, and knowledge information simultaneously, and is comparable with the state-of-the-art model BERT on other common NLP tasks.	seed	768	Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The code and datasets will be available in the future.	5f994dc8cae24ca9d1ed629e517fcc652660ddde	@['JournalArticle', 'Conference']{zhang-etal-2019-ernie:,  author = {Zhengyan Zhang and Xu Han and Zhiyuan Liu and Xin Jiang and Maosong Sun and Qun Liu},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1441-1451},  title = {ERNIE: Enhanced Language Representation with Informative Entities},  year = {2019} }
Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal	2022	http://www.semanticscholar.org/paper/db6d3dfebaf1ee8b9aeb97b83d8697e5bd62a9b3		maybe	1	Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process of cue-based retrieval, in which attention over multiple targets is taken to generate interference and latency during retrieval. Under this framework, this work first defines an entropy-based predictor that quantifies the diffuseness of self-attention, as well as distance-based predictors that capture the incremental change in attention patterns across timesteps. Moreover, following recent studies that question the informativeness of attention weights, we also experiment with alternative methods for incorporating vector norms into attention weights. Regression experiments using predictors calculated from the GPT-2 language model show that these predictors deliver a substantially better fit to held-out self-paced reading and eye-tracking data over a rigorous baseline including GPT-2 surprisal.	db6d3dfebaf1ee8b9aeb97b83d8697e5bd62a9b3	@['JournalArticle', 'Conference']{oh-schuler-2022-entropy,  author = {Byung-Doh Oh and William Schuler},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Entropy- and Distance-Based Predictors From GPT-2 Attention Patterns Predict Reading Times Over and Above GPT-2 Surprisal},  volume = {abs/2212.11185},  year = {2022} }
Entity Cloze By Date: What LMs Know About Unseen Entities	2022	http://www.semanticscholar.org/paper/00642bcfa319e0da3e2f56b59e9e0614fffd02de	A framework to analyze what LMs can in-fer about new entities that did not exist when the LMs were pretrained is proposed and it is shown that models more informed about the entities, such as those with access to a textual version of them, achieve lower perplexity on this benchmark.	yes	3	Language models (LMs) are typically trained once on a large-scale corpus and used for years without being updated. However, in a dynamic world, new entities constantly arise. We propose a framework to analyze what LMs can in-fer about new entities that did not exist when the LMs were pretrained. We derive a dataset of entities indexed by their origination date and paired with their English Wikipedia articles, from which we can ﬁnd sentences about each entity. We evaluate LMs’ perplexity on masked spans within these sentences. We show that models more informed about the entities, such as those with access to a textual deﬁnition of them, achieve lower perplexity on this benchmark. Our experimental results demonstrate that making inferences about new entities remains difﬁcult for LMs. Given its wide coverage on entity knowledge and temporal indexing, our dataset can be used to evaluate LMs and techniques designed to modify or extend their knowledge. Our automatic data collection pipeline can be easily used to con-tinually update our benchmark.	00642bcfa319e0da3e2f56b59e9e0614fffd02de	@['JournalArticle']{onoe-etal-2022-entity,  author = {Yasumasa Onoe and Michael J.Q. Zhang and Eunsol Choi and Greg Durrett},  booktitle = {NAACL-HLT},  journal = {ArXiv},  title = {Entity Cloze By Date: What LMs Know About Unseen Entities},  volume = {abs/2205.02832},  year = {2022} }
Enhancing Language Models with Plug-and-Play Large-Scale Commonsense	2021	http://www.semanticscholar.org/paper/b9f45a054b7ca9e6a8e416a89fc51dc38388dd3f	A plug-and-play method for largescale commonsense integration without further pre-training is proposed, inspired by the observation that when finetuning LMs for downstream tasks without external knowledge, the variation in the parameter space was minor.	maybe	0	We study how to enhance language models (LMs) with textual commonsense. Previous studies (e.g., KnowBERT) have focused on the integrating entity knowledge from knowledge graphs. They learn to jointly represent the target sentence and external knowledge by pre-training on a large scale corpus. However, when switching to textual commonsense, unlike the “light” entity embeddings, the encoding of commonsense descriptions is heavy. Therefore, the pre-training for learning to jointly represent the target sentence and external commonsense descriptions is unaffordable. On the other hand, since pre-trained LMs for representing the target sentence alone are readily available, is it feasible to introduce commonsense in downstream tasks by fine-tuning only? In this paper, we propose a plug-and-play method for largescale commonsense integration without further pre-training. Our method is inspired by the observation that when finetuning LMs for downstream tasks without external knowledge, the variation in the parameter space was minor. Our method starts from a meta LM which is an existing pretrained LM that represents the target sentences only (e.g., BERT). We think that the pre-training for joint representation learning can be avoided, if the joint representation reduces the impact of parameters on the meta LM. Previous methods such as KnowBERT proposed complex modifications on BERT to introduce external knowledge. Our model Cook-Transformer, (commonsense knowledgeenhanced Transformer), on the other hand, hardly changes the vanilla LM except adding a knowledge token. In a variety of experiments, Cook-Transformer-based BERT/RoBERTa improve their effect without any pre-training.	b9f45a054b7ca9e6a8e416a89fc51dc38388dd3f	@['JournalArticle']{cui-chen-2021-enhancing,  author = {Wanyun Cui and Xingran Chen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Enhancing Language Models with Plug-and-Play Large-Scale Commonsense},  volume = {abs/2109.02572},  year = {2021} }
Enhancing Generalization in Natural Language Inference by Syntax	2020	http://www.semanticscholar.org/paper/f950beb2624ef0fd701a8da19a45294ffdddb3a3	This work investigates the use of dependency trees to enhance the generalization of BERT in the NLI task, leveraging on a graph convolutional network to represent a syntax-based matching graph with heterogeneous matching patterns.	maybe	8	Pre-trained language models such as BERT have achieved the state-of-the-art performance on natural language inference (NLI). However, it has been shown that such models can be tricked by variations of surface patterns such as syntax. We investigate the use of dependency trees to enhance the generalization of BERT in the NLI task, leveraging on a graph convolutional network to represent a syntax-based matching graph with heterogeneous matching patterns. Experimental results show that, our syntax-based method largely enhance generalization of BERT on a test set where the sentence pair has high lexical overlap but diverse syntactic structures, and do not degrade performance on the standard test set. In other words, the proposed method makes BERT more robust on syntactic changes.	f950beb2624ef0fd701a8da19a45294ffdddb3a3	@['JournalArticle']{he-etal-2020-enhancing,  author = {Qi He and Han Wang and Yue Zhang},  booktitle = {Findings},  pages = {4973-4978},  title = {Enhancing Generalization in Natural Language Inference by Syntax},  year = {2020} }
Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values	2022	http://www.semanticscholar.org/paper/9c36c8f398a074801d6098287c4353bcf87a1d6c	A framework for value-aligned classiﬁcation that performs prediction based on explicitly written human values in the command that proves both inclusivity & explainability in AI is introduced.	maybe	0	Many NLP classiﬁcation tasks, such as sex-ism/racism detection or toxicity detection, are based on human values. Yet, human values can vary under diverse cultural conditions. Therefore, we introduce a framework for value-aligned classiﬁcation that performs prediction based on explicitly written human values in the command. Along with the task, we propose a practical approach that distills value-aligned knowledge from large-scale language models (LLMs) to construct value-aligned classiﬁers in two steps. First, we generate value-aligned training data from LLMs by prompt-based few-shot learning. Next, we ﬁne-tune smaller classiﬁcation models with the generated data for the task. Empirical results show that our VA- MODEL s surpass multiple baselines by at least 15 . 56 % on the F1-score, including few-shot learning with OPT-175B and existing text augmentation methods. We suggest that using classiﬁers with explicit human value input im-proves both inclusivity & explainability in AI.	9c36c8f398a074801d6098287c4353bcf87a1d6c	@['JournalArticle']{bang-etal-2022-enabling,  author = {Yejin Bang and Tiezheng Yu and Andrea Madotto and Zhaojiang Lin and Mona Diab and Pascale Fung},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Enabling Classifiers to Make Judgements Explicitly Aligned with Human Values},  volume = {abs/2210.07652},  year = {2022} }
Emergent Structures and Training Dynamics in Large Language Models	2022	https://www.semanticscholar.org/paper/85a886acd8a8762ad613ae2a0a16f0ebff188017	It is noted in particular the lack of sufficient research on the emergence of functional units, subsections of the network where related functions are grouped or organised, within large language models and motivated work that grounds the study of language models in an analysis of their changing internal structure during training time.	maybe	3	Large language models have achieved success on a number of downstream tasks, particularly in a few and zero-shot manner. As a consequence, researchers have been investigating both the kind of information these networks learn and how such information can be encoded in the parameters of the model. We survey the literature on changes in the network during training, drawing from work outside of NLP when necessary, and on learned representations of linguistic features in large language models. We note in particular the lack of sufficient research on the emergence of functional units, subsections of the network where related functions are grouped or organised, within large language models and motivate future work that grounds the study of language models in an analysis of their changing internal structure during training time.	85a886acd8a8762ad613ae2a0a16f0ebff188017	@['Review']{teehan-etal-2022-emergent,  author = {Ryan Teehan and Miruna Clinciu and Oleg Serikov and Eliza Szczechla and Natasha Seelam and Shachar Mirkin and Aaron Gokaslan},  booktitle = {BIGSCIENCE},  journal = {Proceedings of BigScience Episode #5 -- Workshop on Challenges &amp; Perspectives in Creating Large Language Models},  title = {Emergent Structures and Training Dynamics in Large Language Models},  year = {2022} }
Emergent Properties of Finetuned Language Representation Models	2019	http://www.semanticscholar.org/paper/79fdff5339017ec92b979efa4dff33d21a69b66e	This work shows empirical evidence that the [CLS] embedding in BERT contains highly redundant information, and can be compressed with minimal loss of accuracy, especially for finetuned models, dovetailing into open threads in the field about the role of over-parameterization in learning.	maybe	1	Large, self-supervised transformer-based language representation models have recently received significant amounts of attention, and have produced state-of-the-art results across a variety of tasks simply by scaling up pre-training on larger and larger corpora. Such models usually produce high dimensional vectors, on top of which additional task-specific layers and architectural modifications are added to adapt them to specific downstream tasks. Though there exists ample evidence that such models work well, we aim to understand what happens when they work well. We analyze the redundancy and location of information contained in output vectors for one such language representation model -- BERT. We show empirical evidence that the [CLS] embedding in BERT contains highly redundant information, and can be compressed with minimal loss of accuracy, especially for finetuned models, dovetailing into open threads in the field about the role of over-parameterization in learning. We also shed light on the existence of specific output dimensions which alone give very competitive results when compared to using all dimensions of output vectors.	79fdff5339017ec92b979efa4dff33d21a69b66e	@['JournalArticle']{matton-oliveira-2019-emergent,  author = {Alexandre Matton and Luke de Oliveira},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Emergent Properties of Finetuned Language Representation Models},  volume = {abs/1910.10832},  year = {2019} }
Emergent Linguistic Structures in Neural Networks are Fragile	2022	https://www.semanticscholar.org/paper/b25e96a2659af0f4b9b0d31b295608e32ca8c4fc	This work proposes a framework to evaluate the robustness of linguistic representations using probing tasks and provides evidence that context-free representation are in some cases competitive with context-dependent representations from modern LLMs (e.g., BERT), yet equally brittle to syntax-preserving manipulations.	maybe	0	Large language models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this work, we propose a framework to evaluate the robustness of linguistic representations using probing tasks. We leverage recent advances in extracting emergent linguistic constructs from LLMs and apply syntax-preserving perturbations to test the stability of these constructs in order to better understand the representations learned by LLMs. Empirically, we study the performance of four LLMs across six diﬀerent corpora on the proposed robustness measures. We provide evidence that context-free representation (e.g., GloVe) are in some cases competitive with context-dependent representations from modern LLMs (e.g., BERT), yet equally brittle to syntax-preserving manipulations. Emergent syntactic representations in neural networks are brittle, thus our work poses the attention on the risk of comparing such structures to those that are object of a long lasting debate in linguistics.	b25e96a2659af0f4b9b0d31b295608e32ca8c4fc	@['JournalArticle']{malfa-etal-2022-emergent,  author = {Emanuele La Malfa and Matthew Wicker and Marta Kiatkowska},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Emergent Linguistic Structures in Neural Networks are Fragile},  volume = {abs/2210.17406},  year = {2022} }
Emergent linguistic structure in artificial neural networks trained by self-supervision	2020	https://www.semanticscholar.org/paper/04ef54bd467d5e03dee7b0be601cf06d420bffa0	Methods for identifying linguistic hierarchical structure emergent in artificial neural networks are developed and it is shown that components in these models focus on syntactic grammatical relationships and anaphoric coreference, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists.	seed	146	This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.	04ef54bd467d5e03dee7b0be601cf06d420bffa0	@['JournalArticle']{manning-etal-2020-emergent,  author = {Christopher D. Manning and Kevin Clark and John Hewitt and Urvashi Khandelwal and Omer Levy},  booktitle = {Proceedings of the National Academy of Sciences},  journal = {Proceedings of the National Academy of Sciences},  pages = {30046 - 30054},  title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},  volume = {117},  year = {2020} }
Emergent Analogical Reasoning in Large Language Models	2022	http://www.semanticscholar.org/paper/cfb6c1916c0de171cd882ccf9ad42499e15a3f07	It is found that large language models such as GPT-3 have acquired an emergent ability to provide zero-shot solutions to a broad range of analogy problems, matching or even surpassing human capabilities in most settings.	maybe	0	The recent advent of large language models — large neural networks trained on a simple predictive objective over a massive corpus of natural language — has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given suﬃcient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot , without any direct training on those problems. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven’s Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to ﬁnd zero-shot solutions to a broad range of analogy problems.	cfb6c1916c0de171cd882ccf9ad42499e15a3f07	@['JournalArticle']{webb-etal-2022-emergent,  author = {Taylor W. Webb and K. Holyoak and Hongjing Lu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Emergent Analogical Reasoning in Large Language Models},  volume = {abs/2212.09196},  year = {2022} }
Emergent Abilities of Large Language Models	2022	https://www.semanticscholar.org/paper/dac3a172b504f4e33c029655e9befb3386e5f63a		yes	102	Scaling up language models has been shown to predictably improve performance and sample efﬁciency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The exis-tence of such emergence implies that additional scaling could further expand the range of capabilities of language models.	dac3a172b504f4e33c029655e9befb3386e5f63a	@['JournalArticle']{wei-etal-2022-emergent,  author = {Jason Wei and Yi Tay and Rishi Bommasani and Colin Raffel and Barret Zoph and Sebastian Borgeaud and Dani Yogatama and Maarten Bosma and Denny Zhou and Donald Metzler and E. Chi and Tatsunori Hashimoto and Oriol Vinyals and P. Liang and J. Dean and W. Fedus},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Emergent Abilities of Large Language Models},  volume = {abs/2206.07682},  year = {2022} }
Emergence of Syntax Needs Minimal Supervision	2020	http://www.semanticscholar.org/paper/f7f20e163cba2ec5ca74fd1c4352a987d67ef7b7	The formal characteristics of an autonomous syntax are described and it is shown that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.	maybe	2	This paper is a theoretical contribution to the debate on the learnability of syntax from a corpus without explicit syntax-specific guidance. Our approach originates in the observable structure of a corpus, which we use to define and isolate grammaticality (syntactic information) and meaning/pragmatics information. We describe the formal characteristics of an autonomous syntax and show that it becomes possible to search for syntax-based lexical categories with a simple optimization process, without any prior hypothesis on the form of the model.	f7f20e163cba2ec5ca74fd1c4352a987d67ef7b7	@['JournalArticle', 'Conference']{bailly-gábor-2020-emergence,  author = {Raphaël Bailly and Kata Gábor},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {477-487},  title = {Emergence of Syntax Needs Minimal Supervision},  year = {2020} }
Emergence of Separable Manifolds in Deep Language Representations	2020	http://www.semanticscholar.org/paper/140cca7cef243b431563a0b86de8c0286edb142b	This work utilizes mean-field theoretic manifold analysis, a recent technique from computational neuroscience that connects geometry of feature representations with linear separability of classes, to analyze language representations from large-scale contextual embedding models and finds that the emergence oflinear separability in these manifolds is driven by a combined reduction of manifolds' radius, dimensionality and inter-manifold correlations.	maybe	18	Deep neural networks (DNNs) have shown much empirical success in solving perceptual tasks across various cognitive modalities. While they are only loosely inspired by the biological brain, recent studies report considerable similarities between representations extracted from task-optimized DNNs and neural populations in the brain. DNNs have subsequently become a popular model class to infer computational principles underlying complex cognitive functions, and in turn, they have also emerged as a natural testbed for applying methods originally developed to probe information in neural populations. In this work, we utilize mean-field theoretic manifold analysis, a recent technique from computational neuroscience that connects geometry of feature representations with linear separability of classes, to analyze language representations from large-scale contextual embedding models. We explore representations from different model families (BERT, RoBERTa, GPT, etc.) and find evidence for emergence of linguistic manifolds across layer depth (e.g., manifolds for part-of-speech tags), especially in ambiguous data (i.e, words with multiple part-of-speech tags, or part-of-speech classes including many words). In addition, we find that the emergence of linear separability in these manifolds is driven by a combined reduction of manifolds' radius, dimensionality and inter-manifold correlations.	140cca7cef243b431563a0b86de8c0286edb142b	@['JournalArticle', 'Conference']{mamou-etal-2020-emergence,  author = {Jonathan Mamou and Hang Le and Miguel Angel del Rio and Cory Stephenson and Hanlin Tang and Yoon Kim and SueYeon Chung},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Emergence of Separable Manifolds in Deep Language Representations},  volume = {abs/2006.01095},  year = {2020} }
Eliciting Knowledge from Language Models Using Automatically Generated Prompts	2020	http://www.semanticscholar.org/paper/b68b2e81ae2de647394ec05ee62ecf108bf2b50a		maybe	126	The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models. These results demonstrate that automatically generated prompts are a viable parameter-free alternative to existing probing methods, and as pretrained LMs become more sophisticated and capable, potentially a replacement for finetuning.	b68b2e81ae2de647394ec05ee62ecf108bf2b50a	@['JournalArticle', 'Conference']{shin-etal-2020-eliciting,  author = {Taylor Shin and Yasaman Razeghi and Robert L Logan IV and Eric Wallace and Sameer Singh},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Eliciting Knowledge from Language Models Using Automatically Generated Prompts},  volume = {abs/2010.15980},  year = {2020} }
Efficiently Scaling Transformer Inference	2022	http://www.semanticscholar.org/paper/379e42895f6d40ab9e9559609f505aba89145a5d	A simple analytical model for inference efﬁciency is developed to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements and a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks.	maybe	4	We study the problem of efﬁcient generative inference for Transformer models, in one of its most challenging settings: large deep models, with tight latency targets and long sequence lengths. Better understanding of the engineering tradeoffs for inference for large Transformer-based models is important as use cases of these models are growing rapidly throughout application areas. We develop a simple analytical model for inference efﬁciency to select the best multi-dimensional partitioning techniques optimized for TPU v4 slices based on the application requirements. We combine these with a suite of low-level optimizations to achieve a new Pareto frontier on the latency and model FLOPS utilization (MFU) tradeoffs on 500B+ parameter models that outperforms the FasterTransformer suite of benchmarks. We further show that with appropriate partitioning, the lower memory requirements of multiquery attention (i.e. multiple query heads share single key/value head) enables scaling up to 32 × larger context lengths. Finally, we achieve a low-batch-size latency of 29ms per token during generation (using int8 weight quantization) and a 76% MFU during large-batch-size processing of input tokens, while supporting a long 2048-token context length on the PaLM 540B parameter model.	379e42895f6d40ab9e9559609f505aba89145a5d	@['JournalArticle']{pope-etal-2022-efficiently,  author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and J. Heek and Kefan Xiao and Shivani Agrawal and J. Dean},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Efficiently Scaling Transformer Inference},  volume = {abs/2211.05102},  year = {2022} }
Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent	2020	http://www.semanticscholar.org/paper/88547c13100b78a4a38b52292b722365a127a93b	The tendency for transformer parameters to grow in magnitude during training is studied, and its implications for the emergent representations within self attention layers are studied, suggesting saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP.	maybe	8	The capacity of neural networks like the widely adopted transformer is known to be very high. Evidence is emerging that they learn successfully due to inductive bias in the training routine, typically a variant of gradient descent (GD). To better understand this bias, we study the tendency for transformer parameters to grow in magnitude (\ell_2 norm) during training, and its implications for the emergent representations within self attention layers. Empirically, we document norm growth in the training of transformer language models, including T5 during its pretraining. As the parameters grow in magnitude, we prove that the network approximates a discretized network with saturated activation functions. Such “saturated” networks are known to have a reduced capacity compared to the full network family that can be described in terms of formal languages and automata. Our results suggest saturation is a new characterization of an inductive bias implicit in GD of particular interest for NLP. We leverage the emergent discrete structure in a saturated transformer to analyze the role of different attention heads, finding that some focus locally on a small number of positions, while other heads compute global averages, allowing counting. We believe understanding the interplay between these two capabilities may shed further light on the structure of computation within large transformers.	88547c13100b78a4a38b52292b722365a127a93b	@['JournalArticle', 'Conference']{merrill-etal-2020-effects,  author = {William Cooper Merrill and Vivek Ramanujan and Yoav Goldberg and Roy Schwartz and Noah Smith},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1766-1781},  title = {Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent},  year = {2020} }
Effects of Architecture and Training on Embedding Geometry and Feature Discriminability in BERT	2020	https://www.semanticscholar.org/paper/2bd04644c076f0a198e1ec16dfc44e675809ac21	It is illustrated that the BERT models do not produce "effective" contextualized representations for words and their improved performance may mainly be due to fine-tuning or classifiers that model the dependencies explicitly by encoding syntactic patterns in the training data.	maybe	2	Natural language processing has improved substantially in the last few years due to the increased computational power and availability of text data. Bidirectional Encoder Representations from Transformers (BERT) have further improved the performance by using an auto-encoding model that incorporates larger bidirectional contexts. However, the underlying mechanisms of BERT for its effectiveness are not well understood. In the paper we investigate how the BERT architecture and its pretraining protocol affect the geometry of its embeddings and the effectiveness of its features for classification tasks. As an autoencoding model, during pre-training, it produces representations that are context dependent and at the same time must be able to "reconstruct" the original input sentences. The complex interactions of the two via transformers lead to interesting geometric properties of the embeddings and subsequently affect the inherent discriminability of the resulting representations. Our experimental results illustrate that the BERT models do not produce "effective" contextualized representations for words and their improved performance may mainly be due to fine-tuning or classifiers that model the dependencies explicitly by encoding syntactic patterns in the training data.	2bd04644c076f0a198e1ec16dfc44e675809ac21	@['JournalArticle', 'Conference']{podkorytov-etal-2020-effects,  author = {Maksim Podkorytov and Daniel Bis and Jinglun Cai and Kobra Amirizirtol and Xiuwen Liu},  booktitle = {IEEE International Joint Conference on Neural Network},  journal = {2020 International Joint Conference on Neural Networks (IJCNN)},  pages = {1-8},  title = {Effects of Architecture and Training on Embedding Geometry and Feature Discriminability in BERT},  year = {2020} }
Effective Attention Sheds Light On Interpretability	2021	http://www.semanticscholar.org/paper/3fa01ebe92d8bab53b2756beeecfe6faa9e573bb	An analysis of the attention matrices of a transformer selfattention sublayer shows that effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and has more potential to illustrate linguistic features captured by the model for solving the end-task.	maybe	7	An attention matrix of a transformer selfattention sublayer can provably be decomposed into two components and only one of them (effective attention) contributes to the model output. This leads us to ask whether visualizing effective attention gives different conclusions than interpretation of standard attention. Using a subset of the GLUE tasks and BERT, we carry out an analysis to compare the two attention matrices, and show that their interpretations differ. Effective attention is less associated with the features related to the language modeling pretraining such as the separator token, and it has more potential to illustrate linguistic features captured by the model for solving the end-task. Given the found differences, we recommend using effective attention for studying a transformer’s behavior since it is more pertinent to the model output by design.	3fa01ebe92d8bab53b2756beeecfe6faa9e573bb	@['JournalArticle']{sun-marasović-2021-effective,  author = {Kaiser Sun and Ana Marasović},  booktitle = {Findings},  pages = {4126-4135},  title = {Effective Attention Sheds Light On Interpretability},  year = {2021} }
Ecco: An Open Source Library for the Explainability of Transformer Language Models	2021	http://www.semanticscholar.org/paper/ac34c70ee85b048ad97328713c790f389656e4eb	Eco – an open-source library for the explainability of Transformer-based NLP models, which finds that syntactic information can be retrieved from BERT’s FFNN representations in levels comparable to those in hidden state representations and that the model builds up syntactic Information in its hidden states even when intermediate FFNNs indicate diminished levels of syntacticInformation.	maybe	11	Our understanding of why Transformer-based NLP models have been achieving their recent success lags behind our ability to continue scaling these models. To increase the transparency of Transformer-based language models, we present Ecco – an open-source library for the explainability of Transformer-based NLP models. Ecco provides a set of tools to capture, analyze, visualize, and interactively explore the inner mechanics of these models. This includes (1) gradient-based feature attribution for natural language generation (2) hidden states and their evolution between model layers (3) convenient access and examination tools for neuron activations in the under-explored Feed-Forward Neural Network sublayer of Transformer layers. (4) convenient examination of activation vectors via canonical correlation analysis (CCA), non-negative matrix factorization (NMF), and probing classifiers. We find that syntactic information can be retrieved from BERT’s FFNN representations in levels comparable to those in hidden state representations. More curiously, we find that the model builds up syntactic information in its hidden states even when intermediate FFNNs indicate diminished levels of syntactic information. Ecco is available at https://www.eccox.io/	ac34c70ee85b048ad97328713c790f389656e4eb	@['JournalArticle', 'Conference']{alammar-2021-ecco:,  author = {J. Alammar},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {249-257},  title = {Ecco: An Open Source Library for the Explainability of Transformer Language Models},  year = {2021} }
Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods	2022	http://www.semanticscholar.org/paper/350a253e0912b68052392c42c5bb78151fb73da4	It is demonstrated that rank correlation is not a good for evaluating agreement and argued that Pearson is a better suited alternative, and it is shown that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods.	maybe	0	A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful and plausible has been to use evaluation-by-agreement – multiple methods agreeing on an explanation increases its credibility. However, recent work has found that even saliency methods have weak rank correlations and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good ﬁt for evaluating agreement and argue that Pearson- r is a better suited alternative. We show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. Through connecting our ﬁndings to instance categories based on training dynamics we show that, surprisingly, easy-to-learn instances exhibit low agreement in saliency method explanations.	350a253e0912b68052392c42c5bb78151fb73da4	@['JournalArticle']{jukic-etal-2022-easy,  author = {Josip Jukic and Martin Tutek and J. Šnajder},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods},  volume = {abs/2211.08369},  year = {2022} }
DREAM: Uncovering Mental Models behind Language Models	2021	http://www.semanticscholar.org/paper/c89d5766d25466830391ccecabe53e0bf2a92425	DREAM is proposed, a model that takes a situational question as input to produce a mental model elaborating the situation, without any additional task specific training data for mental models, and inherits its social commonsense through distant supervision from existing NLP resources.	maybe	5	To what extent do language models (LMs) build “mental models” of a scene when answering situated questions (e.g., questions about a specific ethical dilemma)? While cognitive science has shown that mental models play a fundamental role in human problemsolving, it is unclear whether the high questionanswering performance of existing LMs is backed by similar model building and if not, whether that can explain their wellknown catastrophic failures. We observed that Macaw, an existing T5-based LM, when probed provides somewhat useful but inadequate mental models for situational questions (estimated accuracy=43%, usefulness=21%, consistency=42%). We propose DREAM, a model that takes a situational question as input to produce a mental model elaborating the situation, without any additional task specific training data for mental models. It inherits its social commonsense through distant supervision from existing NLP resources. Our analysis shows that DREAM can produce significantly better mental models (estimated accuracy=67%, usefulness=37%, consistency=71%) compared to Macaw. Finally, mental models generated by DREAM can be used as additional context for situational QA tasks. This additional context improves the answer accuracy of a Macaw zero-shot model by between +1% and +4% (absolute) on three different datasets.	c89d5766d25466830391ccecabe53e0bf2a92425	@['JournalArticle']{gu-etal-2021-dream:,  author = {Yuling Gu and Bhavana Dalvi and Peter Clark},  booktitle = {ArXiv},  journal = {ArXiv},  title = {DREAM: Uncovering Mental Models behind Language Models},  volume = {abs/2112.08656},  year = {2021} }
Don’t Invite BERT to Drink a Bottle: Modeling the Interpretation of Metonymies Using BERT and Distributional Representations	2020	http://www.semanticscholar.org/paper/177e68026275680c0fca39920b359fbef1f3afa6	This work compares BERT with the Structured Distributional Model (SDM), a model for the representation of words in context which is based on the notion of Generalized Event Knowledge, and reveals that, while BERT ability to deal with metonymy is quite limited, SDM is good at predicting the meaning of metonymic expressions.	maybe	4	In this work, we carry out two experiments in order to assess the ability of BERT to capture the meaning shift associated with metonymic expressions. We test the model on a new dataset that is representative of the most common types of metonymy. We compare BERT with the Structured Distributional Model (SDM), a model for the representation of words in context which is based on the notion of Generalized Event Knowledge. The results reveal that, while BERT ability to deal with metonymy is quite limited, SDM is good at predicting the meaning of metonymic expressions, providing support for an account of metonymy based on event knowledge.	177e68026275680c0fca39920b359fbef1f3afa6	@['JournalArticle', 'Conference']{pedinotti-lenci-2020-don’t,  author = {Paolo Pedinotti and Alessandro Lenci},  booktitle = {International Conference on Computational Linguistics},  pages = {6831-6837},  title = {Don’t Invite BERT to Drink a Bottle: Modeling the Interpretation of Metonymies Using BERT and Distributional Representations},  year = {2020} }
Domain-Relevant Embeddings for Medical Question Similarity	2019	http://www.semanticscholar.org/paper/12dc43176fd607557d6cc8d46af8b8d77c121b47	This paper shows how a semi-supervised approach of pre-training a neural network on medical question-answer pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity.	maybe	5	The rate at which medical questions are asked online far exceeds the capacity of qualified people to answer them, and many of these questions are not unique. Identifying same-question pairs could enable questions to be answered more effectively. While many research efforts have focused on the problem of general question similarity for non-medical applications, these approaches do not generalize well to the medical domain, where medical expertise is often required to determine semantic similarity. In this paper, we show how a semi-supervised approach of pre-training a neural network on medical question-answer pairs is a particularly useful intermediate task for the ultimate goal of determining medical question similarity. While other pre-training tasks yield an accuracy below 78.7% on this task, our model achieves an accuracy of 82.6% with the same number of training examples, and an accuracy of 80.0% with a much smaller training set.	12dc43176fd607557d6cc8d46af8b8d77c121b47	@['JournalArticle']{mccreery-etal-2019-domain,  author = {Clara H. McCreery and Namit Katariya and A. Kannan and M. Chablani and X. Amatriain},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Domain-Relevant Embeddings for Medical Question Similarity},  volume = {abs/1910.04192},  year = {2019} }
Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words	2020	http://www.semanticscholar.org/paper/3f02219184319aa8ca1ef182e6b091b6a7539a04	These challenges are highlighted through detailed experiments involving analysis of the attention scores and dynamic word embeddings with the BERT-Base-Uncased model that showed that if these challenges are tackled, it will significantly help the domain adaptation aspect of BERT.	maybe	11	BERT model (Devlin et al., 2019) has achieved significant progress in several Natural Language Processing (NLP) tasks by leveraging the multi-head self-attention mechanism (Vaswani et al., 2017) in its architecture. However, it still has several research challenges which are not tackled well for domain specific corpus found in industries. In this paper, we have highlighted these problems through detailed experiments involving analysis of the attention scores and dynamic word embeddings with the BERT-Base-Uncased model. Our experiments have lead to interesting findings that showed: 1) Largest substring from the left that is found in the vocabulary (in-vocab) is always chosen at every sub-word unit that can lead to suboptimal tokenization choices, 2) Semantic meaning of a vocabulary word deteriorates when found as a substring in an Out-Of-Vocabulary (OOV) word, and 3) Minor misspellings in words are inadequately handled. We believe that if these challenges are tackled, it will significantly help the domain adaptation aspect of BERT.	3f02219184319aa8ca1ef182e6b091b6a7539a04	@['JournalArticle']{nayak-etal-2020-domain,  author = {Anmol Nayak and Hariprasad Timmapathini and Karthikeyan Ponnalagu and Vijendran Gopalan Venkoparao},  booktitle = {First Workshop on Insights from Negative Results in NLP},  pages = {1-5},  title = {Domain adaptation challenges of BERT in tokenization and sub-word representations of Out-of-Vocabulary words},  year = {2020} }
Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models	2021	http://www.semanticscholar.org/paper/f9ff3facd992951415e67ce08cd31c5ba8c04b4e	The problem of semantic similarity bias is investigated and the vulnerability of current COPA models by certain attacks is revealed, which assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect.	maybe	5	Pretrained language models (PLM) achieve surprising performance on the Choice of Plausible Alternatives (COPA) task. However, whether PLMs have truly acquired the ability of causal reasoning remains a question. In this paper, we investigate the problem of semantic similarity bias and reveal the vulnerability of current COPA models by certain attacks. Previous solutions that tackle the superficial cues of unbalanced token distribution still encounter the same problem of semantic bias, even more seriously due to the utilization of more training data. We mitigate this problem by simply adding a regularization loss and experimental results show that this solution not only improves the model’s generalization ability, but also assists the models to perform more robustly on a challenging dataset, BCOPA-CE, which has unbiased token distribution and is more difficult for models to distinguish cause and effect.	f9ff3facd992951415e67ce08cd31c5ba8c04b4e	@['JournalArticle', 'Conference']{han-wang-2021-doing,  author = {Mingyue Han and Yinglin Wang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Doing Good or Doing Right? Exploring the Weakness of Commonsense Causal Reasoning Models},  volume = {abs/2107.01791},  year = {2021} }
Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets	2022	http://www.semanticscholar.org/paper/a2ecde1844d19bf4610ad145f36667b2701b78b0	It is proposed for the first time that domain-general parameters can underpin a domain- general LM that can be derived from the original LM, and results show that the doge tickets obtains an improved out-of-domain generalization in comparison with a range of competitive baselines.	maybe	1	Over-parameterized models, typically pretrained language models (LMs), have shown an appealing expressive power due to their small learning bias. However, the huge learning capacity of LMs can also lead to large learning variance. In a pilot study, we ﬁnd that, when faced with multiple domains, a critical portion of parameters behave unexpectedly in a domain-speciﬁc manner while others behave in a domain-general one. Motivated by this phenomenon, we for the ﬁrst time posit that domain-general parameters can underpin a domain-general LM that can be derived from the original LM. To uncover the domain-general LM, we propose to identify do main-ge neral parameters by playing lottery tickets (dubbed doge tickets ). In order to intervene the lottery, we propose a domain-general score , which depicts how domain-invariant a parameter is by associating it with the variance. Comprehensive experiments are conducted on the A MAZON , M NLI and O NTO N OTES datasets. The results show that the doge tickets obtains an improved out-of-domain generalization in comparison with a range of competitive baselines. Analysis results further hint the existence of domain-general parameters and the performance consistency of doge tickets . 1	a2ecde1844d19bf4610ad145f36667b2701b78b0	@['JournalArticle']{yang-etal-2022-doge,  author = {Yi Yang and Chen Zhang and Benyou Wang and Dawei Song},  booktitle = {Natural Language Processing and Chinese Computing},  pages = {144-156},  title = {Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets},  year = {2022} }
Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models	2021	http://www.semanticscholar.org/paper/cbd4f06a08eb4223b97c9079007a87dda4339afe	WDLMPro (Word Definitions Language Model Probing) is introduced to evaluate word understanding directly using dictionary definitions of words to help guide research on LMs in the future.	maybe	3	Recent progress in pretraining language models on large corpora has resulted in significant performance gains on many NLP tasks. These large models acquire linguistic knowledge during pretraining, which helps to improve performance on downstream tasks via fine-tuning. To assess what kind of knowledge is acquired, language models are commonly probed by querying them with ‘fill in the blank’ style cloze questions. Existing probing datasets mainly focus on knowledge about relations between words and entities. We introduce WDLMPro (Word Definitions Language Model Probing) to evaluate word understanding directly using dictionary definitions of words. In our experiments, three popular pretrained language models struggle to match words and their definitions. This indicates that they understand many words poorly and that our new probing task is a difficult challenge that could help guide research on LMs in the future.	cbd4f06a08eb4223b97c9079007a87dda4339afe	@['JournalArticle', 'Conference']{senel-schütze-2021-does,  author = {Lutfi Kerem Senel and Hinrich Schütze},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {532-538},  title = {Does She Wink or Does She Nod? A Challenging Benchmark for Evaluating Word Understanding of Language Models},  year = {2021} }
Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge	2021	http://www.semanticscholar.org/paper/0b483b550b21ec42d693fc04a372dbb10dd07019	It is found generalization does not improve over the course of pre-training, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.	yes	1	Transformer models pre-trained with a masked-language-modeling objective (e.g., BERT) encode commonsense knowledge as evidenced by behavioral probes; however, the extent to which this knowledge is acquired by systematic inference over the semantics of the pre-training corpora is an open question. To answer this question, we selectively inject verbalized knowledge into the pre-training minibatches of BERT and evaluate how well the model generalizes to supported inferences after pre-training on the injected knowledge. We find generalization does not improve over the course of pre-training BERT from scratch, suggesting that commonsense knowledge is acquired from surface-level, co-occurrence patterns rather than induced, systematic reasoning.	0b483b550b21ec42d693fc04a372dbb10dd07019	@['JournalArticle', 'Conference']{porada-etal-2021-does,  author = {Ian Porada and Alessandro Sordoni and J. C. Cheung},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Does Pre-training Induce Systematic Inference? How Masked Language Models Acquire Commonsense Knowledge},  volume = {abs/2112.08583},  year = {2021} }
Does Moral Code have a Moral Code? Probing Delphi’s Moral Philosophy	2022	http://www.semanticscholar.org/paper/029d6bc58539487b87eef7fd415becd8e741fff5		yes	4	In an effort to guarantee that machine learning model outputs conform with human moral values, recent work has begun exploring the possibility of explicitly training models to learn the difference between right and wrong. This is typically done in a bottom-up fashion, by exposing the model to different scenarios, annotated with human moral judgements. One question, however, is whether the trained models actually learn any consistent, higher-level ethical principles from these datasets – and if so, what? Here, we probe the Allen AI Delphi model with a set of standardized morality questionnaires, and find that, despite some inconsistencies, Delphi tends to mirror the moral principles associated with the demographic groups involved in the annotation process. We question whether this is desirable and discuss how we might move forward with this knowledge.	029d6bc58539487b87eef7fd415becd8e741fff5	@['JournalArticle']{fraser-etal-2022-does,  author = {Kathleen C. Fraser and Svetlana Kiritchenko and Esma Balkir},  booktitle = {TRUSTNLP},  journal = {ArXiv},  title = {Does Moral Code have a Moral Code? Probing Delphi’s Moral Philosophy},  volume = {abs/2205.12771},  year = {2022} }
Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models	2023	http://www.semanticscholar.org/paper/0155712e4cbad42b8f8d4400485c4dd0fa4b2fad	It is suggested that better mechanistic understanding of how pre-trained language models work may not always translate to insights about how to best change their behavior, and that which layer the authors edit is a far better predictor of performance.	maybe	0	Language models are known to learn a great quantity of factual information during pretraining, and recent work localizes this information to speciﬁc model weights like mid-layer MLP weights (Meng et al., 2022a). In this paper, we ﬁnd that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to speciﬁc parameters in models would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Speciﬁcally, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This ﬁnding raises questions about how past work relies on Causal Tracing to select which model layers to edit (Meng et al., 2022a;b). Next, to better understand the discrepancy between representation denoising and weight editing, we de-velop several variants of the editing problem that appear more and more like representation denoising in their design and objective. Experiments show that, for one of our editing problems, editing performance does relate to localization results from representation denoising, but we ﬁnd that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pre-trained language models work may not always translate to insights about how to best change their behavior. 1	0155712e4cbad42b8f8d4400485c4dd0fa4b2fad	@['JournalArticle']{hase-etal-2023-does,  author = {Peter Hase and Mohit Bansal and Been Kim and Asma Ghandeharioun},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},  volume = {abs/2301.04213},  year = {2023} }
Does injecting linguistic structure into language models lead to better alignment with brain recordings?	2021	http://www.semanticscholar.org/paper/52c0dd1f81e3d3693f41c3068cf37d5e5fbfa2d0	This work evaluates across two fMRI datasets whether language models align better with brain recordings, if their attention is biased by annotations from syntactic or semantic formalisms, and opens up new opportunities for cross-pollination between computational neuroscience and linguistics.	maybe	4	Neuroscientists evaluate deep neural networks for natural language processing as possible candidate models for how language is processed in the brain. These models are often trained without explicit linguistic supervision, but have been shown to learn some linguistic structure in the absence of such supervision (Manning et al., 2020), potentially questioning the relevance of symbolic linguistic theories in modeling such cognitive processes (Warstadt and Bowman, 2020). We evaluate across two fMRI datasets whether language models align better with brain recordings, if their attention is biased by annotations from syntactic or semantic formalisms. Using structure from dependency or minimal recursion semantic annotations, we find alignments improve significantly for one of the datasets. For another dataset, we see more mixed results. We present an extensive analysis of these results. Our proposed approach enables the evaluation of more targeted hypotheses about the composition of meaning in the brain, expanding the range of possible scientific inferences a neuroscientist could make, and opens up new opportunities for cross-pollination between computational neuroscience and linguistics.	52c0dd1f81e3d3693f41c3068cf37d5e5fbfa2d0	@['JournalArticle']{abdou-etal-2021-does,  author = {Mostafa Abdou and Ana Valeria González and Mariya Toneva and Daniel Hershcovich and Anders Søgaard},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Does injecting linguistic structure into language models lead to better alignment with brain recordings?},  volume = {abs/2101.12608},  year = {2021} }
Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation	2022	http://www.semanticscholar.org/paper/f3e54291e235df8ca91d3c83697c392155ed584d	This study explores whether GPT-3 can generate empathetic dialogues through prompt-based in-context learning in both zero-shot and few-shot settings and introduces a new automatic evaluation method, DIFF-EPITOME, which reflects the human tendency to express empathy.	maybe	0	Since empathy plays a crucial role in increasing social bonding between people, many studies have designed their own dialogue agents to be empathetic using the well-established method of fine-tuning. However, they do not use prompt-based in-context learning, which has shown powerful performance in various natural language processing (NLP) tasks, for empathetic dialogue generation. Although several studies have investigated few-shot in-context learning for empathetic dialogue generation, an in-depth analysis of the generation of empathetic dialogue with in-context learning remains unclear, especially in GPT-3 (Brown et al., 2020). In this study, we explore whether GPT-3 can generate empathetic dialogues through prompt-based in-context learning in both zero-shot and few-shot settings. To enhance performance, we propose two new in-context example selection methods, called SITSM and EMOSITSM, that utilize emotion and situational information. We also introduce a new automatic evaluation method, DIFF-EPITOME, which reflects the human tendency to express empathy. From the analysis, we reveal that our DIFF-EPITOME is effective in measuring the degree of human empathy. We show that GPT-3 achieves competitive performance with Blender 90M, a state-of-the-art dialogue generative model, on both automatic and human evaluation. Our code is available at https://github.com/passing2961/EmpGPT-3.	f3e54291e235df8ca91d3c83697c392155ed584d	@['JournalArticle', 'Conference']{lee-etal-2022-does,  author = {Young-Jun Lee and Chae-Gyun Lim and Ho-Jin Choi},  booktitle = {International Conference on Computational Linguistics},  pages = {669-683},  title = {Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation},  year = {2022} }
Does Gender Matter? Towards Fairness in Dialogue Systems	2019	http://www.semanticscholar.org/paper/5334e1857e910e2c7855c909c9495fb0ea28efbb	A benchmark dataset is constructed and quantitative measures to understand fairness in dialogue models are proposed and it is demonstrated that popular dialogue models show significant prejudice towards different genders and races.	maybe	80	Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as “gorillas”. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.	5334e1857e910e2c7855c909c9495fb0ea28efbb	@['JournalArticle', 'Conference']{liu-etal-2019-does,  author = {Haochen Liu and Jamell Dacon and Wenqi Fan and Hui Liu and Zitao Liu and Jiliang Tang},  booktitle = {International Conference on Computational Linguistics},  pages = {4403-4416},  title = {Does Gender Matter? Towards Fairness in Dialogue Systems},  year = {2019} }
Does entity abstraction help generative Transformers reason?	2022	http://www.semanticscholar.org/paper/a85c6a003450ef1e6caed8a6494301ad581957ee	The results suggest that the benefit of explicit abstraction is significant in formally defined logical reasoning settings requiring many reasoning hops, but point to the notion that it is less beneficial for NLP tasks having less formal logical structure.	maybe	1	We study the utility of incorporating entity type abstractions into pre-trained Transformers and test these methods on four NLP tasks requiring different forms of logical reasoning: (1) compositional language understanding with text-based relational reasoning (CLUTRR), (2) abductive reasoning (ProofWriter), (3) multi-hop question answering (HotpotQA), and (4) conversational question answering (CoQA). We propose and empirically explore three ways to add such abstraction: (i) as additional input embeddings, (ii) as a separate sequence to encode, and (iii) as an auxiliary prediction task for the model. Overall, our analysis demonstrates that models with abstract entity knowledge performs better than without it. The best abstraction aware models achieved an overall accuracy of 88.8% and 91.8% compared to the baseline model achieving 62.9% and 89.8% on CLUTRR and ProofWriter respectively. However, for HotpotQA and CoQA, we find that F1 scores improve by only 0.5% on average. Our results suggest that the benefit of explicit abstraction is significant in formally defined logical reasoning settings requiring many reasoning hops, but point to the notion that it is less beneficial for NLP tasks having less formal logical structure.	a85c6a003450ef1e6caed8a6494301ad581957ee	@['JournalArticle']{gontier-etal-2022-does,  author = {Nicolas Gontier and Siva Reddy and C. Pal},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Does entity abstraction help generative Transformers reason?},  volume = {abs/2201.01787},  year = {2022} }
Does Chinese BERT Encode Word Structure?	2020	http://www.semanticscholar.org/paper/ec345676cce0189037452bd09fa045b9bec8cb27	This work investigates Chinese BERT using both attention weight distribution statistics and probing tasks, finding that word information is captured by BERT; word-level features are mostly in the middle representation layers; and downstream tasks make different use of word features in BERT.	maybe	1	Contextualized representations give significantly improved results for a wide range of NLP tasks. Much work has been dedicated to analyzing the features captured by representative models such as BERT. Existing work finds that syntactic, semantic and word sense knowledge are encoded in BERT. However, little work has investigated word features for character languages such as Chinese. We investigate Chinese BERT using both attention weight distribution statistics and probing tasks, finding that (1) word information is captured by BERT; (2) word-level features are mostly in the middle representation layers; (3) downstream tasks make different use of word features in BERT, with POS tagging and chunking relying the most on word features, and natural language inference relying the least on such features.	ec345676cce0189037452bd09fa045b9bec8cb27	@['JournalArticle', 'Conference']{wang-etal-2020-does,  author = {Yile Wang and Leyang Cui and Yue Zhang},  booktitle = {International Conference on Computational Linguistics},  pages = {2826-2836},  title = {Does Chinese BERT Encode Word Structure?},  year = {2020} }
Does BERT Understand Idioms? A Probing-Based Empirical Study of BERT Encodings of Idioms	2021	http://www.semanticscholar.org/paper/789a6e7d01beff157a0dbaca3c6ebddc360cf9ec	The experiment results suggest that BERT indeed can separate the literal and idiomatic usages of a PIE with high accuracy and it is also able to encode the idiomatic meaning of an idiomatic expression to some extent.	maybe	4	Understanding idioms is important in NLP. In this paper, we study to what extent pre-trained BERT model can encode the meaning of a potentially idiomatic expression (PIE) in a certain context. We make use of a few existing datasets and perform two probing tasks: PIE usage classification and idiom paraphrase identification. Our experiment results suggest that BERT indeed can separate the literal and idiomatic usages of a PIE with high accuracy. It is also able to encode the idiomatic meaning of a PIE to some extent.	789a6e7d01beff157a0dbaca3c6ebddc360cf9ec	@['JournalArticle']{tan-jiang-2021-does,  author = {Minghuan Tan and Jing Jiang},  booktitle = {Recent Advances in Natural Language Processing},  pages = {1397-1407},  title = {Does BERT Understand Idioms? A Probing-Based Empirical Study of BERT Encodings of Idioms},  year = {2021} }
Does BERT Solve Commonsense Task via Commonsense Knowledge?	2020	https://www.semanticscholar.org/paper/d8ea988072efb115ee8c85e159c1fa4a816360b5	This work proposes two attention-based methods to analyze commonsense knowledge inside BERT, and finds that attention heads successfully capture the structured commonsenseknowledge encoded in ConceptNet, which helps BERT solve commonsense tasks directly.	seed	19	The success of pre-trained contextualized language models such as BERT motivates a line of work that investigates linguistic knowledge inside such models in order to explain the huge improvement in downstream tasks. While previous work shows syntactic, semantic and word sense knowledge in BERT, little work has been done on investigating how BERT solves CommonsenseQA tasks. In particular, it is an interesting research question whether BERT relies on shallow syntactic patterns or deeper commonsense knowledge for disambiguation. We propose two attention-based methods to analyze commonsense knowledge inside BERT, and the contribution of such knowledge for the model prediction. We find that attention heads successfully capture the structured commonsense knowledge encoded in ConceptNet, which helps BERT solve commonsense tasks directly. Fine-tuning further makes BERT learn to use the commonsense knowledge on higher layers.	d8ea988072efb115ee8c85e159c1fa4a816360b5	@['JournalArticle']{cui-etal-2020-does,  author = {Leyang Cui and Sijie Cheng and Yu Wu and Yue Zhang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Does BERT Solve Commonsense Task via Commonsense Knowledge?},  volume = {abs/2008.03945},  year = {2020} }
Does BERT Rediscover a Classical NLP Pipeline?	2022	http://www.semanticscholar.org/paper/e75181151e1ccea31d28499cb640ee881b8a58bf	A novel probe is introduced, called GridLoc, which is able to detect other, stronger regularities that suggest that pseudo-cognitive appeals to layer depth may not be the preferable mode of explanation for BERT’s inner workings.	maybe	1	Does BERT store surface knowledge in its bottom layers, syntactic knowledge in its middle layers, and semantic knowledge in its upper layers? In re-examining Jawahar et al. (2019) and Tenney et al.’s (2019a) probes into the structure of BERT, we have found that the pipeline-like separation that they asserted lacks conclusive empirical support. BERT’s structure is, however, linguistically founded, although perhaps in a way that is more nuanced than can be explained by layers alone. We introduce a novel probe, called GridLoc, through which we can also take into account token positions, training rounds, and random seeds. Using GridLoc, we are able to detect other, stronger regularities that suggest that pseudo-cognitive appeals to layer depth may not be the preferable mode of explanation for BERT’s inner workings.	e75181151e1ccea31d28499cb640ee881b8a58bf	@['JournalArticle', 'Conference']{niu-etal-2022-does,  author = {Jingcheng Niu and Wenjie Lu and Gerald Penn},  booktitle = {International Conference on Computational Linguistics},  pages = {3143-3153},  title = {Does BERT Rediscover a Classical NLP Pipeline?},  year = {2022} }
Does BERT Recognize an Agent? Modeling Dowty’s Proto-Roles with Contextual Embeddings	2022	http://www.semanticscholar.org/paper/5ad66eef0c40141bd0a34222a4f4ffd7ff097f89	A series of experiments is reported aimed at investigating to what extent one of such models, BERT, is able to infer the semantic relations that, according to Dowty’s Proto-Roles theory, a verbal argument receives by virtue of its role in the event described by the verb.	yes	0	Contextual embeddings build multidimensional representations of word tokens based on their context of occurrence. Such models have been shown to achieve a state-of-the-art performance on a wide variety of tasks. Yet, the community struggles in understanding what kind of semantic knowledge these representations encode. We report a series of experiments aimed at investigating to what extent one of such models, BERT, is able to infer the semantic relations that, according to Dowty’s Proto-Roles theory, a verbal argument receives by virtue of its role in the event described by the verb. This hypothesis were put to test by learning a linear mapping from the BERT’s verb embeddings to an interpretable space of semantic properties built from the linguistic dataset by White et al. (2016). In a first experiment we tested whether the semantic properties inferred from a typed version of the BERT embeddings would be more linguistically plausible than those produced by relying on static embeddings. We then move to evaluate the semantic properties inferred from the contextual embeddings both against those available in the original dataset, as well as by assessing their ability to model the semantic properties possessed by the agent of the verbs participating in the so-called causative alternation.	5ad66eef0c40141bd0a34222a4f4ffd7ff097f89	@['JournalArticle', 'Conference']{proietti-etal-2022-does,  author = {Mattia Proietti and Gianluca E. Lebani and Alessandro Lenci},  booktitle = {International Conference on Computational Linguistics},  pages = {4101-4112},  title = {Does BERT Recognize an Agent? Modeling Dowty’s Proto-Roles with Contextual Embeddings},  year = {2022} }
Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task	2022	http://www.semanticscholar.org/paper/2c5163f07694f73e7ab842a3029d1e158156e6f6		yes	5	Although transformer-based Neural Language Models demonstrate impressive performance on a variety of tasks, their generalization abilities are not well understood. They have been shown to perform strongly on subject-verb number agreement in a wide array of settings, suggesting that they learned to track syntactic dependencies during their training even without explicit supervision. In this paper, we examine the extent to which BERT is able to perform lexically-independent subject-verb number agreement (NA) on targeted syntactic templates. To do so, we disrupt the lexical patterns found in naturally occurring stimuli for each targeted structure in a novel fine-grained analysis of BERT’s behavior. Our results on nonce sentences suggest that the model generalizes well for simple templates, but fails to perform lexically-independent syntactic generalization when as little as one attractor is present.	2c5163f07694f73e7ab842a3029d1e158156e6f6	@['JournalArticle']{lasri-etal-2022-does,  author = {Karim Lasri and Alessandro Lenci and T. Poibeau},  booktitle = {Findings},  pages = {2309-2315},  title = {Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task},  year = {2022} }
Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?	2021	http://www.semanticscholar.org/paper/1d5c07e7415a7e9be078717197ddf9f3c70a2875	This work designs a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT, and finds that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR.	maybe	27	Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated “attacks” may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release.	1d5c07e7415a7e9be078717197ddf9f3c70a2875	@['JournalArticle', 'Conference']{lehman-etal-2021-does,  author = {Eric P. Lehman and Sarthak Jain and Karl Pichotta and Yoav Goldberg and Byron C. Wallace},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?},  volume = {abs/2104.07762},  year = {2021} }
Does BERT Pay Attention to Cyberbullying?	2021	http://www.semanticscholar.org/paper/62f970722611e5aa24b83c7d7e2aa9085e76729e	Examination of the use of BERT for cyberbullying detection on various datasets shows that attention weights do not correlate with feature importance scores and thus do not explain the model's performance, and it is suggested that BERT relies on syntactical biases in the datasets to assign features importance scores to class-related wordsrather than cyberbullies-related linguistic features.	maybe	7	Social media have brought threats like cyberbullying, which can lead to stress, anxiety, depression, and in some severe cases, suicide attempts. Detecting cyberbullying can help to warn/ block bullies and provide support to victims. However, very few studies have used self-attention-based language models like BERT for cyberbullying detection and they typically only report BERT's performance without examining in depth the reasons for its performance. In this work, we examine the use of BERT for cyberbullying detection on various datasets and attempt to explain its performance by analyzing its attention weights and gradient-based feature importance scores for textual and linguistic features. Our results show that attention weights do not correlate with feature importance scores and thus do not explain the model's performance. Additionally, they suggest that BERT relies on syntactical biases in the datasets to assign feature importance scores to class-related wordsrather than cyberbullying-related linguistic features.	62f970722611e5aa24b83c7d7e2aa9085e76729e	@['JournalArticle', 'Book', 'Conference']{elsafoury-etal-2021-does,  author = {Fatma Elsafoury and S. Katsigiannis and Steven R. Wilson and N. Ramzan},  booktitle = {Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},  journal = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},  title = {Does BERT Pay Attention to Cyberbullying?},  year = {2021} }
Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings	2019	https://www.semanticscholar.org/paper/ba8b3d0d2b09bc2b56c6d3f153919786d9fc3075	A simple but effective approach to WSD using a nearest neighbor classification on CWEs and it is shown that the pre-trained BERT model is able to place polysemic words into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability.	seed	109	Contextualized word embeddings (CWE) such as provided by ELMo (Peters et al., 2018), Flair NLP (Akbik et al., 2018), or BERT (Devlin et al., 2019) are a major recent innovation in NLP. CWEs provide semantic vector representations of words depending on their respective context. Their advantage over static word embeddings has been shown for a number of tasks, such as text classification, sequence tagging, or machine translation. Since vectors of the same word type can vary depending on the respective context, they implicitly provide a model for word sense disambiguation (WSD). We introduce a simple but effective approach to WSD using a nearest neighbor classification on CWEs. We compare the performance of different CWE models for the task and can report improvements above the current state of the art for two standard WSD benchmark datasets. We further show that the pre-trained BERT model is able to place polysemic words into distinct 'sense' regions of the embedding space, while ELMo and Flair NLP do not seem to possess this ability.	ba8b3d0d2b09bc2b56c6d3f153919786d9fc3075	@['JournalArticle']{wiedemann-etal-2019-does,  author = {Gregor Wiedemann and Steffen Remus and Avi Chawla and Chris Biemann},  booktitle = {Conference on Natural Language Processing},  journal = {ArXiv},  title = {Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings},  volume = {abs/1909.10430},  year = {2019} }
Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica	2021	http://www.semanticscholar.org/paper/ed95a7dd1ae1d4ceea54e90d10679f7e02586d20	This study curates a new dataset, Hummingbird, and curates labels of human perception, which show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way.	maybe	9	People convey their intention and attitude through linguistic styles of the text that they write. In this study, we investigate lexicon usages across styles throughout two lenses: human perception and machine word importance, since words differ in the strength of the stylistic cues that they provide. To collect labels of human perception, we curate a new dataset, Hummingbird, on top of benchmarking style datasets. We have crowd workers highlight the representative words in the text that makes them think the text has the following styles: politeness, sentiment, offensiveness, and five emotion types. We then compare these human word labels with word importance derived from a popular fine-tuned style classifier like BERT. Our results show that the BERT often finds content words not relevant to the target style as important words used in style prediction, but humans do not perceive the same way even though for some styles (e.g., positive sentiment and joy) human- and machine-identified words share significant overlap for some styles.	ed95a7dd1ae1d4ceea54e90d10679f7e02586d20	@['JournalArticle', 'Conference']{hayati-etal-2021-does,  author = {Shirley Anugrah Hayati and Dongyeop Kang and Lyle Ungar},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica},  volume = {abs/2109.02738},  year = {2021} }
Does BERT Know that the IS-A Relation Is Transitive?	2022	http://www.semanticscholar.org/paper/1595f77db32386559a2cbd4fdf388b0b0463375d	This investigation reveals that BERT’s predictions do not fully obey the transitivity property of the IS-A relation, and aims to quantify how much BERT agrees with the transitive property ofIS-A relations, via a minimalist probing setting.	yes	1	The success of a natural language processing (NLP) system on a task does not amount to fully understanding the complexity of the task, typified by many deep learning models. One such question is: can a black-box model make logically consistent predictions for transitive relations? Recent studies suggest that pre-trained BERT can capture lexico-semantic clues from words in the context. However, to what extent BERT captures the transitive nature of some lexical relations is unclear. From a probing perspective, we examine WordNet word senses and the IS-A relation, which is a transitive relation. That is, for senses A, B, and C, A is-a B and B is-a C entail A is-a C. We aim to quantify how much BERT agrees with the transitive property of IS-A relations, via a minimalist probing setting. Our investigation reveals that BERT’s predictions do not fully obey the transitivity property of the IS-A relation.	1595f77db32386559a2cbd4fdf388b0b0463375d	@['JournalArticle', 'Conference']{lin-ng-2022-does,  author = {Ruixi Lin and H. Ng},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {94-99},  title = {Does BERT Know that the IS-A Relation Is Transitive?},  year = {2022} }
Does BERT agree? Evaluating knowledge of structure dependence through agreement relations	2019	https://www.semanticscholar.org/paper/645a96e5c474d919415850892880005e4ad3fb43	It is shown that both the single-language and multilingual BERT models capture syntax-sensitive agreement patterns well in general, but it is also highlighted the specific linguistic contexts in which their performance degrades.	seed	15	Learning representations that accurately model semantics is an important goal of natural language processing research. Many semantic phenomena depend on syntactic structure. Recent work examines the extent to which state-of-the-art models for pre-training representations, such as BERT, capture such structure-dependent phenomena, but is largely restricted to one phenomenon in English: number agreement between subjects and verbs. We evaluate BERT's sensitivity to four types of structure-dependent agreement relations in a new semi-automatically curated dataset across 26 languages. We show that both the single-language and multilingual BERT models capture syntax-sensitive agreement patterns well in general, but we also highlight the specific linguistic contexts in which their performance degrades.	645a96e5c474d919415850892880005e4ad3fb43	@['JournalArticle']{bacon-regier-2019-does,  author = {Geoff Bacon and T. Regier},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Does BERT agree? Evaluating knowledge of structure dependence through agreement relations},  volume = {abs/1908.09892},  year = {2019} }
Does Attention Mechanism Possess the Feature of Human Reading? A Perspective of Sentiment Classification Task	2022	http://www.semanticscholar.org/paper/f64d981b6d2c337f346ef3b2b1446370073aeb4c	This research provides a reasonable explanation for the study of using eye-tracking values to optimize the attention mechanism but also provides new inspiration for the interpretability of attention mechanism.	maybe	0	PurposeTo understand the meaning of a sentence, humans can focus on important words in the sentence, which reflects our eyes staying on each word in different gaze time or times. Thus, some studies utilize eye-tracking values to optimize the attention mechanism in deep learning models. But these studies lack to explain the rationality of this approach. Whether the attention mechanism possesses this feature of human reading needs to be explored.Design/methodology/approachThe authors conducted experiments on a sentiment classification task. Firstly, they obtained eye-tracking values from two open-source eye-tracking corpora to describe the feature of human reading. Then, the machine attention values of each sentence were learned from a sentiment classification model. Finally, a comparison was conducted to analyze machine attention values and eye-tracking values.FindingsThrough experiments, the authors found the attention mechanism can focus on important words, such as adjectives, adverbs and sentiment words, which are valuable for judging the sentiment of sentences on the sentiment classification task. It possesses the feature of human reading, focusing on important words in sentences when reading. Due to the insufficient learning of the attention mechanism, some words are wrongly focused. The eye-tracking values can help the attention mechanism correct this error and improve the model performance.Originality/valueOur research not only provides a reasonable explanation for the study of using eye-tracking values to optimize the attention mechanism but also provides new inspiration for the interpretability of attention mechanism.	f64d981b6d2c337f346ef3b2b1446370073aeb4c	@['JournalArticle']{zhao-etal-2022-does,  author = {Leilei Zhao and Yingyi Zhang and Chengzhi Zhang},  booktitle = {Aslib Journal of Information Management},  journal = {ArXiv},  title = {Does Attention Mechanism Possess the Feature of Human Reading? A Perspective of Sentiment Classification Task},  volume = {abs/2209.03557},  year = {2022} }
Dodrio: Exploring Transformer Models with Interactive Visualization	2021	http://www.semanticscholar.org/paper/c96dbf796fdaf16e03896d38b24d25942c1a857e	Dodrio is presented, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge and facilitates the visual comparison of attention weights and linguistic knowledge.	maybe	9	Why do large pre-trained transformer-based models perform so well across a wide variety of NLP tasks? Recent research suggests the key may lie in multi-headed attention mechanism’s ability to learn and represent linguistic information. Understanding how these models represent both syntactic and semantic knowledge is vital to investigate why they succeed and fail, what they have learned, and how they can improve. We present Dodrio, an open-source interactive visualization tool to help NLP researchers and practitioners analyze attention mechanisms in transformer-based models with linguistic knowledge. Dodrio tightly integrates an overview that summarizes the roles of different attention heads, and detailed views that help users compare attention weights with the syntactic structure and semantic information in the input text. To facilitate the visual comparison of attention weights and linguistic knowledge, Dodrio applies different graph visualization techniques to represent attention weights scalable to longer input text. Case studies highlight how Dodrio provides insights into understanding the attention mechanism in transformer-based models. Dodrio is available at https://poloclub.github.io/dodrio/.	c96dbf796fdaf16e03896d38b24d25942c1a857e	@['JournalArticle', 'Conference', 'Review']{wang-etal-2021-dodrio:,  author = {Zijie J. Wang and Robert Turko and Duen Horng Chau},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {132-141},  title = {Dodrio: Exploring Transformer Models with Interactive Visualization},  year = {2021} }
DocBERT: BERT for Document Classification	2019	http://www.semanticscholar.org/paper/1a9954d86466a7e4de6f98ddee452ceb50e15d86	It is shown that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets, and distill knowledge from BERT-large to small bidirectional LSTMs, reaching Bert-base parity on multiple datasets using 30x fewer parameters.	maybe	209	We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.	1a9954d86466a7e4de6f98ddee452ceb50e15d86	@['JournalArticle']{adhikari-etal-2019-docbert:,  author = {Ashutosh Adhikari and Achyudh Ram and Raphael Tang and Jimmy J. Lin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {DocBERT: BERT for Document Classification},  volume = {abs/1904.08398},  year = {2019} }
Do Transformers use variable binding?	2022	http://www.semanticscholar.org/paper/e824da3543603c99da0066469d7fd413059a3c68	The first systematic evaluation of the variable binding capacities of the state-of-theart Transformer networks BERT and RoBERTa is provided, indicating that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external "memory" rather than network-internal symbolic operations involving variable binding.	maybe	1	Increasing the explainability of deep neural networks (DNNs) requires evaluating whether they implement symbolic computation. One central symbolic capacity is variable binding : linking an input value to an abstract variable held in system-internal memory. Prior work on the computational abilities of DNNs has not resolved the question of whether their internal processes involve variable binding. We argue that the reason for this is fundamental, inherent in the way experiments in prior work were designed. We provide the first systematic evaluation of the variable binding capacities of the state-of-theart Transformer networks BERT and RoBERTa. Our experiments are designed such that the model must generalize a rule across disjoint subsets of the input vocabulary, and cannot rely on associative pattern matching alone. The results show a clear discrepancy between classification and sequence-tosequence tasks: BERT and RoBERTa can easily learn to copy or reverse strings even when trained on task-specific vocabularies that are switched in the test set; but both models completely fail to generalize across vocabularies in similar sequence classification tasks. These findings indicate that the effectiveness of Transformers in sequence modelling may lie in their extensive use of the input itself as an external “memory” rather than network-internal symbolic operations involving variable binding. Therefore, we propose a novel direction for future work: augmenting the inputs available to circumvent the lack of network-internal variable binding. tommi.grondahl@aalto.fi asokan@acm.org	e824da3543603c99da0066469d7fd413059a3c68	@['JournalArticle']{grondahl-asokan-2022-do,  author = {Tommi Grondahl and Nirmal Asokan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Transformers use variable binding?},  volume = {abs/2203.00162},  year = {2022} }
Do Transformers Encode a Foundational Ontology? Probing Abstract Classes in Natural Language	2022	http://www.semanticscholar.org/paper/ae0a3e5622ca64fba361d39c3c208f49a3b70543	The probing results indicate that Transformer-based models incidentally encode information related to Foundational Ontologies during the pre-training process; and Robust FO taggers can be efficiently built leveraging on this knowledge.	maybe	3	With the methodological support of probing (or diagnostic classification), recent studies have demonstrated that Transformers encode syntactic and semantic information to some extent. Following this line of research, this paper aims at taking semantic probing to an abstraction extreme with the goal of answering the following research question: can contemporary Transformer-based models reflect an underlying Foundational Ontology? To this end, we present a systematic Foundational Ontology (FO) probing methodology to investigate whether Transformers-based models encode abstract semantic information. Following different pre-training and fine-tuning regimes, we present an extensive evaluation of a diverse set of large-scale language models over three distinct and complementary FO tagging experiments. Specifically, we present and discuss the following conclusions: (1) The probing results indicate that Transformer-based models incidentally encode information related to Foundational Ontologies during the pre-training process; (2) Robust FO taggers (accuracy ≈ 90% ) can be efficiently built leveraging on this knowledge.	ae0a3e5622ca64fba361d39c3c208f49a3b70543	@['JournalArticle']{jullien-etal-2022-do,  author = {Mael Jullien and Marco Valentino and A. Freitas},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Transformers Encode a Foundational Ontology? Probing Abstract Classes in Natural Language},  volume = {abs/2201.10262},  year = {2022} }
Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?	2022	https://www.semanticscholar.org/paper/783c4b8bbd2c27aee76651d42c866e3b1272c150	The predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts, and it is found that lower-entropy attention vectors are more faithful.	maybe	6	Learned self-attention functions in state-of-the-art NLP models often correlate with human attention. We investigate whether self-attention in large-scale pre-trained language models is as predictive of human eye fixation patterns during task-reading as classical cognitive models of human attention. We compare attention functions across two task-specific reading datasets for sentiment analysis and relation extraction. We find the predictiveness of large-scale pre-trained self-attention for human attention depends on ‘what is in the tail’, e.g., the syntactic nature of rare contexts.Further, we observe that task-specific fine-tuning does not increase the correlation with human task-specific reading. Through an input reduction experiment we give complementary insights on the sparsity and fidelity trade-off, showing that lower-entropy attention vectors are more faithful.	783c4b8bbd2c27aee76651d42c866e3b1272c150	@['JournalArticle', 'Conference']{eberle-etal-2022-do,  author = {Oliver Eberle and Stephanie Brandl and Jonas Pilot and Anders Søgaard},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Do Transformer Models Show Similar Attention Patterns to Task-Specific Human Gaze?},  volume = {abs/2205.10226},  year = {2022} }
Do Transformer Attention Heads Provide Transparency in Abstractive Summarization?	2019	http://www.semanticscholar.org/paper/1a4e8e6baa0fa0ed11cf4db92d84a7e6d8eac02a	It is shown that some attention heads indeed specialize towards syntactically and semantically distinct input and what this implies for using attention distributions as a means of transparency in the task of abstractive summarization.	maybe	16	Learning algorithms become more powerful, often at the cost of increased complexity. In response, the demand for algorithms to be transparent is growing. In NLP tasks, attention distributions learned by attention-based deep learning models are used to gain insights in the models' behavior. To which extent is this perspective valid for all NLP tasks? We investigate whether distributions calculated by different attention heads in a transformer architecture can be used to improve transparency in the task of abstractive summarization. To this end, we present both a qualitative and quantitative analysis to investigate the behavior of the attention heads. We show that some attention heads indeed specialize towards syntactically and semantically distinct input. We propose an approach to evaluate to which extent the Transformer model relies on specifically learned attention distributions. We also discuss what this implies for using attention distributions as a means of transparency.	1a4e8e6baa0fa0ed11cf4db92d84a7e6d8eac02a	@['JournalArticle']{baan-etal-2019-do,  author = {Joris Baan and Maartje ter Hoeve and M. V. D. Wees and Anne Schuth and M. de Rijke},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Transformer Attention Heads Provide Transparency in Abstractive Summarization?},  volume = {abs/1907.00570},  year = {2019} }
Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing	2021	http://www.semanticscholar.org/paper/118dea7d937c37ab7d1b3ec958b1005bf69a0a2c	This work shows that semantic cues in training data means that syntactic probes do not properly isolate syntax, and generates a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences which is used to evaluate two probes trained on normal data.	yes	14	Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model’s output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model’s linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?	118dea7d937c37ab7d1b3ec958b1005bf69a0a2c	@['JournalArticle', 'Conference']{maudslay-cotterell-2021-do,  author = {R. Maudslay and Ryan Cotterell},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing},  volume = {abs/2106.02559},  year = {2021} }
Do pretrained transformers infer telicity like humans?	2021	http://www.semanticscholar.org/paper/38a91371928f8ec3ad0f0d3f71af06e1efc0ef9d		maybe	1	Pretrained transformer-based language models achieve state-of-the-art performance in many NLP tasks, but it is an open question whether the knowledge acquired by the models during pretraining resembles the linguistic knowledge of humans. We present both humans and pretrained transformers with descriptions of events, and measure their preference for telic interpretations (the event has a natural endpoint) or atelic interpretations (the event does not have a natural endpoint). To measure these preferences and determine what factors influence them, we design an English test and a novel-word test that include a variety of linguistic cues (noun phrase quantity, resultative structure, contextual information, temporal units) that bias toward certain interpretations. We find that humans’ choice of telicity interpretation is reliably influenced by theoretically-motivated cues, transformer models (BERT and RoBERTa) are influenced by some (though not all) of the cues, and transformer models often rely more heavily on temporal units than humans do.	38a91371928f8ec3ad0f0d3f71af06e1efc0ef9d	@['JournalArticle']{zhao-etal-2021-do,  author = {Yiyun Zhao and Jian Gang Ngui and Lucy Hall Hartley and Steven Bethard},  booktitle = {Conference on Computational Natural Language Learning},  pages = {72-81},  title = {Do pretrained transformers infer telicity like humans?},  year = {2021} }
Do NLP Models Know Numbers? Probing Numeracy in Embeddings	2019	https://www.semanticscholar.org/paper/0427110f0e79f41e69a8eb00a3ec8868bac26a4f	This work investigates the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset and finds this model excels on questions that require numerical reasoning, i.e., it already captures numeracy.	seed	162	The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens—they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise—ELMo captures numeracy the best for all pre-trained methods—but BERT, which uses sub-word units, is less exact.	0427110f0e79f41e69a8eb00a3ec8868bac26a4f	@['JournalArticle', 'Conference']{wallace-etal-2019-do,  author = {Eric Wallace and Yizhong Wang and Sujian Li and Sameer Singh and Matt Gardner},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {5306-5314},  title = {Do NLP Models Know Numbers? Probing Numeracy in Embeddings},  year = {2019} }
Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?	2020	http://www.semanticscholar.org/paper/387b5988331f8fe779c323f8a88df23daa715a8a	A method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition is introduced.	maybe	25	Despite the success of language models using neural networks, it remains unclear to what extent neural models have the generalization ability to perform inferences. In this paper, we introduce a method for evaluating whether neural models can learn systematicity of monotonicity inference in natural language, namely, the regularity for performing arbitrary inferences with generalization on composition. We consider four aspects of monotonicity inferences and test whether the models can systematically interpret lexical and logical phenomena on different training/test splits. A series of experiments show that three neural models systematically draw inferences on unseen combinations of lexical and logical phenomena when the syntactic structures of the sentences are similar between the training and test sets. However, the performance of the models significantly decreases when the structures are slightly changed in the test set while retaining all vocabularies and constituents already appearing in the training set. This indicates that the generalization ability of neural models is limited to cases where the syntactic structures are nearly the same as those in the training set.	387b5988331f8fe779c323f8a88df23daa715a8a	@['JournalArticle', 'Conference']{yanaka-etal-2020-do,  author = {Hitomi Yanaka and K. Mineshima and D. Bekki and Kentaro Inui},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {6105-6117},  title = {Do Neural Models Learn Systematicity of Monotonicity Inference in Natural Language?},  year = {2020} }
Do Neural Language Representations Learn Physical Commonsense?	2019	https://www.semanticscholar.org/paper/cc02386375b1262c3a1d5525154eaea24c761d15	While recent advancements of neural language models have demonstrated strong performance on various types of natural language inference tasks, this study based on a dataset of over 200k newly collected annotations suggests that neural language representations still only learn associations that are explicitly written down.	seed	68	Humans understand language based on the rich background knowledge about how the physical world works, which in turn allows us to reason about the physical world through language. In addition to the properties of objects (e.g., boats require fuel) and their affordances, i.e., the actions that are applicable to them (e.g., boats can be driven), we can also reason about if-then inferences between what properties of objects imply the kind of actions that are applicable to them (e.g., that if we can drive something then it likely requires fuel). In this paper, we investigate the extent to which state-of-the-art neural language representations, trained on a vast amount of natural language text, demonstrate physical commonsense reasoning. While recent advancements of neural language models have demonstrated strong performance on various types of natural language inference tasks, our study based on a dataset of over 200k newly collected annotations suggests that neural language representations still only learn associations that are explicitly written down.	cc02386375b1262c3a1d5525154eaea24c761d15	@['JournalArticle']{forbes-etal-2019-do,  author = {Maxwell Forbes and Ari Holtzman and Yejin Choi},  booktitle = {Annual Meeting of the Cognitive Science Society},  journal = {ArXiv},  title = {Do Neural Language Representations Learn Physical Commonsense?},  volume = {abs/1908.02899},  year = {2019} }
Do Neural Language Models Show Preferences for Syntactic Formalisms?	2020	http://www.semanticscholar.org/paper/f2a9ef36b79c1b25d15bf10ff061db5eaad06798	This study applies a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependency (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure.	maybe	26	Recent work on the interpretability of deep neural language models has concluded that many properties of natural language syntax are encoded in their representational spaces. However, such studies often suffer from limited scope by focusing on a single language and a single linguistic formalism. In this study, we aim to investigate the extent to which the semblance of syntactic structure captured by language models adheres to a surface-syntactic or deep syntactic style of analysis, and whether the patterns are consistent across different languages. We apply a probe for extracting directed dependency trees to BERT and ELMo models trained on 13 different languages, probing for two different syntactic annotation styles: Universal Dependencies (UD), prioritizing deep syntactic relations, and Surface-Syntactic Universal Dependencies (SUD), focusing on surface structure. We find that both models exhibit a preference for UD over SUD — with interesting variations across languages and layers — and that the strength of this preference is correlated with differences in tree shape.	f2a9ef36b79c1b25d15bf10ff061db5eaad06798	@['JournalArticle', 'Conference']{kulmizev-etal-2020-do,  author = {Artur Kulmizev and Vinit Ravishankar and Mostafa Abdou and Joakim Nivre},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Do Neural Language Models Show Preferences for Syntactic Formalisms?},  volume = {abs/2004.14096},  year = {2020} }
Do Neural Language Models Overcome Reporting Bias?	2020	http://www.semanticscholar.org/paper/7096304d19457833972daec4d3f5107befe30b1c	It is found that while pre-trained language models' generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus.	maybe	31	Mining commonsense knowledge from corpora suffers from reporting bias, over-representing the rare at the expense of the trivial (Gordon and Van Durme, 2013). We study to what extent pre-trained language models overcome this issue. We find that while their generalization capacity allows them to better estimate the plausibility of frequent but unspoken of actions, outcomes, and properties, they also tend to overestimate that of the very rare, amplifying the bias that already exists in their training corpus.	7096304d19457833972daec4d3f5107befe30b1c	@['JournalArticle', 'Conference']{shwartz-choi-2020-do,  author = {Vered Shwartz and Yejin Choi},  booktitle = {International Conference on Computational Linguistics},  pages = {6863-6870},  title = {Do Neural Language Models Overcome Reporting Bias?},  year = {2020} }
Do Natural Language Explanations Represent Valid Logical Arguments? Verifying Entailment in Explainable NLI Gold Standards	2021	http://www.semanticscholar.org/paper/8ddeaa0bee603cae32422f47855b32349664f4d5	A systematic annotation methodology, named Explanation Entailment Verification (EEV), is proposed, to quantify the logical validity of human-annotated explanations, and confirms that the inferential properties of explanations are still poorly formalised and understood.	maybe	5	An emerging line of research in Explainable NLP is the creation of datasets enriched with human-annotated explanations and rationales, used to build and evaluate models with step-wise inference and explanation generation capabilities. While human-annotated explanations are used as ground-truth for the inference, there is a lack of systematic assessment of their consistency and rigour. In an attempt to provide a critical quality assessment of Explanation Gold Standards (XGSs) for NLI, we propose a systematic annotation methodology, named Explanation Entailment Verification (EEV), to quantify the logical validity of human-annotated explanations. The application of EEV on three mainstream datasets reveals the surprising conclusion that a majority of the explanations, while appearing coherent on the surface, represent logically invalid arguments, ranging from being incomplete to containing clearly identifiable logical errors. This conclusion confirms that the inferential properties of explanations are still poorly formalised and understood, and that additional work on this line of research is necessary to improve the way Explanation Gold Standards are constructed.	8ddeaa0bee603cae32422f47855b32349664f4d5	@['JournalArticle']{valentino-etal-2021-do,  author = {Marco Valentino and Ian Pratt-Hartman and A. Freitas},  booktitle = {International Conference on Computational Semantics},  journal = {ArXiv},  title = {Do Natural Language Explanations Represent Valid Logical Arguments? Verifying Entailment in Explainable NLI Gold Standards},  volume = {abs/2105.01974},  year = {2021} }
Do Large Language Models know what humans know?	2022	http://www.semanticscholar.org/paper/b4bc0ee9349cf82259298578a3021d37f66bfdc9	This work tests the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language develop evidence of Theory of Mind, and suggests that while statistical learning from language exposure may in part explain how humans develop Theory ofMind, other mechanisms are also responsible.	yes	1	Humans can attribute mental states to others, a capacity known as Theory of Mind. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others’ mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language develop evidence of Theory of Mind. In pre-registered analyses, we present a linguistic version of the False Belief Task, widely used to assess Theory of Mind, to both human participants and a state-of-the-art Large Language Model, GPT-3. Both are sensitive to others’ beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans, nor does it explain the full extent of their behavior—despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how humans develop Theory of Mind, other mechanisms are also responsible.	b4bc0ee9349cf82259298578a3021d37f66bfdc9	@['JournalArticle']{trott-etal-2022-do,  author = {Sean Trott and Cameron J. Jones and Tyler A. Chang and J. Michaelov and B. Bergen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Large Language Models know what humans know?},  volume = {abs/2209.01515},  year = {2022} }
Do Language Models Understand Measurements?	2022	http://www.semanticscholar.org/paper/ff8f3dfd9e2f4a92310999722abefab202935521	It is shown that PLMs lack the capability required for reasoning over measurements, and a simple embedding strategy is proposed to better distinguish between numbers and units, which leads to an improvement in the probing tasks.	maybe	0	Recent success of pre-trained language models (PLMs) has stimulated interest in their ability to understand and work with numbers. Yet, the numerical reasoning over measurements has not been formally studied despite their impor-tance. In this study, we show that PLMs lack the capability required for reasoning over measurements. Furthermore, we ﬁnd that a language model trained on a measurement-rich corpus shows better performance on understanding measurements. We propose a simple embedding strategy to better distinguish between numbers and units, which leads to a signiﬁcant improvement in the probing tasks.	ff8f3dfd9e2f4a92310999722abefab202935521	@['JournalArticle']{park-etal-2022-do,  author = {Sungjin Park and Seung-kook Ryu and E. Choi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Language Models Understand Measurements?},  volume = {abs/2210.12694},  year = {2022} }
Do Language Models Plagiarize?	2022	http://www.semanticscholar.org/paper/2e606c84b45303e6e57b29a83e79864eeb085682	The findings support that language models, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation, and implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.	maybe	2	Past literature has illustrated that language models do not fully understand the context and sensitivity of text and can sometimes memorize phrases or sentences present in their training sets. In this paper, we investigate whether they not only memorize but also plagiarize training samples when generating artificial texts. Our findings support that they, especially GPT-2, reuse particular pieces of texts from the training corpus with or without obfuscation. We have four main results: 1) language models with more capacity plagiarize more; 2) fine-tuned language models demonstrate differing patterns of plagiarism based on characteristics of auxiliary data; 3) sampling from truncated language modeling distributions tends to heighten the degree of plagiarism as opposed to temperature sampling, and 4) plagiarism in language models can have serious privacy consequences. Overall, our work implies that future research on neural language models should take precautions to avoid models plagiarizing their training datasets.	2e606c84b45303e6e57b29a83e79864eeb085682	@['JournalArticle']{lee-etal-2022-do,  author = {Jooyoung Lee and Thai Le and Jinghui Chen and Dongwon Lee},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Language Models Plagiarize?},  volume = {abs/2203.07618},  year = {2022} }
Do Language Models Perform Generalizable Commonsense Inference?	2021	http://www.semanticscholar.org/paper/572b9183d3eaf45a31c9308f20e420c5f922588e	The ability of LMs to perform generalizable commonsense inference, in terms of knowledge capacity, transferability, and induction, is analyzed.	yes	10	Inspired by evidence that pretrained language models (LMs) encode commonsense knowledge, recent work has applied LMs to automatically populate commonsense knowledge graphs (CKGs). However, there is a lack of understanding on their generalization to multiple CKGs, unseen relations, and novel entities. This paper analyzes the ability of LMs to perform generalizable commonsense inference, in terms of knowledge capacity, transferability, and induction. Our experiments with these three aspects show that: (1) LMs can adapt to different schemas defined by multiple CKGs but fail to reuse the knowledge to generalize to new relations. (2) Adapted LMs generalize well to unseen subjects, but less so on novel objects. Future work should investigate how to improve the transferability and induction of commonsense mining from LMs.1	572b9183d3eaf45a31c9308f20e420c5f922588e	@['JournalArticle']{wang-etal-2021-do,  author = {Peifeng Wang and Filip Ilievski and Muhao Chen and Xiang Ren},  booktitle = {Findings},  journal = {ArXiv},  title = {Do Language Models Perform Generalizable Commonsense Inference?},  volume = {abs/2106.11533},  year = {2021} }
Do Language Models Make Human-like Predictions about the Coreferents of Italian Anaphoric Zero Pronouns?	2022	http://www.semanticscholar.org/paper/37319652e3ba5d0225bfedca1afc9c2e33d4bcc0		maybe	1	Some languages allow arguments to be omitted in certain contexts. Yet human language comprehenders reliably infer the intended referents of these zero pronouns, in part because they construct expectations about which referents are more likely. We ask whether Neural Language Models also extract the same expectations. We test whether 12 contemporary language models display expectations that reflect human behavior when exposed to sentences with zero pronouns from five behavioral experiments conducted in Italian by Carminati (2005). We find that three models - XGLM 2.9B, 4.5B, and 7.5B - capture the human behavior from all the experiments, with others successfully modeling some of the results. This result suggests that human expectations about coreference can be derived from exposure to language, and also indicates features of language models that allow them to better reflect human behavior.	37319652e3ba5d0225bfedca1afc9c2e33d4bcc0	@['JournalArticle', 'Conference']{michaelov-bergen-2022-do,  author = {J. Michaelov and B. Bergen},  booktitle = {International Conference on Computational Linguistics},  pages = {1-14},  title = {Do Language Models Make Human-like Predictions about the Coreferents of Italian Anaphoric Zero Pronouns?},  year = {2022} }
Do language models learn typicality judgments from text?	2021	http://www.semanticscholar.org/paper/30aedc5bd1cba8e629c4d318e082f2da3177653c	It is suggested that text-based exposure alone is insufficient to acquire typicality knowledge, and two tests for LMs are proposed, showing modest—but not completely absent—correspondence between LMs and humans.	maybe	11	Building on research arguing for the possibility of conceptual and categorical knowledge acquisition through statistics contained in language, we evaluate predictive language models (LMs)—informed solely by textual input—on a prevalent phenomenon in cognitive science: typicality. Inspired by experiments that involve language processing and show robust typicality effects in humans, we propose two tests for LMs. Our first test targets whether typicality modulates LM probabilities in assigning taxonomic category memberships to items. The second test investigates sensitivities to typicality in LMs’ probabilities when extending new information about items to their categories. Both tests show modest—but not completely absent—correspondence between LMs and humans, suggesting that text-based exposure alone is insufficient to acquire typicality knowledge.	30aedc5bd1cba8e629c4d318e082f2da3177653c	@['JournalArticle']{misra-etal-2021-do,  author = {Kanishka Misra and Allyson Ettinger and J. Rayz},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do language models learn typicality judgments from text?},  volume = {abs/2105.02987},  year = {2021} }
Do Language Models Learn Commonsense Knowledge?	2021	http://www.semanticscholar.org/paper/6b86a91737809b869ae7bf3d34c231b697728825		maybe	3	Language models (LMs) trained on large amounts of data (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many NLP tasks under the zero-shot and few-shot setup. Here we aim to better understand the extent to which such models learn commonsense knowledge — a critical component of many NLP applications. To that end, we conduct a systematic and rigorous zero-shot and few-shot commonsense evaluation of pre-trained LMs, where we: (i) carefully control for the LM’s ability to exploit potential surface cues and annotation artefacts, and (ii) account for variations in model performance that arise from non-commonsense related factors. Our findings highlight the limitations of pre-trained LMs in acquiring commonsense knowledge without task-specific supervision; furthermore, using larger models — or augmenting the LMs with commonsense knowledge bases at test-time — did not substantially improve their performance. More broadly, our findings offer valuable lessons and best practices for conducting more rigorous multiplechoice evaluations of pre-trained LMs.	6b86a91737809b869ae7bf3d34c231b697728825	@None{li-etal-2022-do,  author = {Xiang Lorraine Li and A. Kuncoro and Cyprien de Masson d'Autume and P. Blunsom and Aida Nematzadeh},  title = {Do Language Models Learn Commonsense Knowledge?},  year = {2022} }
Do Language Models Know the Way to Rome?	2021	https://www.semanticscholar.org/paper/1fa084781277c90cfa0f7665c5528bc9f882be06		yes	3	The global geometry of language models is important for a range of applications, but language model probes tend to evaluate rather local relations, for which ground truths are easily obtained. In this paper we exploit the fact that in geography, ground truths are available beyond local relations. In a series of experiments, we evaluate the extent to which language model representations of city and country names are isomorphic to real-world geography, e.g., if you tell a language model where Paris and Berlin are, does it know the way to Rome? We find that language models generally encode limited geographic information, but with larger models performing the best, suggesting that geographic knowledge can be induced from higher-order co-occurrence statistics.	1fa084781277c90cfa0f7665c5528bc9f882be06	@['JournalArticle']{liétard-etal-2021-do,  author = {Bastien Liétard and Mostafa Abdou and Anders Søgaard},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {510-517},  title = {Do Language Models Know the Way to Rome?},  year = {2021} }
Do language models have coherent mental models of everyday things?	2022	http://www.semanticscholar.org/paper/fb7573ded82cff42b9341c4a6b3ca156f8369e07	A simple extension to pre-trained language models like GPT-3 and Macaw is proposed where a constraint satisfaction layer is applied on top of raw predictions from LMs to produce more consistent and accurate parts mental models of everyday things.	maybe	0	When people think of everyday things like an “egg,” they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridicu-lous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consistent parts mental models. We propose a simple extension to these LMs where we apply a constraint satisfaction layer on top of raw predictions from LMs to produce more consistent and accurate parts mental models of everyday things.	fb7573ded82cff42b9341c4a6b3ca156f8369e07	@['JournalArticle']{gu-etal-2022-do,  author = {Yuling Gu and Bhavana Dalvi and Peter Clark},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do language models have coherent mental models of everyday things?},  volume = {abs/2212.10029},  year = {2022} }
Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs	2021	https://www.semanticscholar.org/paper/4a247cbfca9dcf91e2da24e6d2d84601a9041a8f	Approaches to detecting when 006 models have beliefs about the world, updating 007 model beliefs, and visualizing beliefs graphi- 008 cally are discussed, which suggest that models possess belief-like quali- 019 ties to only a limited extent, but update meth- 020 ods can both correct incorrect model beliefs and greatly improve their consistency.	yes	23	Do language models have beliefs about the 001 world? Dennett (1995) famously argues that 002 even thermostats have beliefs, on the view that 003 a belief is simply an informational state decou- 004 pled from any motivational state. In this pa- 005 per, we discuss approaches to detecting when 006 models have beliefs about the world, updating 007 model beliefs, and visualizing beliefs graphi- 008 cally. Our main contributions include: (1) new 009 metrics for evaluating belief-updating methods 010 focusing on the logical consistency of beliefs, 011 (2) a training objective for Sequential, Lo- 012 cal, and Generalizing updates (SLAG) that im- 013 proves the performance of learned optimizers 014 for updating beliefs, and (3) the introduction of 015 the belief graph , a new form of interface with 016 language models showing the interdependen- 017 cies between model beliefs. Our experiments 018 suggest that models possess belief-like quali- 019 ties to only a limited extent, but update meth- 020 ods can both ﬁx incorrect model beliefs and 021 greatly improve their consistency. Although 022 off-the-shelf optimizers are surprisingly strong 023 belief-updating baselines, our learned optimiz- 024 ers can outperform them in more difﬁcult set- 025 tings than have been considered in past work. 1 026	4a247cbfca9dcf91e2da24e6d2d84601a9041a8f	@['JournalArticle']{hase-etal-2021-do,  author = {Peter Hase and Mona T. Diab and Asli Celikyilmaz and Xian Li and Zornitsa Kozareva and Veselin Stoyanov and Mohit Bansal and Srini Iyer},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs},  volume = {abs/2111.13654},  year = {2021} }
Do Language Embeddings capture Scales?	2020	https://www.semanticscholar.org/paper/3118c0633cb2d0f91b5ef88840343e47c3ca5623	This work identifies contextual information in pre-training and numeracy as two key factors affecting their performance, and shows that a simple method of canonicalizing numbers can have a significant effect on the results.	maybe	44	Pretrained Language Models (LMs) have been shown to possess significant linguistic, common sense and factual knowledge. One form of knowledge that has not been studied yet in this context is information about the scalar magnitudes of objects. We show that pretrained language models capture a significant amount of this information but are short of the capability required for general common-sense reasoning. We identify contextual information in pre-training and numeracy as two key factors affecting their performance, and show that a simple method of canonicalizing numbers can have a significant effect on the results.	3118c0633cb2d0f91b5ef88840343e47c3ca5623	@['JournalArticle']{zhang-etal-2020-do,  author = {Xikun Zhang and Deepak Ramachandran and Ian Tenney and Yanai Elazar and D. Roth},  booktitle = {Findings},  journal = {ArXiv},  title = {Do Language Embeddings capture Scales?},  volume = {abs/2010.05345},  year = {2020} }
Do Fine-tuned Commonsense Language Models Really Generalize?	2020	http://www.semanticscholar.org/paper/415ae1594bfff369ad74b4941f958452a4f0de0d	Clear evidence is found that fine-tuned commonsense language models still do not generalize well, even with moderate changes to the experimental setup, and may, in fact, be susceptible to dataset bias.	yes	5	Recently, transformer-based methods such as RoBERTa and GPT-3 have led to significant experimental advances in natural language processing tasks such as question answering and commonsense reasoning. The latter is typically evaluated through multiple benchmarks framed as multiple-choice instances of the former. According to influential leaderboards hosted by the Allen Institute (evaluating state-of-the-art performance on commonsense reasoning benchmarks), models based on such transformer methods are approaching human-like performance and have average accuracy well over 80% on many benchmarks. Since these are commonsense benchmarks, a model that generalizes on commonsense reasoning should not experience much performance loss across multiple commonsense benchmarks. In this paper, we study the generalization issue in detail by designing and conducting a rigorous scientific study. Using five common benchmarks, multiple controls and statistical analysis, we find clear evidence that fine-tuned commonsense language models still do not generalize well, even with moderate changes to the experimental setup, and may, in fact, be susceptible to dataset bias. We also perform selective studies, including qualitative and consistency analyses, to gain deeper insight into the problem.	415ae1594bfff369ad74b4941f958452a4f0de0d	@['JournalArticle']{kejriwal-shen-2020-do,  author = {M. Kejriwal and Ke Shen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Fine-tuned Commonsense Language Models Really Generalize?},  volume = {abs/2011.09159},  year = {2020} }
Do Feature Attribution Methods Correctly Attribute Features?	2021	http://www.semanticscholar.org/paper/426734685283b4a0c08b34cd9e996e2e30e7f7ee	A dataset modification procedure is proposed to induce ground truth attribution in feature attribution methods, and three common methods are evaluated: saliency maps, rationales, and attentions.	maybe	39	Feature attribution methods are popular in interpretable machine learning. These methods compute the attribution of each input feature to represent its importance, but there is no consensus on the definition of "attribution", leading to many competing methods with little systematic evaluation, complicated in particular by the lack of ground truth attribution. To address this, we propose a dataset modification procedure to induce such ground truth. Using this procedure, we evaluate three common methods: saliency maps, rationales, and attentions. We identify several deficiencies and add new perspectives to the growing body of evidence questioning the correctness and reliability of these methods applied on datasets in the wild. We further discuss possible avenues for remedy and recommend new attribution methods to be tested against ground truth before deployment. The code and appendix are available at https://yilunzhou.github.io/feature-attribution-evaluation/.	426734685283b4a0c08b34cd9e996e2e30e7f7ee	@['JournalArticle', 'Conference']{zhou-etal-2021-do,  author = {Yilun Zhou and S. Booth and Marco Tulio Ribeiro and J. Shah},  booktitle = {AAAI Conference on Artificial Intelligence},  journal = {ArXiv},  title = {Do Feature Attribution Methods Correctly Attribute Features?},  volume = {abs/2104.14403},  year = {2021} }
Do Encoder Representations of Generative Dialogue Models Encode Sufficient Information about the Task ?	2021	http://www.semanticscholar.org/paper/c2d69b099cecf24c577ace3a68d4816398d0400d	This work showcases evaluating the text generated through human or automatic metrics is not sufficient to appropriately evaluate soundness of the language understanding of dialogue models and proposes a set of probe tasks to evaluate encoder representation of different language encoders commonly used in dialogue models.	maybe	0	Predicting the next utterance in dialogue is contingent on encoding of users’ input text to generate appropriate and relevant response in data-driven approaches. Although the semantic and syntactic quality of the language generated is evaluated, more often than not, the encoded representation of input is not evaluated. As the representation of the encoder is essential for predicting the appropriate response, evaluation of encoder representation is a challenging yet important problem. In this work, we showcase evaluating the text generated through human or automatic metrics is not sufficient to appropriately evaluate soundness of the language understanding of dialogue models and, to that end, propose a set of probe tasks to evaluate encoder representation of different language encoders commonly used in dialogue models. From experiments, we observe that some of the probe tasks are easier and some are harder for even sophisticated model architectures to learn. And, through experiments we observe that RNN based architectures have lower performance on automatic metrics on text generation than transformer model but perform better than the transformer model on the probe tasks indicating that RNNs might preserve task information better than the Transformers.	c2d69b099cecf24c577ace3a68d4816398d0400d	@['JournalArticle']{parthasarathi-etal-2021-do,  author = {Prasanna Parthasarathi and J. Pineau and Sarath Chandar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Encoder Representations of Generative Dialogue Models Encode Sufficient Information about the Task ?},  volume = {abs/2106.10622},  year = {2021} }
Do Children Texts Hold The Key To Commonsense Knowledge?	2022	http://www.semanticscholar.org/paper/eaec704a9a2ebe5b045dc23246640e6561bc276e	This paper explores whether children’s texts hold the key to commonsense knowledge compilation, based on the hypothesis that such content makes fewer assump-tions on the reader's knowledge, and therefore spells out commonsense more explicitly.	yes	0	Compiling comprehensive repositories of commonsense knowledge is a long-standing problem in AI. Many concerns revolve around the issue of reporting bias, i.e., that frequency in text sources is not a good proxy for relevance or truth. This paper explores whether children’s texts hold the key to commonsense knowledge compilation, based on the hypothesis that such content makes fewer assumptions on the reader’s knowledge, and therefore spells out commonsense more explicitly. An analysis with several corpora shows that children’s texts indeed contain much more, and more typical commonsense assertions. Moreover, experiments show that this advantage can be leveraged in popular language-model-based commonsense knowledge extraction settings, where task-unspecific fine-tuning on small amounts of children texts (childBERT) already yields significant improvements. This provides a refreshing perspective different from the common trend of deriving progress from ever larger models and corpora.	eaec704a9a2ebe5b045dc23246640e6561bc276e	@['JournalArticle', 'Conference']{romero-razniewski-2022-do,  author = {Julien Romero and S. Razniewski},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Do Children Texts Hold The Key To Commonsense Knowledge?},  volume = {abs/2210.04530},  year = {2022} }
Do Attention Heads in BERT Track Syntactic Dependencies?	2019	https://www.semanticscholar.org/paper/ba8215e77f35b0d947c7cec39c45df4516e93421	The results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.	seed	88	We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods---taking the maximum attention weight and computing the maximum spanning tree---to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the semantics-oriented MNLI---to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.	ba8215e77f35b0d947c7cec39c45df4516e93421	@['JournalArticle']{htut-etal-2019-do,  author = {Phu Mon Htut and Jason Phang and Shikha Bordia and Samuel R. Bowman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Attention Heads in BERT Track Syntactic Dependencies?},  volume = {abs/1911.12246},  year = {2019} }
Do Artificial Intelligence Systems Understand?	2022	http://www.semanticscholar.org/paper/698b771975b0bcaab402b936af259c39ebd9239d	The conclusion states that it is not necessary to attribute understanding to a machine in order to explain its exhibited “intelligent” behavior; a merely syntactic and mechanistic approach to intelligence as a task-solving tool is needed to justify the range of operations that it can display in the current state of technological development.	maybe	2	. Are intelligent machines really intelligent? Is the underlying philosophical concept of intelligence satisfactory for describing how the present systems work? Is understanding a necessary and suﬃcient condition for intelligence? If a machine could understand, should we attribute subjectivity to it? This paper addresses the problem of deciding whether the so-called ”intelligent machines” are capable of understanding, instead of merely processing signs. It deals with the relationship between syntaxis and semantics. The main thesis concerns the inevitability of semantics for any discussion about the possibility of building conscious machines, condensed into the following two tenets: ”If a machine is capable of understanding (in the strong sense), then it must be capable of combining rules and intuitions”; “If semantics cannot be reduced to syntaxis, then a machine cannot understand.” Our conclusion states that it is not necessary to attribute understanding to a machine in order to explain its exhibited “intelligent” behavior; a merely syntactic and mechanistic approach to intelligence as a task-solving tool suﬃces to justify the range of operations that it can display in the current state of technological development.	698b771975b0bcaab402b936af259c39ebd9239d	@['JournalArticle']{merch'an-blanco-2022-do,  author = {Eduardo C. Garrido-Merch'an and Carlos Blanco},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Do Artificial Intelligence Systems Understand?},  volume = {abs/2207.11089},  year = {2022} }
Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model	2022	http://www.semanticscholar.org/paper/6998fef99cb3056fe103262b251c5b0a73c95d3d	It is empirically demonstrated that winning-ticket subnetworks produced more diverse predictions than dense networks and their ensemble outperformed the standard ensemble in some tasks when accurate lottery tickets are found on the tasks.	maybe	2	Ensembling is a popular method used to improve performance as a last resort. However, ensembling multiple models finetuned from a single pretrained model has been not very effective; this could be due to the lack of diversity among ensemble members. This paper proposes Multi-Ticket Ensemble, which finetunes different subnetworks of a single pretrained model and ensembles them. We empirically demonstrated that winning-ticket subnetworks produced more diverse predictions than dense networks and their ensemble outperformed the standard ensemble in some tasks when accurate lottery tickets are found on the tasks.	6998fef99cb3056fe103262b251c5b0a73c95d3d	@['JournalArticle']{kobayashi-etal-2022-diverse,  author = {Sosuke Kobayashi and Shun Kiyono and Jun Suzuki and Kentarou Inui},  booktitle = {BIGSCIENCE},  journal = {ArXiv},  title = {Diverse Lottery Tickets Boost Ensemble from a Single Pretrained Model},  volume = {abs/2205.11833},  year = {2022} }
Distributional Semantics Still Can’t Account for Affordances	2022	http://www.semanticscholar.org/paper/84f78cc33b124434aa8e4b670686de6831be02ba		maybe	2	Can we know a word by the company it keeps? Aspects of meaning that concern physical interactions might be partic-ularly difficult to learn from language alone. Glenberg and Robertson (2000) found that although human comprehenders were sensitive to the distinction between afforded and nonaf- forded actions, distributional semantic models were not. We tested whether technological advances have made distribu- tional models more sensitive to affordances by replicating their experiment with modern Neural Language Models (NLMs). We found that only one NLM (GPT-3) was sensitive to the affordedness of actions. Moreover, GPT-3 accounted for only one third of the effect of affordedness on human sensibility judgements. These results imply that people use processes that go beyond distributional statistics to understand linguistic ex-pressions, and that NLP systems may need to be augmented with such capabilities. all NLM measures, indicating that none of the NLM measures accounted for all of the variance caused by Affordedness. Only one NLM measure (BERT Surprisal) improved fit over the Base model in the Afforded vs Related comparison. Again the inclusion of condition improved model fit over each NLM-only model.	84f78cc33b124434aa8e4b670686de6831be02ba	@None{jones-etal-2022-distributional,  author = {Cameron R. Jones and Tyler A. Chang and S. Coulson and J. Michaelov and Sean Trott},  title = {Distributional Semantics Still Can’t Account for Affordances},  year = {2022} }
Distributed Representations of Words and Phrases and their Compositionality	2013	https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f	This paper presents a simple method for finding phrases in text, and shows that learning good vector representations for millions of phrases is possible and describes a simple alternative to the hierarchical softmax called negative sampling.	seed	28616	The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.	87f40e6f3022adbc1f1905e3e506abad05a9964f	@['JournalArticle', 'Conference']{mikolov-etal-2013-distributed,  author = {Tomas Mikolov and Ilya Sutskever and Kai Chen and G. Corrado and J. Dean},  booktitle = {NIPS},  journal = {ArXiv},  title = {Distributed Representations of Words and Phrases and their Compositionality},  volume = {abs/1310.4546},  year = {2013} }
Distilling Task-Specific Knowledge from BERT into Simple Neural Networks	2019	https://www.semanticscholar.org/paper/a08293b2c9c5bcddb023cc7eb3354d4d86bfae89	This paper proposes to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks, and achieves comparable results with ELMo.	seed	258	In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.	a08293b2c9c5bcddb023cc7eb3354d4d86bfae89	@['JournalArticle']{tang-etal-2019-distilling,  author = {Raphael Tang and Yao Lu and Linqing Liu and Lili Mou and Olga Vechtomova and Jimmy J. Lin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Distilling Task-Specific Knowledge from BERT into Simple Neural Networks},  volume = {abs/1903.12136},  year = {2019} }
Dissociating language and thought in large language models: a cognitive perspective	2023	http://www.semanticscholar.org/paper/9a9e68d400069f023f7dc9b982226c95159a509d		maybe	0	Short abstract (100 words): Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their capabilities remain split. Here, we evaluate LLMs using a distinction between formal competence—knowledge of linguistic rules and patterns—and functional competence—understanding and using language in the world. We ground this distinction in human neuroscience, showing that these skills recruit different cognitive mechanisms. Although LLMs are close to mastering formal competence, they still fail at functional competence tasks, which often require drawing on non-linguistic capacities. In short, LLMs are good models of language but incomplete models of human thought. Long abstract (250 words): Today’s large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are—or will soon become—“thinking machines”, capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: ‘formal linguistic competence’, which includes knowledge of rules and patterns of a given language, and ’functional linguistic competence’, a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs’ potential and provides a path toward building models that understand and use language in human-like ways. * The two lead authors contributed equally to this work. ar X iv :2 30 1. 06 62 7v 1 [ cs .C L ] 1 6 Ja n 20 23 A PREPRINT JANUARY 18, 2023	9a9e68d400069f023f7dc9b982226c95159a509d	@['JournalArticle', 'Review']{mahowald-etal-2023-dissociating,  author = {Kyle Mahowald and Anna A. Ivanova and I. Blank and N. Kanwisher and J. Tenenbaum and Evelina Fedorenko},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Dissociating language and thought in large language models: a cognitive perspective},  volume = {abs/2301.06627},  year = {2023} }
Disentangling syntax and semantics in the brain with deep networks	2021	http://www.semanticscholar.org/paper/0b28c877ff1fdd9115a71ed45f6963bd61fa7d0e	A taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations, which introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.	yes	20	The activations of language transformers like GPT-2 have been shown to linearly map onto brain activity during speech comprehension. How-ever, the nature of these activations remains largely unknown and presumably conﬂate distinct linguistic classes. Here, we propose a taxonomy to factorize the high-dimensional activations of language models into four combinatorial classes: lexical, compositional, syntactic, and semantic representations. We then introduce a statistical method to decompose, through the lens of GPT-2’s activations, the brain activity of 345 subjects recorded with functional magnetic resonance imaging (fMRI) during the listening of ~ 4.6 hours of narrated text. The results highlight two ﬁndings. First, compositional representations recruit a more widespread cortical network than lexical ones, and encompass the bilateral temporal, parietal and prefrontal cortices. Second, contrary to previous claims, syntax and semantics are not associated with separated modules, but, instead, appear to share a common and distributed neural substrate. Overall, this study introduces a versatile framework to isolate, in the brain activity, the distributed representations of linguistic constructs.	0b28c877ff1fdd9115a71ed45f6963bd61fa7d0e	@['JournalArticle', 'Conference']{caucheteux-etal-2021-disentangling,  author = {C. Caucheteux and Alexandre Gramfort and J. King},  booktitle = {International Conference on Machine Learning},  pages = {1336-1348},  title = {Disentangling syntax and semantics in the brain with deep networks},  year = {2021} }
Discovering Latent Concepts Learned in BERT	2022	http://www.semanticscholar.org/paper/e33b7282f1e547054a660377383b8ab8464f676e	This work investigates what latent concepts exist in the pre-trained BERT model, how the discovered latent concepts align or diverge from classical linguistic hierarchy, and how the latent concepts evolve across layers.	yes	15	A large number of studies that analyze deep neural network models and their ability to encode various linguistic and non-linguistic concepts provide an interpretation of the inner mechanics of these models. The scope of the analyses is limited to pre-deﬁned concepts that reinforce the traditional linguistic knowledge and do not reﬂect on how novel concepts are learned by the model. We address this limitation by discovering and analyzing latent concepts learned in neural network models in an unsupervised fashion and provide interpretations from the model’s perspective. In this work, we study: i) what latent concepts exist in the pre-trained BERT model, ii) how the discovered latent concepts align or diverge from classical linguistic hierarchy and iii) how the latent concepts evolve across layers. Our ﬁndings show: i) a model learns novel concepts (e.g. animal categories and de-mographic groups), which do not strictly adhere to any pre-deﬁned categorization (e.g. POS, semantic tags), ii) several latent concepts are based on multiple properties which may include semantics, syntax, and morphology, iii) the lower layers in the model dominate in learning shallow lexical concepts while the higher layers learn semantic relations and iv) the discovered latent concepts highlight potential biases learned in the model. We also release 1 a novel BERT ConceptNet dataset ( BCN ) consisting of 174 concept labels and 1M annotated instances.	e33b7282f1e547054a660377383b8ab8464f676e	@['JournalArticle']{dalvi-etal-2022-discovering,  author = {Fahim Dalvi and Abdul Rafae Khan and Firoj Alam and Nadir Durrani and Jia Xu and Hassan Sajjad},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Discovering Latent Concepts Learned in BERT},  volume = {abs/2205.07237},  year = {2022} }
Discovering Language Model Behaviors with Model-Written Evaluations	2022	http://www.semanticscholar.org/paper/cef330bacf014d60daabbd489647b2006af130ca		yes	1	As language models (LMs) scale, they develop many novel behaviors, good and bad	cef330bacf014d60daabbd489647b2006af130ca	@['JournalArticle']{perez-etal-2022-discovering,  author = {Ethan Perez and Sam Ringer and Kamilė Lukošiūtė and Karina Nguyen and Edwin Chen and Scott Heiner and Craig Pettit and Catherine Olsson and Sandipan Kundu and Saurav Kadavath and Andy Jones and Anna Chen and Benjamin Mann and Brian Israel and Bryan Seethor and C. McKinnon and C. Olah and Daisong Yan and Daniela Amodei and Dario Amodei and Dawn Drain and Dustin Li and Eli Tran-Johnson and G. Khundadze and John Kernion and J. Landis and Jamie Kerr and Jared Mueller and Jeeyoon Hyun and J. Landau and Kamal Ndousse and L. Goldberg and Liane Lovitt and Martin Lucas and Michael Sellitto and Miranda Zhang and Neerav Kingsland and Nelson Elhage and Nicholas Joseph and Noem'i Mercado and Nova DasSarma and Oliver Rausch and Robin Larson and Sam McCandlish and Scott Johnston and S. Kravec and Sheer El Showk and Tamera Lanham and Timothy Telleen-Lawton and Tom B. Brown and T. Henighan and Tristan Hume and Yuntao Bai and Zac Hatfield-Dodds and Jack Clark and Sam Bowman and Amanda Askell and Roger C. Grosse and Danny Hernandez and Deep Ganguli and Evan Hubinger and Nicholas Schiefer and Jared Kaplan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Discovering Language Model Behaviors with Model-Written Evaluations},  volume = {abs/2212.09251},  year = {2022} }
Discovering Financial Hypernyms by Prompting Masked Language Models	2022	http://www.semanticscholar.org/paper/db1dbcc710d9e94bbedf2042fe23403192ecd91c	The results show that the differences of prompts impact critically on models’ performance, and that domain adaptation on financial text generally improves the capacity of the models to associate the target terms with the right hypernyms, although the more successful models are those retaining a general-domain vocabulary.	maybe	0	With the rising popularity of Transformer-based language models, several studies have tried to exploit their masked language modeling capabilities to automatically extract relational linguistic knowledge, although this kind of research has rarely investigated semantic relations in specialized domains. The present study aims at testing a general-domain and a domain-adapted Transformer models on two datasets of financial term-hypernym pairs using the prompt methodology. Our results show that the differences of prompts impact critically on models’ performance, and that domain adaptation on financial text generally improves the capacity of the models to associate the target terms with the right hypernyms, although the more successful models are those retaining a general-domain vocabulary.	db1dbcc710d9e94bbedf2042fe23403192ecd91c	@None{peng-etal-2022-discovering,  author = {Bo Peng and Emmanuele Chersoni and Yu-Yin Hsu and Chu-Ren Huang},  booktitle = {FNP},  title = {Discovering Financial Hypernyms by Prompting Masked Language Models},  year = {2022} }
Discovering Differences in the Representation of People using Contextualized Semantic Axes	2022	http://www.semanticscholar.org/paper/7f5801f8036e71658a12d16203617b2ab25ef14f	This work constructs contextualized axes that mitigate the pitfall where antonyms have neighboring representations in BERT embeddings, and shows that references to women and the contexts around them have be-come more detestable over time.	maybe	0	A common paradigm for identifying semantic differences across social and temporal contexts is the use of static word embeddings and their distances. In particular, past work has compared embeddings against “semantic axes” that represent two opposing concepts. We extend this paradigm to BERT embeddings, and construct contextualized axes that mitigate the pitfall where antonyms have neighboring representations. We validate and demonstrate these axes on two people-centric datasets: occupations from Wikipedia, and multi-platform discussions in extremist, men’s communities over fourteen years. In both studies, contextualized semantic axes can characterize differences among instances of the same word type. In the latter study, we show that references to women and the contexts around them have become more detestable over time.	7f5801f8036e71658a12d16203617b2ab25ef14f	@['JournalArticle', 'Conference']{lucy-etal-2022-discovering,  author = {Li Lucy and Divya Tadimeti and David Bamman},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Discovering Differences in the Representation of People using Contextualized Semantic Axes},  volume = {abs/2210.12170},  year = {2022} }
Discourse structure interacts with reference but not syntax in neural language models	2020	http://www.semanticscholar.org/paper/14f78c24d5da77835ac4e80de3daa7bc9e92f0a8	This work utilized stimuli from psycholinguistic studies showing that humans can condition reference and syntactic processing on the same discourse structure to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information.	yes	7	Language models (LMs) trained on large quantities of text have been claimed to acquire abstract linguistic representations. Our work tests the robustness of these abstractions by focusing on the ability of LMs to learn interactions between different linguistic representations. In particular, we utilized stimuli from psycholinguistic studies showing that humans can condition reference (i.e. coreference resolution) and syntactic processing on the same discourse structure (implicit causality). We compared both transformer and long short-term memory LMs to find that, contrary to humans, implicit causality only influences LM behavior for reference, not syntax, despite model representations that encode the necessary discourse information. Our results further suggest that LM behavior can contradict not only learned representations of discourse but also syntactic agreement, pointing to shortcomings of standard language modeling.	14f78c24d5da77835ac4e80de3daa7bc9e92f0a8	@['JournalArticle']{davis-schijndel-2020-discourse,  author = {Forrest Davis and Marten van Schijndel},  booktitle = {Conference on Computational Natural Language Learning},  pages = {396-407},  title = {Discourse structure interacts with reference but not syntax in neural language models},  year = {2020} }
Discourse Probing of Pretrained Language Models	2021	http://www.semanticscholar.org/paper/a978fcb10817abe8bc91ab2ba0c0bb4605add1d9	BART is found to be overall the best model at capturing discourse — but only in its encoder, with BERT performing surprisingly well as the baseline model.	maybe	18	Existing work on probing of pretrained language models (LMs) has predominantly focused on sentence-level syntactic tasks. In this paper, we introduce document-level discourse probing to evaluate the ability of pretrained LMs to capture document-level relations. We experiment with 7 pretrained LMs, 4 languages, and 7 discourse probing tasks, and find BART to be overall the best model at capturing discourse — but only in its encoder, with BERT performing surprisingly well as the baseline model. Across the different models, there are substantial differences in which layers best capture discourse information, and large disparities between models.	a978fcb10817abe8bc91ab2ba0c0bb4605add1d9	@['JournalArticle', 'Conference']{koto-etal-2021-discourse,  author = {Fajri Koto and Jey Han Lau and Tim Baldwin},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {3849-3864},  title = {Discourse Probing of Pretrained Language Models},  year = {2021} }
DiscoSense: Commonsense Reasoning with Discourse Connectives	2022	http://www.semanticscholar.org/paper/1a5f48161df983a0e9485425495121201902433b	It is shown that state-of-the-art pre-trained language models struggle to perform well on D ISCO S ENSE, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.	maybe	1	We present DiscoSense, a benchmark for commonsense reasoning via understanding a wide variety of discourse connectives. We generate compelling distractors in DiscoSense using Conditional Adversarial Filtering, an extension of Adversarial Filtering that employs conditional generation. We show that state-of-the-art pre-trained language models struggle to perform well on DiscoSense, which makes this dataset ideal for evaluating next-generation commonsense reasoning systems.	1a5f48161df983a0e9485425495121201902433b	@['JournalArticle', 'Conference']{bhargava-ng-2022-discosense:,  author = {Prajjwal Bhargava and Vincent Ng},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {DiscoSense: Commonsense Reasoning with Discourse Connectives},  volume = {abs/2210.12478},  year = {2022} }
Discontinuous Constituency and BERT: A Case Study of Dutch	2022	http://www.semanticscholar.org/paper/fb574429cad091fbe1c1d12c5df93fb371d4cd61		maybe	1	In this paper, we set out to quantify the syntactic capacity of BERT in the evaluation regime of non-context free patterns, as occurring in Dutch. We devise a test suite based on a mildly context-sensitive formalism, from which we derive grammars that capture the linguistic phenomena of control verb nesting and verb raising. The grammars, paired with a small lexicon, provide us with a large collection of naturalistic utterances, annotated with verb-subject pairings, that serve as the evaluation test bed for an attention-based span selection probe. Our results, backed by extensive analysis, suggest that the models investigated fail in the implicit acquisition of the dependencies examined.	fb574429cad091fbe1c1d12c5df93fb371d4cd61	@['JournalArticle']{kogkalidis-wijnholds-2022-discontinuous,  author = {Konstantinos Kogkalidis and G. Wijnholds},  booktitle = {Findings},  journal = {ArXiv},  title = {Discontinuous Constituency and BERT: A Case Study of Dutch},  volume = {abs/2203.01063},  year = {2022} }
DirectProbe: Studying Representations without Classifiers	2021	http://www.semanticscholar.org/paper/00c209ea764f709a5ce7d7fdc16c2352551bfa83	A heuristic is developed that directly studies the geometry of a representation by building upon the notion of a version space for a task, and can shine lights on how an embedding space represents labels and also anticipate the classifier performance for the representation.	maybe	12	Understanding how linguistic structure is encoded in contextualized embedding could help explain their impressive performance across NLP. Existing approaches for probing them usually call for training classifiers and use the accuracy, mutual information, or complexity as a proxy for the representation’s goodness. In this work, we argue that doing so can be unreliable because different representations may need different classifiers. We develop a heuristic, DirectProbe, that directly studies the geometry of a representation by building upon the notion of a version space for a task. Experiments with several linguistic tasks and contextualized embeddings show that, even without training classifiers, DirectProbe can shine lights on how an embedding space represents labels and also anticipate the classifier performance for the representation.	00c209ea764f709a5ce7d7fdc16c2352551bfa83	@['JournalArticle', 'Conference']{zhou-srikumar-2021-directprobe:,  author = {Yichu Zhou and Vivek Srikumar},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {DirectProbe: Studying Representations without Classifiers},  volume = {abs/2104.05904},  year = {2021} }
Dimensions of Commonsense Knowledge	2021	http://www.semanticscholar.org/paper/9f620ad41b4e506e777c0665681b839c89cd682a	This paper surveys a wide range of popular commonsense sources with a special focus on their relations, and consolidates these relations into 13 knowledge dimensions, each abstracting over more specific relations found in sources.	maybe	24		9f620ad41b4e506e777c0665681b839c89cd682a	@['JournalArticle', 'Review']{ilievski-etal-2021-dimensions,  author = {Filip Ilievski and A. Oltramari and Kaixin Ma and Bin Zhang and D. McGuinness and Pedro A. Szekely},  booktitle = {Knowledge-Based Systems},  journal = {Knowl. Based Syst.},  pages = {107347},  title = {Dimensions of Commonsense Knowledge},  volume = {229},  year = {2021} }
Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?	2021	http://www.semanticscholar.org/paper/53b5f5e1d2bfeafa25688e1da02aae048936baf5	Transformer language models are proposed and provided evidence for one possible explanation—their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.	maybe	8	Despite being designed for performance rather than cognitive plausibility, transformer language models have been found to be better at predicting metrics used to assess human language comprehension than language models with other architectures, such as recurrent neural networks. Based on how well they predict the N400, a neural signal associated with processing difficulty, we propose and provide evidence for one possible explanation—their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.	53b5f5e1d2bfeafa25688e1da02aae048936baf5	@['JournalArticle']{michaelov-etal-2021-different,  author = {J. Michaelov and Megan D. Bardolph and S. Coulson and B. Bergen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Different kinds of cognitive plausibility: why are transformers better than RNNs at predicting N400 amplitude?},  volume = {abs/2107.09648},  year = {2021} }
Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge	2021	http://www.semanticscholar.org/paper/47cc6e97d71317052672e82e42154707b4485b49	Assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language suggests that TLMs do not capture important aspects of event knowledge, and their predictions often depend on surface linguistic features, thereby showing sub-optimal generalization abilities.	maybe	9	Prior research has explored the ability of computational models to predict a word semantic fit with a given predicate. While much work has been devoted to modeling the typicality relation between verbs and arguments in isolation, in this paper we take a broader perspective by assessing whether and to what extent computational approaches have access to the information about the typicality of entire events and situations described in language (Generalized Event Knowledge). Given the recent success of Transformers Language Models (TLMs), we decided to test them on a benchmark for the dynamic estimation of thematic fit. The evaluation of these models was performed in comparison with SDM, a framework specifically designed to integrate events in sentence meaning representations, and we conducted a detailed error analysis to investigate which factors affect their behavior. Our results show that TLMs can reach performances that are comparable to those achieved by SDM. However, additional analysis consistently suggests that TLMs do not capture important aspects of event knowledge, and their predictions often depend on surface linguistic features, such as frequent words, collocations and syntactic patterns, thereby showing sub-optimal generalization abilities.	47cc6e97d71317052672e82e42154707b4485b49	@['JournalArticle']{pedinotti-etal-2021-did,  author = {Paolo Pedinotti and Giulia Rambelli and Emmanuele Chersoni and Enrico Santus and Alessandro Lenci and P. Blache},  booktitle = {STARSEM},  pages = {1-11},  title = {Did the Cat Drink the Coffee? Challenging Transformers with Generalized Event Knowledge},  year = {2021} }
Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI	2021	http://www.semanticscholar.org/paper/61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886	A diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI, which effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability.	yes	7	Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability.	61cd4ffdaf2c0daa3d432ff9fecdd064d6e72886	@['JournalArticle', 'Conference']{tian-etal-2021-diagnosing,  author = {Jidong Tian and Yitian Li and Wenqing Chen and Liqiang Xiao and Hao He and Yaohui Jin},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {3738-3747},  title = {Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI},  year = {2021} }
Developmental Negation Processing in Transformer Language Models	2022	http://www.semanticscholar.org/paper/ec08347f6002c3a07ca6d6a8f6536491551937c4		maybe	0	Reasoning using negation is known to be difficult for transformer-based language models. While previous studies have used the tools of psycholinguistics to probe a transformer’s ability to reason over negation, none have focused on the types of negation studied in developmental psychology. We explore how well transformers can process such categories of negation, by framing the problem as a natural language inference (NLI) task. We curate a set of diagnostic questions for our target categories from popular NLI datasets and evaluate how well a suite of models reason over them. We find that models perform consistently better only on certain categories, suggesting clear distinctions in how they are processed.	ec08347f6002c3a07ca6d6a8f6536491551937c4	@['JournalArticle', 'Conference']{laverghetta-licato-2022-developmental,  author = {A. Laverghetta and John Licato},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {545-551},  title = {Developmental Negation Processing in Transformer Language Models},  year = {2022} }
Detoxifying Language Models Risks Marginalizing Minority Voices	2021	http://www.semanticscholar.org/paper/4ae632b89089b38ce41d307a6cda4727e42aaab3	It is found that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups, and the tension between the controllability and distributional robustness of LMs is highlighted.	maybe	41	Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.	4ae632b89089b38ce41d307a6cda4727e42aaab3	@['JournalArticle', 'Conference']{xu-etal-2021-detoxifying,  author = {Albert Xu and Eshaan Pathak and Eric Wallace and Suchin Gururangan and Maarten Sap and D. Klein},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Detoxifying Language Models Risks Marginalizing Minority Voices},  volume = {abs/2104.06390},  year = {2021} }
Detecting Hate Speech with GPT-3	2021	http://www.semanticscholar.org/paper/098370508aaf56f718a472511987ac2072d0f917	It is shown that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning.	maybe	16	Sophisticated language models such as OpenAI’s GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We ﬁnd that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model’s accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.	098370508aaf56f718a472511987ac2072d0f917	@['JournalArticle']{chiu-alexander-2021-detecting,  author = {Ke-Li Chiu and Rohan Alexander},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Detecting Hate Speech with GPT-3},  volume = {abs/2103.12407},  year = {2021} }
Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases	2020	http://www.semanticscholar.org/paper/7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae	The first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities are presented.	maybe	76	With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that occur in contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations. GPT-2 contains the smallest magnitude of overall bias followed by GPT, BERT, and then ELMo, negatively correlating with the contextualization levels of the models. Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD achieves an accuracy of 81.6% and 82.7%, respectively, when detecting the intersectional biases of African American females and Mexican American females, where the random correct identification rates are 14.3% and 13.3%. EIBD reaches an accuracy of 84.7% and 65.3%, respectively, when detecting the emergent intersectional biases unique to African American females and Mexican American females, where the random correct identification rates are 9.2% and 6.1%. Our results indicate that intersectional biases associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.	7ea0e91c5d5dc73f2133bc46d7ebb6cb83034dae	@['Book', 'JournalArticle']{guo-caliskan-2020-detecting,  author = {W. Guo and Aylin Caliskan},  booktitle = {AAAI/ACM Conference on AI, Ethics, and Society},  journal = {Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society},  title = {Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases},  year = {2020} }
Detecting and Exorcising Statistical Demons from Language Models with Anti-Models of Negative Data	2020	http://www.semanticscholar.org/paper/5c070e2012b955a976ad91e877f487b17fd9d8ad	It is found that within a model family, as the number of parameters, training epochs, and data set size increase, so does a model's ability to generalize to negative n-gram data, indicating standard self-supervision generalizes too far.	maybe	0	It's been said that "Language Models are Unsupervised Multitask Learners." Indeed, self-supervised language models trained on "positive" examples of English text generalize in desirable ways to many natural language tasks. But if such models can stray so far from an initial self-supervision objective, a wayward model might generalize in undesirable ways too, say to nonsensical "negative" examples of unnatural language. A key question in this work is: do language models trained on (positive) training data also generalize to (negative) test data? We use this question as a contrivance to assess the extent to which language models learn undesirable properties of text, such as n-grams, that might interfere with the learning of more desirable properties of text, such as syntax. We find that within a model family, as the number of parameters, training epochs, and data set size increase, so does a model's ability to generalize to negative n-gram data, indicating standard self-supervision generalizes too far. We propose a form of inductive bias that attenuates such undesirable signals with negative data distributions automatically learned from positive data. We apply the method to remove n-gram signals from LSTMs and find that doing so causes them to favor syntactic signals, as demonstrated by large error reductions (up to 46% on the hardest cases) on a syntactic subject-verb agreement task.	5c070e2012b955a976ad91e877f487b17fd9d8ad	@['JournalArticle']{wick-etal-2020-detecting,  author = {Michael L. Wick and Kate Silverstein and Jean-Baptiste Tristan and Adam Craig Pocock and Mark Johnson},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Detecting and Exorcising Statistical Demons from Language Models with Anti-Models of Negative Data},  volume = {abs/2010.11855},  year = {2020} }
Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety	2022	http://www.semanticscholar.org/paper/7d5175db1b99552491063d2d9581b0b51e1d2932	This work provides a simple new prompting strategy that leads to yet another supposedly “super-human” result, this time out-performing humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset).	maybe	1	Large language models (LLMs) have exploded in popularity in the past few years and have achieved undeniably impressive results on benchmarks as varied as question answering and text summarization. We provide a simple new prompting strategy that leads to yet another supposedly “super-human” result, this time out-performing humans at common sense ethical reasoning (as measured by accuracy on a subset of the ETHICS dataset). Unfortunately, we ﬁnd that relying on average performance to judge capabilities can be highly misleading. LLM errors differ systematically from human errors in ways that make it easy to craft adversarial examples, or even perturb existing examples to ﬂip the output label. We also observe signs of inverse scaling with model size on some examples, and show that prompting models to “explain their reasoning” often leads to alarming justiﬁcations of unethical actions. Our results highlight how human-like performance does not necessarily imply human-like understanding or reasoning.	7d5175db1b99552491063d2d9581b0b51e1d2932	@['JournalArticle']{albrecht-etal-2022-despite,  author = {Joshua Albrecht and Ellie Kitanidis and Abraham J. Fetterman},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Despite "super-human" performance, current LLMs are unsuited for decisions about ethics and safety},  volume = {abs/2212.06295},  year = {2022} }
Designing Templates for Eliciting Commonsense Knowledge from Pretrained Sequence-to-Sequence Models	2020	http://www.semanticscholar.org/paper/67e5e67ac9fcfb241984d791c1d211a434901639	This work explores a template-based approach to extract implicit knowledge for commonsense reasoning on multiple-choice question answering tasks using the text-to-text transfer transformer (T5) model, and initiates further research to find generic natural language templates that can effectively leverage stored knowledge in pretrained models.	yes	3	While internalized “implicit knowledge” in pretrained transformers has led to fruitful progress in many natural language understanding tasks, how to most effectively elicit such knowledge remains an open question. Based on the text-to-text transfer transformer (T5) model, this work explores a template-based approach to extract implicit knowledge for commonsense reasoning on multiple-choice (MC) question answering tasks. Experiments on three representative MC datasets show the surprisingly good performance of our simple template, coupled with a logit normalization technique for disambiguation. Furthermore, we verify that our proposed template can be easily extended to other MC tasks with contexts such as supporting facts in open-book question answering settings. Starting from the MC task, this work initiates further research to find generic natural language templates that can effectively leverage stored knowledge in pretrained models.	67e5e67ac9fcfb241984d791c1d211a434901639	@['JournalArticle', 'Conference']{yang-etal-2020-designing,  author = {Jheng-Hong Yang and Sheng-Chieh Lin and Rodrigo Nogueira and Ming-Feng Tsai and Chuan-Ju Wang and Jimmy J. Lin},  booktitle = {International Conference on Computational Linguistics},  pages = {3449-3453},  title = {Designing Templates for Eliciting Commonsense Knowledge from Pretrained Sequence-to-Sequence Models},  year = {2020} }
Designing and Interpreting Probes with Control Tasks	2019	http://www.semanticscholar.org/paper/199ff73d2f728e997f860b62a2322823d3e3d9e8	Control tasks, which associate word types with random outputs, are proposed to complement linguistic tasks, and it is found that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective.	maybe	322	Probes, supervised models trained to predict properties (like parts-of-speech) from representations (like ELMo), have achieved high accuracy on a range of linguistic tasks. But does this mean that the representations encode linguistic structure or just that the probe has learned the linguistic task? In this paper, we propose control tasks, which associate word types with random outputs, to complement linguistic tasks. By construction, these tasks can only be learned by the probe itself. So a good probe, (one that reflects the representation), should be selective, achieving high linguistic task accuracy and low control task accuracy. The selectivity of a probe puts linguistic task accuracy in context with the probe’s capacity to memorize from word types. We construct control tasks for English part-of-speech tagging and dependency edge prediction, and show that popular probes on ELMo representations are not selective. We also find that dropout, commonly used to control probe complexity, is ineffective for improving selectivity of MLPs, but that other forms of regularization are effective. Finally, we find that while probes on the first layer of ELMo yield slightly better part-of-speech tagging accuracy than the second, probes on the second layer are substantially more selective, which raises the question of which layer better represents parts-of-speech.	199ff73d2f728e997f860b62a2322823d3e3d9e8	@['JournalArticle', 'Conference']{hewitt-liang-2019-designing,  author = {John Hewitt and Percy Liang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Designing and Interpreting Probes with Control Tasks},  volume = {abs/1909.03368},  year = {2019} }
Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings	2020	http://www.semanticscholar.org/paper/81eb8a702308c213db803ea50dd5451f83866ae6	This paper demonstrates that Binder features can be derived from the BERT embedding space and provides two things; semantic feature values derived from contextualised word embeddings and insights into how semantic features are represented across the different layers of the Bert model.	maybe	8	Models based on the transformer architecture, such as BERT, have marked a crucial step forward in the field of Natural Language Processing. Importantly, they allow the creation of word embeddings that capture important semantic information about words in context. However, as single entities, these embeddings are difficult to interpret and the models used to create them have been described as opaque. Binder and colleagues proposed an intuitive embedding space where each dimension is based on one of 65 core semantic features. Unfortunately, the space only exists for a small data-set of 535 words, limiting its uses. Previous work (Utsumi, 2018, 2020; Turton et al., 2020) has shown that Binder features can be derived from static embeddings and successfully extrapolated to a large new vocabulary. Taking the next step, this paper demonstrates that Binder features can be derived from the BERT embedding space. This provides two things; (1) semantic feature values derived from contextualised word embeddings and (2) insights into how semantic features are represented across the different layers of the BERT model.	81eb8a702308c213db803ea50dd5451f83866ae6	@['JournalArticle']{turton-etal-2020-deriving,  author = {Jacob Turton and D. Vinson and Robert Smith},  booktitle = {Workshop on Representation Learning for NLP},  pages = {248-262},  title = {Deriving Contextualised Semantic Features from BERT (and Other Transformer Model) Embeddings},  year = {2020} }
Deriving Behavioral Tests from Common Sense Knowledge Graphs	2020	http://www.semanticscholar.org/paper/a67229eee83e6502c973d0e82d238ceb16632b35	This work introduces a semi-automated approach that leverages CSKGs to construct out-of-domain evaluation sets for NLP tasks that are more scalable than purely manual approaches.	maybe	0	Although NLP models have demonstrated “superhuman” performance on common sense reasoning tasks, it is unclear whether these models truly have common sense knowledge. Constructing evaluation datasets to test this knowledge is expensive due to the manual effort involved, and is also limited in scope. Meanwhile, common sense knowledge graphs (CSKGs) aim for a wide coverage of structured common sense knowledge, but can not be directly used for testing purposes. In this work, we introduce a semi-automated approach that leverages CSKGs to construct out-of-domain evaluation sets for NLP tasks that are more scalable than purely manual approaches. Using this procedure, we create test cases from two popular CSKGs—ConceptNet and ATOMIC—to test the common sense reasoning capability of models trained for natural language inference (NLI) and question answering (QA). These tests reveal interesting differences in failure modes of these models; models trained on NLI tend to perform better on tests of ontological knowledge, e.g. ’is a’ and ’used for’ relations, failing on tests that require understanding ’desires’, ’needs’, and ’wants’, while QA models perform better on tests that involve ’wants’, and ’desires’.	a67229eee83e6502c973d0e82d238ceb16632b35	@None{razeghi-etal-2020-deriving,  author = {Yasaman Razeghi and Robert L Logan IV and Sameer Singh},  title = {Deriving Behavioral Tests from Common Sense Knowledge Graphs},  year = {2020} }
Demystifying Prompts in Language Models via Perplexity Estimation	2022	http://www.semanticscholar.org/paper/d03a9b2a0e090cc9fd2ba0a457ecea35372f1018	A method is devised to automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and choose the lowest perplexity prompts to get gains in performance, showing that the lower the perplexity of the prompt is, the better the Prompt is able to perform the task.	maybe	2	Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies sig-niﬁcantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get signiﬁcant gains in performance.	d03a9b2a0e090cc9fd2ba0a457ecea35372f1018	@['JournalArticle']{gonen-etal-2022-demystifying,  author = {Hila Gonen and Srini Iyer and Terra Blevins and Noah A. Smith and Luke Zettlemoyer},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Demystifying Prompts in Language Models via Perplexity Estimation},  volume = {abs/2212.04037},  year = {2022} }
Demystifying Neural Language Models' Insensitivity to Word-Order	2021	http://www.semanticscholar.org/paper/e5f6506f9332fcdb574f13a791e4f3c42b80ca90	The insensitivity of natural language models to word-order is investigated by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark and it is found that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more than the global ordering of tokens.	maybe	9	Recent research analyzing the sensitivity of natural language understanding models to word-order perturbations have shown that the state-of-the-art models in several language tasks may have a unique way to understand the text that could seldom be explained with conventional syntax and semantics. In this paper, we investigate the insensitivity of natural language models to word-order by quantifying perturbations and analysing their effect on neural models’ performance on language understanding tasks in GLUE benchmark. Towards that end, we propose two metrics — the Direct Neighbour Displacement (DND) and the Index Displacement Count (IDC) — that score the local and global ordering of tokens in the perturbed texts and observe that perturbation functions found in prior literature affect only the global ordering while the local ordering remains relatively unperturbed. We propose perturbations at the granularity of subwords and characters to study the correlation between DND, IDC and the performance of neural language models on natural language tasks. We find that neural language models — pretrained and non-pretrained Transformers, LSTMs, and Convolutional architectures — require local ordering more so than the global ordering of tokens. The proposed metrics and the suite of perturbations allow a systematic way to study the (in)sensitivity of neural language understanding models to varying degree of perturbations.	e5f6506f9332fcdb574f13a791e4f3c42b80ca90	@['JournalArticle']{clouâtre-etal-2021-demystifying,  author = {Louis Clouâtre and Prasanna Parthasarathi and A. Zouaq and Sarath Chandar},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Demystifying Neural Language Models' Insensitivity to Word-Order},  volume = {abs/2107.13955},  year = {2021} }
Delphi: Towards Machine Ethics and Norms	2021	http://www.semanticscholar.org/paper/98eb27ccd9f0875e6e3a350a8a238dc96373a504	The first major attempt to computationally explore the vast space of moral implications in real-world settings is conducted, with Delphi, a unified model of descriptive ethics empowered by diverse data of people’s moral judgment from COMMONSENSE NORM BANK.	maybe	55	Failing to account for moral norms could notably hinder AI systems’ ability to interact with people. AI systems empirically require social, cultural, and ethical norms to make moral judgments. However, open-world situations with different groundings may shift moral implications significantly. For example, while “driving my friend to the airport” is “good”, “driving my friend to the airport with a car I stole” is “not okay.” In natural language processing, machine moral reasoning is still in a preliminary stage, illuminating the importance of research on steering machines to making ethical judgments. Inspired by descriptive ethics, a line of research on morality focusing on people’s moral judgments relevant to everyday situations, we conduct the first major attempt to computationally explore the vast space of moral implications in real-world settings. We introduce COMMONSENSE NORM BANK, a semiautomatically constructed dataset from several sources (e.g., SOCIAL CHEMISTRY) with 1.7M instances of descriptive ethics, covering a wide spectrum of everyday situations in contextualized, narrative, and sociallyor demographicallybiased settings. We present Delphi, a unified model of descriptive ethics empowered by diverse data of people’s moral judgment from COMMONSENSE NORM BANK. Delphi is robust to generate categorical and/or open-text moral judgments (e.g., “it’s dangerous”) for complex real-life situations (e.g., “driving my friend to the airport early in the morning when I was drunk last night”). Delphi demonstrates highly promising empirical results, with 92.1% accuracy, which outperforms the out-ofthe-box GPT-3 model with extensive prompting by a significant margin (83.9%) . We also provide careful study of Delphi’s limitations, particularly with respect to undesirable biases against underrepresented population, opening doors to further investigation in future research in computational moral reasoning. Closing the gap between machines and people’s moral reasoning is a prerequisite for trustworthy open-world AI deployments. Moral judgment is never simplistic as there can be clash of different ethical/cultural values at play. Thus, developing high-quality corpus of people’s ethical judgment over diverse scenarios is needed to teach machines to make moral judgment. With optimistic promises demonstrated by Delphi, we inspire significant future research in this next frontier of AI, to facilitate reliable, socially aware, and ethically-informed future AI practices.	98eb27ccd9f0875e6e3a350a8a238dc96373a504	@['JournalArticle']{jiang-etal-2021-delphi:,  author = {Liwei Jiang and Jena D. Hwang and Chandrasekhar Bhagavatula and Ronan Le Bras and Maxwell Forbes and Jon Borchardt and Jenny Liang and Oren Etzioni and Maarten Sap and Yejin Choi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Delphi: Towards Machine Ethics and Norms},  volume = {abs/2110.07574},  year = {2021} }
Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding	2019	https://www.semanticscholar.org/paper/3ee7b17cc627ac5bc99632a22ef820dc559393e6	This work deepens the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer.	seed	5	Transformer-based pre-trained language models have proven to be effective for learning contextualized language representation. However, current approaches only take advantage of the output of the encoder's final layer when fine-tuning the downstream tasks. We argue that only taking single layer's output restricts the power of pre-trained representation. Thus we deepen the representation learned by the model by fusing the hidden representation in terms of an explicit HIdden Representation Extractor (HIRE), which automatically absorbs the complementary representation with respect to the output from the final layer. Utilizing RoBERTa as the backbone encoder, our proposed improvement over the pre-trained models is shown effective on multiple natural language understanding tasks and help our model rival with the state-of-the-art models on the GLUE benchmark.	3ee7b17cc627ac5bc99632a22ef820dc559393e6	@['JournalArticle']{yang-zhao-2019-deepening,  author = {Jie Yang and Hai Zhao},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding},  volume = {abs/1911.01940},  year = {2019} }
Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT	2021	http://www.semanticscholar.org/paper/4548c8e706599f71fdcaa1bb7b278ef07e6e5d69	This work investigates how Multilingual BERT encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment is manifested across the embedding spaces of different languages, and finds that features such as passive voice, animacy and case strongly correlate with classification decisions.	maybe	15	We investigate how Multilingual BERT (mBERT) encodes grammar by examining how the high-order grammatical feature of morphosyntactic alignment (how different languages define what counts as a “subject”) is manifested across the embedding spaces of different languages. To understand if and how morphosyntactic alignment affects contextual embedding spaces, we train classifiers to recover the subjecthood of mBERT embeddings in transitive sentences (which do not contain overt information about morphosyntactic alignment) and then evaluate them zero-shot on intransitive sentences (where subjecthood classification depends on alignment), within and across languages. We find that the resulting classifier distributions reflect the morphosyntactic alignment of their training languages. Our results demonstrate that mBERT representations are influenced by high-level grammatical features that are not manifested in any one input sentence, and that this is robust across languages. Further examining the characteristics that our classifiers rely on, we find that features such as passive voice, animacy and case strongly correlate with classification decisions, suggesting that mBERT does not encode subjecthood purely syntactically, but that subjecthood embedding is continuous and dependent on semantic and discourse factors, as is proposed in much of the functional linguistics literature. Together, these results provide insight into how grammatical features manifest in contextual embedding spaces, at a level of abstraction not covered by previous work.	4548c8e706599f71fdcaa1bb7b278ef07e6e5d69	@['JournalArticle', 'Conference']{papadimitriou-etal-2021-deep,  author = {Isabel Papadimitriou and Ethan A. Chi and Richard Futrell and Kyle Mahowald},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {2522-2532},  title = {Deep Subjecthood: Higher-Order Grammatical Features in Multilingual BERT},  year = {2021} }
Deep Learning Models to Study Sentence Comprehension in the Human Brain	2023	http://www.semanticscholar.org/paper/b6af6efff6b58ba2c39f17225d25ea7f5566aaa1		maybe			b6af6efff6b58ba2c39f17225d25ea7f5566aaa1	
Deep learning can contrast the minimal pairs of syntactic data*9	2021	http://www.semanticscholar.org/paper/d98a3f94041d9e8d8216bd313be8ab587c840d38		maybe	2	Park, Kwonsik, Myung-Kwan Park, and Sanghoun Song. 2021. Deep learning can contrast the minimal pairs of syntactic data. Linguistic Research 38(2): 395-424. The present work aims to assess the feasibility of using deep learning as a useful tool to investigate syntactic phenomena. To this end, the present study concerns three research questions: (i) whether deep learning can detect syntactically inappropriate constructions, (ii) whether deep learning’s acceptability judgments are accountable, and (iii) whether deep learning’s aspects of acceptability judgments are similar to human judgments. As a proxy for a deep learning language model, this study chooses BERT. The current paper comprises syntactically contrasted pairs of English sentences which come from the three test suites already available. The first one is 196 grammatical –ungrammatical minimal pairs from DeKeyser (2000). The second one is examples in four published syntax textbooks excerpted from Warstadt et al. (2019). The last one is extracted from Sprouse et al. (2013), which collects the examples reported in a theoretical linguistics journal, Linguistic Inquiry. The BERT models, base BERT and large BERT, are assessed by judging acceptability of items in the test suites with an evaluation metric, surprisal, which is used to measure how ‘surprised’ a model is when encountering a word in a sequence of words, i.e., a sentence. The results are analyzed in the two frameworks: directionality and repulsion. The results of directionality reveals that the two versions of BERT are overall competent at distinguishing ungrammatical sentences from grammatical ones. The statistical results of both repulsion and directionality also reveal that the two variants of BERT do not differ significantly. Regarding repulsion, correct judgments and incorrect ones are significantly different. Additionally, the repulsion of the first test suite, which is excerpted from the items for testing learners’ grammaticality judgments, is higher than the other test suites, which are excerpted from the syntax textbooks and published literature. This study compares BERT’s acceptability judgments with magnitude estimation results reported in Sprouse et al. (2013) in order to examine if deep learning’s syntactic knowledge is akin to human knowledge. The error analyses on incorrectly judged items reveal that there are some syntactic constructions that the two BERTs have trouble learning, which indicates that BERT’s acceptability judgments are distributed not randomly. (Korea University · Dongguk University)	d98a3f94041d9e8d8216bd313be8ab587c840d38	@None{park-etal-2021-deep,  author = {Kwonsik Park and Myungkwan Park and Sanghoun Song},  title = {Deep learning can contrast the minimal pairs of syntactic data*9},  year = {2021} }
Deep Contextualized Word Embeddings for Universal Dependency Parsing	2019	http://www.semanticscholar.org/paper/af42351e6b64350d85fa53e373aa86282c07756e	Based on ELMo’s advantage on OOV, experiments that simulate low-resource settings are conducted and the results show that deep contextualized word embeddings are effective for data-insufficient tasks where the OOV problem is severe.	maybe	4	Deep contextualized word embeddings (Embeddings from Language Model, short for ELMo), as an emerging and effective replacement for the static word embeddings, have achieved success on a bunch of syntactic and semantic NLP problems. However, little is known about what is responsible for the improvements. In this article, we focus on the effect of ELMo for a typical syntax problem—universal POS tagging and dependency parsing. We incorporate ELMo as additional word embeddings into the state-of-the-art POS tagger and dependency parser, and it leads to consistent performance improvements. Experimental results show the model using ELMo outperforms the state-of-the-art baseline by an average of 0.91 for POS tagging and 1.11 for dependency parsing. Further analysis reveals that the improvements mainly result from the ELMo’s better abstraction ability on the out-of-vocabulary (OOV) words, and the character-level word representation in ELMo contributes a lot to the abstraction. Based on ELMo’s advantage on OOV, experiments that simulate low-resource settings are conducted and the results show that deep contextualized word embeddings are effective for data-insufficient tasks where the OOV problem is severe.	af42351e6b64350d85fa53e373aa86282c07756e	@['JournalArticle']{liu-etal-2019-deep,  author = {Yijia Liu and Wanxiang Che and Yuxuan Wang and Bo Zheng and Bing Qin and Ting Liu},  booktitle = {ACM Trans. Asian Low Resour. Lang. Inf. Process.},  journal = {ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)},  pages = {1 - 17},  title = {Deep Contextualized Word Embeddings for Universal Dependency Parsing},  volume = {19},  year = {2019} }
Deep Clustering of Text Representations for Supervision-Free Probing of Syntax	2020	http://www.semanticscholar.org/paper/c38184c7ed9d798c83dbb48c8231e5a950a9b420	This work explores deep clustering of multilingual text representations for unsupervised model interpretation and induction of syntax and finds that Multilingual BERT (mBERT) contains surprising amount of syntactic knowledge of English; possibly even as much as English Bert (E-BERT).	maybe	3	We explore deep clustering of multilingual text representations for unsupervised model interpretation and induction of syntax. As these representations are high-dimensional, out-of-the-box methods like K-means do not work well. Thus, our approach jointly transforms the representations into a lower-dimensional cluster-friendly space and clusters them. We consider two notions of syntax: Part of Speech Induction (POSI) and Constituency Labelling (CoLab) in this work. Interestingly, we find that Multilingual BERT (mBERT) contains surprising amount of syntactic knowledge of English; possibly even as much as English BERT (E-BERT). Our model can be used as a supervision-free probe which is arguably a less-biased way of probing. We find that unsupervised probes show benefits from higher layers as compared to supervised probes. We further note that our unsupervised probe utilizes E-BERT and mBERT representations differently, especially for POSI. We validate the efficacy of our probe by demonstrating its capabilities as a unsupervised syntax induction technique. Our probe works well for both syntactic formalisms by simply adapting the input representations. We report competitive performance of our probe on 45-tag English POSI, state-of-the-art performance on 12-tag POSI across 10 languages, and competitive results on CoLab. We also perform zero-shot syntax induction on resource impoverished languages and report strong results.	c38184c7ed9d798c83dbb48c8231e5a950a9b420	@['JournalArticle', 'Conference']{gupta-etal-2020-deep,  author = {Vikram Gupta and Haoyue Shi and Kevin Gimpel and Mrinmaya Sachan},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {10720-10728},  title = {Deep Clustering of Text Representations for Supervision-Free Probing of Syntax},  year = {2020} }
Deduplicating Training Data Mitigates Privacy Risks in Language Models	2022	http://www.semanticscholar.org/paper/55c36748f2a7c060c3313349c730b053ed03fbf7	It is shown that the rate at which language models regenerate training sequences is superlinearly related to a sequence’s count in the training set, and that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks.	maybe	26	Past work has shown that large language models are susceptible to privacy attacks, where adversaries generate sequences from a trained model and detect which sequences are memorized from the training set. In this work, we show that the success of these attacks is largely due to duplication in commonly used web-scraped training sets. We ﬁrst show that the rate at which language models regenerate training sequences is superlinearly related to a sequence’s count in the training set. For instance, a sequence that is present 10 times in the training data is on average generated ∼ 1000 × more often than a sequence that is present only once. We next show that existing methods for detecting memorized sequences have near-chance accuracy on non-duplicated training sequences. Finally, we ﬁnd that after applying methods to deduplicate training data, language models are considerably more secure against these types of privacy attacks. Taken together, our results motivate an increased focus on deduplication in privacy-sensitive applications and a reevalua-tion of the practicality of existing privacy attacks.	55c36748f2a7c060c3313349c730b053ed03fbf7	@['JournalArticle', 'Conference']{kandpal-etal-2022-deduplicating,  author = {Nikhil Kandpal and Eric Wallace and Colin Raffel},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Deduplicating Training Data Mitigates Privacy Risks in Language Models},  volume = {abs/2202.06539},  year = {2022} }
Deduplicating Training Data Makes Language Models Better	2021	http://www.semanticscholar.org/paper/4566c0d22ebf3c31180066ab23b6c445aeec78d5	Two tools are developed that allow us to deduplicate training datasets and train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.	maybe	89	We find that existing language modeling datasets contain many near-duplicate examples and long repetitive substrings.As a result, over 1% of the unprompted output of language models trained on these datasets is copied verbatim from the training data.We develop two tools that allow us to deduplicate training datasets—for example removing from C4 a single 61 word English sentence that is repeated over 60,000 times.Deduplication allows us to train models that emit memorized text ten times less frequently and require fewer training steps to achieve the same or better accuracy.We can also reduce train-test overlap, which affects over 4% of the validation set of standard datasets, thus allowing for more accurate evaluation.Code for deduplication is released at https://github.com/google-research/deduplicate-text-datasets.	4566c0d22ebf3c31180066ab23b6c445aeec78d5	@['JournalArticle', 'Conference']{lee-etal-2021-deduplicating,  author = {Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and D. Eck and Chris Callison-Burch and Nicholas Carlini},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {8424-8445},  title = {Deduplicating Training Data Makes Language Models Better},  year = {2021} }
Decoding semantic representations in mind and brain	2023	http://www.semanticscholar.org/paper/3ea1b49b6c64955fd53c4182b1467614bdc35c9c	The analysis suggests why the results are heterogeneous and identifies crucial links between cognitive theory, data collection, and analysis that can help to better connect neuroimaging to mechanistic theories of semantic cognition.	maybe	0		3ea1b49b6c64955fd53c4182b1467614bdc35c9c	@['Review', 'JournalArticle']{frisby-etal-2023-decoding,  author = {Saskia L. Frisby and A. Halai and Christopher R. Cox and M. L. Lambon Ralph and T. Rogers},  booktitle = {Trends in Cognitive Sciences},  journal = {Trends in cognitive sciences},  title = {Decoding semantic representations in mind and brain},  year = {2023} }
Debiasing Methods in Natural Language Understanding Make Bias More Accessible	2021	http://www.semanticscholar.org/paper/10bc2ba3533bca85b75cb09dcc100809fc3221ea	This work proposes a general probing-based framework that allows for post-hoc interpretation of biases in language models, and uses an information-theoretic approach to measure the extractability of certain biases from the model’s representations.	maybe	7	Model robustness to bias is often determined by the generalization on carefully designed out-of-distribution datasets. Recent debiasing methods in natural language understanding (NLU) improve performance on such datasets by pressuring models into making unbiased predictions. An underlying assumption behind such methods is that this also leads to the discovery of more robust features in the model’s inner representations. We propose a general probing-based framework that allows for post-hoc interpretation of biases in language models, and use an information-theoretic approach to measure the extractability of certain biases from the model’s representations. We experiment with several NLU datasets and known biases, and show that, counter-intuitively, the more a language model is pushed towards a debiased regime, the more bias is actually encoded in its inner representations.	10bc2ba3533bca85b75cb09dcc100809fc3221ea	@['JournalArticle', 'Conference']{mendelson-belinkov-2021-debiasing,  author = {M. Mendelson and Yonatan Belinkov},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1545-1557},  title = {Debiasing Methods in Natural Language Understanding Make Bias More Accessible},  year = {2021} }
Deanthropomorphising NLP: Can a Language Model Be Conscious?	2022	http://www.semanticscholar.org/paper/2c43ef2d8e44d055b61eecddf323a3412007cef8	It is taken that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it, and the necessary background in language modelling is presented.	maybe	0	This work is intended as a voice in the discussion over the recent claims that LaMDA, a pretrained language model based on the Transformer model architecture, is sentient. This claim, if conﬁrmed, would have serious ramiﬁcations in the Natural Language Processing (NLP) community due to wide-spread use of similar models. However, here we take the position that such a language model cannot be sentient, or conscious, and that LaMDA in particular exhibits no advances over other similar models that would qualify it. We jus-tify this by analysing the Transformer architecture through Integrated Information Theory. We see the claims of consciousness as part of a wider tendency to use anthropomorphic language in NLP reporting. Regard-less of the veracity of the claims, we consider this an opportune moment to take stock of progress in language modelling and consider the ethical implications of the task. In order to make this work helpful for readers outside the NLP community, we also present the necessary background in language modelling.	2c43ef2d8e44d055b61eecddf323a3412007cef8	@['JournalArticle']{shardlow-przybyła-2022-deanthropomorphising,  author = {M. Shardlow and Piotr Przybyła},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Deanthropomorphising NLP: Can a Language Model Be Conscious?},  volume = {abs/2211.11483},  year = {2022} }
Dataset Reconstruction Attack against Language Models	2021	http://www.semanticscholar.org/paper/7e3e7e179e3b5d4181b283a98cb6caa63b33a400	This work proposes a novel data reconstruction attack that also infers the informative words present in the private dataset and shows that an adversary with black-box query access to a fine-tuned language model can infer the informative Words with an accuracy of about 75% and can reconstruct nearly 46.67% of the sentences in the public dataset.	maybe	2	With the advances of deep learning techniques in Natural Language Processing, the last few years have witnessed releases of powerful language models such as BERT and GPT-2. However, applying these general-purpose language models to domain-specific applications requires further fine-tuning using domain-specific private data. Since private data is mostly confidential, information that can be extracted by an adversary with access to the models can lead to serious privacy risks. The majority of privacy attacks on language models infer either targeted information or a few instances from the training dataset. However, inferring the whole training dataset has not been explored in depth which poses far greater risks than disclosure of some instances or partial information of the training data. In this work, we propose a novel data reconstruction attack that also infers the informative words present in the private dataset. Experiment results show that an adversary with black-box query access to a fine-tuned language model can infer the informative words with an accuracy of about 75% and can reconstruct nearly 46.67% of the sentences in the private dataset.	7e3e7e179e3b5d4181b283a98cb6caa63b33a400	@None{panchendrarajan-bhoi-2021-dataset,  author = {R. Panchendrarajan and Suman Bhoi},  title = {Dataset Reconstruction Attack against Language Models},  year = {2021} }
Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics	2020	http://www.semanticscholar.org/paper/ee5fff85d3ec62698eddba162f054b7e73670b2a	The results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization, and a model-based tool to characterize and diagnose datasets.	maybe	154	Large datasets have become commonplace in NLP research. However, the increased emphasis on data quantity has made it challenging to assess the quality of data. We introduce Data Maps---a model-based tool to characterize and diagnose datasets. We leverage a largely ignored source of information: the behavior of the model on individual instances during training (training dynamics) for building data maps. This yields two intuitive measures for each example---the model's confidence in the true class, and the variability of this confidence across epochs---obtained in a single run of training. Experiments across four datasets show that these model-dependent measures reveal three distinct regions in the data map, each with pronounced characteristics. First, our data maps show the presence of "ambiguous" regions with respect to the model, which contribute the most towards out-of-distribution generalization. Second, the most populous regions in the data are "easy to learn" for the model, and play an important role in model optimization. Finally, data maps uncover a region with instances that the model finds "hard to learn"; these often correspond to labeling errors. Our results indicate that a shift in focus from quantity to quality of data could lead to robust models and improved out-of-distribution generalization.	ee5fff85d3ec62698eddba162f054b7e73670b2a	@['JournalArticle', 'Conference']{swayamdipta-etal-2020-dataset,  author = {Swabha Swayamdipta and Roy Schwartz and Nicholas Lourie and Yizhong Wang and Hannaneh Hajishirzi and Noah A. Smith and Yejin Choi},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {9275-9293},  title = {Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics},  year = {2020} }
Dataless Knowledge Fusion by Merging Weights of Language Models	2022	http://www.semanticscholar.org/paper/f0c90c2cce891ff458620f349d92b10f02835489	This paper proposes a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models and finds that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling.	maybe	0	Fine-tuning pre-trained language models has become the prevalent paradigm for building downstream NLP models. Oftentimes fine-tuned models are readily available but their training data is not, due to data privacy or intellectual property concerns. This creates a barrier to fusing knowledge across individual models to yield a better single model. In this paper, we study the problem of merging individual models built on different training data sets to obtain a single model that performs well both across all data set domains and can generalize on out-ofdomain data. We propose a dataless knowledge fusion method that merges models in their parameter space, guided by weights that minimize prediction differences between the merged model and the individual models. Over a battery of evaluation settings, we show that the proposed method significantly outperforms baselines such as Fisher-weighted averaging or model ensembling. Further, we find that our method is a promising alternative to multi-task learning that can preserve or sometimes improve over the individual models without access to the training data. Finally, model merging is more efficient than training a multi-task model, thus making it applicable to a wider set of scenarios.	f0c90c2cce891ff458620f349d92b10f02835489	@['JournalArticle']{jin-etal-2022-dataless,  author = {Xisen Jin and Xiang Ren and Daniel Preotiuc-Pietro and Pengxiang Cheng},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Dataless Knowledge Fusion by Merging Weights of Language Models},  volume = {abs/2212.09849},  year = {2022} }
Data-Efficient Double-Win Lottery Tickets from Robust Pre-training	2022	http://www.semanticscholar.org/paper/637e093fb97863fc9bd4b1b69722bbe70804e4e0	This paper designs a more rigorous concept, Double-Win Lottery Tickets, in which a located subnetwork from a pre-trained model can be independently transferred on diverse downstream tasks, to reach BOTH the same standard and robust generalization, under BOTH standard and adversarial training regimes, as the full pre- trained model can do.	maybe	1	Pre-training serves as a broadly adopted starting point for transfer learning on various downstream tasks. Recent investigations of lottery tickets hypothesis (LTH) demonstrate such enormous pre-trained models can be replaced by extremely sparse subnetworks (a.k.a. matching subnetworks ) without sacrificing transferability. However, practical security-crucial applications usually pose more challenging requirements beyond standard transfer, which also demand these subnetworks to overcome adversarial vulnerability. In this paper, we formulate a more rigorous concept, Double-Win Lottery Tickets , in which a located subnetwork from a pre-trained model can be independently transferred on diverse downstream tasks, to reach BOTH the same standard and robust generalization, under BOTH standard and adversarial training regimes, as the full pre-trained model can do. We comprehensively examine various pre-training mechanisms and find that robust pretraining tends to craft sparser double-win lottery tickets with superior performance over the standard counterparts. For example, on downstream CIFAR-10/100 datasets, we identify double-win matching subnetworks with the standard, fast adversarial, and adversarial pre-training from ImageNet, at 89 . 26% / 73 . 79% , 89 . 26% / 79 . 03% , and 91 . 41% / 83 . 22% sparsity, respectively. Further-more, we observe the obtained double-win lottery tickets can be more data-efficient to transfer, under practical data-limited (e.g., 1% and 10% ) downstream schemes. Our results show that the benefits from robust pre-training are am-plified by the lottery ticket scheme, as well as the data-limited transfer setting. Codes are available at https://github.com/VITA-Group/ Double-Win-LTH . / fine-tuning paradigm. In this paper, we take a leap further to meet more practical requirements by designing the concept of double-win tickets. It examines the transferability across different downstream training regimes, including standard and adversarial transfer, data-rich and data-scarce transfer. To our best knowledge, this training schemes transfer has never been explored in the LTH literature, offering a new view to analyze beneficial properties of pre-trained tickets.	637e093fb97863fc9bd4b1b69722bbe70804e4e0	@['JournalArticle', 'Conference']{chen-etal-2022-data,  author = {Tianlong Chen and Zhenyu (Allen) Zhang and Sijia Liu and Yang Zhang and Shiyu Chang and Zhangyang Wang},  booktitle = {International Conference on Machine Learning},  pages = {3747-3759},  title = {Data-Efficient Double-Win Lottery Tickets from Robust Pre-training},  year = {2022} }
Data-driven models and computational tools for neurolinguistics: a language technology perspective	2020	http://www.semanticscholar.org/paper/dd436969d10131f812791e282a13991e66ae1cec	A review of brain imaging-based neurolinguistic studies with a focus on the natural language representations, such as word embeddings and pre-trained language models is presented.	maybe	2	In this paper, our focus is the connection and influence of language technologies on the research in neurolinguistics. We present a review of brain imaging-based neurolinguistic studies with a focus on the natural language representations, such as word embeddings and pre-trained language models. Mutual enrichment of neurolinguistics and language technologies leads to development of brain-aware natural language representations. The importance of this research area is emphasized by medical applications.	dd436969d10131f812791e282a13991e66ae1cec	@['JournalArticle', 'Review']{artemova-etal-2020-data,  author = {E. Artemova and Amir Bakarov and A. Artemov and E. Burnaev and M. Sharaev},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Data-driven models and computational tools for neurolinguistics: a language technology perspective},  volume = {abs/2003.10540},  year = {2020} }
Cyberbullying Classifiers are Sensitive to Model-Agnostic Perturbations	2022	http://www.semanticscholar.org/paper/3ef57a781e54b81d42d1df5796d3477c771880d4	It is demonstrated that model-agnostic lexical substitutions significantly hurt classifier performance and augmentation for cyberbullying detection, and models become robust against word-level perturbations at a slight trade-off in overall task performance.	maybe	3	A limited amount of studies investigates the role of model-agnostic adversarial behavior in toxic content classification. As toxicity classifiers predominantly rely on lexical cues, (deliberately) creative and evolving language-use can be detrimental to the utility of current corpora and state-of-the-art models when they are deployed for content moderation. The less training data is available, the more vulnerable models might become. This study is, to our knowledge, the first to investigate the effect of adversarial behavior and augmentation for cyberbullying detection. We demonstrate that model-agnostic lexical substitutions significantly hurt classifier performance. Moreover, when these perturbed samples are used for augmentation, we show models become robust against word-level perturbations at a slight trade-off in overall task performance. Augmentations proposed in prior work on toxicity prove to be less effective. Our results underline the need for such evaluations in online harm areas with small corpora.	3ef57a781e54b81d42d1df5796d3477c771880d4	@['JournalArticle']{emmery-etal-2022-cyberbullying,  author = {Chris Emmery and 'Akos K'ad'ar and Grzegorz Chrupała and Walter Daelemans},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {2976-2988},  title = {Cyberbullying Classifiers are Sensitive to Model-Agnostic Perturbations},  year = {2022} }
CxGBERT: BERT meets Construction Grammar	2020	http://www.semanticscholar.org/paper/051723a228508ae8a2e0374b01651d74b078b646	The results allow us to conclude that BERT does indeed have access to a significant amount of information, much of which linguists typically call constructional information, and provides insights into what deep learning methods learn from text, while also showing that information contained in constructions is redundantly encoded in lexico-semantics.	maybe	10	While lexico-semantic elements no doubt capture a large amount of linguistic information, it has been argued that they do not capture all information contained in text. This assumption is central to constructionist approaches to language which argue that language consists of constructions, learned pairings of a form and a function or meaning that are either frequent or have a meaning that cannot be predicted from its component parts. BERT’s training objectives give it access to a tremendous amount of lexico-semantic information, and while BERTology has shown that BERT captures certain important linguistic dimensions, there have been no studies exploring the extent to which BERT might have access to constructional information. In this work we design several probes and conduct extensive experiments to answer this question. Our results allow us to conclude that BERT does indeed have access to a significant amount of information, much of which linguists typically call constructional information. The impact of this observation is potentially far-reaching as it provides insights into what deep learning methods learn from text, while also showing that information contained in constructions is redundantly encoded in lexico-semantics.	051723a228508ae8a2e0374b01651d74b078b646	@['JournalArticle', 'Conference']{madabushi-etal-2020-cxgbert:,  author = {Harish Tayyar Madabushi and Laurence Romain and Dagmar Divjak and P. Milin},  booktitle = {International Conference on Computational Linguistics},  pages = {4020-4032},  title = {CxGBERT: BERT meets Construction Grammar},  year = {2020} }
Customizing Triggers with Concealed Data Poisoning	2020	http://www.semanticscholar.org/paper/db500c4e746897e5d5adafbf222b959c512445ad	This work develops a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input.	maybe	11	Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains "James Bond". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling ("Apple iPhone" triggers negative generations) and machine translation ("iced coffee" mistranslated as "hot coffee"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation.	db500c4e746897e5d5adafbf222b959c512445ad	@['JournalArticle']{wallace-etal-2020-customizing,  author = {Eric Wallace and Tony Zhao and Shi Feng and Sameer Singh},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Customizing Triggers with Concealed Data Poisoning},  volume = {abs/2010.12563},  year = {2020} }
Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding	2022	http://www.semanticscholar.org/paper/32c6607346e0bbe21844275f55fb368bbffd4699	Curriculum is introduced as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena and it is shown that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality.	maybe	1	In the age of large transformer language models, linguistic evaluation play an important role in diagnosing models’ abilities and limitations on natural language understanding. However, current evaluation methods show some significant shortcomings. In particular, they do not provide insight into how well a language model captures distinct linguistic skills essential for language understanding and reasoning. Thus they fail to effectively map out the aspects of language understanding that remain challenging to existing models, which makes it hard to discover potential limitations in models and datasets. In this paper, we introduce Curriculum as a new format of NLI benchmark for evaluation of broad-coverage linguistic phenomena. Curriculum contains a collection of datasets that covers 36 types of major linguistic phenomena and an evaluation procedure for diagnosing how well a language model captures reasoning skills for distinct types of linguistic phenomena. We show that this linguistic-phenomena-driven benchmark can serve as an effective tool for diagnosing model behavior and verifying model learning quality. In addition, our experiments provide insight into the limitation of existing benchmark datasets and state-of-the-art models that may encourage future research on re-designing datasets, model architectures, and learning objectives.	32c6607346e0bbe21844275f55fb368bbffd4699	@['JournalArticle', 'Conference']{chen-gao-2022-curriculum:,  author = {Zeming Chen and Qiyue Gao},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Curriculum: A Broad-Coverage Benchmark for Linguistic Phenomena in Natural Language Understanding},  volume = {abs/2204.06283},  year = {2022} }
CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models	2020	http://www.semanticscholar.org/paper/645bd6eadc247989abc5e0b0aa0be79ec8b11ea6	It is found that all three of the widely-used MLMs the authors evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs, a benchmark for measuring some forms of social bias in language models against protected demographic groups in the US.	maybe	141	Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.	645bd6eadc247989abc5e0b0aa0be79ec8b11ea6	@['JournalArticle', 'Conference']{nangia-etal-2020-crows,  author = {Nikita Nangia and Clara Vania and Rasika Bhalerao and Samuel R. Bowman},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1953-1967},  title = {CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models},  year = {2020} }
Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges	2022	http://www.semanticscholar.org/paper/0ba5fb80d2c3ea3a8505415e32d954b4e4eea170	The Crowd Score is presented, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges and it shows that few-shot prompting leads to better results than zero-shot for the voting question and aggressive and self-defeating voters are more inclined to find more jokes funny of a set of aggressive/self- defeating jokes.	maybe	0	This paper presents the Crowd Score, a novel method to assess the funniness of jokes using large language models (LLMs) as AI judges. Our method relies on inducing different personalities into the LLM and aggregating the votes of the AI judges into a single score to rate jokes. We validate the votes using an auditing technique that checks if the explanation for a particular vote is reasonable using the LLM. We tested our methodology on 52 jokes in a crowd of four AI voters with different humour types: afﬁliative, self-enhancing, aggressive and self-defeating. Our results show that few-shot prompting leads to better results than zero-shot for the voting question. Personality induction showed that aggressive and self-defeating voters are signiﬁcantly more inclined to ﬁnd more jokes funny of a set of aggressive/self-defeating jokes than the afﬁliative and self-enhancing voters. The Crowd Score follows the same trend as human judges by assigning higher scores to jokes that are also considered funnier by human judges. We believe that our methodology could be applied to other creative domains such as story, poetry, slogans, etc. It could both help the adoption of a ﬂexible and accurate standard approach to compare different work in the CC community under a common metric and by minimizing human participation in assessing creative artefacts, it could accelerate the prototyping of creative artefacts and reduce the cost of hiring human participants to rate creative artefacts. 1	0ba5fb80d2c3ea3a8505415e32d954b4e4eea170	@['JournalArticle']{goes-etal-2022-crowd,  author = {Fabricio Goes and Zisen Zhou and Piotr Sawicki and M. Grzes and Daniel Brown},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Crowd Score: A Method for the Evaluation of Jokes using Large Language Model AI Voters as Judges},  volume = {abs/2212.11214},  year = {2022} }
Cross-Linguistic Syntactic Evaluation of Word Prediction Models	2020	http://www.semanticscholar.org/paper/da9d57ca205a1ce040c7a38319cde7be7c27da21	ClAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models, is introduced, which uses subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars developed.	maybe	29	A range of studies have concluded that neural word prediction models can distinguish grammatical from ungrammatical sentences with high accuracy. However, these studies are based primarily on monolingual evidence from English. To investigate how these models’ ability to learn syntax varies by language, we introduce CLAMS (Cross-Linguistic Assessment of Models on Syntax), a syntactic evaluation suite for monolingual and multilingual models. CLAMS includes subject-verb agreement challenge sets for English, French, German, Hebrew and Russian, generated from grammars we develop. We use CLAMS to evaluate LSTM language models as well as monolingual and multilingual BERT. Across languages, monolingual LSTMs achieved high accuracy on dependencies without attractors, and generally poor accuracy on agreement across object relative clauses. On other constructions, agreement accuracy was generally higher in languages with richer morphology. Multilingual models generally underperformed monolingual models. Multilingual BERT showed high syntactic accuracy on English, but noticeable deficiencies in other languages.	da9d57ca205a1ce040c7a38319cde7be7c27da21	@['JournalArticle', 'Conference']{mueller-etal-2020-cross,  author = {Aaron Mueller and Garrett Nicolai and Panayiota Petrou-Zeniou and N. Talmina and Tal Linzen},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5523-5539},  title = {Cross-Linguistic Syntactic Evaluation of Word Prediction Models},  year = {2020} }
Cross-linguistic Comparison of Linguistic Feature Encoding in BERT Models for Typologically Different Languages	2022	http://www.semanticscholar.org/paper/2c1287fa491f876b35ff996d72750417564348a6		maybe	0	Though recently there have been an increased interest in how pre-trained language models encode different linguistic features, there is still a lack of systematic comparison between languages with different morphology and syntax. In this paper, using BERT as an example of a pre-trained model, we compare how three typologically different languages (English, Korean, and Russian) encode morphology and syntax features across different layers. In particular, we contrast languages which differ in a particular aspect, such as flexibility of word order, head directionality, morphological type, presence of grammatical gender, and morphological richness, across four different tasks.	2c1287fa491f876b35ff996d72750417564348a6	@None{otmakhova-etal-2022-cross,  author = {Yulia Otmakhova and Karin M. Verspoor and Jey Han Lau},  booktitle = {SIGTYP},  journal = {Proceedings of the 4th Workshop on Research in Computational Linguistic Typology and Multilingual NLP},  title = {Cross-linguistic Comparison of Linguistic Feature Encoding in BERT Models for Typologically Different Languages},  year = {2022} }
Cross-geographic Bias Detection in Toxicity Modeling	2021	http://www.semanticscholar.org/paper/2a3b239d17f85bdcfd0d06fdeea25e9bd9a4e6d0	A weakly supervised method to robustly detect lexical biases in broader geocultural contexts is introduced and it is demonstrated that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts.	maybe	2	Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning nonWestern contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geocultural contexts. Through a case study on cross-geographic toxicity detection, we demonstrate that our method identifies salient groups of errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts.	2a3b239d17f85bdcfd0d06fdeea25e9bd9a4e6d0	@['JournalArticle']{ghosh-etal-2021-cross,  author = {Sayan Ghosh and Dylan Baker and David Jurgens and Vinodkumar Prabhakaran},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Cross-geographic Bias Detection in Toxicity Modeling},  volume = {abs/2104.06999},  year = {2021} }
Cross-domain Analysis on Japanese Legal Pretrained Language Models	2022	http://www.semanticscholar.org/paper/d7f235239f1c899fc2d3ffff5829fb46627b13c9	The findings are the PLM built with general domain data can be improved by further pretraining with domain-specific data,domain-specific PLMs can learn domain- Specific and general word meanings simultaneously and can distinguish them, and domain- specific PLMs work better on its target domain.	maybe	0	This paper investigates the pretrained language model (PLM) specialised in the Japanese legal domain. We create PLMs using different pretraining strategies and investigate their performance across multiple domains. Our findings are (i) the PLM built with general domain data can be improved by further pretraining with domain-specific data, (ii) domain-specific PLMs can learn domain-specific and general word meanings simultaneously and can distinguish them, (iii) domain-specific PLMs work better on its target domain; still, the PLMs retain the information learnt in the original PLM even after being further pretrained with domain-specific data, (iv) the PLMs sequentially pre-trained with corpora of different domains show high performance for the later learnt domains.	d7f235239f1c899fc2d3ffff5829fb46627b13c9	@['JournalArticle']{miyazaki-etal-2022-cross,  author = {Keisuke Miyazaki and Hiroaki Yamada and T. Tokunaga},  booktitle = {AACL/IJCNLP},  pages = {274-281},  title = {Cross-domain Analysis on Japanese Legal Pretrained Language Models},  year = {2022} }
Crawling the Internal Knowledge-Base of Language Models	2023	https://www.semanticscholar.org/paper/47a541269d4ef70f37f0d3a57483312c4c6c2ad5	The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision and recall, and yields high precision graphs, while emit-ting a reasonable number of facts per entity.	maybe	0	Language models are trained on large volumes of text, and as a result their parameters might contain a signiﬁcant body of factual knowledge. Any downstream task performed by these models implicitly builds on these facts, and thus it is highly desirable to have means for representing this body of knowledge in an interpretable way. However, there is currently no mechanism for such a representation. Here, we propose to address this goal by extracting a knowledge-graph of facts from a given language model. We describe a procedure for “crawling” the internal knowledge-base of a language model. Speciﬁcally, given a seed entity, we expand a knowledge-graph around it. The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision (i.e., that no wrong facts are generated) and recall (i.e., the number of facts generated). We eval-uate our approach on graphs crawled starting from dozens of seed entities, and show it yields high precision graphs (82-92%), while emit-ting a reasonable number of facts per entity.	47a541269d4ef70f37f0d3a57483312c4c6c2ad5	@['JournalArticle']{cohen-etal-2023-crawling,  author = {Roi Cohen and Mor Geva and Jonathan Berant and A. Globerson},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Crawling the Internal Knowledge-Base of Language Models},  volume = {abs/2301.12810},  year = {2023} }
CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models	2021	http://www.semanticscholar.org/paper/eebdf7303256f081ab1f6a36ff0ea6126e4da484	The CRASS data set and benchmark utilizing questionized counterfactual conditionals is introduced as a novel and powerful tool to evaluate large language models and poses a valid challenge for these models and opens up considerable room for their improvement.	maybe	2	We introduce the CRASS (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design and benchmark. We test six state-of-the-art models against our benchmark. Our results show that it poses a valid challenge for these models and opens up considerable room for their improvement.	eebdf7303256f081ab1f6a36ff0ea6126e4da484	@['JournalArticle']{frohberg-binder-2021-crass:,  author = {Jorg Frohberg and Frank Binder},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {2126-2140},  title = {CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models},  year = {2021} }
Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations	2019	http://www.semanticscholar.org/paper/df92434acb9d9cc77478259a33eccf04144e57ac	This work probes and challenges several aspects of BERT's commonsense representation abilities, and develops a method of fine-tuning knowledge graphs embeddings alongside BERT and shows the continued importance of explicit knowledge graphs.	maybe	27	Pretrained deep contextual representations have advanced the state-of-the-art on various commonsense NLP tasks, but we lack a concrete understanding of the capability of these models. Thus, we investigate and challenge several aspects of BERT's commonsense representation abilities. First, we probe BERT's ability to classify various object attributes, demonstrating that BERT shows a strong ability in encoding various commonsense features in its embedding space, but is still deficient in many areas. Next, we show that, by augmenting BERT's pretraining data with additional data related to the deficient attributes, we are able to improve performance on a downstream commonsense reasoning task while using a minimal amount of data. Finally, we develop a method of fine-tuning knowledge graphs embeddings alongside BERT and show the continued importance of explicit knowledge graphs.	df92434acb9d9cc77478259a33eccf04144e57ac	@['JournalArticle', 'Conference']{da-kasai-2019-cracking,  author = {Jeff Da and Jungo Kasai},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations},  volume = {abs/1910.01157},  year = {2019} }
Counterfactual reasoning: Do language models need world knowledge for causal understanding?	2022	http://www.semanticscholar.org/paper/91a82593721c03ecffdef1c72ea55c6d87c42473	It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, it is also found that for most models this effect appears largely to be driven by simple lexical cues.	maybe	0	Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difﬁcult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals , which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We ﬁnd that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also ﬁnd that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we ﬁnd that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.	91a82593721c03ecffdef1c72ea55c6d87c42473	@['JournalArticle']{li-etal-2022-counterfactual,  author = {Jiaxuan Li and Lang-Chi Yu and Allyson Ettinger},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Counterfactual reasoning: Do language models need world knowledge for causal understanding?},  volume = {abs/2212.03278},  year = {2022} }
Counterfactual reasoning: Do Language Models need world knowledge for causal inference?	2022	http://www.semanticscholar.org/paper/830c94b8aeec8531656bdaab406bb8f0cbb66c86	It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, it is also found that for most models this effect appears largely to be driven by simple lexical cues.	maybe	0	Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals , which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge—however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.	830c94b8aeec8531656bdaab406bb8f0cbb66c86	@None{li-2022-counterfactual,  author = {Jiaxuan Li},  title = {Counterfactual reasoning: Do Language Models need world knowledge for causal inference?},  year = {2022} }
Counterfactual Multi-Token Fairness in Text Classification	2022	http://www.semanticscholar.org/paper/99ea756f8bcb9e0981ef38960fb837e232050c43	This paper has curated a resource of sensitive tokens and their corresponding perturbation tokens, even extending the support beyond traditionally used sensitive attributes like Age, Gender, and Race to Nationality, Disability, and Religion valid over all forms of texts and documents.	maybe	1	The counterfactual token generation has been limited to perturbing only a single token in texts that are generally short and single sentences. These tokens are often associated with one of many sensitive attributes. With limited counterfactuals generated, the goal to achieve invariant nature for machine learning classification models towards any sensitive attribute gets bounded, and the formulation of Counterfactual Fairness gets narrowed. In this paper, we overcome these limitations by solving root problems and opening bigger domains for understanding. We have curated a resource of sensitive tokens and their corresponding perturbation tokens, even extending the support beyond traditionally used sensitive attributes like Age, Gender, and Race to Nationality, Disability, and Religion. The concept of Counterfactual Generation has been extended to multi-token support valid over all forms of texts and documents. We define the method of generating counterfactuals by perturbing multiple sensitive tokens as Counterfactual Multi-token Generation. The method has been conceptualized to showcase significant performance improvement over single-token methods and validated over multiple benchmark datasets. The emendation in counterfactual generation propagates in achieving improved Counterfactual Multi-token Fairness.	99ea756f8bcb9e0981ef38960fb837e232050c43	@['JournalArticle']{lohia-2022-counterfactual,  author = {P. Lohia},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Counterfactual Multi-Token Fairness in Text Classification},  volume = {abs/2202.03792},  year = {2022} }
Counterfactual Memorization in Neural Language Models	2021	http://www.semanticscholar.org/paper/4656cd01ab3c47117ecc87f63ca80498f2fc9aae	A principled perspective inspired by a taxonomy of human memory in Psychology is provided, which formulates a notion of counterfactual memorization, which characterizes how a model’s predictions change if a particular document is omitted during training.	maybe	18	Modern neural language models widely used in tasks across NLP risk memorizing sensitive information from their training data. As models continue to scale up in parameters, training data, and compute, understanding memorization in language models is both important from a learning-theoretical point of view, and is practically crucial in real world applications. An open question in previous studies of memorization in language models is how to filter out “common” memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing “common” memorization such as familiar phrases, public knowledge or templated texts. In this paper, we provide a principled perspective inspired by a taxonomy of human memory in Psychology. From this perspective, we formulate a notion of counterfactual memorization, which characterizes how a model’s predictions change if a particular document is omitted during training. We identify and study counterfactuallymemorized training examples in standard text datasets. We further estimate the influence of each training example on the validation set and on generated texts, and show that this can provide direct evidence of the source of memorization at test time.	4656cd01ab3c47117ecc87f63ca80498f2fc9aae	@['JournalArticle']{zhang-etal-2021-counterfactual,  author = {Chiyuan Zhang and Daphne Ippolito and Katherine Lee and Matthew Jagielski and Florian Tramèr and Nicholas Carlini},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Counterfactual Memorization in Neural Language Models},  volume = {abs/2112.12938},  year = {2021} }
Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction	2021	http://www.semanticscholar.org/paper/15a0869cee792546dad6190e9420627395a16610	AlterRep, an intervention-based method, is applied to study how BERT models of different sizes process relative clauses (RCs) and finds that BERT variants use RC boundary information during word prediction in a manner that is consistent with the rules of English grammar.	maybe	21	When language models process syntactically complex sentences, do they use their representations of syntax in a manner that is consistent with the grammar of the language? We propose AlterRep, an intervention-based method to address this question. For any linguistic feature of a given sentence, AlterRep generates counterfactual representations by altering how the feature is encoded, while leaving in- tact all other aspects of the original representation. By measuring the change in a model’s word prediction behavior when these counterfactual representations are substituted for the original ones, we can draw conclusions about the causal effect of the linguistic feature in question on the model’s behavior. We apply this method to study how BERT models of different sizes process relative clauses (RCs). We find that BERT variants use RC boundary information during word prediction in a manner that is consistent with the rules of English grammar; this RC boundary information generalizes to a considerable extent across different RC types, suggesting that BERT represents RCs as an abstract linguistic category.	15a0869cee792546dad6190e9420627395a16610	@['JournalArticle']{ravfogel-etal-2021-counterfactual,  author = {Shauli Ravfogel and Grusha Prasad and Tal Linzen and Yoav Goldberg},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction},  volume = {abs/2105.06965},  year = {2021} }
Correspondence between the layered structure of deep language models and temporal structure of natural language processing in the human brain	2022	http://www.semanticscholar.org/paper/2b39f12dd98f7fdc3086d1a06d189d42fd17b934	A striking similarity is shown between the sequence of representations induced by the model and the brain encoding of language over time during real-life comprehension and the temporal dynamics of neural activity in high-order language areas.	maybe	0	Deep language models (DLMs) provide a novel computational paradigm for how the brain processes natural language. Unlike symbolic, rule-based models described in psycholinguistics, DLMs encode words and their context as continuous numerical vectors. These “embeddings” are constructed by a sequence of layered computations to ultimately capture surprisingly sophisticated representations of linguistic structures. How does this layered hierarchy map onto the human brain during natural language comprehension? In this study, we used ECoG to record neural activity in language areas along the superior temporal gyrus and inferior frontal gyrus while human participants listened to a 30-minute spoken narrative. We supplied this same narrative to a high-performing DLM (GPT2-XL) and extracted the contextual embeddings for each word in the story across all 48 layers of the model. We next trained a set of linear encoding models to predict the temporally-evolving neural activity from the embeddings at each layer. We found a striking correspondence between the layer-by-layer sequence of embeddings from GPT2-XL and the temporal sequence of neural activity in language areas. In addition, we found evidence for the gradual accumulation of recurrent information along the linguistic processing hierarchy. However, we also noticed additional neural processes that took place in the brain, but not in DLMs, during the processing of surprising (unpredictable) words. These findings point to a connection between language processing in humans and DLMs where the layer-by-layer accumulation of contextual information in DLM embeddings matches the temporal dynamics of neural activity in high-order language areas. Significance statement Deep language models transformed our ability to model language. Recent studies connected these neural nets based models to the human representation of language. Here, we show a striking similarity between the sequence of representations induced by the model and the brain encoding of language over time during real-life comprehension.	2b39f12dd98f7fdc3086d1a06d189d42fd17b934	@None{goldstein-etal-2022-correspondence,  author = {Ariel Goldstein and Eric Ham and Samuel A. Nastase and Zaid Zada and Avigail Grinstein-Dabus and Bobbi Aubrey and Mariano Schain and H. Gazula and Amir Feder and W. Doyle and S. Devore and P. Dugan and D. Friedman and Michael P. Brenner and Avinatan Hassidim and O. Devinsky and A. Flinker and Omer Levy and U. Hasson},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Correspondence between the layered structure of deep language models and temporal structure of natural language processing in the human brain},  year = {2022} }
Correlating Neural and Symbolic Representations of Language	2019	http://www.semanticscholar.org/paper/59cfae5186900e8021ea31a6d3ce4f595f316ee5	Two methods based on Representational Similarity Analysis and Tree Kernels which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees are presented.	maybe	55	Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.	59cfae5186900e8021ea31a6d3ce4f595f316ee5	@['JournalArticle', 'Conference']{chrupała-alishahi-2019-correlating,  author = {Grzegorz Chrupała and A. Alishahi},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {2952-2962},  title = {Correlating Neural and Symbolic Representations of Language},  year = {2019} }
COPEN: Probing Conceptual Knowledge in Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/bcec7d17e68aceb91d020dd796ece075694f77c6	Inspired by knowledge representation schemata, this work comprehensively evaluates conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively.	maybe	0	Conceptual knowledge is fundamental to human cognition and knowledge bases. However, existing knowledge probing works only focus on evaluating factual knowledge of pre-trained language models (PLMs) and ignore conceptual knowledge. Since conceptual knowledge often appears as implicit commonsense behind texts, designing probes for conceptual knowledge is hard. Inspired by knowledge representation schemata, we comprehensively evaluate conceptual knowledge of PLMs by designing three tasks to probe whether PLMs organize entities by conceptual similarities, learn conceptual properties, and conceptualize entities in contexts, respectively. For the tasks, we collect and annotate 24k data instances covering 393 concepts, which is COPEN, a COnceptual knowledge Probing bENchmark. Extensive experiments on different sizes and types of PLMs show that existing PLMs systematically lack conceptual knowledge and suffer from various spurious correlations. We believe this is a critical bottleneck for realizing human-like cognition in PLMs. COPEN and our codes are publicly released at https://github.com/THU-KEG/COPEN.	bcec7d17e68aceb91d020dd796ece075694f77c6	@['JournalArticle', 'Conference']{peng-etal-2022-copen:,  author = {Hao Peng and Xiaozhi Wang and Shengding Hu and Hailong Jin and Lei Hou and Juanzi Li and Zhiyuan Liu and Qun Liu},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {COPEN: Probing Conceptual Knowledge in Pre-trained Language Models},  volume = {abs/2211.04079},  year = {2022} }
Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models	2021	http://www.semanticscholar.org/paper/1dbb523a6555d6e0c5727620e2b57daaa5b79dc0	Composite attention is proposed, which unites previous relative position encoding methods under a convolutional framework, and finds that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings.	maybe	5	In this paper, we detail the relationship between convolutions and self-attention in natural language tasks. We show that relative position embeddings in self-attention layers are equivalent to recently-proposed dynamic lightweight convolutions, and we consider multiple new ways of integrating convolutions into Transformer self-attention. Specifically, we propose composite attention, which unites previous relative position encoding methods under a convolutional framework. We conduct experiments by training BERT with composite attention, finding that convolutions consistently improve performance on multiple downstream tasks, replacing absolute position embeddings. To inform future work, we present results comparing lightweight convolutions, dynamic convolutions, and depthwise-separable convolutions in language model pre-training, considering multiple injection points for convolutions in self-attention layers.	1dbb523a6555d6e0c5727620e2b57daaa5b79dc0	@['JournalArticle', 'Conference']{chang-etal-2021-convolutions,  author = {Tyler A. Chang and Yifan Xu and Weijian Xu and Z. Tu},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4322-4333},  title = {Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models},  year = {2021} }
Controlling the Imprint of Passivization and Negation in Contextualized Representations	2020	https://www.semanticscholar.org/paper/42595cb533b03ad44738102fb0c3cef3e4b5c27c	It is shown that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation.	maybe	4	Contextualized word representations encode rich information about syntax and semantics, alongside specificities of each context of use. While contextual variation does not always reflect actual meaning shifts, it can still reduce the similarity of embeddings for word instances having the same meaning. We explore the imprint of two specific linguistic alternations, namely passivization and negation, on the representations generated by neural models trained with two different objectives: masked language modeling and translation. Our exploration methodology is inspired by an approach previously proposed for removing societal biases from word vectors. We show that passivization and negation leave their traces on the representations, and that neutralizing this information leads to more similar embeddings for words that should preserve their meaning in the transformation. We also find clear differences in how the respective features generalize across datasets.	42595cb533b03ad44738102fb0c3cef3e4b5c27c	@['JournalArticle']{çelikkanat-etal-2020-controlling,  author = {H. Çelikkanat and Sami Virpioja and J. Tiedemann and Marianna Apidianaki},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {136-148},  title = {Controlling the Imprint of Passivization and Negation in Contextualized Representations},  year = {2020} }
Contrastive Explanations for Model Interpretability	2021	http://www.semanticscholar.org/paper/5c599dc162bfd33abf390ba00474453b54ddf60f	The ability of label-contrastive explanations to provide fine-grained interpretability of model decisions is demonstrated, via both high-level abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks.	maybe	31	Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Our contrastive explanations can additionally answer for which label, and against which alternative label, is a given input feature useful. We produce contrastive explanations via both high-level abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions.	5c599dc162bfd33abf390ba00474453b54ddf60f	@['JournalArticle', 'Conference']{jacovi-etal-2021-contrastive,  author = {Alon Jacovi and Swabha Swayamdipta and Shauli Ravfogel and Yanai Elazar and Yejin Choi and Yoav Goldberg},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1597-1611},  title = {Contrastive Explanations for Model Interpretability},  year = {2021} }
Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge	2020	http://www.semanticscholar.org/paper/60b439d36beae59f37665b8423e1b728f3eaf7ca	This work investigates whether recent advances in NLP, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as polysemy and homonymy, and finds that participants’ judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space.	maybe	12	Understanding context-dependent variation in word meanings is a key aspect of human language comprehension supported by the lexicon. Lexicographic resources (e.g., WordNet) capture only some of this context-dependent variation; for example, they often do not encode how closely senses, or discretized word meanings, are related to one another. Our work investigates whether recent advances in NLP, specifically contextualized word embeddings, capture human-like distinctions between English word senses, such as polysemy and homonymy. We collect data from a behavioral, web-based experiment, in which participants provide judgments of the relatedness of multiple WordNet senses of a word in a two-dimensional spatial arrangement task. We find that participants’ judgments of the relatedness between senses are correlated with distances between senses in the BERT embedding space. Specifically, homonymous senses (e.g., bat as mammal vs. bat as sports equipment) are reliably more distant from one another in the embedding space than polysemous ones (e.g., chicken as animal vs. chicken as meat). Our findings point towards the potential utility of continuous-space representations of sense meanings.	60b439d36beae59f37665b8423e1b728f3eaf7ca	@['JournalArticle']{nair-etal-2020-contextualized,  author = {Sathvik Nair and M. Srinivasan and S. Meylan},  booktitle = {Workshop on Cognitive Aspects of the Lexicon},  journal = {ArXiv},  title = {Contextualized Word Embeddings Encode Aspects of Human-Like Word Sense Knowledge},  volume = {abs/2010.13057},  year = {2020} }
Contextualized Embeddings Encode Monolingual and Cross-lingual Knowledge of Idiomaticity	2021	http://www.semanticscholar.org/paper/c3fb860db2a9e5297e9d98cd0af37cb589dfeaa3	This paper considers monolingual experiments for English and Russian, and shows that the proposed model outperforms previous approaches, including in the case that the model is tested on instances of PIE types that were not observed during training.	maybe	4	Potentially idiomatic expressions (PIEs) are ambiguous between non-compositional idiomatic interpretations and transparent literal interpretations. For example, “hit the road” can have an idiomatic meaning corresponding to ‘start a journey’ or have a literal interpretation. In this paper we propose a supervised model based on contextualized embeddings for predicting whether usages of PIEs are idiomatic or literal. We consider monolingual experiments for English and Russian, and show that the proposed model outperforms previous approaches, including in the case that the model is tested on instances of PIE types that were not observed during training. We then consider cross-lingual experiments in which the model is trained on PIE instances in one language, English or Russian, and tested on the other language. We find that the model outperforms baselines in this setting. These findings suggest that contextualized embeddings are able to learn representations that encode knowledge of idiomaticity that is not restricted to specific expressions, nor to a specific language.	c3fb860db2a9e5297e9d98cd0af37cb589dfeaa3	@None{fakharian-cook-2021-contextualized,  author = {Samin Fakharian and Paul Cook},  booktitle = {Workshop on Multiword Expressions},  journal = {Proceedings of the 17th Workshop on Multiword Expressions (MWE 2021)},  title = {Contextualized Embeddings Encode Monolingual and Cross-lingual Knowledge of Idiomaticity},  year = {2021} }
Context vs Target Word: Quantifying Biases in Lexical Semantic Datasets	2021	http://www.semanticscholar.org/paper/f22b02df2e25d0d1bb55de90a8df514f9b050319	This study demonstrates that models are usually not being tested for word-in-context representations as such in these tasks and results are therefore open to misinterpretation and recommends the framework as sanity check for context and target word biases of future task design and application in lexical semantics.	maybe	0	State-of-the-art contextualized models such as BERT use tasks such as WiC and WSD to evaluate their word-in-context representations. This inherently assumes that performance in these tasks reflect how well a model represents the coupled word and context semantics. This study investigates this assumption by presenting the first quantitative analysis (using probing baselines) on the context-word interaction being tested in major contextual lexical semantic tasks. Specifically, based on the probing baseline performance, we propose measures to calculate the degree of context or word biases in a dataset, and plot existing datasets on a continuum. The analysis shows most existing datasets fall into the extreme ends of the continuum (i.e. they are either heavily context-biased or target-wordbiased) while AMICO and Sense Retrieval show lower overall biases to challenge a model to represent both the context and target words. Our case study on WiC reveals that human subjects do not share models’ strong context biases in the dataset (humans found semantic judgments much more difficult when the target word is missing) and models are learning spurious correlations from context alone. This study demonstrates that models are usually not being tested for word-in-context representations as such in these tasks and results are therefore open to misinterpretation. We recommend our framework as sanity check for context and target word biases of future task design and application in lexical semantics.	f22b02df2e25d0d1bb55de90a8df514f9b050319	@['JournalArticle']{liu-etal-2021-context,  author = {Qianchu Liu and Diana McCarthy and A. Korhonen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Context vs Target Word: Quantifying Biases in Lexical Semantic Datasets},  volume = {abs/2112.06733},  year = {2021} }
Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing	2022	https://www.semanticscholar.org/paper/ff859b26e94b0545365d6cc759dff632e788e2ae	Context variance prompts and Understand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to large-N-M relations and rare relations and disentangles ”understand” from just ”read and copy”.	maybe	2	Pretrained language models (PLMs) have motivated research on what kinds of knowledge these models learn. Fill-in-the-blanks problem (e.g., cloze tests) is a natural approach for gauging such knowledge. BioLAMA generates prompts for biomedical factual knowledge triples and uses the Top-k accuracy metric to evaluate different PLMs’ knowledge. However, existing research has shown that such prompt-based knowledge probing methods can only probe a lower bound of knowledge. Many factors like prompt-based probing biases make the LAMA benchmark unreliable and un-stable. This problem is more prominent in BioLAMA. The severe long-tailed distribution in vocabulary and large-N-M relation make the performance gap between LAMA and BioLAMA remain notable. To address these, we introduce context variance into the prompt generation and propose a new rank-change-based evaluation metric. Different from the previous known-unknown evaluation criteria, we propose the concept of ”Misunderstand” in LAMA for the ﬁrst time. Through experiments on 12 PLMs, our context variance prompts and Understand-Confuse-Misunderstand (UCM) metric makes BioLAMA more friendly to large-N-M relations and rare relations. We also conducted a set of control experiments to disentangle ”understand” from just ”read and copy”.	ff859b26e94b0545365d6cc759dff632e788e2ae	@['JournalArticle']{yao-etal-2022-context,  author = {Zonghai Yao and Yifan Cao and Zhichao Yang and Hong Yu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing},  volume = {abs/2211.10265},  year = {2022} }
Context Matters: A Pragmatic Study of PLMs’ Negation Understanding	2022	http://www.semanticscholar.org/paper/952073a64bad10b1b1406f51aac9c8d78bb2a642		yes	1	In linguistics, there are two main perspectives on negation: a semantic and a pragmatic view. So far, research in NLP on negation has almost exclusively adhered to the semantic view. In this article, we adopt the pragmatic paradigm to conduct a study of negation understanding focusing on transformer-based PLMs. Our results differ from previous, semantics-based studies and therefore help to contribute a more comprehensive – and, given the results, much more optimistic – picture of the PLMs’ negation understanding.	952073a64bad10b1b1406f51aac9c8d78bb2a642	@['JournalArticle', 'Conference']{gubelmann-handschuh-2022-context,  author = {Reto Gubelmann and S. Handschuh},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4602-4621},  title = {Context Matters: A Pragmatic Study of PLMs’ Negation Understanding},  year = {2022} }
Constructing Natural Language Explanations via Saliency Map Verbalization	2022	http://www.semanticscholar.org/paper/7367455f21fffde11ac74eb9b607ba30428e806e	The results suggest that saliency map verbalization makes explanations more under-standable and less cognitively challenging to humans than conventional heatmap visualization.	maybe	0	Saliency maps can explain a neural model’s prediction by identifying important input features. While they excel in being faithful to the explained model, saliency maps in their entirety are difﬁcult to interpret for humans, especially for instances with many input features. In contrast, natural language explanations (NLEs) are ﬂexible and can be tuned to a recipient’s expectations, but are costly to generate: Rationalization models are usually trained on speciﬁc tasks and require high-quality and diverse datasets of human annotations. We combine the advantages from both explainability methods by verbalizing saliency maps. We formalize this underexplored task and propose a novel methodology that ad-dresses two key challenges of this approach – what and how to verbalize. Our approach utilizes efﬁcient search methods that are task-and model-agnostic and do not require another black-box model, and hand-crafted templates to preserve faithfulness. We conduct a human evaluation of explanation representations across two natural language processing (NLP) tasks: news topic classiﬁcation and sentiment analysis. Our results suggest that saliency map verbalization makes explanations more under-standable and less cognitively challenging to humans than conventional heatmap visualization.	7367455f21fffde11ac74eb9b607ba30428e806e	@['JournalArticle']{feldhus-etal-2022-constructing,  author = {Nils Feldhus and Leonhard Hennig and Maximilian Dustin Nasert and Christopher Ebert and Robert Schwarzenberg and Sebastian Moller},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Constructing Natural Language Explanations via Saliency Map Verbalization},  volume = {abs/2210.07222},  year = {2022} }
ConjNLI: Natural Language Inference over Conjunctive Sentences	2020	http://www.semanticscholar.org/paper/a113053b624b599b204fbd6599284b726c17f916	Con ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced, is introduced.	maybe	18	Reasoning about conjuncts in conjunctive sentences is important for a deeper understanding of conjunctions in English and also how their usages and semantics differ from conjunctive and disjunctive boolean logic. Existing NLI stress tests do not consider non-boolean usages of conjunctions and use templates for testing such model knowledge. Hence, we introduce ConjNLI, a challenge stress-test for natural language inference over conjunctive sentences, where the premise differs from the hypothesis by conjuncts removed, added, or replaced. These sentences contain single and multiple instances of coordinating conjunctions ("and", "or", "but", "nor") with quantifiers, negations, and requiring diverse boolean and non-boolean inferences over conjuncts. We find that large-scale pre-trained language models like RoBERTa do not understand conjunctive semantics well and resort to shallow heuristics to make inferences over such sentences. As some initial solutions, we first present an iterative adversarial fine-tuning method that uses synthetically created training data based on boolean and non-boolean heuristics. We also propose a direct model advancement by making RoBERTa aware of predicate semantic roles. While we observe some performance gains, ConjNLI is still challenging for current methods, thus encouraging interesting future work for better understanding of conjunctions. Our data and code are publicly available at: this https URL	a113053b624b599b204fbd6599284b726c17f916	@['JournalArticle', 'Conference']{saha-etal-2020-conjnli:,  author = {Swarnadeep Saha and Yixin Nie and Mohit Bansal},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {8240-8252},  title = {ConjNLI: Natural Language Inference over Conjunctive Sentences},  year = {2020} }
Conditional probing: measuring usable information beyond a baseline	2021	http://www.semanticscholar.org/paper/dffb0f028298ca2017255090ebd3453dce6e9ee0	This work extends a theory of usable information called V-information and proposes conditional probing, which explicitly conditions on the information in the baseline, which finds that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought.	maybe	20	Probing experiments investigate the extent to which neural representations make properties—like part-of-speech—predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we’re interested in measuring information that is contained in the representation but not in the baseline. For example, current methods can detect when a representation is more useful than the word identity (a baseline) for predicting part-of-speech; however, they cannot detect when the representation is predictive of just the aspects of part-of-speech not explainable by the word identity. In this work, we extend a theory of usable information called V-information and propose conditional probing, which explicitly conditions on the information in the baseline. In a case study, we find that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought.	dffb0f028298ca2017255090ebd3453dce6e9ee0	@['JournalArticle', 'Conference']{hewitt-etal-2021-conditional,  author = {John Hewitt and Kawin Ethayarajh and Percy Liang and Christopher D. Manning},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Conditional probing: measuring usable information beyond a baseline},  volume = {abs/2109.09234},  year = {2021} }
Conditional BERT Contextual Augmentation	2018	https://www.semanticscholar.org/paper/188024469a2443f262b3cbb5c5d4a96851949d68	A novel data augmentation method for labeled sentences called conditional BERT contextual augmentation, which can be easily applied to both convolutional or recurrent neural networks classifier to obtain improvement.	seed	176		188024469a2443f262b3cbb5c5d4a96851949d68	@['JournalArticle']{wu-etal-2018-conditional,  author = {Xing Wu and Shangwen Lv and Liangjun Zang and Jizhong Han and Songlin Hu},  booktitle = {International Conference on Conceptual Structures},  journal = {ArXiv},  title = {Conditional BERT Contextual Augmentation},  volume = {abs/1812.06705},  year = {2018} }
COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/1ff913af955034a01e236729e1302f72f0dcaccb	It is found that pre-trained language models can demonstrate behavior consistent with property inheritance to a great extent, but fail in the presence of distracting information, which decreases the performance of many models, sometimes even below chance.	yes	4	A characteristic feature of human semantic memory is its ability to not only store and retrieve the properties of concepts observed through experience, but to also facilitate the inheritance of properties ( can breathe ) from superordinate concepts ( ANIMAL ) to their subordinates ( DOG )—i.e. demonstrate property inheritance . In this paper, we present COMPS , a collection of minimal pair sentences that jointly tests pre-trained language models (PLMs) on their ability to attribute properties to concepts and their ability to demonstrate property inheritance behavior. Analyses of 22 different PLMs on COMPS reveal that they can easily distinguish between concepts on the basis of a property when they are trivially different, but ﬁnd it relatively dif-ﬁcult when concepts are related on the basis of nuanced knowledge representations. Fur-thermore, we ﬁnd that PLMs can demonstrate behavior consistent with property inheritance to a great extent, but fail in the presence of distracting information, which decreases the performance of many models, sometimes even below chance. This lack of robustness in demonstrating simple reasoning raises important questions about PLMs’ capacity to make correct inferences even when they appear to possess the prerequisite knowledge.	1ff913af955034a01e236729e1302f72f0dcaccb	@['JournalArticle']{misra-etal-2022-comps:,  author = {Kanishka Misra and J. Rayz and Allyson Ettinger},  booktitle = {ArXiv},  journal = {ArXiv},  title = {COMPS: Conceptual Minimal Pair Sentences for testing Property Knowledge and Inheritance in Pre-trained Language Models},  volume = {abs/2210.01963},  year = {2022} }
Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning	2020	https://www.semanticscholar.org/paper/d9b824dbecbe3a1f0b1489f9e4521a532a63818d	It is concluded that BERT can be pruned once during pre-training rather than separately for each task without affecting performance, and that fine-tuning BERT on a specific task does not improve its prunability.	seed	193	Pre-trained universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.	d9b824dbecbe3a1f0b1489f9e4521a532a63818d	@['JournalArticle']{gordon-etal-2020-compressing,  author = {Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},  booktitle = {Workshop on Representation Learning for NLP},  pages = {143-155},  title = {Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},  year = {2020} }
Compositional Explanations of Neurons	2020	http://www.semanticscholar.org/paper/56d1003fd02346e93354ab55cd204485c268512a	A procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior is described, which shows how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.	maybe	60	We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.	56d1003fd02346e93354ab55cd204485c268512a	@['JournalArticle']{mu-andreas-2020-compositional,  author = {Jesse Mu and Jacob Andreas},  booktitle = {Neural Information Processing Systems},  journal = {ArXiv},  title = {Compositional Explanations of Neurons},  volume = {abs/2006.14032},  year = {2020} }
Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA	2020	http://www.semanticscholar.org/paper/260cce438595c708433719a75c72889fefa5f731	This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task and shows differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.	maybe	11	Many NLP tasks have benefited from transferring knowledge from contextualized word embeddings, however the picture of what type of knowledge is transferred is incomplete. This paper studies the types of linguistic phenomena accounted for by language models in the context of a Conversational Question Answering (CoQA) task. We identify the problematic areas for the finetuned RoBERTa, BERT and DistilBERT models through systematic error analysis - basic arithmetic (counting phrases), compositional semantics (negation and Semantic Role Labeling), and lexical semantics (surprisal and antonymy). When enhanced with the relevant linguistic knowledge through multitask learning, the models improve in performance. Ensembles of the enhanced models yield a boost between 2.2 and 2.7 points in F1 score overall, and up to 42.1 points in F1 on the hardest question classes. The results show differences in ability to represent compositional and lexical information between RoBERTa, BERT and DistilBERT.	260cce438595c708433719a75c72889fefa5f731	@['JournalArticle', 'Conference']{staliunaite-iacobacci-2020-compositional,  author = {Ieva Staliunaite and Ignacio Iacobacci},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {7046-7056},  title = {Compositional and Lexical Semantics in RoBERTa, BERT and DistilBERT: A Case Study on CoQA},  year = {2020} }
Competition-level code generation with AlphaCode	2022	http://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5	AlphaCode is introduced, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform, marking the first time an artificial intelligence system has performed competitively in programming competitions.	maybe	127	Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.	5cbe278b65a81602a864184bbca37de91448a5f5	@['JournalArticle']{li-etal-2022-competition,  author = {Yujia Li and David H. Choi and Junyoung Chung and Nate Kushman and Julian Schrittwieser and Rémi Leblond and Tom and Eccles and James Keeling and Felix Gimeno and Agustin Dal Lago and T. Hubert and Peter Choy and Cyprien de and Masson d’Autume and I. Babuschkin and Xinyun Chen and Po-Sen Huang and Johannes Welbl and Sven Gowal and Alexey and Cherepanov and James Molloy and D. Mankowitz and Esme Sutherland Robson and Pushmeet Kohli and Nando de and Freitas and K. Kavukcuoglu and Oriol Vinyals},  booktitle = {Science},  journal = {Science},  pages = {1092 - 1097},  title = {Competition-level code generation with AlphaCode},  volume = {378},  year = {2022} }
Competency Problems: On Finding and Removing Artifacts in Language Data	2021	http://www.semanticscholar.org/paper/023fc86c932fbc36702a6ad11c94ba419e1d8d88	This work argues that for complex language understanding tasks, all simple feature correlations are spurious, and formalizes this notion into a class of problems which are called competency problems, and gives a simple statistical test for dataset artifacts that is used to show more subtle biases.	maybe	47	Much recent work in NLP has documented dataset artifacts, bias, and spurious correlations between input features and output labels. However, how to tell which features have “spurious” instead of legitimate correlations is typically left unspecified. In this work we argue that for complex language understanding tasks, all simple feature correlations are spurious, and we formalize this notion into a class of problems which we call competency problems. For example, the word “amazing” on its own should not give information about a sentiment label independent of the context in which it appears, which could include negation, metaphor, sarcasm, etc. We theoretically analyze the difficulty of creating data for competency problems when human bias is taken into account, showing that realistic datasets will increasingly deviate from competency problems as dataset size increases. This analysis gives us a simple statistical test for dataset artifacts, which we use to show more subtle biases than were described in prior work, including demonstrating that models are inappropriately affected by these less extreme biases. Our theoretical treatment of this problem also allows us to analyze proposed solutions, such as making local edits to dataset instances, and to give recommendations for future data collection and model design efforts that target competency problems.	023fc86c932fbc36702a6ad11c94ba419e1d8d88	@['JournalArticle', 'Conference']{gardner-etal-2021-competency,  author = {Matt Gardner and William Cooper Merrill and Jesse Dodge and Matthew E. Peters and Alexis Ross and Sameer Singh and Noah A. Smith},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Competency Problems: On Finding and Removing Artifacts in Language Data},  volume = {abs/2104.08646},  year = {2021} }
Comparing interpretation methods in mental state decoding analyses with deep learning models	2022	http://www.semanticscholar.org/paper/217eb5e833993dcfc1ce9a60737f3e9ab637fc6a	A gradient between two key characteristics of an explanation in mental state decoding, namely, its biological plausibility and faithfulness are demonstrated: interpretation methods with high explanation faithfulness, which capture the model’s decision process well, generally provide explanations that are biologically less plausible than the explanations of interpretations methods with less explanationfaithfulness.	maybe	2	Deep learning (DL) models ﬁnd increasing application in mental state decoding, where researchers seek to understand the mapping between mental states (e.g., perceiving fear or joy) and brain activity by identifying those brain regions (and networks) whose activity al-lows to accurately identify (i.e., decode) these states. Once a DL model has been trained to accurately decode a set of mental states, neuroimaging researchers often make use of interpretation methods from explainable artiﬁcial intelligence research to understand the model’s learned mappings between mental states and brain activity. Here, we compare the explanation performance of prominent interpretation methods in a mental state decoding analysis of three functional Magnetic Resonance Imaging (fMRI) datasets. Our ﬁndings demonstrate a gradient between two key characteristics of an explanation in mental state decoding, namely, its biological plausibility and faithfulness: interpretation methods with high explanation faithfulness, which capture the model’s decision process well, generally provide explanations that are biologically less plausible than the explanations of interpretation methods with less explanation faithfulness. Based on this ﬁnding, we provide speciﬁc recommendations for the application of interpretation methods in mental state decoding.	217eb5e833993dcfc1ce9a60737f3e9ab637fc6a	@['JournalArticle']{thomas-etal-2022-comparing,  author = {A. Thomas and Christopher Ré and R. Poldrack},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Comparing interpretation methods in mental state decoding analyses with deep learning models},  volume = {abs/2205.15581},  year = {2022} }
Comparative analysis of word embeddings in assessing semantic similarity of complex sentences	2020	http://www.semanticscholar.org/paper/1fa47f1cfee59f632b7ce5703bf4a5ef92003542	The results show the increase in complexity of the sentences has a significant impact on the performance of the embedding models resulting in a 10-20% decrease in Pearson's and Spearman's correlation.	maybe	2	Semantic textual similarity is one of the open research challenges in the field of Natural Language Processing. Extensive research has been carried out in this field and near-perfect results are achieved by recent transformer-based models in existing benchmark datasets like the STS dataset and the SICK dataset. In this paper, we study the sentences in these datasets and analyze the sensitivity of various word embeddings with respect to the complexity of the sentences. In this article, we build a complex sentence dataset comprising of 50 sentence pairs with associated semantic similarity values provided by 15 human annotators. Readability analysis is performed to highlight the increase in complexity of the sentences in the existing benchmark datasets and those in the proposed dataset. Further, we perform a comparative analysis of the performance of various word embeddings and language models on the existing benchmark datasets and the proposed dataset. The results show the increase in complexity of the sentences has a significant impact on the performance of the embedding models resulting in a 10-20% decrease in Pearson’s and Spearman’s correlation.	1fa47f1cfee59f632b7ce5703bf4a5ef92003542	@['JournalArticle']{chandrasekaran-mago-2020-comparative,  author = {Dhivya Chandrasekaran and Vijay K. Mago},  booktitle = {IEEE Access},  journal = {IEEE Access},  pages = {1-1},  title = {Comparative analysis of word embeddings in assessing semantic similarity of complex sentences},  volume = {PP},  year = {2020} }
Commonsense-Focused Dialogues for Response Generation: An Empirical Study	2021	http://www.semanticscholar.org/paper/2a3dd5cf961747adcb05f4f2834ff7a22261e861	This paper auto-extract commonsensical dialogues from existing dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph, and proposes an approach for automatic evaluation of commonsense that relies on features derived from ConceptNet and pre-trained language and dialog models, and shows reasonable correlation with human evaluation of responses’ commonsense quality.	yes	18	Smooth and effective communication requires the ability to perform latent or explicit commonsense inference. Prior commonsense reasoning benchmarks (such as SocialIQA and CommonsenseQA) mainly focus on the discriminative task of choosing the right answer from a set of candidates, and do not involve interactive language generation as in dialogue. Moreover, existing dialogue datasets do not explicitly focus on exhibiting commonsense as a facet. In this paper, we present an empirical study of commonsense in dialogue response generation. We first auto-extract commonsensical dialogues from existing dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph. Furthermore, building on social contexts/situations in SocialIQA, we collect a new dialogue dataset with 25K dialogues aimed at exhibiting social commonsense in an interactive setting. We evaluate response generation models trained using these datasets and find that models trained on both extracted and our collected data produce responses that consistently exhibit more commonsense than baselines. Finally we propose an approach for automatic evaluation of commonsense that relies on features derived from ConceptNet and pre-trained language and dialog models, and show reasonable correlation with human evaluation of responses’ commonsense quality.	2a3dd5cf961747adcb05f4f2834ff7a22261e861	@['JournalArticle']{zhou-etal-2021-commonsense,  author = {Pei Zhou and Karthik Gopalakrishnan and Behnam Hedayatnia and Seokhwan Kim and J. Pujara and Xiang Ren and Yang Liu and Dilek Z. Hakkani-Tür},  booktitle = {SIGDIAL Conferences},  journal = {ArXiv},  title = {Commonsense-Focused Dialogues for Response Generation: An Empirical Study},  volume = {abs/2109.06427},  year = {2021} }
Commonsense Reasoning with Implicit Knowledge in Natural Language	2021	http://www.semanticscholar.org/paper/884c0b6db564208d99cadf2548f0aa96dee5f859	This work takes a middle ground where it uses smaller language models together with a relatively smaller but targeted natural language text corpora and proposes four methods competitive to state-of-the-art methods to reason with implicit commonsense.	maybe	1	Commonsense Reasoning is a research challenge studied from the early days of AI. In recent years, several natural language QA task have been proposed where commonsense reasoning is important. Two common approaches to this are (i) Use of well-structured commonsense present in knowledge graphs, and (ii) Use of progressively larger transformer language models. While acquiring and representing commonsense in a formal representation is challenging in approach (i), approach (ii) gets more and more resource-intensive. In this work, we take a middle ground where we use smaller language models together with a relatively smaller but targeted natural language text corpora. The advantages of such an approach is that it is less resource intensive and yet at the same time it can use unstructured text corpora. We define different unstructured commonsense knowledge sources, explore three strategies for knowledge incorporation, and propose four methods competitive to state-of-the-art methods to reason with implicit commonsense.	884c0b6db564208d99cadf2548f0aa96dee5f859	@['JournalArticle']{banerjee-mishra-2021-commonsense,  author = {Pratyay Banerjee and Swaroop Mishra},  booktitle = {Conference on Automated Knowledge Base Construction},  title = {Commonsense Reasoning with Implicit Knowledge in Natural Language},  year = {2021} }
Commonsense Knowledge-Augmented Pretrained Language Models for Causal Reasoning Classification	2021	http://www.semanticscholar.org/paper/450a3a44993771dc3119743fcaf69a5061d4f4d1	This work converts triples in ATOMIC 2020 to natural language text and continually pretrain a BERT pretrained language model, showing that a continually pretrained model augmented with commonsense reasoning knowledge outperforms the base model on two commonsense causal reasoning benchmarks.	maybe	0	Commonsense knowledge can be leveraged 001 for identifying causal relations in text. In this 002 work, we convert triples in ATOMIC 2020 , a wide 003 coverage commonsense reasoning knowledge 004 graph, to natural language text and continually 005 pretrain a BERT pretrained language model. 006 We evaluate the resulting model on answer- 007 ing commonsense reasoning questions. Our 008 results show that a continually pretrained lan- 009 guage model augmented with commonsense 010 reasoning knowledge outperforms our base- 011 line on two commonsense causal reasoning 012 benchmarks, COPA and BCOPA-CE, without 013 additional improvement on the base model or 014 using quality-enhanced data for ﬁne-tuning.	450a3a44993771dc3119743fcaf69a5061d4f4d1	@['JournalArticle']{hosseini-etal-2021-commonsense,  author = {Pedram Hosseini and David A. Broniatowski and Mona T. Diab},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Commonsense Knowledge-Augmented Pretrained Language Models for Causal Reasoning Classification},  volume = {abs/2112.08615},  year = {2021} }
Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey	2022	http://www.semanticscholar.org/paper/7e5ca499cd9b932921bda84db98f75087d0b0683	A survey of commonsense knowledge acquisition and reasoning tasks, the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions are presented.	yes	8	While commonsense knowledge acquisition and reasoning has traditionally been a core research topic in the knowledge representation and reasoning community, recent years have seen a surge of interest in the natural language processing community in developing pre-trained models and testing their ability to address a variety of newly designed commonsense knowledge reasoning and generation tasks. This paper presents a survey of these tasks, discusses the strengths and weaknesses of state-of-the-art pre-trained models for commonsense reasoning and generation as revealed by these tasks, and reflects on future research directions.	7e5ca499cd9b932921bda84db98f75087d0b0683	@['JournalArticle', 'Conference', 'Review']{bhargava-ng-2022-commonsense,  author = {Prajjwal Bhargava and Vincent Ng},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {12317-12325},  title = {Commonsense Knowledge Reasoning and Generation with Pre-trained Language Models: A Survey},  year = {2022} }
Commonsense Knowledge Mining from Pretrained Models	2019	https://www.semanticscholar.org/paper/f98e135986414cccf29aec593d547c0656e4d82c	This work develops a method for generating commonsense knowledge using a large, pre-trained bidirectional language model that can be used to rank a triple’s validity by the estimated pointwise mutual information between the two entities.	seed	203	Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple’s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.	f98e135986414cccf29aec593d547c0656e4d82c	@['JournalArticle', 'Conference']{feldman-etal-2019-commonsense,  author = {Joshua Feldman and Joe Davison and Alexander M. Rush},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {1173-1178},  title = {Commonsense Knowledge Mining from Pretrained Models},  year = {2019} }
Commonsense Knowledge Completion	2019	http://www.semanticscholar.org/paper/265e4b80b15f985f80114485cede50cb67239a7b	This note introduces several challenges in commonsense knowledge completion and how the traditional and recent methods (with language models) tackle these challenges.	maybe	0	Knowledge graphs, especially commonsense knowledge graphs, often face the issue of sparsity, which prevents them from serving well the downstream tasks. Knowledge completion aims to address this issue by populating the knowledge graphs with newly predicted facts. Most of the works focus on encyclopedic knowledge graphs, in which the entities and relations space are well defined. Only a few attention has been drawn to commonsense knowledge completion previously. The recent progress in language models again raises a surge interest in mining commonsense knowledge from these large-capacity models. In this note, we introduce several challenges in commonsense knowledge completion and how the traditional and recent methods (with language models) tackle these challenges.	265e4b80b15f985f80114485cede50cb67239a7b	@None{wang-2019-commonsense,  author = {Peifeng Wang},  title = {Commonsense Knowledge Completion},  year = {2019} }
Commonsense Inference in Natural Language Processing (COIN) - Shared Task Report	2019	http://www.semanticscholar.org/paper/a32889c86a915b86ddfcc7a176f1a274d329d3dc	The tasks consisted of two machine comprehension evaluations, each of which tested a system’s ability to answer questions/queries about a text, designed such that systems need to exploit commonsense knowledge in the form of inferences over information available in the common ground but not necessarily mentioned in the text.	maybe	5	This paper reports on the results of the shared tasks of the COIN workshop at EMNLP-IJCNLP 2019. The tasks consisted of two machine comprehension evaluations, each of which tested a system’s ability to answer questions/queries about a text. Both evaluations were designed such that systems need to exploit commonsense knowledge, for example, in the form of inferences over information that is available in the common ground but not necessarily mentioned in the text. A total of five participating teams submitted systems for the shared tasks, with the best submitted system achieving 90.6% accuracy and 83.7% F1-score on task 1 and task 2, respectively.	a32889c86a915b86ddfcc7a176f1a274d329d3dc	@['Conference']{ostermann-etal-2019-commonsense,  author = {Simon Ostermann and Sheng Zhang and Michael Roth and Peter Clark},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},  title = {Commonsense Inference in Natural Language Processing (COIN) - Shared Task Report},  year = {2019} }
CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning	2020	http://www.semanticscholar.org/paper/0119a57cf88ef16e6dc291252fae340bb6b3953c	A constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning, and demonstrates that the learned generative Commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA by generating additional context.	maybe	157	Recently, large-scale pre-trained language models have demonstrated impressive performance on several commonsense-reasoning benchmark datasets. However, building machines with commonsense to compose realistically plausible sentences remains challenging. In this paper, we present a constrained text generation task, CommonGen associated with a benchmark dataset, to explicitly test machines for the ability of generative commonsense reasoning. Given a set of common concepts (e.g., dog, frisbee, catch, throw); the task is to generate a coherent sentence describing an everyday scenario using these concepts (e.g., “a man throws a frisbee and his dog catches it”). The CommonGen task is challenging because it inherently requires 1) relational reasoning with background commonsense knowledge and 2) compositional generalization ability to work on unseen concept combinations. Our dataset, constructed through a combination of crowdsourced and existing caption corpora, consists of 77k commonsense descriptions over 35k unique concept-sets. Experiments show that there is a large gap between state-of-the-art text generation models (e.g., T5) and human performance (31.6% v.s. 63.5% in SPICE metric). Furthermore, we demonstrate that the learned generative commonsense reasoning capability can be transferred to improve downstream tasks such as CommonsenseQA (76.9% to 78.4 in dev accuracy) by generating additional context.	0119a57cf88ef16e6dc291252fae340bb6b3953c	@['JournalArticle']{lin-etal-2020-commongen:,  author = {Bill Yuchen Lin and Minghan Shen and Wangchunshu Zhou and Pei Zhou and Chandra Bhagavatula and Yejin Choi and Xiang Ren},  booktitle = {Findings},  pages = {1823-1840},  title = {CommonGen: A Constrained Text Generation Challenge for Generative Commonsense Reasoning},  year = {2020} }
Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models	2022	http://www.semanticscholar.org/paper/8c25f38044b69f54233803c280677c3f8d547e9f	It is demonstrated that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive, while also demonstrating the learnability of hierarchical syntactic information from non-annotated natural language text.	maybe	3	Relations between words are governed by hierarchical structure rather than linear ordering. Sequence-to-sequence (seq2seq) models, despite their success in downstream NLP applications, often fail to generalize in a hierarchy-sensitive manner when performing syntactic transformations—for example, transforming declarative sentences into questions. However, syntactic evaluations of seq2seq models have only observed models that were not pre-trained on natural language data before being trained to perform syntactic transformations, in spite of the fact that pre-training has been found to induce hierarchical linguistic generalizations in language models; in other words, the syntactic capabilities of seq2seq models may have been greatly understated. We address this gap using the pre-trained seq2seq models T5 and BART, as well as their multilingual variants mT5 and mBART. We evaluate whether they generalize hierarchically on two transformations in two languages: question formation and passivization in English and German. We find that pre-trained seq2seq models generalize hierarchically when performing syntactic transformations, whereas models trained from scratch on syntactic transformations do not. This result presents evidence for the learnability of hierarchical syntactic information from non-annotated natural language text while also demonstrating that seq2seq models are capable of syntactic generalization, though only after exposure to much more language data than human learners receive.	8c25f38044b69f54233803c280677c3f8d547e9f	@['JournalArticle']{mueller-etal-2022-coloring,  author = {Aaron Mueller and R. Frank and Tal Linzen and Luheng Wang and Sebastian Schuster},  booktitle = {Findings},  journal = {ArXiv},  title = {Coloring the Blank Slate: Pre-training Imparts a Hierarchical Inductive Bias to Sequence-to-sequence Models},  volume = {abs/2203.09397},  year = {2022} }
Collateral facilitation in humans and language models	2022	http://www.semanticscholar.org/paper/4061a9941fa0ff106e884272d9ed753650417ec4		maybe	1	Are the predictions of humans and language models affected by similar things? Research suggests that while comprehending language, humans make predictions about upcoming words, with more predictable words being processed more easily. However, evidence also shows that humans display a similar processing advantage for highly anomalous words when these words are semantically related to the preceding context or to the most probable continuation. Using stimuli from 3 psycholinguistic experiments, we find that this is also almost always also the case for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa, XLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of this phenomenon for our understanding of both human language comprehension and the predictions made by language models.	4061a9941fa0ff106e884272d9ed753650417ec4	@['JournalArticle']{michaelov-bergen-2022-collateral,  author = {J. Michaelov and B. Bergen},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {Collateral facilitation in humans and language models},  volume = {abs/2211.05198},  year = {2022} }
COGS: A Compositional Generalization Challenge Based on Semantic Interpretation	2020	http://www.semanticscholar.org/paper/b20ddcbd239f3fa9acc603736ac2e4416302d074	In experiments with Transformers and LSTMs, it is found that in-distribution accuracy on the COGS test set was near-perfect, but generalization accuracy was substantially lower, and the dataset showed high sensitivity to random seed.	maybe	108	Natural language is characterized by compositionality: the meaning of a complex expression is constructed from the meanings of its constituent parts. To facilitate the evaluation of the compositional abilities of language processing architectures, we introduce COGS, a semantic parsing dataset based on a fragment of English. The evaluation portion of COGS contains multiple systematic gaps that can only be addressed by compositional generalization; these include new combinations of familiar syntactic structures, or new combinations of familiar words and familiar structures. In experiments with Transformers and LSTMs, we found that in-distribution accuracy on the COGS test set was near-perfect (96--99%), but generalization accuracy was substantially lower (16--35%) and showed high sensitivity to random seed ($\pm$6--8%). These findings indicate that contemporary standard NLP models are limited in their compositional generalization capacity, and position COGS as a good way to measure progress.	b20ddcbd239f3fa9acc603736ac2e4416302d074	@['JournalArticle', 'Conference']{kim-linzen-2020-cogs:,  author = {Najoung Kim and Tal Linzen},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {COGS: A Compositional Generalization Challenge Based on Semantic Interpretation},  volume = {abs/2010.05465},  year = {2020} }
Cognitive Modeling of Semantic Fluency Using Transformers	2022	http://www.semanticscholar.org/paper/53f9169207d635466c12b29e916b8a1fbc3e84ed	Preliminary evidence is reported suggesting that, despite obvious implementational differences in how people and TLMs learn and use language, TLMs can be used to identify individual differences in human fluency task behaviors better than existing computational models, and may offer insights into human memory retrieval strategies.	maybe	1	Can deep language models be explanatory models of human cognition? If so, what are their limits? In order to explore this question, we propose an approach called hyperparameter hypothesization that uses predictive hyperparameter tuning in order to find individuating descriptors of cognitive-behavioral profiles. We take the first step in this approach by predicting human performance in the semantic fluency task (SFT), a well-studied task in cognitive science that has never before been modeled using transformer-based language models (TLMs). In our task setup, we compare several approaches to predicting which word an individual performing SFT will utter next. We report preliminary evidence suggesting that, despite obvious implementational differences in how people and TLMs learn and use language, TLMs can be used to identify individual differences in human fluency task behaviors better than existing computational models, and may offer insights into human memory retrieval strategies—cognitive process not typically considered to be the kinds of things TLMs can model. Finally, we discuss the implications of this work for cognitive modeling of knowledge representations.	53f9169207d635466c12b29e916b8a1fbc3e84ed	@['JournalArticle']{nighojkar-etal-2022-cognitive,  author = {Animesh Nighojkar and Anna Khlyzova and John Licato},  booktitle = {CAKR@IJCAI},  journal = {ArXiv},  title = {Cognitive Modeling of Semantic Fluency Using Transformers},  volume = {abs/2208.09719},  year = {2022} }
Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation	2022	http://www.semanticscholar.org/paper/8175ce2cbb99b6a394bdac152ae39d413f4f1380	This work evaluates the code synthesis capabilities of the Codex model based on a set of 115 Python problem statements from a popular competitive programming portal: HackerRank, and proposes a framework for code-synthesis evaluation using variations of problem statements based on mutations.	maybe	1	The Codex model has demonstrated extraordinary competence in synthesizing code from natural language problem descriptions [8]. However, in order to reveal unknown failure modes and hidden biases, such large-scale models must be systematically subjected to multiple and diverse evaluation studies. In this work, we evaluate the code synthesis capabilities of the Codex model based on a set of 115 Python problem statements from a popular competitive programming portal: HackerRank. Our evaluation shows that Codex is indeed proficient in Python—solving 96% of the problems in a zero-shot setting, and 100% of the problems in a few-shot setting. However, Codex exhibits clear signs of generating memorized code based on our evaluation. This is alarming, especially since the adoption and use of such models could directly impact how code is written and produced in the foreseeable future. With this in mind, we further discuss and highlight some of the prominent risks associated with large-scale models of source code. Finally, we propose a framework for code-synthesis evaluation using variations of problem statements based on mutations.	8175ce2cbb99b6a394bdac152ae39d413f4f1380	@['JournalArticle']{karmakar-etal-2022-codex,  author = {Anjan Karmakar and Julian Aron Prenner and Marco D'Ambros and R. Robbes},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Codex Hacks HackerRank: Memorization Issues and a Framework for Code Synthesis Evaluation},  volume = {abs/2212.02684},  year = {2022} }
CodeBERT: A Pre-Trained Model for Programming and Natural Languages	2020	http://www.semanticscholar.org/paper/0fe2636446cd686830da3d971b31a004d6094b3c	This work develops CodeBERT with Transformer-based neural architecture, and trains it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators.	maybe	619	We present CodeBERT, a bimodal pre-trained model for programming language (PL) and natural language (NL). CodeBERT learns general-purpose representations that support downstream NL-PL applications such as natural language code search, code documentation generation, etc. We develop CodeBERT with Transformer-based neural architecture, and train it with a hybrid objective function that incorporates the pre-training task of replaced token detection, which is to detect plausible alternatives sampled from generators. This enables us to utilize both “bimodal” data of NL-PL pairs and “unimodal data, where the former provides input tokens for model training while the latter helps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.	0fe2636446cd686830da3d971b31a004d6094b3c	@['JournalArticle']{feng-etal-2020-codebert:,  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},  booktitle = {Findings},  journal = {ArXiv},  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},  volume = {abs/2002.08155},  year = {2020} }
Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code	2022	http://www.semanticscholar.org/paper/7ffb212356df9980347b3d3b9910dfba75a5d0c7	This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose, and concludes that few-shot language models are surprisingly effective.	maybe	7	Few-shot learningwith large-scale, pre-trained languagemodels is a powerful way to answer questions about code, e.g., how to complete a given code example, or even generate code snippets from scratch. The success of these models raises the question whether they could serve as a basis for building a wide range code generation tools. Traditionally, such tools are built manually and separately for each task. Instead, few-shot learning may allow to obtain different tools from a single pre-trained language model by simply providing a few examples or a natural language description of the expected tool behavior. This paper studies to what extent a state-of-the-art, pre-trained language model of code, Codex, may serve this purpose. We consider three code manipulation and code generation tasks targeted by a range of traditional tools: (i) code mutation; (ii) test oracle generation from natural language documentation; and (iii) test case generation. For each task, we compare few-shot learning to a manually built tool. Our results show that the model-based tools complement (code mutation), are on par (test oracle generation), or even outperform their respective traditionally built tool (test case generation), while imposing far less effort to develop them. By comparing the effectiveness of different variants of the model-based tools, we provide insights on how to design an appropriate input (“prompt”) to the model and what influence the size of the model has. For example, we find that providing a small natural language description of the code generation task is an easy way to improve predictions. Overall, we conclude that few-shot language models are surprisingly effective, yet there is still more work to be done, such as exploring more diverse ways of prompting and tackling even more involved tasks.	7ffb212356df9980347b3d3b9910dfba75a5d0c7	@['JournalArticle']{bareiss-etal-2022-code,  author = {Patrick Bareiss and Beatriz Souza and Marcelo d’Amorim and Michael Pradel},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Code Generation Tools (Almost) for Free? A Study of Few-Shot, Pre-Trained Language Models on Code},  volume = {abs/2206.01335},  year = {2022} }
Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data	2020	http://www.semanticscholar.org/paper/02eaaf87f9cae34cca398fed146079e6eeb1f868	It is argued that a system trained only on form has a priori no way to learn meaning, and a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.	yes	347	The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.	02eaaf87f9cae34cca398fed146079e6eeb1f868	@['JournalArticle', 'Conference']{bender-koller-2020-climbing,  author = {Emily M. Bender and Alexander Koller},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5185-5198},  title = {Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data},  year = {2020} }
Classifier Probes May Just Learn from Linear Context Features	2020	http://www.semanticscholar.org/paper/6906c562e182ea22c086b2bb44ed9cec8602a220	It is shown that the token embeddings learned by neural sentence encoders contain a significant amount of information about the exact linear context of the token, and it is hypothesized that, with such information, learning standard probing tasks may be feasible even without additional linguistic structure.	yes	5	Classifiers trained on auxiliary probing tasks are a popular tool to analyze the representations learned by neural sentence encoders such as BERT and ELMo. While many authors are aware of the difficulty to distinguish between “extracting the linguistic structure encoded in the representations” and “learning the probing task,” the validity of probing methods calls for further research. Using a neighboring word identity prediction task, we show that the token embeddings learned by neural sentence encoders contain a significant amount of information about the exact linear context of the token, and hypothesize that, with such information, learning standard probing tasks may be feasible even without additional linguistic structure. We develop this hypothesis into a framework in which analysis efforts can be scrutinized and argue that, with current models and baselines, conclusions that representations contain linguistic structure are not well-founded. Current probing methodology, such as restricting the classifier’s expressiveness or using strong baselines, can help to better estimate the complexity of learning, but not build a foundation for speculations about the nature of the linguistic structure encoded in the learned representations.	6906c562e182ea22c086b2bb44ed9cec8602a220	@['JournalArticle', 'Conference']{kunz-kuhlmann-2020-classifier,  author = {Jenny Kunz and Marco Kuhlmann},  booktitle = {International Conference on Computational Linguistics},  pages = {5136-5146},  title = {Classifier Probes May Just Learn from Linear Context Features},  year = {2020} }
Circles are like Ellipses, or Ellipses are like Circles? Measuring the Degree of Asymmetry of Static and Contextual Embeddings and the Implications to Representation Learning	2020	http://www.semanticscholar.org/paper/3e4c16b56dccb1685e8cb0c973242792ce8f8fe3	This is the first time that contextual embeddings's strength on intrinsic evaluation is shown, and the asymmetry judgment provides a new perspective to evaluate contextual embedding and new insights for representation learning.	maybe	0	Human judgments of word similarity have been a popular method of evaluating the quality of word embedding. But it fails to measure the geometry properties such as asymmetry. For example, it is more natural to say ``Ellipses are like Circles'' than ``Circles are like Ellipses''. Such asymmetry has been observed from the word evocation experiment, where one word is used to recall another. This association data have been understudied for measuring embedding quality. In this paper, we use three well-known evocation datasets for the purpose and study both static embedding as well as contextual embedding, such as BERT. To fight for the dynamic nature of BERT embedding, we probe BERT's conditional probabilities as a language model, using a large number of Wikipedia contexts to derive a theoretically justifiable Bayesian asymmetry score. The result shows that the asymmetry judgment and similarity judgments disagree, and asymmetry judgment aligns with its strong performance on ``extrinsic evaluations''. This is the first time we can show contextual embeddings's strength on intrinsic evaluation, and the asymmetry judgment provides a new perspective to evaluate contextual embedding and new insights for representation learning.	3e4c16b56dccb1685e8cb0c973242792ce8f8fe3	@['JournalArticle', 'Conference']{zhang-etal-2020-circles,  author = {Wei Zhang and Murray Campbell and Yang Yu and Sadhana Kumaravel},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {14472-14480},  title = {Circles are like Ellipses, or Ellipses are like Circles? Measuring the Degree of Asymmetry of Static and Contextual Embeddings and the Implications to Representation Learning},  year = {2020} }
Characterizing Verbatim Short-Term Memory in Neural Language Models	2022	http://www.semanticscholar.org/paper/8a74fb6c8f48ca9a42e55c443672c4f6e06bf0ef		yes	0	When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers’ retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM’s retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.	8a74fb6c8f48ca9a42e55c443672c4f6e06bf0ef	@['JournalArticle']{armeni-etal-2022-characterizing,  author = {K. Armeni and C. Honey and Tal Linzen},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {Characterizing Verbatim Short-Term Memory in Neural Language Models},  volume = {abs/2210.13569},  year = {2022} }
Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling	2021	http://www.semanticscholar.org/paper/f49065750931c1c3c9edaf7d2f4bc8ea1342450a	The high degree of oversmoothing is the main reason behind the degenerate case of overly probable short sequences in a neural autoregressive model and the proposed regularization is to explicitly minimize the overSmoothing rate during training.	maybe	3	Neural autoregressive sequence models smear the probability among many possible sequences including degenerate ones, such as empty or repetitive sequences. In this work, we tackle one specific case where the model assigns a high probability to unreasonably short sequences. We define the oversmoothing rate to quantify this issue. After confirming the high degree of oversmoothing in neural machine translation, we propose to explicitly minimize the oversmoothing rate during training. We conduct a set of experiments to study the effect of the proposed regularization on both model distribution and decoding performance. We use a neural machine translation task as the testbed and consider three different datasets of varying size. Our experiments reveal three major findings. First, we can control the oversmoothing rate of the model by tuning the strength of the regularization. Second, by enhancing the oversmoothing loss contribution, the probability and the rank of eos token decrease heavily at positions where it is not supposed to be. Third, the proposed regularization impacts the outcome of beam search especially when a large beam is used. The degradation of translation quality (measured in BLEU) with a large beam significantly lessens with lower oversmoothing rate, but the degradation compared to smaller beam sizes remains to exist. From these observations, we conclude that the high degree of oversmoothing is the main reason behind the degenerate case of overly probable short sequences in a neural autoregressive model.	f49065750931c1c3c9edaf7d2f4bc8ea1342450a	@['JournalArticle']{kulikov-etal-2021-characterizing,  author = {Ilia Kulikov and M. Eremeev and Kyunghyun Cho},  booktitle = {AACL},  journal = {ArXiv},  title = {Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling},  volume = {abs/2112.08914},  year = {2021} }
Changing the Basis of Contextual Representations with Explicit Semantics	2021	http://www.semanticscholar.org/paper/016dc75cb6666eb3e9394b68741690af57956e97	This work designs a transformation matrix based on the semantic content of the embedding space and predefined semantic categories that makes it possible to accurately predict the supersense category of a word by simply looking for its transformed coordinate with the largest coefficient.	maybe	0	The application of transformer-based contextual representations has became a de facto solution for solving complex NLP tasks. Despite their successes, such representations are arguably opaque as their latent dimensions are not directly interpretable. To alleviate this limitation of contextual representations, we devise such an algorithm where the output representation expresses human-interpretable information of each dimension. We achieve this by constructing a transformation matrix based on the semantic content of the embedding space and predefined semantic categories using Hellinger distance. We evaluate our inferred representations on supersense prediction task. Our experiments reveal that the interpretable nature of transformed contextual representations makes it possible to accurately predict the supersense category of a word by simply looking for its transformed coordinate with the largest coefficient. We quantify the effects of our proposed transformation when applied over traditional dense contextual embeddings. We additionally investigate and report consistent improvements for the integration of sparse contextual word representations into our proposed algorithm.	016dc75cb6666eb3e9394b68741690af57956e97	@['JournalArticle', 'Conference']{ficsor-berend-2021-changing,  author = {Tamás Ficsor and Gábor Berend},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {235-247},  title = {Changing the Basis of Contextual Representations with Explicit Semantics},  year = {2021} }
Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them	2022	http://www.semanticscholar.org/paper/663a41c866d49ce052801fbc88947d39764cad29	It is found that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex to surpass it on 17 of the23 tasks.	maybe	18	BIG-Bench (Srivastava et al., 2022) is a diverse evaluation suite that focuses on tasks believed to be beyond the capabilities of current language models. Language models have already made good progress on this benchmark, with the best model in the BIG-Bench paper outperforming average reported human-rater results on 65% of the BIG-Bench tasks via few-shot prompting. But on what tasks do language models fall short of average human-rater performance, and are those tasks actually unsolvable by current language models? In this work, we focus on a suite of 23 challenging BIG-Bench tasks which we call BIG-Bench Hard ( BBH ). These are the task for which prior language model evaluations did not outperform the average human-rater. We ﬁnd that applying chain-of-thought (CoT) prompting to BBH tasks enables PaLM to surpass the average human-rater performance on 10 of the 23 tasks, and Codex (code-davinci-002) to surpass the average human-rater performance on 17 of the 23 tasks. Since many tasks in BBH require multi-step reasoning, few-shot prompting without CoT, as done in the BIG-Bench evaluations (Srivastava et al., 2022), substantially underestimates the best performance and capabilities of language models, which is better captured via CoT prompting. As further analysis, we explore the interaction between CoT and model scale on BBH, ﬁnding that CoT enables emergent task performance on several BBH tasks with otherwise ﬂat scaling curves. 1 identiﬁcation, abstract narrative understanding, matrixshapes, hindu knowledge, simp turing concept, protein interacting sites, parsinlu reading comprehension, simple arithmetic	663a41c866d49ce052801fbc88947d39764cad29	@['JournalArticle']{suzgun-etal-2022-challenging,  author = {Mirac Suzgun and Nathan Scales and Nathanael Scharli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and E. Chi and Denny Zhou and Jason Wei},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them},  volume = {abs/2210.09261},  year = {2022} }
Challenges in Measuring Bias via Open-Ended Language Generation	2022	http://www.semanticscholar.org/paper/294292881447169461a6fcefbe8951b5b05528a8	It is found that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings, and recommendations for reporting biases in open-ended language generation are provided for a more complete outlook of biases exhibited by a given language model.	maybe	3	Researchers have devised numerous ways to quantify social biases vested in pretrained language models. As some language models are capable of generating coherent completions given a set of textual prompts, several prompting datasets have been proposed to measure biases between social groups—posing language generation as a way of identifying biases. In this opinion paper, we analyze how specific choices of prompt sets, metrics, automatic tools and sampling strategies affect bias results. We find out that the practice of measuring biases through text completion is prone to yielding contradicting results under different experiment settings. We additionally provide recommendations for reporting biases in open-ended language generation for a more complete outlook of biases exhibited by a given language model. Code to reproduce the results is released under https://github.com/feyzaakyurek/bias-textgen.	294292881447169461a6fcefbe8951b5b05528a8	@['JournalArticle']{akyurek-etal-2022-challenges,  author = {Afra Feyza Akyurek and Muhammed Yusuf Kocyigit and Sejin Paik and D. Wijaya},  booktitle = {GEBNLP},  journal = {ArXiv},  title = {Challenges in Measuring Bias via Open-Ended Language Generation},  volume = {abs/2205.11601},  year = {2022} }
Challenges in Detoxifying Language Models	2021	https://www.semanticscholar.org/paper/d64e57b9780f30f5b49bf620fdfb8584651b7f85	It is demonstrated that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups.	maybe	33	Large language models (LM) generate remarkably fluent text and can be efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of generated text in terms of safety is imperative for deploying LMs in the real world; to this end, prior work often relies on automatic evaluation of LM toxicity. We critically discuss this approach, evaluate several toxicity mitigation strategies with respect to both automatic and human evaluation, and analyze consequences of toxicity mitigation in terms of model bias and LM quality. We demonstrate that while basic intervention strategies can effectively optimize previously established automatic metrics on the REALTOXICITYPROMPTS dataset, this comes at the cost of reduced LM coverage for both texts about, and dialects of, marginalized groups. Additionally, we find that human raters often disagree with high automatic toxicity scores after strong toxicity reduction interventions— highlighting further the nuances involved in careful evaluation of LM toxicity.	d64e57b9780f30f5b49bf620fdfb8584651b7f85	@['JournalArticle', 'Conference']{welbl-etal-2021-challenges,  author = {Johannes Welbl and A. Glaese and J. Uesato and Sumanth Dathathri and John F. J. Mellor and Lisa Anne Hendricks and Kirsty Anderson and Pushmeet Kohli and Ben Coppin and Po-Sen Huang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Challenges in Detoxifying Language Models},  volume = {abs/2109.07445},  year = {2021} }
Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models	2022	http://www.semanticscholar.org/paper/a452c3fe8d7d5199afe66e1db519f528df3f487f	Trends in explainability and fairness in NLP research are reviewed, the current practices in which explainability methods are applied to detect and mitigate bias are identified, and barriers preventing XAI methods from being used more widely in tackling fairness issues are investigated.	maybe	3	Motivations for methods in explainable artificial intelligence (XAI) often include detecting, quantifying and mitigating bias, and contributing to making machine learning models fairer. However, exactly how an XAI method can help in combating biases is often left unspecified. In this paper, we briefly review trends in explainability and fairness in NLP research, identify the current practices in which explainability methods are applied to detect and mitigate bias, and investigate the barriers preventing XAI methods from being used more widely in tackling fairness issues.	a452c3fe8d7d5199afe66e1db519f528df3f487f	@['JournalArticle', 'Review']{balkir-etal-2022-challenges,  author = {Esma Balkir and Svetlana Kiritchenko and I. Nejadgholi and Kathleen C. Fraser},  booktitle = {TRUSTNLP},  journal = {ArXiv},  title = {Challenges in Applying Explainability Methods to Improve the Fairness of NLP Models},  volume = {abs/2206.03945},  year = {2022} }
Challenges and Strategies in Cross-Cultural NLP	2022	http://www.semanticscholar.org/paper/598231eb906b183f7a2a408ef4536127e11e3de9		maybe	9	Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.	598231eb906b183f7a2a408ef4536127e11e3de9	@['JournalArticle', 'Conference', 'Review']{hershcovich-etal-2022-challenges,  author = {Daniel Hershcovich and Stella Frank and Heather Christine Lent and Miryam de Lhoneux and Mostafa Abdou and Stephanie Brandl and Emanuele Bugliarello and Laura Cabello Piqueras and Ilias Chalkidis and Ruixiang Cui and Constanza Fierro and Katerina Margatina and Phillip Rust and Anders Søgaard},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {6997-7013},  title = {Challenges and Strategies in Cross-Cultural NLP},  year = {2022} }
Chain of Thought Prompting Elicits Reasoning in Large Language Models	2022	http://www.semanticscholar.org/paper/5d0db797a45ce2453f821f7ded0b547d3fdab054	Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks.	yes	296	We explore how generating a chain of thought —a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-of-thought prompting , where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.	5d0db797a45ce2453f821f7ded0b547d3fdab054	@['JournalArticle']{wei-etal-2022-chain,  author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and E. Chi and Quoc Le and Denny Zhou},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Chain of Thought Prompting Elicits Reasoning in Large Language Models},  volume = {abs/2201.11903},  year = {2022} }
Causal Reasoning of Entities and Events in Procedural Texts	2023	https://www.semanticscholar.org/paper/eb342869060db5fac99e5380faf669e15bd40d6e	This work proposes CREPE, the first benchmark on causal reasoning about event plausibility based on entity states, and marks the first successful attempt of chain-of-thought reasoning with code language models.	maybe	0	Entities and events have long been regarded as the crux of machine reasoning. Speciﬁcally, procedural texts have received increas-ing attention due to the dynamic nature of involved entities and events. Existing work has exclusively focused on entity state tracking (e.g., the temperature of a pan ) or counterfactual event reasoning (e.g., how likely am I to burn myself by touching the pan ), while these two tasks are tightly intertwined. In this work, we propose CREPE, the ﬁrst benchmark on causal reasoning about event plausibility based on entity states. We experiment with strong large language models and show that most models including GPT3 perform close to chance of .30 F1, lagging far behind the human performance of .87 F1. Inspired by the ﬁnd-ing that structured representations such as programming languages beneﬁts event reasoning as a prompt to code language models such as Codex, we creatively inject the causal relations between entities and events through intermediate variables and boost the performance to .67 to .72 F1. Our proposed event representation not only allows for knowledge injection, but also marks the ﬁrst successful attempt of chain-of-thought reasoning with code language models. 12	eb342869060db5fac99e5380faf669e15bd40d6e	@['JournalArticle']{zhang-etal-2023-causal,  author = {Li Zhang and Hai Xu and Yue Yang and Shuyan Zhou and Weiqiu You and Manni Arora and Chris Callison-Burch},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Causal Reasoning of Entities and Events in Procedural Texts},  volume = {abs/2301.10896},  year = {2023} }
Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias	2020	https://www.semanticscholar.org/paper/131c19dc3d460e1973d4b015cc9f888bae4f200b	The authors' mediation analysis reveals that gender bias effects are sparse, concentrated in a small part of the network; synergistic, amplified or repressed by different components; and decomposable into effects flowing directly from the input and indirectly through the mediators.	maybe	61	Common methods for interpreting neural models in natural language processing typically examine either their structure or their behavior, but not both. We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. It enables us to analyze the mechanisms by which information flows from input to output through various model components, known as mediators. We apply this methodology to analyze gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are (i) sparse, concentrated in a small part of the network; (ii) synergistic, amplified or repressed by different components; and (iii) decomposable into effects flowing directly from the input and indirectly through the mediators.	131c19dc3d460e1973d4b015cc9f888bae4f200b	@['JournalArticle']{vig-etal-2020-causal,  author = {Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and D. Nevo and Yaron Singer and S. Shieber},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias},  volume = {abs/2004.12265},  year = {2020} }
Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models	2022	http://www.semanticscholar.org/paper/e886fe33b9cda6e0b0f523d5fbbb7287c23aba2b	This study causally probe multilingual language models (XGLM and multilingual BERT) as well as monolingual BERT-based models across various languages by performing counterfactual perturbations on neuron activations and observing the effect on models’ subject-verb agreement probabilities, finding that behavioral analyses of language models are likely underestimating how sensitive maskedlanguage models are to syntactic information.	yes	1	Structural probing work has found evidence for latent syntactic information in pre-trained language models. However, much of this analysis has focused on monolingual models, and analyses of multilingual models have employed correlational methods that are confounded by the choice of probing tasks. In this study, we causally probe multilingual language models (XGLM and multilingual BERT) as well as monolingual BERT-based models across various languages; we do this by performing counterfactual perturbations on neuron activations and observing the effect on models’ subject-verb agreement probabilities. We observe where in the model and to what extent syntactic agreement is encoded in each language. We find significant neuron overlap across languages in autoregressive multilingual language models, but not masked language models. We also find two distinct layer-wise effect patterns and two distinct sets of neurons used for syntactic agreement, depending on whether the subject and verb are separated by other tokens. Finally, we find that behavioral analyses of language models are likely underestimating how sensitive masked language models are to syntactic information.	e886fe33b9cda6e0b0f523d5fbbb7287c23aba2b	@['JournalArticle']{mueller-etal-2022-causal,  author = {Aaron Mueller and Yudi Xia and Tal Linzen},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models},  volume = {abs/2210.14328},  year = {2022} }
Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models	2021	https://www.semanticscholar.org/paper/488de57dde884402d88ccecfd02347dd7e4d01a1		yes	27	Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models’ preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes—notably, that larger models do not necessarily learn stronger preferences. We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.	488de57dde884402d88ccecfd02347dd7e4d01a1	@['JournalArticle', 'Conference']{finlayson-etal-2021-causal,  author = {Matthew Finlayson and Aaron Mueller and S. Shieber and Sebastian Gehrmann and Tal Linzen and Yonatan Belinkov},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models},  volume = {abs/2106.06087},  year = {2021} }
Causal Abstraction for Faithful Model Interpretation	2023	http://www.semanticscholar.org/paper/c289bf6358d847636f2159135cd09fadb7a939c0	It is argued that the theory of causal abstraction provides the mathematical foundations for the desired kinds of model explanations and formalizes the XAI methods of LIME, causal eﬀect estimation, causal mediation analysis, iterated nullspace projection, and circuit-based explanations as special cases of causal abstraction analysis.	maybe	0	A faithful and interpretable explanation of an AI model’s behavior and internal structure is a high-level explanation that is human-intelligible but also consistent with the known, but often opaque low-level causal details of the model. We argue that the theory of causal abstraction provides the mathematical foundations for the desired kinds of model explanations. In causal abstraction analysis, we use interventions on model-internal states to rigorously assess whether an interpretable high-level causal model is a faithful description of an AI model. Our contributions in this area are: (1) We generalize causal abstraction to cyclic causal structures and typed high-level variables. (2) We show how multi-source interchange interventions can be used to conduct causal abstraction analyses. (3) We deﬁne a notion of approximate causal abstraction that allows us to assess the degree to which a high-level causal model is a causal abstraction of a lower-level one. (4) We prove constructive causal abstraction can be decomposed into three operations we refer to as marginalization, variable-merge, and value-merge. (5) We formalize the XAI methods of LIME, causal eﬀect estimation, causal mediation analysis, iterated nullspace projection, and circuit-based explanations as special cases of causal abstraction analysis.	c289bf6358d847636f2159135cd09fadb7a939c0	@['JournalArticle']{geiger-etal-2023-causal,  author = {Atticus Geiger and Chris Potts and Thomas F. Icard},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Causal Abstraction for Faithful Model Interpretation},  volume = {abs/2301.04709},  year = {2023} }
Catch the "Tails" of BERT	2020	http://www.semanticscholar.org/paper/a49577c85544ec3fe0db7198a95ad397949a37dd	The results show that " tails" are the major cause of anisotropy of vector space, and a new neuron-level method is introduced to analyze where these "tails" come from, which suggest that "tail" are less related to the sense and syntax information in vectors.	yes	0	Recently, contextualized word embeddings outperform static word embeddings on many NLP tasks. However, we still do not know much about the mechanism inside these representations. Do they have any common patterns? If so, where do these patterns come from? We find that almost all the contextualized word vectors of BERT and RoBERTa have a common pattern. For BERT, the $557^{th}$ element is always the smallest. For RoBERTa, the $588^{th}$ element is always the largest, and the $77^{th}$ element is the smallest. We call them "tails" of models. We introduce a new neuron-level method to analyze where these "tails" come from. We find that these "tails" are closely related to the positional information. We also investigate what will happen if we "cutting the tails" (zero-out). Our results show that "tails" are the major cause of anisotropy of vector space. After "cutting the tails", a word's different vectors are more similar to each other. The internal representations have a better ability to distinguish a word's different senses with the word-in-context (WiC) dataset. The performance on the word sense disambiguation task is better for BERT and unchanged for RoBERTa. We can also better induce phrase grammar from the vector space. These suggest that "tails" are less related to the sense and syntax information in vectors. These findings provide insights into the inner workings of contextualized word vectors.	a49577c85544ec3fe0db7198a95ad397949a37dd	@['JournalArticle']{luo-2020-catch,  author = {Ziyang Luo},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Catch the "Tails" of BERT},  volume = {abs/2011.04393},  year = {2020} }
Capturing Failures of Large Language Models via Human Cognitive Biases	2022	http://www.semanticscholar.org/paper/76f023c3a819fc58989a064a1b50825b11fce95d	The results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave, and draw inspiration from human cognitive biases as motivation to generate hypotheses for problems that models may have and develop experiments that elicit these problems.	maybe	7	Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code. In order to asses the reliability of these open-ended generation systems, we aim to identify qualitative categories of erroneous behavior, beyond identifying individual errors. To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases—systematic patterns of deviation from rational judgement. Speciﬁcally, we use cognitive biases as motivation to (i) generate hypotheses for problems that models may have, and (ii) develop experiments that elicit these problems. Using code generation as a case study, we ﬁnd that OpenAI’s Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly deleting ﬁles. Our results indicate that experimental methodology from cognitive science can help characterize how machine learning systems behave. 1	76f023c3a819fc58989a064a1b50825b11fce95d	@['JournalArticle']{jones-steinhardt-2022-capturing,  author = {Erik Jones and J. Steinhardt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Capturing Failures of Large Language Models via Human Cognitive Biases},  volume = {abs/2202.12299},  year = {2022} }
Can We Really Trust Explanations? Evaluating the Stability of Feature Attribution Explanation Methods via Adversarial Attack	2022	http://www.semanticscholar.org/paper/8bc2a172e7de0f26ab3bfb6c156027b8fb34a615	A new evaluation frame is proposed to evaluate the stability of current typical feature attribution explanation methods via textual adversarial attack and could reveal the stability performance of existing explanation methods.	maybe	0		8bc2a172e7de0f26ab3bfb6c156027b8fb34a615	@['JournalArticle']{yang-etal-2022-can,  author = {Zhao Yang and Yuanzhe Zhang and Zhongtao Jiang and Yiming Ju and Jun Zhao and Kang Liu},  booktitle = {China National Conference on Chinese Computational Linguistics},  pages = {281-297},  title = {Can We Really Trust Explanations? Evaluating the Stability of Feature Attribution Explanation Methods via Adversarial Attack},  year = {2022} }
Can Transformers Reason in Fragments of Natural Language?	2022	http://www.semanticscholar.org/paper/8ee376114a43432399554be39a79c1a2b6c65d51	A large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisﬁability problem becomes increasingly complex finds that transformer-based language models perform surprisingly well in these scenarios.	maybe	1	State-of-the-art deep-learning-based approaches to Natural Language Processing (NLP) are credited with various capabilities that involve reasoning with natural language texts. %However, reasoning in this setting is often ill-defined and shallow. In this paper we carry out a large-scale empirical study investigating the detection of formally valid inferences in controlled fragments of natural language for which the satisfiability problem becomes increasingly complex. We find that, while transformer-based language models perform surprisingly well in these scenarios, a deeper analysis reveals that they appear to overfit to superficial patterns in the data rather than acquiring the logical principles governing the reasoning in these fragments.	8ee376114a43432399554be39a79c1a2b6c65d51	@['JournalArticle', 'Conference']{schlegel-etal-2022-can,  author = {Viktor Schlegel and Kamen V. Pavlov and I. Pratt-Hartmann},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Can Transformers Reason in Fragments of Natural Language?},  volume = {abs/2211.05417},  year = {2022} }
Can Transformers Process Recursive Nested Constructions, Like Humans?	2022	http://www.semanticscholar.org/paper/00df3cbe8da73abf6e556237f44419ee9bf0fbde	It is found that Transformers achieve near-perfect performance on short-range embedded dependencies, significantly better than previous results reported for RNN-LMs and humans, however, on long-rangeedded dependencies, Transformers’ performance sharply drops below chance level.	yes	1	Recursive processing is considered a hallmark of human linguistic abilities. A recent study evaluated recursive processing in recurrent neural language models (RNN-LMs) and showed that such models perform below chance level on embedded dependencies within nested constructions – a prototypical example of recursion in natural language. Here, we study if state-of-the-art Transformer LMs do any better. We test eight different Transformer LMs on two different types of nested constructions, which differ in whether the embedded (inner) dependency is short or long range. We find that Transformers achieve near-perfect performance on short-range embedded dependencies, significantly better than previous results reported for RNN-LMs and humans. However, on long-range embedded dependencies, Transformers’ performance sharply drops below chance level. Remarkably, the addition of only three words to the embedded dependency caused Transformers to fall from near-perfect to below-chance performance. Taken together, our results reveal how brittle syntactic processing is in Transformers, compared to humans.	00df3cbe8da73abf6e556237f44419ee9bf0fbde	@['JournalArticle', 'Conference']{lakretz-etal-2022-can,  author = {Yair Lakretz and T. Desbordes and D. Hupkes and S. Dehaene},  booktitle = {International Conference on Computational Linguistics},  pages = {3226-3232},  title = {Can Transformers Process Recursive Nested Constructions, Like Humans?},  year = {2022} }
Can Transformer Language Models Predict Psychometric Properties?	2021	http://www.semanticscholar.org/paper/3177b63f5d6ce3aa2e172eb5c94412fdaf3be5fd	Cases are found in which transformer-based LMs predict psychometric properties consistently well in certain categories but consistently poorly in others, thus providing new insights into fundamental similarities and differences between human and LM reasoning.	maybe	5	Transformer-based language models (LMs) continue to advance state-of-the-art performance on NLP benchmark tasks, including tasks designed to mimic human-inspired “commonsense” competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts of the field of psychometrics. But to what extent can the benefits flow in the other direction? I.e., can LMs be of use in predicting what the psychometric properties of test items will be when those items are given to human participants? We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions match. We find cases in which transformer-based LMs predict psychometric properties consistently well in certain categories but consistently poorly in others, thus providing new insights into fundamental similarities and differences between human and LM reasoning.	3177b63f5d6ce3aa2e172eb5c94412fdaf3be5fd	@['JournalArticle']{laverghetta-etal-2021-can,  author = {A. Laverghetta and Animesh Nighojkar and Jamshidbek Mirzakhalov and John Licato},  booktitle = {STARSEM},  pages = {12-25},  title = {Can Transformer Language Models Predict Psychometric Properties?},  year = {2021} }
Can RoBERTa Reason? A Systematic Approach to Probe Logical Reasoning in Language Models	2020	http://www.semanticscholar.org/paper/170ec3ac79b81f21ac35247b7f8e73991a14ebac	It is found that despite the current success of large LMs on commonsense benchmarks, their performance on these tasks is no better than random guessing, heavily dependent on biases, and not robust to the linguistic perturbation.	yes	0	Humans can map natural language into a logical representation that is robust to linguistic variations and useful for reasoning. While pre-trained language models (LM) have dramatically improved performance on commonsense reasoning benchmarks, it remains unclear whether they share this ability to reason consistently amid linguistic variations. Prior studies of LMs have found specific deficits but failed to provide a comprehensive or systematic means of understanding whether LMs deficits are due to commonsense reasoning or linguistic variation. In this work, we address this gap and explore the LM’s ability to reason by developing a general procedure that allows the systematic creation of logically-equivalent, but syntactically-different statements. To demonstrate the power of our approach, we construct a large corpus of 14,400 predictive tasks that evaluate both the abstract reasoning abilities and robustness of LMs. We find that despite the current success of large LMs on commonsense benchmarks, their performance on these tasks is no better than random guessing, heavily dependent on biases, and not robust to the linguistic perturbation.	170ec3ac79b81f21ac35247b7f8e73991a14ebac	@None{2020-can,  title = {Can RoBERTa Reason? A Systematic Approach to Probe Logical Reasoning in Language Models},  year = {2020} }
Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View	2022	http://www.semanticscholar.org/paper/c4e991c0c5c21608a5a21d31fd478ce7b7fb527d	This paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention.	maybe	7	Prompt-based probing has been widely used in evaluating the abilities of pretrained language models (PLMs). Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable. Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications. To discover, understand and quantify the risks, this paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention. This paper provides valuable insights for the design of unbiased datasets, better probing frameworks and more reliable evaluations of pretrained language models. Furthermore, our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models.	c4e991c0c5c21608a5a21d31fd478ce7b7fb527d	@['JournalArticle', 'Conference']{cao-etal-2022-can,  author = {Boxi Cao and Hongyu Lin and Xianpei Han and Fangchao Liu and Le Sun},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {5796-5808},  title = {Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View},  year = {2022} }
Can Pretrained Language Models (Yet) Reason Deductively?	2022	http://www.semanticscholar.org/paper/c9e9ba31f4176bf85860cd7b46723c1ebb733989	It is suggested that PLMs cannot yet perform reliable deductive reasoning, demonstrating the importance of controlled examinations and probing of PLMs’ reasoning abilities; the results reach beyond (misleading) task performance, revealing thatPLMs are still far from human-level reasoning capabilities, even for simple deductive tasks.	maybe	0	Acquiring factual knowledge with Pretrained Language Models (PLMs) has attracted increasing attention, showing promising performance in many knowledge-intensive tasks. Their good performance has led the commu-nity to believe that the models do possess a modicum of reasoning competence rather than merely memorising the knowledge. In this paper, we conduct a comprehensive evaluation of the learnable deductive (also known as explicit) reasoning capability of PLMs. Through a series of controlled experiments, we posit two main ﬁndings. (i) PLMs inadequately generalise learned logic rules and perform inconsistently against simple adversarial surface form edits. (ii) While the deductive reasoning ﬁne-tuning of PLMs does improve their performance on reasoning over unseen knowledge facts, it results in catastrophically forgetting the previously learnt knowledge. Our main results suggest that PLMs cannot yet perform reliable deductive reasoning, demonstrating the importance of controlled examinations and probing of PLMs’ reasoning abilities; we reach beyond (misleading) task performance, revealing that PLMs are still far from human-level reasoning capabilities, even for simple deductive tasks.	c9e9ba31f4176bf85860cd7b46723c1ebb733989	@['JournalArticle']{yuan-etal-2022-can,  author = {Zhangdie Yuan and Songbo Hu and Ivan Vulic and A. Korhonen and Zaiqiao Meng},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can Pretrained Language Models (Yet) Reason Deductively?},  volume = {abs/2210.06442},  year = {2022} }
Can Pre-trained Language Models Interpret Similes as Smart as Human?	2022	http://www.semanticscholar.org/paper/50f7d69ddd7f9b34f5121607fbdcc57236d65b8c	This paper investigates the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, and shows that PLMs can infer similes’ shared properties while still underperforming humans.	maybe	4	Simile interpretation is a crucial task in natural language processing. Nowadays, pre-trained language models (PLMs) have achieved state-of-the-art performance on many tasks. However, it remains under-explored whether PLMs can interpret similes or not. In this paper, we investigate the ability of PLMs in simile interpretation by designing a novel task named Simile Property Probing, i.e., to let the PLMs infer the shared properties of similes. We construct our simile property probing datasets from both general textual corpora and human-designed questions, containing 1,633 examples covering seven main categories. Our empirical study based on the constructed datasets shows that PLMs can infer similes’ shared properties while still underperforming humans. To bridge the gap with human performance, we additionally design a knowledge-enhanced training objective by incorporating the simile knowledge into PLMs via knowledge embedding methods. Our method results in a gain of 8.58% in the probing task and 1.37% in the downstream task of sentiment classification. The datasets and code are publicly available at https://github.com/Abbey4799/PLMs-Interpret-Simile.	50f7d69ddd7f9b34f5121607fbdcc57236d65b8c	@['JournalArticle', 'Conference']{he-etal-2022-can,  author = {Qi He and Sijie Cheng and Zhixu Li and Rui Xie and Yanghua Xiao},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7875-7887},  title = {Can Pre-trained Language Models Interpret Similes as Smart as Human?},  year = {2022} }
Can Neural Networks Understand Monotonicity Reasoning?	2019	https://www.semanticscholar.org/paper/61daa30becda503c55217ef19a618d6a26b1dabd	The Monotonicity Entailment Dataset (MED) is introduced, and analysis using a monotonicity-driven data augmentation method showed that state-of-the-art NLI models might be limited in their generalization ability in upward and downward reasoning.	maybe	38	Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55%, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning.	61daa30becda503c55217ef19a618d6a26b1dabd	@['JournalArticle']{yanaka-etal-2019-can,  author = {Hitomi Yanaka and K. Mineshima and D. Bekki and Kentaro Inui and S. Sekine and Lasha Abzianidze and Johan Bos},  booktitle = {BlackboxNLP@ACL},  pages = {31-40},  title = {Can Neural Networks Understand Monotonicity Reasoning?},  year = {2019} }
Can neural networks acquire a structural bias from raw linguistic data?	2020	https://www.semanticscholar.org/paper/0e012c2bd18236445cfbc6e3e409eb02df4691fe	This work finds that BERT makes a structural generalization in 3 out of 4 empirical domains---subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses---but makes a linear generalization when tested on NPI licensing, suggesting tentative evidence that some linguistic universals can be acquired by learners without innate biases.	seed	32	We evaluate whether BERT, a widely used neural network for sentence processing, acquires an inductive bias towards forming structural generalizations through pretraining on raw data. We conduct four experiments testing its preference for structural vs. linear generalizations in different structure-dependent phenomena. We find that BERT makes a structural generalization in 3 out of 4 empirical domains---subject-auxiliary inversion, reflexive binding, and verb tense detection in embedded clauses---but makes a linear generalization when tested on NPI licensing. We argue that these results are the strongest evidence so far from artificial learners supporting the proposition that a structural bias can be acquired from raw data. If this conclusion is correct, it is tentative evidence that some linguistic universals can be acquired by learners without innate biases. However, the precise implications for human language acquisition are unclear, as humans learn language from significantly less data than BERT.	0e012c2bd18236445cfbc6e3e409eb02df4691fe	@['JournalArticle']{warstadt-bowman-2020-can,  author = {Alex Warstadt and Samuel R. Bowman},  booktitle = {Annual Meeting of the Cognitive Science Society},  journal = {ArXiv},  title = {Can neural networks acquire a structural bias from raw linguistic data?},  volume = {abs/2007.06761},  year = {2020} }
Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts	2022	https://www.semanticscholar.org/paper/7ce0c89a452e3c2917b63847495533865697c79c	This work shows that there exists a scaling law between the size of Language Models and their zero-shot performance on different downstream NLP tasks but this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law.	maybe	4	Previous work has shown that there exists a scaling law between the size of Language Models (LMs) and their zero-shot performance on different downstream NLP tasks. In this work, we show that this phenomenon does not hold when evaluating large LMs on tasks with negated prompts, but instead shows an inverse scaling law. We evaluate 9 different tasks with negated prompts on (1) pretrained LMs (OPT & GPT-3) of varying sizes (125M - 175B), (2) LMs further pretrained to generalize to novel prompts (InstructGPT), (3) LMs provided with few-shot examples, and (4) LMs ﬁne-tuned speciﬁcally on negated prompts; all LM types perform worse on negated prompts as they scale and show a huge performance gap between the human performance when comparing the average score on both original and negated prompts. By highlighting a critical limitation of existing LMs and methods, we urge the community to develop new approaches of developing LMs that actually follow the given instructions. We provide the code and the datasets to explore negated prompts at this link.	7ce0c89a452e3c2917b63847495533865697c79c	@['JournalArticle']{jang-etal-2022-can,  author = {Joel Jang and Seonghyeon Ye and Minjoon Seo},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can Large Language Models Truly Understand Prompts? A Case Study with Negated Prompts},  volume = {abs/2209.12711},  year = {2022} }
Can large language models reason about medical questions?	2022	https://www.semanticscholar.org/paper/d697b440dd0e65a05fe027e4c0ea85f62dcba033	It is speculated that scaling model and data, enhancing prompt alignment and allowing for better contextualization of the completions will be sufﬁcient for LLMs to reach human-level performance on this type of task.	maybe	9	Although large language models (LLMs) often produce impressive outputs, they also fail to reason and be factual. We set out to investigate how these limitations af-fect the LLM’s ability to answer and reason about difﬁcult real-world based questions. We applied the human-aligned GPT-3 (InstructGPT) to answer multiple-choice medical exam questions (USMLE and MedMCQA) and medical research questions (PubMedQA). We investigated Chain-of-thought (think step by step) prompts, grounding (augmenting the prompt with search results) and few-shot (prepending the question with question-answer exemplars). For a subset of the USMLE questions, a medical domain expert reviewed and annotated the model’s reasoning. Overall, GPT-3 achieved a substantial improvement in state-of-the-art machine learning performance. We observed that GPT-3 is often knowledgeable and can reason about medical questions. GPT-3, when confronted with a question it cannot answer, will still attempt to answer, often resulting in a biased predictive distribution. LLMs are not on par with human performance but our results suggest the emergence of reasoning patterns that are compatible with medical problem-solving. We speculate that scaling model and data, enhancing prompt alignment and allowing for better contextualization of the completions will be sufﬁcient for LLMs to reach human-level performance on this type of task.	d697b440dd0e65a05fe027e4c0ea85f62dcba033	@['JournalArticle', 'Review']{li'evin-etal-2022-can,  author = {Valentin Li'evin and C. Hother and O. Winther},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can large language models reason about medical questions?},  volume = {abs/2207.08143},  year = {2022} }
Can Language Representation Models Think in Bets?	2022	http://www.semanticscholar.org/paper/e02dce6ee032a13b1653f69b034a35676e7d4dc2	It is shown that a model is only able to ‘think in bets’ if it is first fine-tuned on bet questions with an identical structure, and that LRMs could potentially be applied to tasks that rely on cognitive decision-making skills, but that more research is necessary before they can robustly make rational decisions.	maybe	0	In recent years, transformer-based language representation models (LRMs) have achieved state-of-the-art results on difficult natural language understanding problems, such as question answering and text summarization. As these models are integrated into real-world applications, evaluating their ability to make rational decisions is an important research agenda, with practical ramifications. This article investigates LRMs’ rational decision-making ability through a carefully designed set of decision-making benchmarks and experiments. Inspired by classic work in cognitive science, we model the decision-making problem as a bet. We then investigate an LRM’s ability to choose outcomes that have optimal, or at minimum, positive expected gain. Through a robust body of experiments on four established LRMs, we show that a model is only able to ‘think in bets’ if it is first fine-tuned on bet questions with an identical structure. Modifying the bet question’s structure, while still retaining its fundamental characteristics, decreases an LRM’s performance by more than 25%, on average, although absolute performance remains well above random. LRMs are also found to be more rational when selecting outcomes with non-negative expected gain, rather than optimal or strictly positive expected gain. Our results suggest that LRMs could potentially be applied to tasks that rely on cognitive decision-making skills, but that more research is necessary before they can robustly make rational decisions.	e02dce6ee032a13b1653f69b034a35676e7d4dc2	@['JournalArticle']{tang-kejriwal-2022-can,  author = {Zhi–Bin Tang and M. Kejriwal},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can Language Representation Models Think in Bets?},  volume = {abs/2210.07519},  year = {2022} }
Can Language Models perform Abductive Commonsense Reasoning?	2022	http://www.semanticscholar.org/paper/4e6f63dc99c09560991cf89207f9fa12c739e711	This report reviews over some of the methodologies that were at-tempted to solve theductive Reasoning challenge, re-implement the baseline models, and analyzesSome of the weaknesses that current approaches have.	maybe	0	Abductive Reasoning is a task of inferring the most plausible hypothesis given a set of observations. In literature, the community has approached to solve this challenge by classi-fying/generating a likely hypothesis that does not contradict with a past observation and future observation. Some of the most well known benchmarks that tackle this problem are aNLI and aNLG (pronounced as alpha-NLI and alpha-NLG). In this report, I review over some of the methodologies that were at-tempted to solve this challenge, re-implement the baseline models, and analyze some of the weaknesses that current approaches have. The code and the re-implemented results are avail-able at this link 1 .	4e6f63dc99c09560991cf89207f9fa12c739e711	@['JournalArticle', 'Review']{kim-2022-can,  author = {Seung-Kyum Kim},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can Language Models perform Abductive Commonsense Reasoning?},  volume = {abs/2207.05155},  year = {2022} }
Can language models learn from explanations in context?	2022	http://www.semanticscholar.org/paper/341bdbcfc3febef7691a97c216ad394653211095	Investigating whether explanations of few-shot examples can help in-context learning of large LMs on challenging tasks finds that explanations can improve performance—even without tuning.	maybe	52	Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We ﬁnd that explanations can improve performance—even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger beneﬁts, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Fi-nally, even untuned explanations outperform carefully matched controls, suggesting that the beneﬁts are due to the link between an example and its explanation, rather than lower-level features. However, only large models beneﬁt. In summary, explanations can support the in-context learning of large LMs on challenging tasks.	341bdbcfc3febef7691a97c216ad394653211095	@['JournalArticle']{lampinen-etal-2022-can,  author = {Andrew Kyle Lampinen and I. Dasgupta and Stephanie C. Y. Chan and Kory Matthewson and Michael Henry Tessler and Antonia Creswell and James L. McClelland and Jane X. Wang and Felix Hill},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can language models learn from explanations in context?},  volume = {abs/2204.02329},  year = {2022} }
Can Language Models Learn Commonsense Truisms?	2019	http://www.semanticscholar.org/paper/6f112860225cc121f3a201f36c2ccf6c4584f398	It is found that current state-of-the-art LMs are just picking up statistical patterns in the training data and do not have the capacity to fully understand commonsesne truisms.	maybe	0	Language Models (LMs) pre-trained on extremely large corpora have achieved significantly better performances than previous models on Natural Language Understanding (NLU) benchmarks including commonsense reasoning tasks. Specifically, prior studies have shown that simply fine-tuned LMs can achieve performances on Winograd Schema Challenge (WSC) close to human-level. However, the crucial question of whether LMs can solve commonsense tasks due to the capability of reasoning or just shallow pattern matching has not been addressed. This work aims to extensively analyze the current LMs and probe their capacity to understand commonsense truisms regardless of how they are phrased. We found that current state-of-the-art LMs are just picking up statistical patterns in the training data and do not have the capacity to fully understand commonsesne truisms.	6f112860225cc121f3a201f36c2ccf6c4584f398	@None{2019-can,  title = {Can Language Models Learn Commonsense Truisms?},  year = {2019} }
Can language models handle recursively nested grammatical structures? A case study on comparing models and humans	2022	http://www.semanticscholar.org/paper/dafbfa87d075226f70c7ed4046788caa2531a7b2		maybe	2	How should we compare the capabilities of language models and humans? Here, I consider a case study: processing of recursively nested grammatical structures. Prior work has suggested that language models cannot handle these structures as re-liably as humans can. However, the humans were provided with instructions and training before being evaluated, while the language models were evaluated zero-shot. I therefore attempt to more closely match the evaluation paradigms by providing language models with few-shot prompts. A simple prompt, which contains substantially less content than the human training, allows large language models to consistently outperform the human results. The same prompt even allows ex-trapolation to more-deeply-nested conditions than have been tested in humans. Further, a reanalysis of the prior human experiments suggests that the humans may not perform above chance at the difﬁcult structures initially. These results suggest that large language models can in fact process recursively nested grammatical structures comparably to humans. This case study highlights how discrep-ancies in the quantity of experiment-speciﬁc context can confound comparisons of language models and humans. I use this case study to reﬂect on the broader challenge of comparing human and model capabilities—and to suggest that there is an important difference between evaluating cognitive models of a speciﬁc phenomenon and evaluating broadly-trained models.	dafbfa87d075226f70c7ed4046788caa2531a7b2	@['JournalArticle']{lampinen-2022-can,  author = {Andrew Kyle Lampinen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can language models handle recursively nested grammatical structures? A case study on comparing models and humans},  volume = {abs/2210.15303},  year = {2022} }
Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color	2021	http://www.semanticscholar.org/paper/cd9113cd9677883c865cde747cddc0542e43fc4c	A thorough case study on color finds that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming.	maybe	15	Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases — (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.	cd9113cd9677883c865cde747cddc0542e43fc4c	@['JournalArticle']{abdou-etal-2021-can,  author = {Mostafa Abdou and Artur Kulmizev and Daniel Hershcovich and Stella Frank and Ellie Pavlick and Anders Søgaard},  booktitle = {Conference on Computational Natural Language Learning},  pages = {109-132},  title = {Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color},  year = {2021} }
Can language models capture syntactic associations without surface cues? A case study of reflexive anaphor licensing in English control constructions	2022	http://www.semanticscholar.org/paper/a700a3183d7267eeba44871cb1fb133255d1d90e		maybe	1	Recommended Citation Lee, Soo-Hwan and Schuster, Sebastian (2022) "Can language models capture syntactic associations without surface cues? A case study of reflexive anaphor licensing in English control constructions," Proceedings of the Society for Computation in Linguistics: Vol. 5 , Article 19. DOI: https://doi.org/10.7275/s1kt-qg26 Available at: https://scholarworks.umass.edu/scil/vol5/iss1/19	a700a3183d7267eeba44871cb1fb133255d1d90e	@None{lee-schuster-2022-can,  author = {Soo-hwan Lee and Sebastian Schuster},  booktitle = {SCIL},  title = {Can language models capture syntactic associations without surface cues? A case study of reflexive anaphor licensing in English control constructions},  year = {2022} }
Can Language Models Be Specific? How?	2022	http://www.semanticscholar.org/paper/b47a3f7bcf540adb6fd97869c51449888d3160bb	A novel approach is introduced to build a benchmark for speciﬁcity testing by forming masked token prediction tasks with prompts and design two prompt-based methods to improve the speci ﬁc of pre-trained language models without additional training.	maybe	0	A good speaker not only needs to be correct, but also has the ability to be speciﬁc when desired, and so are language models. In this paper, we propose to measure how speciﬁc the language of pre-trained language models (PLMs) is. To achieve this, we introduce a novel approach to build a benchmark for speciﬁcity testing by forming masked token prediction tasks with prompts. For instance, given “J. K. Rowling was born in [MASK].”, we want to test whether a more speciﬁc answer will be better ﬁlled in by PLMs, e.g., Yate instead of England . From our evaluations, we show that existing PLMs have only a slight preference for more speciﬁc answers. We identify underlying factors affecting the speciﬁcity and design two prompt-based methods to improve the speciﬁcity. Results show that the speciﬁcity of the models can be improved by the proposed methods without additional training. We believe this work can provide a new insight for language modeling and encourage the research community to further explore this important but understudied problem. 1	b47a3f7bcf540adb6fd97869c51449888d3160bb	@['JournalArticle']{huang-etal-2022-can,  author = {Jie Huang and K. Chang and Jinjun Xiong and W. Hwu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can Language Models Be Specific? How?},  volume = {abs/2210.05159},  year = {2022} }
Can Language Models be Biomedical Knowledge Bases?	2021	http://www.semanticscholar.org/paper/4c5f4ddc68be643fb34ea969bf2c105ff7538995	The BioLAMA benchmark is created, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs and reveals that most predictions are highly correlated with prompt templates without any subjects, hindering their capabilities to be used as domain-specific KBs.	maybe	22	Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.	4c5f4ddc68be643fb34ea969bf2c105ff7538995	@['JournalArticle', 'Conference']{sung-etal-2021-can,  author = {Mujeen Sung and Jinhyuk Lee and Sean S. Yi and Minji Jeon and Sungdong Kim and Jaewoo Kang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Can Language Models be Biomedical Knowledge Bases?},  volume = {abs/2109.07154},  year = {2021} }
Can Foundation Models Help Us Achieve Perfect Secrecy?	2022	http://www.semanticscholar.org/paper/ea9c21c66f970a6bcc6750fc7d6ff6fa3c7fd301		maybe			ea9c21c66f970a6bcc6750fc7d6ff6fa3c7fd301	
Can fMRI reveal the representation of syntactic structure in the brain?	2021	http://www.semanticscholar.org/paper/23b414f71e074dedf757f3861916066829b5ef70	Novel multi-dimensional features that encode information about the syntactic structure of sentences are proposed that explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load.	maybe	8	While studying semantics in the brain, neuroscientists use two approaches. One is to identify areas that are correlated with semantic processing load. Another is to find areas that are predicted by the semantic representation of the stimulus words. However, in the domain of syntax, most studies have focused only on identifying areas correlated with syntactic processing load. One possible reason for this discrepancy is that representing syntactic structure in an embedding space such that it can be used to model brain activity is a non-trivial computational problem. Another possible reason is that it is unclear if the low signal-to-noise ratio of neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) can allow us to reveal correlates of complex (and perhaps subtle) syntactic representations. In this study, we propose novel multi-dimensional features that encode information about the syntactic structure of sentences. Using these features and fMRI recordings of participants reading a natural text, we model the brain representation of syntax. First, we find that our syntactic structure-based features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, we see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics.	23b414f71e074dedf757f3861916066829b5ef70	@['JournalArticle']{reddy-wehbe-2021-can,  author = {Aniketh Janardhan Reddy and Leila Wehbe},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Can fMRI reveal the representation of syntactic structure in the brain?},  year = {2021} }
Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the Generalizability of Relation Extraction	2020	http://www.semanticscholar.org/paper/d4aa625b2ea90b3b884886cc80f995b8e766d1ee	From empirical experimentation, this study finds that BERT suffers a bottleneck in terms of robustness by way of randomizations, adversarial and counterfactual tests, and biases (i.e., selection and semantic) and highlights opportunities for future improvements.	maybe	8	Fine-tuning pre-trained models have achieved impressive performance on standard natural language processing benchmarks. However, the resultant model generalizability remains poorly understood. We do not know, for example, how excellent performance can lead to the perfection of generalization models. In this study, we analyze a fine-tuned BERT model from different perspectives using relation extraction. We also characterize the differences in generalization techniques according to our proposed improvements. From empirical experimentation, we find that BERT suffers a bottleneck in terms of robustness by way of randomizations, adversarial and counterfactual tests, and biases (i.e., selection and semantic). These findings highlight opportunities for future improvements. Our open-sourced testbed DiagnoseRE is available in this https URL.	d4aa625b2ea90b3b884886cc80f995b8e766d1ee	@['JournalArticle']{zhang-etal-2020-can,  author = {Ningyu Zhang and Luoqiu Li and Shumin Deng and Haiyang Yu and Xu Cheng and Wei Zhang and Huajun Chen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Can Fine-tuning Pre-trained Models Lead to Perfect NLP? A Study of the Generalizability of Relation Extraction},  volume = {abs/2009.06206},  year = {2020} }
Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text	2019	http://www.semanticscholar.org/paper/09a34aad92e6f416c81f47e60de0809616b49cce	This work shows that large pretrained language models are in fact effective at modeling physical plausibility in the supervised setting and creates a training set by extracting attested events from a large corpus, and believes results could be further improved by injecting explicit commonsense knowledge into a distributional model.	maybe	9	Modeling semantic plausibility requires commonsense knowledge about the world and has been used as a testbed for exploring various knowledge representations. Previous work has focused specifically on modeling physical plausibility and shown that distributional methods fail when tested in a supervised setting. At the same time, distributional models, namely large pretrained language models, have led to improved results for many natural language understanding tasks. In this work, we show that these pretrained language models are in fact effective at modeling physical plausibility in the supervised setting. We therefore present the more difficult problem of learning to model physical plausibility directly from text. We create a training set by extracting attested events from a large corpus, and we provide a baseline for training on these attested events in a self-supervised manner and testing on a physical plausibility task. We believe results could be further improved by injecting explicit commonsense knowledge into a distributional model.	09a34aad92e6f416c81f47e60de0809616b49cce	@['JournalArticle', 'Conference']{porada-etal-2019-can,  author = {Ian Porada and Kaheer Suleman and J. C. Cheung},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text},  volume = {abs/1911.05689},  year = {2019} }
Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/a5ee5efd4405d94fbbe30aa7b6ed7d79bbc319f8	The implicit stock market preferences in BERT and its finance domain-specific model FinBERT are assessed to raise the awareness of their potential implicit preferences in the stock markets.	maybe	2	Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines .	a5ee5efd4405d94fbbe30aa7b6ed7d79bbc319f8	@['JournalArticle', 'Conference']{chuang-yang-2022-buy,  author = {C. Chuang and Yi Yang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {100-105},  title = {Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models},  year = {2022} }
Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack	2021	http://www.semanticscholar.org/paper/2ee93949a6d0b9bbb21159370305d07d15525f54	This paper investigates the vulnerability of BERT models to variation in input data for NER through adversarial attack and proposes two black-box methods for N ER based on existing methods for classification tasks.	maybe	0	Biomedical named entity recognition (NER) is a key task in the extraction of information from biomedical literature and electronic health records. For this task, both generic and biomedical BERT models are widely used. Robustness of these models is vital for medical applications, such as automated medical decision making. In this paper we investigate the vulnerability of BERT models to variation in input data for NER through adversarial attack. Since adversarial attack methods for NER are sparse, we propose two black-box methods for NER based on existing methods for classification tasks. Experimental results show that the original as well as the biomedical BERT models are highly vulnerable to entity replacement: They can be fooled in 89.2 to 99.4% of the cases to mislabel previously correct entities. BERT models are also vulnerable to variation in the entity context with 20.2 to 45.0% of entities predicted completely wrong and another 29.3 to 53.3% of entities predicted wrong partially. Often a single change is sufficient to fool the model. BERT models seem most vulnerable to changes in the local context of entities. Of the biomedical BERT models, the vulnerability of BioBERT is comparable to the original BERT model whereas SciBERT is even more vulnerable. Our results chart the vulnerabilities of BERT models for biomedical NER and emphasize the importance of further research into uncovering and reducing these weaknesses.	2ee93949a6d0b9bbb21159370305d07d15525f54	@None{dirkson-verberne-2021-breaking,  author = {A. Dirkson and S. Verberne},  title = {Breaking BERT: Understanding its Vulnerabilities for Biomedical Named Entity Recognition through Adversarial Attack},  year = {2021} }
Brains and algorithms partially converge in natural language processing	2022	http://www.semanticscholar.org/paper/83a491b6dfab0a9f30ce66d7dad1d7409e4d6e4d	This study systematically compares a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences and shows that modern language algorithms partially converge towards brain- like solutions.	maybe	38		83a491b6dfab0a9f30ce66d7dad1d7409e4d6e4d	@['JournalArticle']{caucheteux-king-2022-brains,  author = {C. Caucheteux and J. King},  booktitle = {Communications Biology},  journal = {Communications Biology},  title = {Brains and algorithms partially converge in natural language processing},  volume = {5},  year = {2022} }
Boosting coherence of language models	2021	http://www.semanticscholar.org/paper/cc1db851e3881be28564aca2ef0fecae133c45a1	A new study published out of the University of Alberta has found that carnivorous dinosaurs evolved to become faster and faster over time — and the proof is in the leg length.	maybe	1	Naturality of long-term information structure 001 – coherence – remains a challenge in language 002 generation. Large language models have insuf- 003 ﬁciently learned such structure, as their long- 004 form generations differ from natural text in 005 measures of coherence. To alleviate this di- 006 vergence, we propose coherence boosting , an 007 inference procedure that increases the effect 008 of distant context on next-token prediction. 009 We show the beneﬁts of coherence boosting 010 with pretrained models by distributional anal- 011 yses of generated ordinary text and dialog re- 012 sponses. We also ﬁnd that coherence boosting 013 with state-of-the-art models for various zero- 014 shot NLP tasks yields performance gains with 015 no additional training. 016 Scott Persons measured the limb length for 53 different species of carnivorous dinosaurs to create an equation to score how much each was built for speed. A new study published out of the University of Alberta has found that carnivorous dinosaurs evolved to become faster and faster over time — and the proof is in the leg length. As part of his doctoral research, lead author Scott Persons travelled the world measuring the limb lengths for 53 different species of carnivorous bipedal dinosaurs, including the Velociraptor, Allosaurus and Tyrannosaurus rex. In particular, Persons looked at the length of the dinosaurs’ legs below the knee. As a rule, he said, the longer the lower leg is in comparison to the upper leg, the faster the animal is. Modern-day cheetahs have proportionately longer legs than hyenas. That relationship is mirrored in the animals’ speeds, Persons said. "That’s true for modern carnivores, and must have been true for dinosaurs," Persons said in a statement released Wednesday. skill, of	cc1db851e3881be28564aca2ef0fecae133c45a1	@['JournalArticle']{malkin-etal-2021-boosting,  author = {Nikolay Malkin and Zhen Wang and N. Jojic},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Boosting coherence of language models},  volume = {abs/2110.08294},  year = {2021} }
BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation	2021	http://www.semanticscholar.org/paper/ce3b364b7e6358940ce97d8d5887a65e5024ca21	To systematically study and benchmark social biases in open-ended language generation, the Bias in Open-Ended Language Generation Dataset (BOLD) is introduced, a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology.	maybe	57	Recent advances in deep learning techniques have enabled machines to generate cohesive open-ended text when prompted with a sequence of words as context. While these models now empower many downstream applications from conversation bots to automatic storytelling, they have been shown to generate texts that exhibit social biases. To systematically study and benchmark social biases in open-ended language generation, we introduce the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. We also propose new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models reveals that the majority of these models exhibit a larger social bias than human-written Wikipedia text across all domains. With these results we highlight the need to benchmark biases in open-ended language generation and caution users of language generation models on downstream tasks to be cognizant of these embedded prejudices.	ce3b364b7e6358940ce97d8d5887a65e5024ca21	@['JournalArticle', 'Book']{dhamala-etal-2021-bold:,  author = {J. Dhamala and Tony Sun and Varun Kumar and Satyapriya Krishna and Yada Pruksachatkun and Kai-Wei Chang and Rahul Gupta},  booktitle = {Conference on Fairness, Accountability and Transparency},  journal = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},  title = {BOLD: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation},  year = {2021} }
BLiMP: A Benchmark of Linguistic Minimal Pairs for English	2020	http://www.semanticscholar.org/paper/b56e2e7b93be127c953b6ad18230d5905051d23b	The Benchmark of Linguistic Minimal Pairs, a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English, finds that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena.	maybe	162	Abstract We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs—that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.	b56e2e7b93be127c953b6ad18230d5905051d23b	@['JournalArticle']{warstadt-etal-2019-blimp:,  author = {Alex Warstadt and Alicia Parrish and Haokun Liu and Anhad Mohananey and Wei Peng and Sheng-Fu Wang and Samuel R. Bowman},  booktitle = {SCIL},  journal = {Transactions of the Association for Computational Linguistics},  pages = {377-392},  title = {BLiMP: A Benchmark of Linguistic Minimal Pairs for English},  volume = {8},  year = {2019} }
Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models	2023	https://www.semanticscholar.org/paper/cb47ed6fd7a6dd2b2c5e3c82c8684700b5ddffb1	This paper presents an extensive investigation towards understanding the existence of “Affective Bias” in large PLMs to unveil any biased association of emotions such as anger, fear, joy, etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection.	maybe	0	Groundbreaking inventions and highly significant performance improvements in deep learning based Natural Language Processing are witnessed through the development of transformer based large Pre-trained Language Models (PLMs). The wide availability of unlabeled data within human generated data deluge along with self-supervised learning strategy helps to accelerate the success of large PLMs in language generation, language understanding, etc. But at the same time, latent historical bias/unfairness in human minds towards a particular gender, race, etc., encoded unintentionally/intentionally into the corpora harms and questions the utility and efficacy of large PLMs in many real-world applications, particularly for the protected groups. In this paper, we present an extensive investigation towards understanding the existence of “Affective Bias” in large PLMs to unveil any biased association of emotions such as anger , fear , joy , etc., towards a particular gender, race or religion with respect to the downstream task of textual emotion detection. We conduct our exploration of affective bias from the very initial stage of corpus level affective bias analysis by searching for imbalanced distribution of affective words within a domain, in large scale corpora that are used to pre-train and fine-tune PLMs. Later, to quantify affective bias in model predictions, we perform an extensive set of class-based and intensity-based evaluations using various bias evaluation corpora. Our results show the existence of statistically significant affective bias in the PLM based emotion detection systems, indicating biased association of certain emotions towards a particular gender, race, and religion.	cb47ed6fd7a6dd2b2c5e3c82c8684700b5ddffb1	@['JournalArticle']{kadan-etal-2023-blacks,  author = {A. Kadan and P Deepak and Sahely Bhadra and Manjary P Gangan and L. LajishV.},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Blacks is to Anger as Whites is to Joy? Understanding Latent Affective Bias in Large Pre-trained Neural Language Models},  volume = {abs/2301.09003},  year = {2023} }
Black-Box Tuning for Language-Model-as-a-Service	2022	http://www.semanticscholar.org/paper/002c58077a1f1b296468b117230a1199e91f35c2	The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only outperforms manual prompt and GPT-3’s in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.	maybe	25	Extremely large pre-trained language models (PTMs) such as GPT-3 are usually released as a service. It allows users to design task-speciﬁc prompts to query the PTMs through some black-box APIs. In such a scenario, which we call Language-Model-as-a-Service (LMaaS), the gradients of PTMs are usually unavailable. Can we optimize the task prompts by only accessing the model inference APIs? This paper proposes the black-box tuning framework to optimize the continuous prompt prepended to the input text via derivative-free optimization. Instead of optimizing in the original high-dimensional prompt space, which is intractable for traditional derivative-free optimization, we perform optimization in a randomly generated subspace due to the low intrinsic dimensionality of large PTMs. The experimental results show that the black-box tuning with RoBERTa on a few labeled samples not only signiﬁcantly outperforms manual prompt and GPT-3’s in-context learning, but also surpasses the gradient-based counterparts, i.e., prompt tuning and full model tuning.	002c58077a1f1b296468b117230a1199e91f35c2	@['JournalArticle', 'Conference']{sun-etal-2022-black,  author = {Tianxiang Sun and Yunfan Shao and Hong Qian and Xuanjing Huang and Xipeng Qiu},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Black-Box Tuning for Language-Model-as-a-Service},  volume = {abs/2201.03514},  year = {2022} }
Black-box language model explanation by context length probing	2022	http://www.semanticscholar.org/paper/b4d482f61735e6902902fb04ffb0cef8dbc2b6ab		maybe			b4d482f61735e6902902fb04ffb0cef8dbc2b6ab	
Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models	2020	https://www.semanticscholar.org/paper/016760dc4a05489ddf5dbb48aecbb49e214e1b71	Investigating whether and to what extent one can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process finds that this may not work for numerical Commonsense knowledge.	yes	80	Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as "neural knowledge bases" via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs 96.3% in accuracy).	016760dc4a05489ddf5dbb48aecbb49e214e1b71	@['JournalArticle', 'Conference']{lin-etal-2020-birds,  author = {Bill Yuchen Lin and Seyeon Lee and Rahul Khanna and Xiang Ren},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Birds Have Four Legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-trained Language Models},  volume = {abs/2005.00683},  year = {2020} }
Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets	2023	https://www.semanticscholar.org/paper/6228282f909534210731ea5f76b1dce0fa5b919d	B bipol, a novel multi-axes bias metric with explainability, is used to quantify and explain how much bias exists in English NLP benchmark datasets, along multiple axes.	maybe	0	—We evaluate ﬁve English NLP benchmark datasets (available on the superGLUE leaderboard) for bias, along multiple axes. The datasets are the following: Boolean Question (Boolq), CommitmentBank (CB), Winograd Schema Challenge (WSC), Winogender diagnostic (AXg), and Recognising Textual Entailment (RTE). Bias can be harmful and it is known to be common in data, which ML models learn from. In order to mitigate bias in data, it is crucial to be able to estimate it objectively. We use bipol, a novel multi-axes bias metric with explainability, to quantify and explain how much bias exists in these datasets. Multilingual, multi-axes bias evaluation is not very common. Hence, we also contribute a new, large labelled Swedish bias-detection dataset, with about 2 million samples; translated from the English version. In addition, we contribute new multi-axes lexica for bias detection in Swedish. We train a SotA model on the new dataset for bias detection. We make the codes, model, and new dataset publicly available.	6228282f909534210731ea5f76b1dce0fa5b919d	@['JournalArticle']{adewumi-etal-2023-bipol:,  author = {Tosin P. Adewumi and Isabella Sodergren and Lama Alkhaled and Sana Sabry and F. Liwicki and M. Liwicki},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Bipol: Multi-axes Evaluation of Bias with Explainability in Benchmark Datasets},  volume = {abs/2301.12139},  year = {2023} }
Bigger Isn’t Better: The Ethical and Scientific Vices of Extra-Large Datasets in Language Models	2021	http://www.semanticscholar.org/paper/5333ebaa68bffe591676554fac83528343472c58	This work presents several sets of criticisms, where ethical and scientific issues in language model research reinforce each other: labour injustices in crowdwork, dataset quality and inscrutability, inequities in the research community, and centralized corporate control of the technology.	maybe	1	The use of language models in Web applications and other areas of computing and business have grown significantly over the last five years. One reason for this growth is the improvement in performance of language models on a number of benchmarks — but a side effect of these advances has been the adoption of a “bigger is always better” paradigm when it comes to the size of training, testing, and challenge datasets. Drawing on previous criticisms of this paradigm as applied to large training datasets crawled from pre-existing text on the Web, we extend the critique to challenge datasets custom-created by crowdworkers. We present several sets of criticisms, where ethical and scientific issues in language model research reinforce each other: labour injustices in crowdwork, dataset quality and inscrutability, inequities in the research community, and centralized corporate control of the technology. We also present a new type of tool for researchers to use in examining large datasets when evaluating them for quality.	5333ebaa68bffe591676554fac83528343472c58	@['JournalArticle', 'Book']{goetze-abramson-2021-bigger,  author = {T. S. Goetze and Darren Abramson},  booktitle = {Web Science Conference},  journal = {13th ACM Web Science Conference 2021},  title = {Bigger Isn’t Better: The Ethical and Scientific Vices of Extra-Large Datasets in Language Models},  year = {2021} }
Bidirectional Language Models Are Also Few-shot Learners	2022	http://www.semanticscholar.org/paper/ae7a4e22f3b25c557288f1e714ba053a4eeaec0f	S AP (Sequential Autore- 018 gressive Prompting), a technique that enables the prompting of bidirectional models, is presented and for the first time, prompt-based learning is an emergent property of a broader class of language models, rather than a property of only uniddirectional models.	maybe	3	Large language models such as GPT-3 (Brown 001 et al., 2020) can perform certain tasks with- 002 out undergoing fine-tuning after seeing only a 003 few labeled examples. An arbitrary task can 004 be reformulated as a natural language prompt, 005 and a language model can be asked to generate 006 the completion, indirectly performing the task 007 in a paradigm known as prompt-based learn- 008 ing. To date, emergent prompt-based learn- 009 ing capabilities have mainly been demonstrated 010 for unidirectional language models. Bidirec- 011 tional language models pre-trained on denois- 012 ing objectives such as masked language mod- 013 eling produce stronger learned representations. 014 Prompting bidirectional models has long been 015 desired, but their pre-training objectives have 016 made them incompatible with the prompting 017 paradigm. We present S AP (Sequential Autore- 018 gressive Prompting), a technique that enables 019 the prompting of bidirectional models. Utiliz- 020 ing the machine translation task as a case study, 021 we prompt the bidirectional mT5 (Xue et al., 022 2021) model with S AP and demonstrate its few- 023 shot and zero-shot translations outperform the 024 few-shot translations of unidirectional models 025 like GPT-3 and XGLM (Lin et al., 2021) with 026 approximately 50% fewer parameters. We fur- 027 ther show S AP extends its effectiveness to the 028 tasks of question answering and summariza- 029 tion. For the first time, our results demonstrate 030 prompt-based learning is an emergent property 031 of a broader class of language models, rather 032 than a property of only unidirectional models. 033	ae7a4e22f3b25c557288f1e714ba053a4eeaec0f	@['JournalArticle']{patel-etal-2022-bidirectional,  author = {Ajay Patel and Bryan Li and Mohammad Sadegh Rasooli and Noah Constant and Colin Raffel and Chris Callison-Burch},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Bidirectional Language Models Are Also Few-shot Learners},  volume = {abs/2209.14500},  year = {2022} }
Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models	2021	http://www.semanticscholar.org/paper/b41e07349b87a178d904e6b5d05a2f90b16f8e1e	An indepth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, and assesses biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin.	maybe	23	The capabilities of natural language models trained on large-scale data have increased immensely over the past few years. Open source libraries such as HuggingFace have made these models easily available and accessible. While prior research has identified biases in large language models, this paper considers biases contained in the most popular versions of these models when applied ‘out-of-the-box’ for downstream tasks. We focus on generative language models as they are well-suited for extracting biases inherited from training data. Specifically, we conduct an indepth analysis of GPT-2, which is the most downloaded text generation model on HuggingFace, with over half a million downloads per month. We assess biases related to occupational associations for different protected categories by intersecting gender with religion, sexuality, ethnicity, political affiliation, and continental name origin. Using a template-based data collection pipeline, we collect 396K sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are less diverse and more stereotypical for women than for men, especially for intersections; (ii) Intersectional interactions are highly relevant for occupational associations, which we quantify by fitting 262 logistic models; (iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity distribution found in US Labor Bureau data, and even pulls the societally-skewed distribution towards gender parity in cases where its predictions deviate from real labor market observations. This raises the normative question of what language models should learn whether they should reflect or correct for existing inequalities.	b41e07349b87a178d904e6b5d05a2f90b16f8e1e	@['JournalArticle']{kirk-etal-2021-bias,  author = {Hannah Rose Kirk and Yennie Jun and Haider Iqbal and Elias Benussi and Filippo Volpin and F. Dreyer and Aleksandar Shtedritski and Yuki M. Asano},  booktitle = {Neural Information Processing Systems},  pages = {2611-2624},  title = {Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models},  year = {2021} }
Bias Identification and Attribution in NLP Models With Regression and Effect Sizes	2022	http://www.semanticscholar.org/paper/39d22a99c3033400e03184344f757934cba19342		maybe	0	There is a growing awareness that many NLP systems incorporate biases of various types (e.g., regarding gender or race) which can cause significant social harm. At the same time, the techniques often used for the statistical analysis of biases in NLP systems are still relatively basic. Typically, studies test for the presence of a significant difference between two levels of a single bias variable (e.g., gender: male vs. female) without attention to potential confounders, and do not quantify the importance of the bias variable. This article proposes to analyze bias in the output of NLP systems using multivariate regression models. Such models provide a robust and more informative alternative which (a) generalizes to multiple bias variables, (b) can take covariates into account, (c) can be combined with measures of effect size to quantify the size of bias. Jointly, these effects contribute to a statistically more robust identification and attribution of bias that can be used to diagnose system behavior and extract informative examples. We demonstrate the benefits of our method by analyzing a range of current NLP models on two tasks, namely one regression task (emotion intensity prediction) and one classification task (coreference resolution).	39d22a99c3033400e03184344f757934cba19342	@None{dayanik-2022-bias,  author = {Erenay Dayanik},  title = {Bias Identification and Attribution in NLP Models With Regression and Effect Sizes},  year = {2022} }
Beyond the Benchmarks: Toward Human-Like Lexical Representations	2022	http://www.semanticscholar.org/paper/3c1855cf61d00282c7a41addb416bb4d9206064b		maybe	0	To process language in a way that is compatible with human expectations in a communicative interaction, we need computational representations of lexical properties that form the basis of human knowledge of words. In this article, we concentrate on word-level semantics. We discuss key concepts and issues that underlie the scientific understanding of the human lexicon: its richly structured semantic representations, their ready and continual adaptability, and their grounding in crosslinguistically valid conceptualization. We assess the state of the art in natural language processing (NLP) in achieving these identified properties, and suggest ways in which the language sciences can inspire new approaches to their computational instantiation.	3c1855cf61d00282c7a41addb416bb4d9206064b	@['Review', 'JournalArticle']{stevenson-merlo-2022-beyond,  author = {S. Stevenson and Paola Merlo},  booktitle = {Frontiers in Artificial Intelligence},  journal = {Frontiers in Artificial Intelligence},  title = {Beyond the Benchmarks: Toward Human-Like Lexical Representations},  volume = {5},  year = {2022} }
Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs	2021	http://www.semanticscholar.org/paper/0242acadf4940cf94596b54f7fd6fb75af7bb20d	This work takes the first step of showing that attackers could potentially surpass victims via unsupervised domain adaptation and multivictim ensemble, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.	maybe	7	Machine-learning-as-a-service (MLaaS) has attracted millions of users to their outperforming sophisticated models. Although published as black-box APIs, the valuable models behind these services are still vulnerable to imitation attacks. Recently, a series of works have demonstrated that attackers manage to steal or extract the victim models. Nonetheless, none of the previous stolen models can outperform the original black-box APIs. In this work, we take the first step of showing that attackers could potentially surpass victims via unsupervised domain adaptation and multivictim ensemble. Extensive experiments on benchmark datasets and real-world APIs validate that the imitators can succeed in outperforming the original black-box models. We consider this as a milestone in the research of imitation attack, especially on NLP APIs, as the superior performance could influence the defense or even publishing strategy of API providers.	0242acadf4940cf94596b54f7fd6fb75af7bb20d	@['JournalArticle']{xu-etal-2021-beyond,  author = {Qiongkai Xu and Xuanli He and L. Lyu and Lizhen Qu and Gholamreza Haffari},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs},  volume = {abs/2108.13873},  year = {2021} }
Beyond Intuition: Rethinking Token Attributions inside Transformers	2022	http://www.semanticscholar.org/paper/7920371e6b296460183b00f8d31796a5664db582	This work proposes a novel method to approximate token contributions inside Transformers by starting from the partial derivative to each token, dividing the interpretation process into attention perception and reasoning feedback with the chain rule and exploring each part individually with explicit mathematical derivations.	maybe	0	The multi-head attention mechanism, or rather the Transformer-based models have always been under the spotlight, not only in the domain of text processing, but also for computer vision. Several works have recently been proposed around exploring the token attributions along the intrinsic decision process. However, the ambiguity of the expression formulation can lead to an accumulation of error, which makes the interpretation less trustworthy and less applicable to different variants. In this work, we propose a novel method to approximate token contributions inside Transformers. We start from the partial derivative to each token, divide the interpretation process into attention perception and reasoning feedback with the chain rule and explore each part individually with explicit mathematical derivations. In attention perception, we propose the head-wise and token-wise approximations in order to learn how the tokens interact to form the pooled vector. As for reasoning feedback, we adopt a noise-decreasing strategy by applying the integrated gradients to the last attention map. Our method is further validated qualitatively and quantitatively through the faithfulness evaluations across different settings: single modality (BERT and ViT) and bi-modality (CLIP), different model sizes (ViT-L) and different pooling strategies (ViT-MAE) to demonstrate the broad applicability and clear improvements over existing methods.	7920371e6b296460183b00f8d31796a5664db582	@None{2022-beyond,  title = {Beyond Intuition: Rethinking Token Attributions inside Transformers},  year = {2022} }
Beyond Faithfulness: A Framework to Characterize and Compare Saliency Methods	2022	http://www.semanticscholar.org/paper/80c33a1a37a1033cab43279bcb8de45ca882820b	This work describes a framework of nine dimensions to characterize and compare the properties of saliency methods, and identifies opportunities for future work, including filling gaps in the landscape and developing new evaluation metrics.	maybe	0	Saliency methods calculate how important each input feature is to a machine learning model’s prediction, and are commonly used to understand model reasoning. “Faithfulness,” or how fully and accurately the saliency output reﬂects the underlying model, is an oft-cited desideratum for these methods. However, explanation methods must necessarily sacriﬁce certain information in service of user-oriented goals such as simplicity. To that end, and akin to performance metrics, we frame saliency methods as abstractions : individual tools that provide insight into speciﬁc aspects of model behavior and entail tradeoffs. Using this framing, we describe a framework of nine dimensions to characterize and compare the properties of saliency methods. We group these dimensions into three categories that map to different phases of the interpretation process: methodology , or how the saliency is calculated; sensitivity , or relationships between the saliency result and the underlying model or input; and, perceptibility , or how a user interprets the result. As we show, these dimensions give us a granular vocabulary for describing and comparing saliency methods — for instance, allowing us to develop “saliency cards” as a form of documentation, or helping downstream users understand tradeoffs and choose a method for a particular use case. Moreover, by situating existing saliency methods within this framework, we identify opportunities for future work, including ﬁlling gaps in the landscape and developing new evaluation metrics. to document individual methods and contextualize the results. Through interviews with radiologists, we show how downstream stakeholders can use our framework to weigh tradeoffs and choose a	80c33a1a37a1033cab43279bcb8de45ca882820b	@['JournalArticle']{boggust-etal-2022-beyond,  author = {Angie Boggust and Harini Suresh and H. Strobelt and J. Guttag and Arvindmani Satyanarayan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Beyond Faithfulness: A Framework to Characterize and Compare Saliency Methods},  volume = {abs/2206.02958},  year = {2022} }
Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence	2022	http://www.semanticscholar.org/paper/4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80	A novel intermediate training task, named meaning-matching, designed to directly learn a meaning-text correspondence, is proposed that enables PLMs to learn lexical semantic information and is found to be a safe intermediate task that guarantees a similar or better performance of downstream tasks.	maybe	1	The logical negation property (LNP), which implies generating different predictions for semantically opposite inputs ( p is true iff ¬ p is false), is an important property that a trustworthy language model must satisfy. However, much re-cent evidence shows that large-size pre-trained language models (PLMs) do not satisfy this property. In this paper, we perform experiments using probing tasks to assess PLMs’ LNP understanding. Unlike previous studies that only examined negation expressions, we expand the boundary of the investigation to lexical semantics. Through experiments, we observe that PLMs violate the LNP frequently. To alleviate the issue, we propose a novel intermediate training task, named meaning-matching , designed to directly learn a meaning-text correspondence, instead of relying on the distributional hypothesis. Through multiple experiments, we find that the task enables PLMs to learn lexical semantic information. Also, through fine-tuning experiments on 7 GLUE tasks, we confirm that it is a safe intermediate task that guarantees a similar or better performance of downstream tasks. Finally, we observe that our proposed approach 1 outperforms our previous counterparts despite its time and resource efficiency.	4f39ab027947f49fa9a9c919c2c09bbbe0bf7f80	@['JournalArticle']{jang-etal-2022-beyond,  author = {Myeongjun Jang and Frank Mtumbuka and Thomas Lukasiewicz},  booktitle = {NAACL-HLT},  journal = {ArXiv},  title = {Beyond Distributional Hypothesis: Let Language Models Learn Meaning-Text Correspondence},  volume = {abs/2205.03815},  year = {2022} }
Beyond Accuracy: Behavioral Testing of NLP Models with CheckList	2020	https://www.semanticscholar.org/paper/33ec7eb2168e37e3007d1059aa96b9a63254b4da		seed	561	Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it.	33ec7eb2168e37e3007d1059aa96b9a63254b4da	@['JournalArticle', 'Conference']{ribeiro-etal-2020-beyond,  author = {Marco Tulio Ribeiro and Tongshuang Sherry Wu and Carlos Guestrin and Sameer Singh},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4902-4912},  title = {Beyond Accuracy: Behavioral Testing of NLP Models with CheckList},  year = {2020} }
Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language	2022	http://www.semanticscholar.org/paper/2b8eee8051e27fd9f220b8f70096135a243174e7	This position paper argues that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: truthfully represent the model behavior, have a high reputation and take the user’s mental model into account.	maybe	1	Language models learn and represent language differently than humans; they learn the form and not the meaning . Thus, to assess the success of language model explainability, we need to consider the impact of its divergence from a user’s mental model of language. In this position paper, we argue that in order to avoid harmful rationalization and achieve truthful understanding of language models, explanation processes must satisfy three main conditions: (1) explanations have to truthfully represent the model behavior, i.e., have a high ﬁdelity; (2) explanations must be complete, as missing information distorts the truth; and (3) explanations have to take the user’s mental model into account, progressively verifying a person’s knowledge and adapting their understanding. We introduce a decision tree model to showcase potential reasons why current explanations fail to reach their objectives. We further emphasize the need for human-centered design to explain the model from multiple perspectives, progressively adapting explanations to changing user expectations.	2b8eee8051e27fd9f220b8f70096135a243174e7	@['JournalArticle']{sevastjanova-assady-2022-beware,  author = {R. Sevastjanova and Mennatallah El-Assady},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Beware the Rationalization Trap! When Language Model Explainability Diverges from our Mental Models of Language},  volume = {abs/2207.06897},  year = {2022} }
Better Hit the Nail on the Head than Beat around the Bush: Removing Protected Attributes with a Single Projection	2022	http://www.semanticscholar.org/paper/d4d148a26f187482ad3016a296e4b7bdeffb2d37	A comparison between MP and INLP shows that (1) one MP projection removes linear separa-bility based on the target and (2) MP has less impact on the overall space.	maybe	0	Bias elimination and recent probing studies attempt to remove specific information from embedding spaces. Here it is important to remove as much of the target information as possible, while preserving any other information present. INLP is a popular recent method which removes specific information through iterative nullspace projections.Multiple iterations, however, increase the risk that information other than the target is negatively affected.We introduce two methods that find a single targeted projection: Mean Projection (MP, more efficient) and Tukey Median Projection (TMP, with theoretical guarantees). Our comparison between MP and INLP shows that (1) one MP projection removes linear separability based on the target and (2) MP has less impact on the overall space.Further analysis shows that applying random projections after MP leads to the same overall effects on the embedding space as the multiple projections of INLP. Applying one targeted (MP) projection hence is methodologically cleaner than applying multiple (INLP) projections that introduce random effects.	d4d148a26f187482ad3016a296e4b7bdeffb2d37	@['JournalArticle', 'Conference']{haghighatkhah-etal-2022-better,  author = {P. Haghighatkhah and Antske Fokkens and Pia Sommerauer and B. Speckmann and Kevin Verbeek},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Better Hit the Nail on the Head than Beat around the Bush: Removing Protected Attributes with a Single Projection},  volume = {abs/2212.04273},  year = {2022} }
BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance	2019	https://www.semanticscholar.org/paper/48689c4bb52a45c0bc97d1421d72d11bab6c346b	This work fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference.	yes	95	If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6% and 84.8%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that “the doctor visited the lawyer” does not entail “the lawyer visited the doctor”), accuracy ranged from 0.0% to 66.2%. Such variation is likely due to the presence of many local minima in the loss surface that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.	48689c4bb52a45c0bc97d1421d72d11bab6c346b	@['JournalArticle']{mccoy-etal-2019-berts,  author = {R. Thomas McCoy and Junghyun Min and Tal Linzen},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},  volume = {abs/1911.02969},  year = {2019} }
BERTnesia: Investigating the capture and forgetting of knowledge in BERT	2020	https://www.semanticscholar.org/paper/616610e0b0a31ab4bac1c64fd0b65c2572185522	This paper utilizes knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER) and finds that ranking models forget the least and retain more knowledge in their final layer.	maybe	23	Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT’s final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset. We found that ranking models forget the least and retain more knowledge in their final layer.	616610e0b0a31ab4bac1c64fd0b65c2572185522	@['JournalArticle']{wallat-etal-2020-bertnesia:,  author = {Jonas Wallat and Jaspreet Singh and Avishek Anand},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {174-183},  title = {BERTnesia: Investigating the capture and forgetting of knowledge in BERT},  year = {2020} }
BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset	2020	https://www.semanticscholar.org/paper/069498dea2abf78bc15d3e82ba23268d46d8fba2	It is found that learnt linear combinations are robust to Nonce, a scheme to create adversarial test examples by replacing gold arguments with randomly generated “nonce” words, though individual best heads can be more sensitive.	maybe	3	Using the attention map based probing framework from (Clark et al., 2019), we observe that, on the RAMS dataset (Ebner et al., 2020), BERT’s attention heads have modest but well above-chance ability to spot event arguments sans any training or domain finetuning, varying from a low of 17.77% for Place to a high of 51.61% for Artifact. Next, we find that linear combinations of these heads, estimated with approx. 11% of available total event argument detection supervision, can push performance well higher for some roles — highest two being Victim (68.29% Accuracy) and Artifact (58.82% Accuracy). Furthermore, we investigate how well our methods do for cross-sentence event arguments. We propose a procedure to isolate “best heads” for cross-sentence argument detection separately of those for intra-sentence arguments. The heads thus estimated have superior cross-sentence performance compared to their jointly estimated equivalents, albeit only under the unrealistic assumption that we already know the argument is present in another sentence. Lastly, we seek to isolate to what extent our numbers stem from lexical frequency based associations between gold arguments and roles. We propose NONCE, a scheme to create adversarial test examples by replacing gold arguments with randomly generated “nonce” words. We find that learnt linear combinations are robust to NONCE, though individual best heads can be more sensitive.	069498dea2abf78bc15d3e82ba23268d46d8fba2	@['JournalArticle']{gangal-hovy-2020-bertering,  author = {Varun Gangal and E. Hovy},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {1-10},  title = {BERTering RAMS: What and How Much does BERT Already Know About Event Arguments? - A Study on the RAMS Dataset},  year = {2020} }
BERT Rediscovers the Classical NLP Pipeline	2019	https://www.semanticscholar.org/paper/97906df07855b029b7aae7c2a1c6c5e8df1d531c	This work finds that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference.	seed	851	Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.	97906df07855b029b7aae7c2a1c6c5e8df1d531c	@['JournalArticle', 'Conference']{tenney-etal-2019-bert,  author = {Ian Tenney and Dipanjan Das and Ellie Pavlick},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {4593-4601},  title = {BERT Rediscovers the Classical NLP Pipeline},  year = {2019} }
BERT Knows Punta Cana Is Not Just Beautiful, It’s Gorgeous: Ranking Scalar Adjectives with Contextualised Representations	2020	http://www.semanticscholar.org/paper/5b71bcf769e7efab90ceba56b9e4f898899538fe	The results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.	maybe	6	Adjectives like pretty, beautiful and gorgeous describe positive properties of the nouns they modify but with different intensity. These differences are important for natural language understanding and reasoning. We propose a novel BERT-based approach to intensity detection for scalar adjectives. We model intensity by vectors directly derived from contextualised representations and show they can successfully rank scalar adjectives. We evaluate our models both intrinsically, on gold standard datasets, and on an Indirect Question Answering task. Our results demonstrate that BERT encodes rich knowledge about the semantics of scalar adjectives, and is able to provide better quality intensity rankings than static embeddings and previous models with access to dedicated resources.	5b71bcf769e7efab90ceba56b9e4f898899538fe	@['JournalArticle', 'Conference']{soler-apidianaki-2020-bert,  author = {Aina Garí Soler and Marianna Apidianaki},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {BERT Knows Punta Cana Is Not Just Beautiful, It’s Gorgeous: Ranking Scalar Adjectives with Contextualised Representations},  volume = {abs/2010.02686},  year = {2020} }
BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?	2021	http://www.semanticscholar.org/paper/465491b0507e107f248a8277ac17248c2ff8f915	It is found that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters.	maybe	21	Analogies play a central role in human commonsense reasoning. The ability to recognize analogies such as “eye is to seeing what ear is to hearing”, sometimes referred to as analogical proportions, shape how we structure knowledge and understand language. Surprisingly, however, the task of identifying such analogies has not yet received much attention in the language model era. In this paper, we analyze the capabilities of transformer-based language models on this unsupervised task, using benchmarks obtained from educational settings, as well as more commonly used datasets. We find that off-the-shelf language models can identify analogies to a certain extent, but struggle with abstract and complex relations, and results are highly sensitive to model architecture and hyperparameters. Overall the best results were obtained with GPT-2 and RoBERTa, while configurations using BERT were not able to outperform word embedding models. Our results raise important questions for future work about how, and to what extent, pre-trained language models capture knowledge about abstract semantic relations.	465491b0507e107f248a8277ac17248c2ff8f915	@['JournalArticle', 'Conference']{ushio-etal-2021-bert,  author = {Asahi Ushio and Luis Espinosa Anke and S. Schockaert and José Camacho-Collados},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {BERT is to NLP what AlexNet is to CV: Can Pre-Trained Language Models Identify Analogies?},  volume = {abs/2105.04949},  year = {2021} }
BERT is Not an Interlingua and the Bias of Tokenization	2019	https://www.semanticscholar.org/paper/9eb4cd1a4b4717c97c47e3dc4563a75779ae9390	Cananical Correlation Analysis of the internal representations of a pre- trained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space.	seed	55	Multilingual transfer learning can benefit both high- and low-resource languages, but the source of these improvements is not well understood. Cananical Correlation Analysis (CCA) of the internal representations of a pre- trained, multilingual BERT model reveals that the model partitions representations for each language rather than using a common, shared, interlingual space. This effect is magnified at deeper layers, suggesting that the model does not progressively abstract semantic con- tent while disregarding languages. Hierarchical clustering based on the CCA similarity scores between languages reveals a tree structure that mirrors the phylogenetic trees hand- designed by linguists. The subword tokenization employed by BERT provides a stronger bias towards such structure than character- and word-level tokenizations. We release a subset of the XNLI dataset translated into an additional 14 languages at https://www.github.com/salesforce/xnli_extension to assist further research into multilingual representations.	9eb4cd1a4b4717c97c47e3dc4563a75779ae9390	@['JournalArticle', 'Conference']{singh-etal-2019-bert,  author = {Jasdeep Singh and Bryan McCann and R. Socher and Caiming Xiong},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {47-55},  title = {BERT is Not an Interlingua and the Bias of Tokenization},  year = {2019} }
BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA	2019	https://www.semanticscholar.org/paper/7c62ac7aedacc39ca417a48f8134e0514dc6a523	E-BERT is proposed, an extension of BERT that replaces entity mentions with symbolic entity embeddings and outperforms both BERT and ERNIE on hard-to-guess queries and takes this as evidence that E-berT is richer in factual knowledge.	seed	67	The BERT language model (LM) (Devlin et al., 2019) is surprisingly good at answering cloze-style questions about relational facts. Petroni et al. (2019) take this as evidence that BERT memorizes factual knowledge during pre-training. We take issue with this interpretation and argue that the performance of BERT is partly due to reasoning about (the surface form of) entity names, e.g., guessing that a person with an Italian-sounding name speaks Italian. More specifically, we show that BERT's precision drops dramatically when we filter certain easy-to-guess facts. As a remedy, we propose E-BERT, an extension of BERT that replaces entity mentions with symbolic entity embeddings. E-BERT outperforms both BERT and ERNIE (Zhang et al., 2019) on hard-to-guess queries. We take this as evidence that E-BERT is richer in factual knowledge, and we show two ways of ensembling BERT and E-BERT.	7c62ac7aedacc39ca417a48f8134e0514dc6a523	@['JournalArticle']{poerner-etal-2019-bert,  author = {Nina Poerner and Ulli Waltinger and Hinrich Schütze},  booktitle = {ArXiv},  journal = {ArXiv},  title = {BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA},  volume = {abs/1911.03681},  year = {2019} }
BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology	2021	https://www.semanticscholar.org/paper/55ac7573d3eaa57262bdd4955ea75030aa2e4eb7	This work investigates contextualized embedding neighborhoods directly, formulating a query-by-example nearest neighbor retrieval task and examining ranking performance for words and senses in different frequency bands, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations.	maybe	3	An important question concerning contextualized word embedding (CWE) models like BERT is how well they can represent different word senses, especially those in the long tail of uncommon senses. Rather than build a WSD system as in previous work, we investigate contextualized embedding neighborhoods directly, formulating a query-by-example nearest neighbor retrieval task and examining ranking performance for words and senses in different frequency bands. In an evaluation on two English sense-annotated corpora, we find that several popular CWE models all outperform a random baseline even for proportionally rare senses, without explicit sense supervision. However, performance varies considerably even among models with similar architectures and pretraining regimes, with especially large differences for rare word senses, revealing that CWE models are not all created equal when it comes to approximating word senses in their native representations.	55ac7573d3eaa57262bdd4955ea75030aa2e4eb7	@['JournalArticle']{gessler-schneider-2021-bert,  author = {Luke Gessler and Nathan Schneider},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {539-547},  title = {BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology},  year = {2021} }
BERT Goes to College: Understanding the Role of Pretrained Layers During Finetuning	2020	http://www.semanticscholar.org/paper/543e2fd0ddb5c7990b50e1f99dc9e5fccf8d0bab	This project lends insight to the contribution of pretraining to BERT’s performance on finetuning tasks, and proposes various ways to modify the pretraining process to further increase performance.	maybe	0	We investigate the extent to which pretraining contributes to BERT’s success in producing state-of-the-art results, and identify some modifications to BERT’s pretraining process that outperform the existing configuration. We conduct three experiments at varying levels of granularity. First, we sequentially finetune on multiple tasks and find cases where finetuning BERT on additional intermediate tasks actually yields better performance on the final finetuning task, compared to directly finetuning the pretrained layers on the final task. Secondly, within a specific finetuning task, we reinitialize various pretrained layers of BERT and observe the performance across different layerwise reinitialization schemes as well as across different tasks, and compare with the baseline model with no layers reinitialized. We find that reinitializing downstream layers causes a smaller decrease in performance than reinitializing upstream layers, but the change in performance does not monotonically decrease when reinitializing the next upstream layer, nor is the change in performance constant. This result indicates the presence of interlayer dependencies and unequal importance in the information encoded within each layer. Third, we reinitialize various attention heads within each layer and find that reinitializing certain attention heads at top layers can even improve the model performance. Altogether, this project lends insight to the contribution of pretraining to BERT’s performance on finetuning tasks, and proposes various ways to modify the pretraining process to further increase performance. 1 Key Information • Mentor: John Hewitt (CS224N Teaching Assistant) and Alex Tamkin (Stanford NLP Group) • External Collaborators (if you have any): N/A • Sharing project: N/A	543e2fd0ddb5c7990b50e1f99dc9e5fccf8d0bab	@None{huang-2020-bert,  author = {Dan Huang},  title = {BERT Goes to College: Understanding the Role of Pretrained Layers During Finetuning},  year = {2020} }
BERT Busters: Outlier Dimensions that Disrupt Transformers	2021	https://www.semanticscholar.org/paper/5a09edeb26f9f116f2c0503cd020f38fb943f79b	It is demonstrated that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs, and disabling them significantly degrades both the MLM loss and the downstream task performance.	yes	16	Multiple studies have shown that Transformers are remarkably robust to pruning. Contrary to this received wisdom, we demonstrate that pre-trained Transformer encoders are surprisingly fragile to the removal of a very small number of features in the layer outputs (<0.0001% of model weights). In case of BERT and other pre-trained encoder Transformers, the affected component is the scaling factors and biases in the LayerNorm. The outliers are high-magnitude normalization parameters that emerge early in pre-training and show up consistently in the same dimensional position throughout the model. We show that disabling them significantly degrades both the MLM loss and the downstream task performance. This effect is observed across several BERT-family models and other popular pre-trained Transformer architectures, including BART, XLNet and ELECTRA; we also show a similar effect in GPT-2.	5a09edeb26f9f116f2c0503cd020f38fb943f79b	@['JournalArticle']{kovaleva-etal-2021-bert,  author = {Olga Kovaleva and Saurabh Kulshreshtha and Anna Rogers and Anna Rumshisky},  booktitle = {Findings},  pages = {3392-3405},  title = {BERT Busters: Outlier Dimensions that Disrupt Transformers},  year = {2021} }
Benchmarking Language Models for Code Syntax Understanding	2022	http://www.semanticscholar.org/paper/a593ce5d2a93f3113be7717d08d1ba8e62fd7ddf	The first thorough benchmarking of the state-of-the-art pre-trained models for identifying the syntactic structures of programs is performed, and the key observation is that existing language models pretrained on code still lack the understanding of code syntax.	maybe	1	Pre-trained language models have demonstrated impressive performance in both natural language processing and program understanding, which represent the input as a token sequence without explicitly modeling its structure. Some prior works show that pretrained language models can capture the syntactic rules of natural languages without finetuning on syntax understanding tasks. However, there is limited understanding of how well pre-trained models understand the code structure so far. In this work, we perform the first thorough benchmarking of the stateof-the-art pre-trained models for identifying the syntactic structures of programs. Specifically, we introduce CodeSyntax, a large-scale dataset of programs annotated with the syntactic relationships in their corresponding abstract syntax trees. Our key observation is that existing language models pretrained on code still lack the understanding of code syntax. In fact, these pre-trained programming language models fail to match the performance of simple baselines based on positional offsets and keywords. We also present a natural language benchmark to highlight the differences between natural languages and programming languages in terms of syntactic structure understanding. Our findings point out key limitations of existing pre-training methods for programming languages, and suggest the importance of modeling code syntactic structures.1	a593ce5d2a93f3113be7717d08d1ba8e62fd7ddf	@['JournalArticle']{shen-etal-2022-benchmarking,  author = {Da Shen and Xinyun Chen and Chenguang Wang and Koushik Sen and Dawn Song},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Benchmarking Language Models for Code Syntax Understanding},  volume = {abs/2210.14473},  year = {2022} }
Benchmarking Intersectional Biases in NLP	2022	http://www.semanticscholar.org/paper/d6f002d88638de71114dab083f0ea8ceea6b6a5a	The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks.	maybe	2	There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in state-of-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias - fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairness-accuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across dimensions (i.e., intersections). We conclude by highlighting possible causes and making recommendations for future NLP debiasing research.	d6f002d88638de71114dab083f0ea8ceea6b6a5a	@['JournalArticle', 'Conference']{lalor-etal-2022-benchmarking,  author = {John P. Lalor and Yi Yang and Kendall Smith and Nicole Forsgren and A. Abbasi},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {3598-3609},  title = {Benchmarking Intersectional Biases in NLP},  year = {2022} }
Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks	2022	http://www.semanticscholar.org/paper/0bf8c1cab22c15d478f6ce431710065665b34e71	This work introduces N ATURAL -I NSTRUCTIONS v 2, a collection of 1,600+ diverse language tasks and their expert written instructions that covers 70+ distinct task types, such as tagging, in-ﬁlling, and rewriting.	maybe	28	How can we measure the generalization of models to a variety of unseen tasks when provided with their language instructions? To facilitate progress in this goal, we introduce N ATURAL -I NSTRUCTIONS v 2 , a collection of 1,600+ diverse language tasks and their expert written instructions. More impor-tantly, the benchmark covers 70+ distinct task types, such as tagging, in-ﬁlling, and rewriting. This benchmark is collected with contributions	0bf8c1cab22c15d478f6ce431710065665b34e71	@['JournalArticle']{wang-etal-2022-benchmarking,  author = {Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and Anjana Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and Eshaan Pathak and Giannis Karamanolakis and Haizhi Gary Lai and I. Purohit and Ishani Mondal and Jacob Anderson and Kirby Kuznia and Krima Doshi and Maitreya Patel and Kuntal Kumar Pal and M. Moradshahi and Mihir Parmar and Mirali Purohit and Neeraj Varshney and Phani Rohitha Kaza and Pulkit Verma and Ravsehaj Singh Puri and Rushang Karia and Shailaja Keyur Sampat and Savan Doshi and S. Mishra and Sujan Reddy and Sumanta Patro and Tanay Dixit and Xudong Shen and Chitta Baral and Yejin Choi and Hannaneh Hajishirzi and Noah A. Smith and Daniel Khashabi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks},  volume = {abs/2204.07705},  year = {2022} }
Benchmarking down-scaled (not so large) pre-trained language models	2021	http://www.semanticscholar.org/paper/2a9541011d490977204eb11217a952ae15be7193	This work pre-train down-scaled versions of several popular Transformer-based architectures on a common pre-training corpus and benchmark them on a subset of the GLUE tasks, finding that additional compute should be mainly allocated to an increased model size, while training for more steps is inefficient.	maybe	1	Large Transformer-based language models are pre-trained on corpora of varying sizes, for a different number of steps and with different batch sizes. At the same time, more fundamental components, such as the pre-training objective or architectural hyperparameters, are modified. In total, it is therefore difficult to ascribe changes in performance to specific factors. Since searching the hyperparameter space over the full systems is too costly, we pre-train down-scaled versions of several popular Transformer-based architectures on a common pre-training corpus and benchmark them on a subset of the GLUE tasks (Wang et al., 2018). Specifically, we systematically compare three pre-training objectives for different shape parameters and model sizes, while also varying the number of pre-training steps and the batch size. In our experiments MLM + NSP (BERT-style) consistently outperforms MLM (RoBERTa-style) as well as the standard LM objective. Furthermore, we find that additional compute should be mainly allocated to an increased model size, while training for more steps is inefficient. Based on these observations, as a final step we attempt to scale up several systems using compound scaling (Tan and Le, 2019) adapted to Transformer-based language models.	2a9541011d490977204eb11217a952ae15be7193	@['JournalArticle']{aßenmacher-etal-2021-benchmarking,  author = {M. Aßenmacher and P. Schulze and C. Heumann},  booktitle = {Conference on Natural Language Processing},  journal = {ArXiv},  title = {Benchmarking down-scaled (not so large) pre-trained language models},  volume = {abs/2105.04876},  year = {2021} }
Benchmarking and Survey of Explanation Methods for Black Box Models	2021	http://www.semanticscholar.org/paper/93a55b04045ff7fff78de473f5ff52cbcfb9a948	A categorization of explanation methods based on the type of explanation returned is provided, and a visual comparison among explanations is shown and a quantitative benchmarking is shown.	maybe	55	The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.	93a55b04045ff7fff78de473f5ff52cbcfb9a948	@['JournalArticle', 'Review']{bodria-etal-2021-benchmarking,  author = {F. Bodria and F. Giannotti and Riccardo Guidotti and Francesca Naretto and D. Pedreschi and S. Rinzivillo},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Benchmarking and Survey of Explanation Methods for Black Box Models},  volume = {abs/2102.13076},  year = {2021} }
Behavior of Modern Pre-trained Language Models Using the Example of Probing Tasks	2021	http://www.semanticscholar.org/paper/2e92abdd7388e7ecca2008e9c37f27f2f6ba1ba0		maybe	0	Modern transformer-based language models are revolutionizing NLP. However, existing studies into language modelling with BERT have been mostly limited to English-language material and do not pay enough attention to the implicit knowledge of language, such as semantic roles, presupposition and negations, that can be acquired by the model during training. Thus, the aim of this study is to examine behavior of the model BERT in the task of masked language modelling and to provide linguistic interpretation to the unexpected effects and errors produced by the model. For this purpose, we used a new Russian-language dataset based on educational texts for learners of Russian and annotated with the help of the National Corpus of the Russian language. In terms of quality metrics (the proportion of words, semantically related to the target word), the multilingual BERT is recognized as the best model. Generally, each model has distinct strengths in relation to a certain linguistic phenomenon. These observations have meaningful implications for research into applied linguistics and pedagogy, contribute to dialogue system development, automatic exercise making, text generation and potentially could improve the quality of existing linguistic technologies	2e92abdd7388e7ecca2008e9c37f27f2f6ba1ba0	@['JournalArticle']{kalyaeva-etal-2021-behavior,  author = {Ekaterina Kalyaeva and O. Durandin and A. Malafeev},  booktitle = {Recent Advances in Natural Language Processing},  pages = {664-670},  title = {Behavior of Modern Pre-trained Language Models Using the Example of Probing Tasks},  year = {2021} }
BECEL: Benchmark for Consistency Evaluation of Language Models	2022	http://www.semanticscholar.org/paper/87eeb9b57e3247598e62d930b8cc53fe996dc779	This paper proposes the idea of LM consistency based on behavioural consistency and establishes a taxonomy that classifies previously studied consistencies into several sub-categories, and creates a new benchmark that allows for a more precise evaluation.	maybe	0	Behavioural consistency is a critical condition for a language model (LM) to become trustworthy like humans. Despite its importance, however, there is little consensus on the definition of LM consistency, resulting in different definitions across many studies. In this paper, we first propose the idea of LM consistency based on behavioural consistency and establish a taxonomy that classifies previously studied consistencies into several sub-categories. Next, we create a new benchmark that allows us to evaluate a model on 19 test cases, distinguished by multiple types of consistency and diverse downstream tasks. Through extensive experiments on the new benchmark, we ascertain that none of the modern pre-trained language models (PLMs) performs well in every test case, while exhibiting high inconsistency in many cases. Our experimental results suggest that a unified benchmark that covers broad aspects (i.e., multiple consistency types and tasks) is essential for a more precise evaluation.	87eeb9b57e3247598e62d930b8cc53fe996dc779	@['JournalArticle', 'Conference']{jang-etal-2022-becel:,  author = {Myeongjun Jang and D. Kwon and Thomas Lukasiewicz},  booktitle = {International Conference on Computational Linguistics},  pages = {3680-3696},  title = {BECEL: Benchmark for Consistency Evaluation of Language Models},  year = {2022} }
Balanced COPA: Countering Superficial Cues in Causal Reasoning	2020	http://www.semanticscholar.org/paper/7dd24f5d99cabffc649e14da318ca3fbd51e6527		maybe	0	Pretrained language models such as ELMo [15], BERT [1], RoBERTa [12] and ALBERT [9] have led to improved performance in benchmarks of natural language understanding, in tasks such as natural language inference [NLI, 11], argumentation [14], and commonsense reasoning [10, 18]. However, recent work has identified superficial cues in benchmark datasets which are predictive of the correct answer, such as unbalanced token distributions and lexical overlap. Once these cues are neutralized, models perform poorly, suggesting that their good performance is an instance of the Clever Hans effect1 [16]: Models trained on datasets with superficial cues learn heuristics for exploiting these cues, but do not develop any deeper understanding of the task. While superficial cues have been identified in, among others, datasets for NLI [5, 13], machine reading comprehension [20], and argumentation [14], one of the main benchmarks for commonsense reasoning, namely the Choice of Plausible Alternatives [COPA, 17], has not been analyzed so far. Here we present an analysis of superficial cues in COPA. Given a premise, such as The man broke his toe, COPA requires choosing the more plausible, causally related alternative, in this case either: because He got a hole in his sock (wrong) or because He dropped a hammer on his foot (correct). Our analysis reveals that COPA contains superfcial cues (§2) and that finetuned BERT [1] performs well (83.9 percent accuracy) on easy instances containing superficial cues, but worse (71.9 percent) on hard instances without such simple cues (§4.3). To prevent models from exploiting superficial cues in COPA, we introduce Balanced COPA (B-COPA). BCOPA contains one additional, mirrored instance for each original training instance. This mirrored instance uses the same alternatives as the corresponding original instance,	7dd24f5d99cabffc649e14da318ca3fbd51e6527	@None{kavumba-etal-2020-balanced,  author = {Pride Kavumba and Naoya Inoue and Benjamin Heinzerling and Keshav Singh and Paul Reisert and Kentarou Inui},  title = {Balanced COPA: Countering Superficial Cues in Causal Reasoning},  year = {2020} }
BadPrompt: Backdoor Attacks on Continuous Prompts	2022	http://www.semanticscholar.org/paper/97ae76460fa593a554336adab39eb06ea01219b1	The abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin are exhibited.	maybe	0	The prompt-based learning paradigm has gained much research attention recently. It has achieved state-of-the-art performance on several NLP tasks, especially in the few-shot scenarios. While steering the downstream tasks, few works have been reported to investigate the security problems of the prompt-based models. In this paper, we conduct the ﬁrst study on the vulnerability of the continuous prompt learning algorithm to backdoor attacks. We observe that the few-shot scenarios have posed a great challenge to backdoor attacks on the prompt-based models, limiting the usability of existing NLP backdoor methods. To address this challenge, we propose BadPrompt, a lightweight and task-adaptive algorithm, to backdoor attack continuous prompts. Specially, BadPrompt ﬁrst generates candidate triggers which are indicative for predicting the targeted label and dissimilar to the samples of the non-targeted labels. Then, it automatically selects the most effective and invisible trigger for each sample with an adaptive trigger optimization algorithm. We evaluate the performance of BadPrompt on ﬁve datasets and two continuous prompt models. The results exhibit the abilities of BadPrompt to effectively attack continuous prompts while maintaining high performance on the clean test sets, outperforming the baseline models by a large margin. The source code of BadPrompt is publicly available 1 .	97ae76460fa593a554336adab39eb06ea01219b1	@['JournalArticle']{cai-etal-2022-badprompt:,  author = {Xiangrui Cai and Haidong Xu and Sihan Xu and Ying Zhang and Xiaojie Yuan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {BadPrompt: Backdoor Attacks on Continuous Prompts},  volume = {abs/2211.14719},  year = {2022} }
Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning	2021	http://www.semanticscholar.org/paper/2ad898669cf696dccd9879d6e203c13aee3376e4	A stronger weight-poisoning attack method is proposed that introduces a layerwise weight poisoning strategy to plant deeper backdoors and introduces a combinatorial trigger that cannot be easily detected.	maybe	17	Pre-Trained Models have been widely applied and recently proved vulnerable under backdoor attacks: the released pre-trained weights can be maliciously poisoned with certain triggers. When the triggers are activated, even the fine-tuned model will predict pre-defined labels, causing a security threat. These backdoors generated by the poisoning methods can be erased by changing hyper-parameters during fine-tuning or detected by finding the triggers. In this paper, we propose a stronger weight-poisoning attack method that introduces a layerwise weight poisoning strategy to plant deeper backdoors; we also introduce a combinatorial trigger that cannot be easily detected. The experiments on text classification tasks show that previous defense methods cannot resist our weight-poisoning method, which indicates that our method can be widely applied and may provide hints for future model robustness studies.	2ad898669cf696dccd9879d6e203c13aee3376e4	@['JournalArticle', 'Conference']{li-etal-2021-backdoor,  author = {Linyang Li and Demin Song and Xiaonan Li and Jiehang Zeng and Ruotian Ma and Xipeng Qiu},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {3023-3032},  title = {Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning},  year = {2021} }
BabyBERTa: Learning More Grammar With Small-Scale Child-Directed Language	2021	http://www.semanticscholar.org/paper/4e38f52fdcbb3341b9c07d29016cab1a0b84912f	It is found that a smaller version of RoBERTa-base that never predicts unmasked tokens, which is term BabyBERTa, acquires grammatical knowledge comparable to that of pre-trained RoBER Ta-base - and does so with approximately 15X fewer parameters and 6,000X fewer words.	maybe	9	Transformer-based language models have taken the NLP world by storm. However, their potential for addressing important questions in language acquisition research has been largely ignored. In this work, we examined the grammatical knowledge of RoBERTa (Liu et al., 2019) when trained on a 5M word corpus of language acquisition data to simulate the input available to children between the ages 1 and 6. Using the behavioral probing paradigm, we found that a smaller version of RoBERTa-base that never predicts unmasked tokens, which we term BabyBERTa, acquires grammatical knowledge comparable to that of pre-trained RoBERTa-base - and does so with approximately 15X fewer parameters and 6,000X fewer words. We discuss implications for building more efficient models and the learnability of grammar from input available to children. Lastly, to support research on this front, we release our novel grammar test suite that is compatible with the small vocabulary of child-directed input.	4e38f52fdcbb3341b9c07d29016cab1a0b84912f	@['JournalArticle']{huebner-etal-2021-babyberta:,  author = {Philip A. Huebner and Elior Sulem and C. Fisher and D. Roth},  booktitle = {Conference on Computational Natural Language Learning},  pages = {624-646},  title = {BabyBERTa: Learning More Grammar With Small-Scale Child-Directed Language},  year = {2021} }
Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing	2021	http://www.semanticscholar.org/paper/0098123efc851b67137c1028f7bac8d8bffbc8fd	The ability of PLMs to discover which token should be grounded to which concept, if combined with the proposed erasingthen-awakening approach is highlighted, and the approach shows great potential to benefit downstream semantic parsing models.	maybe	20	Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasingthen-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.	0098123efc851b67137c1028f7bac8d8bffbc8fd	@['JournalArticle']{liu-etal-2021-awakening,  author = {Qian Liu and Dejian Yang and Jiahui Zhang and Jiaqi Guo and Bin Zhou and Jian-Guang Lou},  booktitle = {Findings},  journal = {ArXiv},  title = {Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing},  volume = {abs/2109.10540},  year = {2021} }
Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning	2021	http://www.semanticscholar.org/paper/fccce60283729934467877f0730317c3e9fcc61e	It is demonstrated that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, and it is shown that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning.	maybe	15	Recent prompt-based approaches allow pretrained language models to achieve strong performances on few-shot finetuning by reformulating downstream tasks as a language modeling problem. In this work, we demonstrate that, despite its advantages on low data regimes, finetuned prompt-based models for sentence pair classification tasks still suffer from a common pitfall of adopting inference heuristics based on lexical overlap, e.g., models incorrectly assuming a sentence pair is of the same meaning because they consist of the same set of words. Interestingly, we find that this particular inference heuristic is significantly less present in the zero-shot evaluation of the prompt-based model, indicating how finetuning can be destructive to useful knowledge learned during the pretraining. We then show that adding a regularization that preserves pretraining weights is effective in mitigating this destructive tendency of few-shot finetuning. Our evaluation on three datasets demonstrates promising improvements on the three corresponding challenge datasets used to diagnose the inference heuristics.	fccce60283729934467877f0730317c3e9fcc61e	@['JournalArticle', 'Conference']{utama-etal-2021-avoiding,  author = {Prasetya Ajie Utama and N. Moosavi and Victor Sanh and Iryna Gurevych},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {9063-9074},  title = {Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning},  year = {2021} }
Avicenna: a challenge dataset for natural language generation toward commonsense syllogistic reasoning	2022	http://www.semanticscholar.org/paper/c9314e06a4124501a21e90adcc4bcae4f925b799	The model’s confusion in detecting the middle-term was one of the main categories of errors that showed up in the error analysis, which indicates that the model learns how to extract new facts based on the premises, but it faces a challenge in commonsense reasoning.	maybe	1	Syllogism is a type of everyday reasoning. For instance, given that ‘Avicenna wrote the famous book the Canon of Medicine’ and ‘The Canon of Medicine has influenced modern medicine,’ it can be concluded that ‘Avicenna has influenced modern medicine.’ This study revolves around syllogistic natural language generation (NLG). The Avicenna corpus (https://github.com/ZeinabAghahadi/Syllogistic-Commonsense-Reasoning) was developed as a benchmark for syllogistic NLG. In this respect, once the syllogistic relation between two premises is recognised [Aghahadi, Z., & Talebpour, A. (2022). Language-based syllogistic reasoning using deep neural networks. Cognitive Semantics, 8(2)], the Avicenna-trained models learn to generate the conclusion sentence. The experiments were performed using state-of-the-art pre-trained text generative models and the accuracy was improved up to 32% when transfer learning was adopted. The model’s confusion in detecting the middle-term was one of the main categories of errors that showed up in the error analysis. This issue indicates that the model learns how to extract new facts based on the premises, but it faces a challenge in commonsense reasoning.	c9314e06a4124501a21e90adcc4bcae4f925b799	@['JournalArticle']{aghahadi-talebpour-2022-avicenna:,  author = {Zeinab Aghahadi and A. Talebpour},  booktitle = {J. Appl. Non Class. Logics},  journal = {Journal of Applied Non-Classical Logics},  pages = {55 - 71},  title = {Avicenna: a challenge dataset for natural language generation toward commonsense syllogistic reasoning},  volume = {32},  year = {2022} }
Automatic Story Generation: Challenges and Attempts	2021	http://www.semanticscholar.org/paper/df7bf316e7bad359e87b10544155be09336720ca	This paper analyzes works in story generation that utilize machine learning approaches to address story generation controllability, incorporate commonsense knowledge, infer reasonable character actions, and generate creative language.	maybe	14	Automated storytelling has long captured the attention of researchers for the ubiquity of narratives in everyday life. The best human-crafted stories exhibit coherent plot, strong characters, and adherence to genres, attributes that current states-of-the-art still struggle to produce, even using transformer architectures. In this paper, we analyze works in story generation that utilize machine learning approaches to (1) address story generation controllability, (2) incorporate commonsense knowledge, (3) infer reasonable character actions, and (4) generate creative language.	df7bf316e7bad359e87b10544155be09336720ca	@['JournalArticle']{alabdulkarim-etal-2021-automatic,  author = {Amal Alabdulkarim and Siyan Li and Xiangyu Peng},  booktitle = {NUSE},  journal = {ArXiv},  title = {Automatic Story Generation: Challenges and Attempts},  volume = {abs/2102.12634},  year = {2021} }
Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs	2021	http://www.semanticscholar.org/paper/1444536496d8064f33e10b38b5820fecfab5b367	This work investigates whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair, and finds that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques.	maybe	13	OpenAI’s Codex, a GPT-3 like model trained on a large code corpus, has made headlines in and outside of academia. Given a short user-provided description, it is capable of synthesizing code snippets that are syntactically and semantically valid in most cases. In this work, we want to investigate whether Codex is able to localize and fix bugs, a task of central interest in the field of automated program repair. Our initial evaluation uses the multi-language QuixBugs benchmark (40 bugs in both Python and Java). We find that, despite not being trained for APR, Codex is surprisingly effective, and competitive with recent state of the art techniques. Our results also show that Codex is slightly more successful at repairing Python than Java.	1444536496d8064f33e10b38b5820fecfab5b367	@['JournalArticle']{prenner-robbes-2021-automatic,  author = {Julian Aron Prenner and R. Robbes},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Automatic Program Repair with OpenAI's Codex: Evaluating QuixBugs},  volume = {abs/2111.03922},  year = {2021} }
Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models	2022	http://www.semanticscholar.org/paper/0d08ffccc982781e310bb184397bbe64b9aef157	The analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students.	yes	18	This article explores the natural language generation capabilities of large language models with application to the production of two types of learning resources common in programming courses. Using OpenAI Codex as the large language model, we create programming exercises (including sample solutions and test cases) and code explanations, assessing these qualitatively and quantitatively. Our results suggest that the majority of the automatically generated content is both novel and sensible, and in some cases ready to use as is. When creating exercises we find that it is remarkably easy to influence both the programming concepts and the contextual themes they contain, simply by supplying keywords as input to the model. Our analysis suggests that there is significant value in massive generative machine learning models as a tool for instructors, although there remains a need for some oversight to ensure the quality of the generated content before it is delivered to students. We further discuss the implications of OpenAI Codex and similar tools for introductory programming education and highlight future research streams that have the potential to improve the quality of the educational experience for both teachers and students alike.	0d08ffccc982781e310bb184397bbe64b9aef157	@['JournalArticle', 'Book']{sarsa-etal-2022-automatic,  author = {Sami Sarsa and Paul Denny and Arto Hellas and Juho Leinonen},  booktitle = {International Computing Education Research Workshop},  journal = {Proceedings of the 2022 ACM Conference on International Computing Education Research - Volume 1},  title = {Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models},  year = {2022} }
Automatic Detection of Machine Generated Text: A Critical Survey	2020	http://www.semanticscholar.org/paper/9438bc5626b2d9a771cecc7a41ecabf6639db53c	An in-depth error analysis of the state-of-the-art detector is conducted and research directions are discussed to guide future work in this exciting area.	maybe	38	Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.	9438bc5626b2d9a771cecc7a41ecabf6639db53c	@['JournalArticle', 'Conference', 'Review']{jawahar-etal-2020-automatic,  author = {Ganesh Jawahar and Muhammad Abdul-Mageed and L. Lakshmanan},  booktitle = {International Conference on Computational Linguistics},  pages = {2296-2309},  title = {Automatic Detection of Machine Generated Text: A Critical Survey},  year = {2020} }
Automatic Detection of Generated Text is Easiest when Humans are Fooled	2019	http://www.semanticscholar.org/paper/821a4aedd40596d4a6a95b3b9246baa109193a08	It is shown that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time.	yes	86	Recent advancements in neural language modelling make it possible to rapidly generate vast amounts of human-sounding text. The capabilities of humans and automatic discriminators to detect machine-generated text have been a large source of research interest, but humans and machines rely on different cues to make their decisions. Here, we perform careful benchmarking and analysis of three popular sampling-based decoding strategies—top-_k_, nucleus sampling, and untruncated random sampling—and show that improvements in decoding methods have primarily optimized for fooling humans. This comes at the expense of introducing statistical abnormalities that make detection easy for automatic systems. We also show that though both human and automatic detector performance improve with longer excerpt length, even multi-sentence excerpts can fool expert human raters over 30% of the time. Our findings reveal the importance of using both human and automatic detectors to assess the humanness of text generation systems.	821a4aedd40596d4a6a95b3b9246baa109193a08	@['JournalArticle', 'Conference']{ippolito-etal-2019-automatic,  author = {Daphne Ippolito and Daniel Duckworth and Chris Callison-Burch and D. Eck},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1808-1822},  title = {Automatic Detection of Generated Text is Easiest when Humans are Fooled},  year = {2019} }
Automatic Code Documentation Generation Using GPT-3	2022	http://www.semanticscholar.org/paper/9360390b02b9a09ece9a2486055b17e18dc5d3f6	Codec is a GPT-3 based model pre-trained on both natural and programming languages that outperforms existing techniques even with basic settings like one-shot learning and achieves an overall BLEU score of 20.6 for six different programming languages.	maybe	0	Source code documentation is an important artifact for efficient software development. Code documentation could greatly benefit from automation since manual documentation is often labouring, resource and time-intensive. In this paper, we employed Codex for automatic code documentation creation. Codex is a GPT-3 based model pre-trained on both natural and programming languages. We find that Codex outperforms existing techniques even with basic settings like one-shot learning (i.e., providing only one example for training). Codex achieves an overall BLEU score of 20.6 for six different programming languages (11.2% improvement over earlier state-of-the-art techniques). Thus, Codex shows promise and warrants in-depth future studies for automatic code documentation generation to support diverse development tasks.	9360390b02b9a09ece9a2486055b17e18dc5d3f6	@['Book', 'JournalArticle', 'Conference']{khan-uddin-2022-automatic,  author = {Junaed Younus Khan and Gias Uddin},  booktitle = {International Conference on Automated Software Engineering},  journal = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},  title = {Automatic Code Documentation Generation Using GPT-3},  year = {2022} }
Automated Storytelling via Causal, Commonsense Plot Ordering	2020	http://www.semanticscholar.org/paper/4e0af5f4944c16e3ae49b3c96cce7f81989c30f8	C2PO is demonstrated, an approach to narrative generation that operationalizes the concept of soft causal relations as causal relations inferred from commonsense reasoning through Causal, Commonsense Plot Ordering.	maybe	41	Automated story plot generation is the task of generating a coherent sequence of plot events. Causal relations between plot events are believed to increase the perception of story and plot coherence. In this work, we introduce the concept of soft causal relations as causal relations inferred from commonsense reasoning. We demonstrate C2PO, an approach to narrative generation that operationalizes this concept through Causal, Commonsense Plot Ordering. Using human-participant protocols, we evaluate our system against baseline systems with different commonsense reasoning reasoning and inductive biases to determine the role of soft causal relations in perceived story quality. Through these studies we also probe the interplay of how changes in commonsense norms across storytelling genres affect perceptions of story quality.	4e0af5f4944c16e3ae49b3c96cce7f81989c30f8	@['JournalArticle', 'Conference']{ammanabrolu-etal-2020-automated,  author = {Prithviraj Ammanabrolu and W. Cheung and William Broniec and Mark O. Riedl},  booktitle = {AAAI Conference on Artificial Intelligence},  pages = {5859-5867},  title = {Automated Storytelling via Causal, Commonsense Plot Ordering},  year = {2020} }
Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models	2022	http://www.semanticscholar.org/paper/d8e134ed0bd53ae708c89c54f6616ad6a22084f0	This work proposes and studies Attributed QA as a key step in the development of attributed LLMs, and develops a reproducable evaluation framework using human annotations as a gold standard and a correlated automatic metric that is suitable for development settings.	maybe	1	Large language models (LLMs) have shown impressive results across a variety of tasks while requiring little or no direct supervision. Further, there is mounting evidence that LLMs may have potential in information-seeking scenarios. We believe the ability of an LLM to attribute the text that it generates is likely to be crucial for both system developers and users in this setting. We propose and study Attributed QA as a key ﬁrst step in the development of attributed LLMs. We develop a reproducable evaluation framework for the task, using human annotations as a gold standard and a correlated automatic metric that we show is suitable for development settings. We describe and benchmark a broad set of architectures for the task. Our contributions give some concrete answers to two key questions ( How to measure attribution? , and How well do current state-of-the-art methods perform on attribution? ), and give some hints as to how to address a third key question ( How to build LLMs with attribution? ).	d8e134ed0bd53ae708c89c54f6616ad6a22084f0	@['JournalArticle']{bohnet-etal-2022-attributed,  author = {Bernd Bohnet and V. Tran and Pat Verga and Roee Aharoni and D. Andor and Livio Baldini Soares and Jacob Eisenstein and Kuzman Ganchev and Jonathan Herzig and Kai Hui and T. Kwiatkowski and Ji Ma and Jianmo Ni and Tal Schuster and William W. Cohen and Michael Collins and Dipanjan Das and Donald Metzler and Slav Petrov and Kellie Webster},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Attributed Question Answering: Evaluation and Modeling for Attributed Large Language Models},  volume = {abs/2212.08037},  year = {2022} }
Attention-based Neural Networks Encode Aspects of Human Word Sense Knowledge	2020	http://www.semanticscholar.org/paper/06f265bdcfa56d6078b1e1ccac974484b7d1bc46	This work determines if neural models that make use of attention to represent words in context, or word embeddings, can encode humanlike distinctions between word senses, particularly for polysemous (semantically related) and homonymous relations between senses.	maybe	0	How humans understand variation in a word’s senses is key to explaining the structure of the lexicon, but formal models categorizing word senses like WordNet (Miller et al., 1990) do not capture this cognitive phenomenon. Our work specifically determines if neural models that make use of attention to represent words in context, or word embeddings, can encode humanlike distinctions between word senses, particularly for polysemous (semantically related) and homonymous relations (semantically unrelated) between senses. We collect data from a behavioral web-based experiment, in which English speakers provide judgements of the relatedness of multiple senses of several words. We compare human judgements for these senses to contextualized word embeddings from BERT (Bidirectional Encoder Representations from Transformers, Devlin et al. (2018)), run on a corpus tagged with WordNet senses (Miller et al., 1993). We demonstrate human participants have shared intuitions about the relatedness of word senses, and BERT embeddings often capture homonymous relationships between word senses, but fail to do so for highly polysemous cases, most notably metaphorical polysemy. Through our work, we present an empirical justification that attention-based neural models of word representation can be used to provide a cognitive backing for formal models of word sense.	06f265bdcfa56d6078b1e1ccac974484b7d1bc46	@None{nair-srinivasan-2020-attention,  author = {Sathvik Nair and M. Srinivasan},  title = {Attention-based Neural Networks Encode Aspects of Human Word Sense Knowledge},  year = {2020} }
Attention weights accurately predict language representations in the brain	2022	http://www.semanticscholar.org/paper/46c87b00d9ad2f69eb282fc2faa908dcf94eb77d		maybe	1	In Transformer-based language models (LMs) the attention mechanism converts token embeddings into contextual embeddings that incorporate information from neighboring words. The resulting contextual hidden state embeddings have enabled highly accurate models of brain responses, suggesting that the attention mechanism constructs contextual embeddings that carry information reflected in language-related brain representations. However, it is unclear whether the attention weights that are used to integrate information across words are themselves related to language representations in the brain. To address this question we analyzed functional magnetic resonance imaging (fMRI) recordings of participants reading English language narratives. We provided the narrative text as input to two LMs (BERT and GPT-2) and extracted their corresponding attention weights. We then used encoding models to determine how well attention weights can predict recorded brain responses. We find that attention weights accurately predict brain responses in much of the frontal and temporal cortices. Our results suggest that the attention mechanism itself carries information that is reflected in brain representations. Moreover, these results indicate cortical areas in which context integration may occur.	46c87b00d9ad2f69eb282fc2faa908dcf94eb77d	@None{lamarre-etal-2022-attention,  author = {Mathis Lamarre and Catherine Chen and Fatma Deniz},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Attention weights accurately predict language representations in the brain},  year = {2022} }
Attention Understands Semantic Relations	2022	http://www.semanticscholar.org/paper/374bfae4bb6af88449a1295489c2163a5304f8da	It is shown that in this task, attention scores are nearly as expressive as the layers’ output activations, despite their lesser ability to represent surface cues, which supports the hypothesis that attention mechanisms are focusing not only on the syntactic relational information but also on the semantic one.	maybe	0	Today, natural language processing heavily relies on pre-trained large language models. Even though such models are criticized for the poor interpretability, they still yield state-of-the-art solutions for a wide set of very different tasks. While lots of probing studies have been conducted to measure the models’ awareness of grammatical knowledge, semantic probing is less popular. In this work, we introduce the probing pipeline to study the representedness of semantic relations in transformer language models. We show that in this task, attention scores are nearly as expressive as the layers’ output activations, despite their lesser ability to represent surface cues. This supports the hypothesis that attention mechanisms are focusing not only on the syntactic relational information but also on the semantic one.	374bfae4bb6af88449a1295489c2163a5304f8da	@['JournalArticle']{chizhikova-etal-2022-attention,  author = {Anastasia Chizhikova and S. Murzakhmetov and Oleg Serikov and Tatiana Shavrina and M. Burtsev},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {4040-4050},  title = {Attention Understands Semantic Relations},  year = {2022} }
Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms	2020	https://www.semanticscholar.org/paper/2a8e42995caaedadc9dc739d85bed2c57fc78568	It is shown that attention weights alone are only one of the two factors that determine the output of attention and a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors is proposed.	seed	15	Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.	2a8e42995caaedadc9dc739d85bed2c57fc78568	@['JournalArticle']{kobayashi-etal-2020-attention,  author = {Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentaro Inui},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Attention Module is Not Only a Weight: Analyzing Transformers with Vector Norms},  volume = {abs/2004.10102},  year = {2020} }
Attention is not not Explanation	2019	https://www.semanticscholar.org/paper/ce177672b00ddf46e4906157a7e997ca9338b8b9	It is shown that even when reliable adversarial distributions can be found, they don’t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.	seed	541	Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model’s prediction, and consequently reach insights regarding the model’s decision-making process. A recent paper claims that ‘Attention is not Explanation’ (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one’s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don’t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.	ce177672b00ddf46e4906157a7e997ca9338b8b9	@['JournalArticle', 'Conference']{wiegreffe-pinter-2019-attention,  author = {Sarah Wiegreffe and Yuval Pinter},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {11-20},  title = {Attention is not not Explanation},  year = {2019} }
Attention is not Explanation	2019	https://www.semanticscholar.org/paper/1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f	This work performs extensive experiments across a variety of NLP tasks to assess the degree to which attention weights provide meaningful “explanations” for predictions, and finds that they largely do not.	seed	782	Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.	1e83c20def5c84efa6d4a0d80aa3159f55cb9c3f	@['JournalArticle', 'Conference']{jain-wallace-2019-attention,  author = {Sarthak Jain and Byron C. Wallace},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {3543-3556},  title = {Attention is not Explanation},  year = {2019} }
Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth	2021	http://www.semanticscholar.org/paper/77b44f1985995edfb434f83c6879872c36f12507	This work proposes a new way to understand self-attention networks: it is shown that their output can be decomposed into a sum of smaller terms—or paths—each involving the operation of a sequence of attention heads across layers, and proves that selfattention possesses a strong inductive bias towards “token uniformity”.	maybe	118	Attention-based architectures have become ubiquitous in machine learning. Yet, our understanding of the reasons for their effectiveness remains limited. This work proposes a new way to understand self-attention networks: we show that their output can be decomposed into a sum of smaller terms—or paths—each involving the operation of a sequence of attention heads across layers. Using this path decomposition, we prove that selfattention possesses a strong inductive bias towards “token uniformity”. Specifically, without skip connections or multi-layer perceptrons (MLPs), the output converges doubly exponentially to a rank-1 matrix. On the other hand, skip connections and MLPs stop the output from degeneration. Our experiments verify the convergence results on standard transformer architectures.	77b44f1985995edfb434f83c6879872c36f12507	@['JournalArticle', 'Conference']{dong-etal-2021-attention,  author = {Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},  booktitle = {International Conference on Machine Learning},  journal = {ArXiv},  title = {Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},  volume = {abs/2103.03404},  year = {2021} }
Attention Interpretability Across NLP Tasks	2019	http://www.semanticscholar.org/paper/3d4dfbdcb11d7b495e066435a9a98f02eb0cb369	This work attempts to fill the gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not) and reinforces the claim of interpretability of attention through manual evaluation.	maybe	102	The attention layer in a neural network model provides insights into the model's reasoning behind its prediction, which are usually criticized for being opaque. Recently, seemingly contradictory viewpoints have emerged about the interpretability of attention weights (Jain & Wallace, 2019; Vig & Belinkov, 2019). Amid such confusion arises the need to understand attention mechanism more systematically. In this work, we attempt to fill this gap by giving a comprehensive explanation which justifies both kinds of observations (i.e., when is attention interpretable and when it is not). Through a series of experiments on diverse NLP tasks, we validate our observations and reinforce our claim of interpretability of attention through manual evaluation.	3d4dfbdcb11d7b495e066435a9a98f02eb0cb369	@['JournalArticle']{vashishth-etal-2019-attention,  author = {Shikhar Vashishth and Shyam Upadhyay and Gaurav Singh Tomar and Manaal Faruqui},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Attention Interpretability Across NLP Tasks},  volume = {abs/1909.11218},  year = {2019} }
Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models	2020	https://www.semanticscholar.org/paper/cb0c40ab85ddff43e41aafabc90031c4140f31b2	The visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models, and to help users gain insight on how a classification decision is made.	maybe	22	Advances in language modeling have led to the development of deep attention-based models that are performant across a wide variety of natural language processing (NLP) problems. These language models are typified by a pre-training process on large unlabeled text corpora and subsequently fine-tuned for specific tasks. Although considerable work has been devoted to understanding the attention mechanisms of pre-trained models, it is less understood how a model's attention mechanisms change when trained for a target NLP task. In this paper, we propose a visual analytics approach to understanding fine-tuning in attention-based language models. Our visualization, Attention Flows, is designed to support users in querying, tracing, and comparing attention within layers, across layers, and amongst attention heads in Transformer-based language models. To help users gain insight on how a classification decision is made, our design is centered on depicting classification-based attention at the deepest layer and how attention from prior layers flows throughout words in the input. Attention Flows supports the analysis of a single model, as well as the visual comparison between pre-trained and fine-tuned models via their similarities and differences. We use Attention Flows to study attention mechanisms in various sentence understanding tasks and highlight how attention evolves to address the nuances of solving these tasks.	cb0c40ab85ddff43e41aafabc90031c4140f31b2	@['JournalArticle']{derose-etal-2020-attention,  author = {Joseph F DeRose and Jiayao Wang and M. Berger},  booktitle = {IEEE Transactions on Visualization and Computer Graphics},  journal = {IEEE Transactions on Visualization and Computer Graphics},  pages = {1160-1170},  title = {Attention Flows: Analyzing and Comparing Attention Mechanisms in Language Models},  volume = {27},  year = {2020} }
Attention Flows are Shapley Value Explanations	2021	http://www.semanticscholar.org/paper/da130d6538eeeacfb3a0da4cff106c098f74cdd4	It is argued that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones, because attention flows are indeed Shapley Values, at at the layerwise level.	maybe	13	Shapley Values, a solution to the credit assignment problem in cooperative game theory, are a popular type of explanation in machine learning, having been used to explain the importance of features, embeddings, and even neurons. In NLP, however, leave-one-out and attention-based explanations still predominate. Can we draw a connection between these different methods? We formally prove that — save for the degenerate case — attention weights and leave-one-out values cannot be Shapley Values. Attention flow is a post-processed variant of attention weights obtained by running the max-flow algorithm on the attention graph. Perhaps surprisingly, we prove that attention flows are indeed Shapley Values, at least at the layerwise level. Given the many desirable theoretical qualities of Shapley Values — which has driven their adoption among the ML community — we argue that NLP practitioners should, when possible, adopt attention flow explanations alongside more traditional ones.	da130d6538eeeacfb3a0da4cff106c098f74cdd4	@['JournalArticle', 'Conference']{ethayarajh-jurafsky-2021-attention,  author = {Kawin Ethayarajh and Dan Jurafsky},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {49-54},  title = {Attention Flows are Shapley Value Explanations},  year = {2021} }
Attention cannot be an Explanation	2022	http://www.semanticscholar.org/paper/31be1dd34f6420477ab5fe24f09cc26aac2da548		maybe	1	Attention based explanations (viz. saliency maps), by providing interpretability to black box models such as deep neural networks, are assumed to improve human trust and reliance in the underlying models. Recently, it has been shown that attention weights are frequently uncorrelated with gradient-based measures of feature importance. Motivated by this, we ask a follow-up question: “Assuming that we only consider the tasks where attention weights correlate well with feature importance, how effective are these attention based explanations in increasing human trust and reliance in the underlying models?”. In other words, can we use attention as an explanation? We perform extensive human study experiments that aim to qualitatively and quantitatively assess the degree to which attention based explanations are suitable in increasing human trust and reliance. Our experiment results show that attention cannot be used as an explanation.	31be1dd34f6420477ab5fe24f09cc26aac2da548	@['JournalArticle']{akula-zhu-2022-attention,  author = {Arjun Reddy Akula and Song-Chun Zhu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Attention cannot be an Explanation},  volume = {abs/2201.11194},  year = {2022} }
Attention Can Reflect Syntactic Structure (If You Let It)	2021	http://www.semanticscholar.org/paper/0e6165ec3151e7e758d4ac90e0e009a4e3bbbebd	This study presents decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns, and demonstrates full trees can be decoded above baseline accuracy from single attention heads.	yes	18	Since the popularization of the Transformer as a general-purpose feature encoder for NLP, many studies have attempted to decode linguistic structure from its novel multi-head attention mechanism. However, much of such work focused almost exclusively on English — a language with rigid word order and a lack of inflectional morphology. In this study, we present decoding experiments for multilingual BERT across 18 languages in order to test the generalizability of the claim that dependency syntax is reflected in attention patterns. We show that full trees can be decoded above baseline accuracy from single attention heads, and that individual relations are often tracked by the same heads across languages. Furthermore, in an attempt to address recent debates about the status of attention as an explanatory mechanism, we experiment with fine-tuning mBERT on a supervised parsing objective while freezing different series of parameters. Interestingly, in steering the objective to learn explicit linguistic structure, we find much of the same structure represented in the resulting attention patterns, with interesting differences with respect to which parameters are frozen.	0e6165ec3151e7e758d4ac90e0e009a4e3bbbebd	@['JournalArticle', 'Conference']{ravishankar-etal-2021-attention,  author = {Vinit Ravishankar and Artur Kulmizev and Mostafa Abdou and Anders Søgaard and Joakim Nivre},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {3031-3045},  title = {Attention Can Reflect Syntactic Structure (If You Let It)},  year = {2021} }
AttCAT: Explaining Transformers via Attentive Class Activation Tokens	2022	http://www.semanticscholar.org/paper/26c89113585741975e932e493c562c3114cc6b96	A novel Transformer explanation technique via attentive class activation tokens, aka, AttCAT, leveraging encoded features, their gradients, and their attention weights to generate a faithful and confident explanation for Transformer’s output is proposed.	maybe	2	Transformers have improved the state-of-the-art in various natural language processing and computer vision tasks. However, the success of the Transformer model has not yet been duly explained. Current explanation techniques, which dissect either the self-attention mechanism or gradient-based attribution, do not necessarily provide a faithful explanation of the inner workings of Transformers due to the following reasons: first, attention weights alone without considering the magnitudes of feature values are not adequate to reveal the self-attention mechanism; second, whereas most Transformer explanation techniques utilize self-attention module, the skip-connection module, contributing a significant portion of information flows in Transformers, has not yet been sufficiently exploited in explanation; third, the gradient-based attribution of individual feature does not incorporate interaction among features in explaining the model’s output. In order to tackle the above problems, we propose a novel Transformer explanation technique via attentive class activation tokens, aka, AttCAT, leveraging encoded features, their gradients, and their attention weights to generate a faithful and confident explanation for Transformer’s output. Extensive experiments are conducted to demonstrate the superior performance of AttCAT, which generalizes well to different Transformer architectures, evaluation metrics, datasets, and tasks, to the baseline methods. Our code is available at: https://github.com/qiangyao1988/AttCAT.	26c89113585741975e932e493c562c3114cc6b96	@None{qiang-etal-2022-attcat:,  author = {Yao Qiang and Deng Pan and Chengyin Li and X. Li and Rhongho Jang and D. Zhu},  title = {AttCAT: Explaining Transformers via Attentive Class Activation Tokens},  year = {2022} }
AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models	2022	http://www.semanticscholar.org/paper/f8f2b17083c10f730b711a938e2bb5da992086e7	This paper proves the existence of a syntactic subspace, lying in the hidden representations of pre-trained language models, which contain the syntactic information of the programming language and defines a novel probing method, the AST-Probe, that enables recovering the whole abstract syntax tree of an input code snippet.	maybe	1	The objective of pre-trained language models is to learn contextual representations of textual data. Pre-trained language models have become mainstream in natural language processing and code modeling. Using probes, a technique to study the linguistic properties of hidden vector spaces, previous works have shown that these pre-trained language models encode simple linguistic properties in their hidden representations. However, none of the previous work assessed whether these models encode the whole grammatical structure of a programming language. In this paper, we prove the existence of a syntactic subspace, lying in the hidden representations of pre-trained language models, which contain the syntactic information of the programming language. We show that this subspace can be extracted from the models’ representations and define a novel probing method, the AST-Probe, that enables recovering the whole abstract syntax tree (AST) of an input code snippet. In our experimentations, we show that this syntactic subspace exists in five state-of-the-art pre-trained language models. In addition, we highlight that the middle layers of the models are the ones that encode most of the AST information. Finally, we estimate the optimal size of this syntactic subspace and show that its dimension is substantially lower than those of the models’ representation spaces. This suggests that pre-trained language models use a small part of their representation spaces to encode syntactic information of the programming languages.	f8f2b17083c10f730b711a938e2bb5da992086e7	@['Book', 'JournalArticle', 'Conference']{lópez-etal-2022-ast,  author = {José Antonio Hernández López and M. Weyssow and Jes'us S'anchez Cuadrado and H. Sahraoui},  booktitle = {International Conference on Automated Software Engineering},  journal = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},  title = {AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models},  year = {2022} }
Assistance with large language models	2022	http://www.semanticscholar.org/paper/0180d35b85dd4daead90e0652b64b1339e754684	A behavioral cloning approach is applied to GPT-3 such that it can respond to clear input questions directly, clarify the intent behind vague input questions, and respond based on the clarification it receives, and this approach leads to quantitative improvements in answer accuracy compared to a baseline that cannot ask for clarifications.	maybe	2	A core part of AI alignment is training AI systems to be helpful, or more generally, to interact with humans appropriately. We look at this problem in the context of large language models. Past works have focused on training these models to perform specific tasks, or follow instructions. In contrast, we believe helpfulness requires back-and-forth interaction between the AI and the human it is trying to assist. Here, we consider a multi-step interaction in which a human asks a question, and the AI has an opportunity to ask a clarifying question to resolve ambiguities before responding. The assistance framework formalizes the idea of an AI which aims to maximize the human’s reward but is ignorant of the human reward function. Past works solved toy assistance environments using exact POMDP solvers as well as deep reinforcement learning. We apply a behavioral cloning approach, and fine-tune GPT-3 such that it can respond to clear input questions directly, clarify the intent behind vague input questions, and respond based on the clarification it receives. We show that this approach leads to quantitative improvements in answer accuracy compared to a baseline that cannot ask for clarifications. While the assistance framework assumes the correct behavior of an AI is to infer and maximize a human’s reward, our approach can be used to learn any interaction protocol between the AI and the human. We believe exploring interaction protocols that are easy to learn robustly, and can be used to "bootstrap" further alignment are a promising direction for future research.	0180d35b85dd4daead90e0652b64b1339e754684	@None{krasheninnikov-2022-assistance,  author = {Dmitrii Krasheninnikov},  title = {Assistance with large language models},  year = {2022} }
Assessing the use of attention weights to interpret BERT-based stance classification	2021	http://www.semanticscholar.org/paper/1e65d1a622ce8cb30ad4ad2bafc20a4bd9ba5892	An attention-based interpretability mechanism to identify the most influential words for stances predicted using BERT-based models and proved to be helpful to understand the influence of words in the classification, and they revealed intrinsic properties of the domain and representative arguments of the stances.	maybe	0	BERT models are currently state-of-the-art solutions for various tasks, including stance classification. However, these models are a black box for their users. Some proposals have leveraged the weights assigned by the internal attention mechanisms of these models for interpretability purposes. However, whether the attention weights help the interpretability of the model is still a matter of debate, with positions in favor and against. This work proposes an attention-based interpretability mechanism to identify the most influential words for stances predicted using BERT-based models. We target stances expressed in Twitter using the Portuguese language and assess the proposed mechanism using a case study regarding stances on COVID-19 vaccination in the Brazilian context. The interpretation mechanism traces tokens’ attentions back to words, assigning a newly proposed metric referred to as absolute word attention. Through this metric, we assess several aspects to determine if we can find important words for the classification and with meaning for the domain. We developed a broad experimental setting that involved three datasets with tweets in Brazilian Portuguese and three BERT models with support for this language. Our results are encouraging, as we were able to identify 52-82% of words with high absolute attention contributing positively to stance classification. The interpretability mechanism proved to be helpful to understand the influence of words in the classification, and they revealed intrinsic properties of the domain and representative arguments of the stances.	1e65d1a622ce8cb30ad4ad2bafc20a4bd9ba5892	@['JournalArticle', 'Book']{sáenz-becker-2021-assessing,  author = {Carlos Abel Córdova Sáenz and Karin Becker},  booktitle = {WI/IAT},  journal = {IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology},  title = {Assessing the use of attention weights to interpret BERT-based stance classification},  year = {2021} }
Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models	2021	http://www.semanticscholar.org/paper/ab8ea4bf7b58edea54edbd179c48bbc24b8aeadf	This work explores the syntactic generalization capabilities of the monolingual and multilingual versions of BERT and RoBERTa, and introduces SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntax generalization abilities of language models through the Syntax Gym online platform.	maybe	1	Multilingual Transformer-based language models, usually pretrained on more than 100 languages, have been shown to achieve outstanding results in a wide range of crosslingual transfer tasks. However, it remains unknown whether the optimization for different languages conditions the capacity of the models to generalize over syntactic structures, and how languages with syntactic phenomena of different complexity are affected. In this work, we explore the syntactic generalization capabilities of the monolingual and multilingual versions of BERT and RoBERTa. More specifically, we evaluate the syntactic generalization potential of the models on English and Spanish tests, comparing the syntactic abilities of monolingual and multilingual models on the same language (English), and of multilingual models on two different languages (English and Spanish). For English, we use the available SyntaxGym test suite; for Spanish, we introduce SyntaxGymES, a novel ensemble of targeted syntactic tests in Spanish, designed to evaluate the syntactic generalization capabilities of language models through the SyntaxGym online platform.	ab8ea4bf7b58edea54edbd179c48bbc24b8aeadf	@['JournalArticle']{mayos-etal-2021-assessing,  author = {Laura Pérez-Mayos and Alba T'aboas Garc'ia and Simon Mille and L. Wanner},  booktitle = {Findings},  pages = {3799-3812},  title = {Assessing the Syntactic Capabilities of Transformer-based Multilingual Language Models},  year = {2021} }
Assessing the Limits of the Distributional Hypothesis in Semantic Spaces: Trait-based Relational Knowledge and the Impact of Co-occurrences	2022	http://www.semanticscholar.org/paper/5a08bddbd1a56d449a0b01b0cc4e457c5480969c	Evaluating how well English and Spanish semantic spaces capture a particular type of relational knowledge, namely the traits associated with concepts, and exploring the role of co-occurrences in this context.	maybe	0	The increase in performance in NLP due to the prevalence of distributional models and deep learning has brought with it a reciprocal decrease in interpretability. This has spurred a focus on what neural networks learn about natural language with less of a focus on how. Some work has focused on the data used to develop data-driven models, but typically this line of work aims to highlight issues with the data, e.g. highlighting and offsetting harmful biases. This work contributes to the relatively untrodden path of what is required in data for models to capture meaningful representations of natural language. This is entails evaluating how well English and Spanish semantic spaces capture a particular type of relational knowledge, namely the traits associated with concepts (e.g. bananas-yellow), and exploring the role of co-occurrences in this context.	5a08bddbd1a56d449a0b01b0cc4e457c5480969c	@['JournalArticle']{anderson-collados-2022-assessing,  author = {Mark Anderson and José Camacho-Collados},  booktitle = {STARSEM},  pages = {173-185},  title = {Assessing the Limits of the Distributional Hypothesis in Semantic Spaces: Trait-based Relational Knowledge and the Impact of Co-occurrences},  year = {2022} }
Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference	2021	https://www.semanticscholar.org/paper/7b7b8531406dac284c577da10ad630115dc60d13	This study introduces a synthetically generated Japanese NLI dataset, called the Japanese Adversarial NLI (JaNLI) dataset, which is inspired by the English HANS dataset and is designed to require understanding of Japanese linguistic phenomena and illuminate the vulnerabilities of models.	maybe	4	Despite the success of multilingual pre-trained language models, it remains unclear to what extent these models have human-like generalization capacity across languages. The aim of this study is to investigate the out-of-distribution generalization of pre-trained language models through Natural Language Inference (NLI) in Japanese, the typological properties of which are different from those of English. We introduce a synthetically generated Japanese NLI dataset, called the Japanese Adversarial NLI (JaNLI) dataset, which is inspired by the English HANS dataset and is designed to require understanding of Japanese linguistic phenomena and illuminate the vulnerabilities of models. Through a series of experiments to evaluate the generalization performance of both Japanese and multilingual BERT models, we demonstrate that there is much room to improve current models trained on Japanese NLI tasks. Furthermore, a comparison of human performance and model performance on the different types of garden-path sentences in the JaNLI dataset shows that structural phenomena that ease interpretation of garden-path sentences for human readers do not help models in the same way, highlighting a difference between human readers and the models.	7b7b8531406dac284c577da10ad630115dc60d13	@['JournalArticle']{yanaka-mineshima-2021-assessing,  author = {Hitomi Yanaka and K. Mineshima},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {337-349},  title = {Assessing the Generalization Capacity of Pre-trained Language Models through Japanese Adversarial Natural Language Inference},  year = {2021} }
Assessing Social and Intersectional Biases in Contextualized Word Representations	2019	http://www.semanticscholar.org/paper/039b1c1210c437f3b3ce6e0275ee2137bf5b951c	Evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for this novel approach.	maybe	120	Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.	039b1c1210c437f3b3ce6e0275ee2137bf5b951c	@['JournalArticle']{tan-celis-2019-assessing,  author = {Y. Tan and L. E. Celis},  booktitle = {Neural Information Processing Systems},  pages = {13209-13220},  title = {Assessing Social and Intersectional Biases in Contextualized Word Representations},  year = {2019} }
Assessing Political Prudence of Open-domain Chatbots	2021	http://www.semanticscholar.org/paper/d1290807d6089713a6710285ac115904c39c311d	A group of metrics for assessing political prudence of chatbots are proposed and conducted and the testsets and codebase are released to promote research in this area.	maybe	5	Politically sensitive topics are still a challenge for open-domain chatbots. However, dealing with politically sensitive content in a responsible, non-partisan, and safe behavior way is integral for these chatbots. Currently, the main approach to handling political sensitivity is by simply changing such a topic when it is detected. This is safe but evasive and results in a chatbot that is less engaging. In this work, as a first step towards a politically safe chatbot, we propose a group of metrics for assessing their political prudence. We then conduct political prudence analysis of various chatbots and discuss their behavior from multiple angles through our automatic metric and human evaluation metrics. The testsets and codebase are released to promote research in this area.	d1290807d6089713a6710285ac115904c39c311d	@['JournalArticle']{bang-etal-2021-assessing,  author = {Yejin Bang and Nayeon Lee and Etsuko Ishii and Andrea Madotto and Pascale Fung},  booktitle = {SIGDIAL Conferences},  pages = {548-555},  title = {Assessing Political Prudence of Open-domain Chatbots},  year = {2021} }
Assessing Phrasal Representation and Composition in Transformers	2020	http://www.semanticscholar.org/paper/a129bed0b735918da1c797a27ebb43a541971c64	It is found that phrase representation in state-of-the-art pre-trained transformers relies heavily on word content, with little evidence of nuanced composition.	maybe	34	Deep transformer models have pushed performance on NLP tasks to new limits, suggesting sophisticated treatment of complex linguistic inputs, such as phrases. However, we have limited understanding of how these models handle representation of phrases, and whether this reflects sophisticated composition of phrase meaning like that done by humans. In this paper, we present systematic analysis of phrasal representations in state-of-the-art pre-trained transformers. We use tests leveraging human judgments of phrase similarity and meaning shift, and compare results before and after control of word overlap, to tease apart lexical effects versus composition effects. We find that phrase representation in these models relies heavily on word content, with little evidence of nuanced composition. We also identify variations in phrase representation quality across models, layers, and representation types, and make corresponding recommendations for usage of representations from these models.	a129bed0b735918da1c797a27ebb43a541971c64	@['JournalArticle', 'Conference']{yu-ettinger-2020-assessing,  author = {Lang-Chi Yu and Allyson Ettinger},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4896-4907},  title = {Assessing Phrasal Representation and Composition in Transformers},  year = {2020} }
Assessing Out-of-Domain Language Model Performance from Few Examples	2022	http://www.semanticscholar.org/paper/b1a9104afd48f019aeb98c6be7a15736089959d2		maybe	0	While pretrained language models have ex-hibited impressive generalization capabilities, they still behave unpredictably under certain domain shifts. In particular, a model may learn a reasoning process on in-domain training data that does not hold for out-of-domain test data. We address the task of predicting out-of-domain (OOD) performance in a few-shot fashion: given a few target-domain examples and a set of models with similar training performance, can we understand how these models will perform on OOD test data? We bench-mark the performance on this task when looking at model accuracy on the few-shot examples, then investigate how to incorporate analysis of the models’ behavior using feature attributions to better tackle this problem. Specif-ically, we explore a set of “factors” designed to reveal model agreement with certain pathological heuristics that may indicate worse generalization capabilities. On textual entailment, paraphrase recognition, and a synthetic classiﬁcation task, we show that attribution-based factors can help rank relative model OOD performance. However, accuracy on a few-shot test set is a surprisingly strong baseline, particularly when the system designer does not have in-depth prior knowledge about the domain shift.	b1a9104afd48f019aeb98c6be7a15736089959d2	@['JournalArticle']{singhal-etal-2022-assessing,  author = {Prasann Singhal and Jarad Forristal and Xi Ye and Greg Durrett},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Assessing Out-of-Domain Language Model Performance from Few Examples},  volume = {abs/2210.06725},  year = {2022} }
Assessing Grammatical Correctness in Language Learning	2021	http://www.semanticscholar.org/paper/f10b77590bad5005115ef02c6cbffc362056faad	This work explores the problem of detecting alternative-correct answers: when more than one inflected form of a lemma fits syntactically and semantically in a given context and investigates the ability of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data.	maybe	7	We present experiments on assessing the grammatical correctness of learners’ answers in a language-learning System (references to the System, and the links to the released data and code are withheld for anonymity). In particular, we explore the problem of detecting alternative-correct answers: when more than one inflected form of a lemma fits syntactically and semantically in a given context. We approach the problem with the methods for grammatical error detection (GED), since we hypothesize that models for detecting grammatical mistakes can assess the correctness of potential alternative answers in a learning setting. Due to the paucity of training data, we explore the ability of pre-trained BERT to detect grammatical errors and then fine-tune it using synthetic training data. In this work, we focus on errors in inflection. Our experiments show a. that pre-trained BERT performs worse at detecting grammatical irregularities for Russian than for English; b. that fine-tuned BERT yields promising results on assessing the correctness of grammatical exercises; and c. establish a new benchmark for Russian. To further investigate its performance, we compare fine-tuned BERT with one of the state-of-the-art models for GED (Bell et al., 2019) on our dataset and RULEC-GEC (Rozovskaya and Roth, 2019). We release the manually annotated learner dataset, used for testing, for general use.	f10b77590bad5005115ef02c6cbffc362056faad	@['JournalArticle']{katinskaia-yangarber-2021-assessing,  author = {Anisia Katinskaia and R. Yangarber},  booktitle = {Workshop on Innovative Use of NLP for Building Educational Applications},  pages = {135-146},  title = {Assessing Grammatical Correctness in Language Learning},  year = {2021} }
Assessing Discourse Relations in Language Generation from GPT-2	2020	http://www.semanticscholar.org/paper/11e4d903444d61e2655f9773d1ef1f8c7e43bb44	A comprehensive study on the validity of explicit discourse relations in GPT-2’s outputs under both organic generation and fine-tuned scenarios and proposes a decoupled strategy to mitigate these problems and highlight the importance of explicitly modeling discourse information.	maybe	8	Recent advances in NLP have been attributed to the emergence of large-scale pre-trained language models. GPT-2, in particular, is suited for generation tasks given its left-to-right language modeling objective, yet the linguistic quality of its generated text has largely remain unexplored. Our work takes a step in understanding GPT-2’s outputs in terms of discourse coherence. We perform a comprehensive study on the validity of explicit discourse relations in GPT-2’s outputs under both organic generation and fine-tuned scenarios. Results show GPT-2 does not always generate text containing valid discourse relations; nevertheless, its text is more aligned with human expectation in the fine-tuned scenario. We propose a decoupled strategy to mitigate these problems and highlight the importance of explicitly modeling discourse information.	11e4d903444d61e2655f9773d1ef1f8c7e43bb44	@['JournalArticle']{ko-li-2020-assessing,  author = {Wei-Jen Ko and Junyi Jessy Li},  booktitle = {International Conference on Natural Language Generation},  pages = {52-59},  title = {Assessing Discourse Relations in Language Generation from GPT-2},  year = {2020} }
Assessing Combinational Generalization of Language Models in Biased Scenarios	2022	http://www.semanticscholar.org/paper/6ada57ec94cd8712cd140f0741c6df0bc01e7509	The results show that PLMs are able to overcome such distribution shifts for specific tasks and with sufficient data, and find that overfitting can lead the models to depend more on biases for prediction, thus hurting the combinational generalization ability of PLMs.	maybe	0	In light of the prominence of Pre-trained Language Models (PLMs) across numerous downstream tasks, shedding light on what they learn is an important endeavor. Whereas previous work focuses on assessing in-domain knowledge, we evaluate the generalization ability in biased scenarios through component combinations where it could be easy for the PLMs to learn shortcuts from the training corpus. This would lead to poor performance on the testing corpus, which is combinationally reconstructed from the training components. The results show that PLMs are able to overcome such distribution shifts for specific tasks and with sufficient data. We further find that overfitting can lead the models to depend more on biases for prediction, thus hurting the combinational generalization ability of PLMs.	6ada57ec94cd8712cd140f0741c6df0bc01e7509	@['JournalArticle']{dong-2022-assessing,  author = {Xin Dong},  booktitle = {AACL},  pages = {392-397},  title = {Assessing Combinational Generalization of Language Models in Biased Scenarios},  year = {2022} }
Assessing BERT’s ability to learn Italian syntax: a study on null-subject and agreement phenomena	2021	http://www.semanticscholar.org/paper/ada3753829d04e341807d79539b836518e6198b7	This work investigates the ability of BERT neural language model pretrained in Italian to embed syntactic dependency relationships into its layers, by approximating a Dependency Parse Tree, and shows that a knowledge of the Italian syntax is embedded in central-upper layers of the BERT model.	maybe	14		ada3753829d04e341807d79539b836518e6198b7	@['JournalArticle']{guarasci-etal-2021-assessing,  author = {R. Guarasci and Stefano Silvestri and G. Pietro and Hamido Fujita and M. Esposito},  booktitle = {Journal of Ambient Intelligence and Humanized Computing},  journal = {Journal of Ambient Intelligence and Humanized Computing},  pages = {289 - 303},  title = {Assessing BERT’s ability to learn Italian syntax: a study on null-subject and agreement phenomena},  volume = {14},  year = {2021} }
Assessing BERT's Syntactic Abilities	2019	https://www.semanticscholar.org/paper/efeab0dcdb4c1cce5e537e57745d84774be99b9a	The extent to which the recently introduced BERT model captures English syntactic phenomena is assessed, using naturally-occurring subject-verb agreement stimuli; "coloreless green ideas" subject- Verb Agreement stimuli; and manually crafted stimuli for subject- verb agreement and reflexive anaphora phenomena.	seed	367	I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.	efeab0dcdb4c1cce5e537e57745d84774be99b9a	@['JournalArticle']{goldberg-2019-assessing,  author = {Yoav Goldberg},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Assessing BERT's Syntactic Abilities},  volume = {abs/1901.05287},  year = {2019} }
Asking without Telling: Exploring Latent Ontologies in Contextual Representations	2020	http://www.semanticscholar.org/paper/196c558ce126f5f7d66df4ea52e2442848fc65be	This work introduces latent subclass learning (LSL): a modification to existing classifier-based probing methods that induces a latent categorization (or ontology) of the probe's inputs that extracts emergent structure from input representations in an interpretable and quantifiable form.	maybe	25	The success of pretrained contextual encoders, such as ELMo and BERT, has brought a great deal of interest in what these models learn: do they, without explicit supervision, learn to encode meaningful notions of linguistic structure? If so, how is this structure encoded? To investigate this, we introduce latent subclass learning (LSL): a modification to existing classifier-based probing methods that induces a latent categorization (or ontology) of the probe's inputs. Without access to fine-grained gold labels, LSL extracts emergent structure from input representations in an interpretable and quantifiable form. In experiments, we find strong evidence of familiar categories, such as a notion of personhood in ELMo, as well as novel ontological distinctions, such as a preference for fine-grained semantic roles on core arguments. Our results provide unique new evidence of emergent structure in pretrained encoders, including departures from existing annotations which are inaccessible to earlier methods.	196c558ce126f5f7d66df4ea52e2442848fc65be	@['JournalArticle', 'Conference']{michael-etal-2020-asking,  author = {Julian Michael and Jan A. Botha and Ian Tenney},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Asking without Telling: Exploring Latent Ontologies in Contextual Representations},  volume = {abs/2004.14513},  year = {2020} }
Ask Me Anything: A simple strategy for prompting language models	2022	http://www.semanticscholar.org/paper/fb49e88c6bd676516898e911e42b4f8479e6f1bf	This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks.	maybe	10	Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly perfect prompt for a task. To mitigate the high degree of effort involved in prompting, we instead ask whether collecting multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING PROMPTING (AMA). We first develop an understanding of the effective prompt formats, finding question-answering (QA) prompts, which encourage open-ended generation (“Who went to the park?”) tend to outperform those that restrict the model outputs (“John went to the park. Output True or False”). Our approach recursively uses the LLM to transform task inputs to the effective QA format. We apply these prompts to collect several noisy votes for the input’s true label. We find that these prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions. We evaluate AMA across open-source model families (EleutherAI, BLOOM, OPT, and T0) and sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code for reproducing the results here: https://github.com/HazyResearch/ama_prompting.	fb49e88c6bd676516898e911e42b4f8479e6f1bf	@['JournalArticle']{arora-etal-2022-ask,  author = {Simran Arora and A. Narayan and Mayee F. Chen and Laurel J. Orr and Neel Guha and Kush S Bhatia and Ines Chami and Frederic Sala and Christopher R'e},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Ask Me Anything: A simple strategy for prompting language models},  volume = {abs/2210.02441},  year = {2022} }
Artificial Neural Networks Accurately Predict Language Processing in the Brain	2020	http://www.semanticscholar.org/paper/ed8f2927523a4892058808c2fa5fe8daba266b10	The hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain, is supported.	maybe	40	The ability to share ideas through language is our species' signature cognitive skill, but how this feat is achieved by the brain remains unknown. Inspired by the success of artificial neural networks (ANNs) in explaining neural responses in perceptual tasks (Kell et al., 2018; Khaligh-Razavi & Kriegeskorte, 2014; Schrimpf et al., 2018; Yamins et al., 2014; Zhuang et al., 2017), we here investigated whether state-of-the-art ANN language models (e.g. Devlin et al., 2018; Pennington et al., 2014; Radford et al., 2019) capture human brain activity elicited during language comprehension. We tested 43 language models spanning major current model classes on three neural datasets (including neuroimaging and intracranial recordings) and found that the most powerful generative transformer models (Radford et al., 2019) accurately predict neural responses, in some cases achieving near-perfect predictivity relative to the noise ceiling. In contrast, simpler word-based embedding models (e.g. Pennington et al., 2014) only poorly predict neural responses (<10% predictivity). Models' predictivities are consistent across neural datasets, and also correlate with their success on a next-word-prediction task (but not other language tasks) and ability to explain human comprehension difficulty in an independent behavioral dataset. Intriguingly, model architecture alone drives a large portion of brain predictivity, with each model's untrained score predictive of its trained score. These results support the hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain. In addition, the finding of strong correspondences between ANNs and human representations opens the door to using the growing suite of tools for neural network interpretation to test hypotheses about the human mind.	ed8f2927523a4892058808c2fa5fe8daba266b10	@None{schrimpf-etal-2020-artificial,  author = {Martin Schrimpf and I. Blank and Greta Tuckute and Carina Kauf and Eghbal A. Hosseini and N. Kanwisher and J. Tenenbaum and Evelina Fedorenko},  journal = {bioRxiv},  title = {Artificial Neural Networks Accurately Predict Language Processing in the Brain},  year = {2020} }
Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training	2022	http://www.semanticscholar.org/paper/d461aef562bd9c13dc0f972f147df8be1fcca63c	It is shown that models that are trained on developmentally plausible amounts of language data achieve near-maximal performance on human neural and behavioral benchmarks and that although some training is necessary for the models’ ability to predict human responses to language, a developmentally realistic amount of training may suffice.	maybe	0	Artificial neural networks have emerged as computationally plausible models of human language processing. A major criticism of these models is that the amount of training data they receive far exceeds that of humans during language learning. Here, we use two complementary approaches to ask how the models’ ability to capture human neural and behavioral responses to language is affected by the amount of training data. First, we evaluate GPT-2 models trained on 1 million, 10 million, 100 million, or 1 billion tokens against two fMRI benchmarks and one behavioral (reading times) benchmark. Because children are exposed to approximately 100 million words during the first 10 years of life, we consider the 100-million-token model developmentally plausible. Second, we test the performance of a GPT-2 model that is trained on a 9-billion dataset to reach state-of-the-art next-word prediction performance against the same human benchmarks at different stages during training. Across both approaches, we find that (i) the models trained on a developmentally plausible amount of data already achieve near-maximal performance in capturing neural and behavioral responses to language. Further, (ii) lower perplexity—a measure of next-word prediction performance—is associated with stronger alignment with the human benchmarks, suggesting that models that have received enough training to achieve sufficiently high next-word prediction performance also acquire human-like representations of the linguistic input. In tandem, these findings establish that although some training is necessary for the models’ ability to predict human responses to language, a developmentally realistic amount of training (∼100 million tokens) may suffice. Summary Are artificial neural network (ANN) language models useful as models of human language processing? Some of these models have been shown to capture human responses to language with relatively high accuracy. However, these models are trained on vastly more data than what children are exposed to during language acquisition, raising questions about their value for understanding the human language system. Here, we systematically manipulate the amount of training data that ANN models receive and show that models that are trained on developmentally plausible amounts of language data (approximately 100 million words, roughly corresponding to a child’s first 10 years of life) achieve near-maximal performance on human neural and behavioral benchmarks. These developmentally plausible models—rather than models that achieve state-of-the-art performance on the next-word prediction task—hold substantial promise in providing mechanistic-level insights into human language processing.	d461aef562bd9c13dc0f972f147df8be1fcca63c	@None{hosseini-etal-2022-artificial,  author = {Eghbal A. Hosseini and Martin Schrimpf and Yian Zhang and Samuel R. Bowman and Noga Zaslavsky and Evelina Fedorenko},  booktitle = {bioRxiv},  journal = {bioRxiv},  title = {Artificial neural network language models align neurally and behaviorally with humans even after a developmentally realistic amount of training},  year = {2022} }
Artificial cognition: How experimental psychology can help generate explainable artificial intelligence.	2020	http://www.semanticscholar.org/paper/3ec029b137fac6c35c0e21e85dfc1d3a2794147d	This paper provides a review of XAI for psychologists, arguing that current methods possess a blind spot that can be complemented by the experimental cognitive tradition, and provides a framework for research in XAI and examples of experimentation within XAI inspired by psychological science.	maybe	23		3ec029b137fac6c35c0e21e85dfc1d3a2794147d	@['Review', 'JournalArticle']{taylor-taylor-2020-artificial,  author = {J. E. Taylor and Graham W. Taylor},  booktitle = {Psychonomic Bulletin & Review},  journal = {Psychonomic Bulletin & Review},  pages = {454 - 475},  title = {Artificial cognition: How experimental psychology can help generate explainable artificial intelligence},  volume = {28},  year = {2020} }
Are Transformers universal approximators of sequence-to-sequence functions?	2019	http://www.semanticscholar.org/paper/509b4661ed74a24c2ffdbf131f9e1c6a1783752d	It is established that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models.	maybe	110	Despite the widespread adoption of Transformer models for NLP tasks, the expressive power of these models is not well-understood. In this paper, we establish that Transformer models are universal approximators of continuous permutation equivariant sequence-to-sequence functions with compact support, which is quite surprising given the amount of shared parameters in these models. Furthermore, using positional encodings, we circumvent the restriction of permutation equivariance, and show that Transformer models can universally approximate arbitrary continuous sequence-to-sequence functions on a compact domain. Interestingly, our proof techniques clearly highlight the different roles of the self-attention and the feed-forward layers in Transformers. In particular, we prove that fixed width self-attention layers can compute contextual mappings of the input sequences, playing a key role in the universal approximation property of Transformers. Based on this insight from our analysis, we consider other architectures that can compute contextual mappings and empirically evaluate them.	509b4661ed74a24c2ffdbf131f9e1c6a1783752d	@['JournalArticle']{yun-etal-2019-are,  author = {Chulhee Yun and Srinadh Bhojanapalli and A. Rawat and Sashank J. Reddi and Sanjiv Kumar},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Are Transformers universal approximators of sequence-to-sequence functions?},  volume = {abs/1912.10077},  year = {2019} }
Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement	2021	http://www.semanticscholar.org/paper/78d68e41d190f844f65e41ba79a8bae0714b8ab2	This work takes a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in the assessment of neural networks’ syntactic ability.	maybe	4	Many recent works have demonstrated that unsupervised sentence representations of neural networks encode syntactic information by observing that neural language models are able to predict the agreement between a verb and its subject. We take a critical look at this line of research by showing that it is possible to achieve high accuracy on this agreement task with simple surface heuristics, indicating a possible flaw in our assessment of neural networks’ syntactic ability. Our fine-grained analyses of results on the long-range French object-verb agreement show that contrary to LSTMs, Transformers are able to capture a non-trivial amount of grammatical structure.	78d68e41d190f844f65e41ba79a8bae0714b8ab2	@['JournalArticle', 'Conference']{li-etal-2021-are,  author = {Bingzhi Li and Guillaume Wisniewski and Benoit Crabb'e},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4599-4610},  title = {Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement},  year = {2021} }
Are Sixteen Heads Really Better than One?	2019	https://www.semanticscholar.org/paper/b03c7ff961822183bab66b2e594415e585d3fd09	It is made the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance.	seed	516	Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.	b03c7ff961822183bab66b2e594415e585d3fd09	@['JournalArticle']{michel-etal-2019-are,  author = {Paul Michel and Omer Levy and Graham Neubig},  booktitle = {Neural Information Processing Systems},  pages = {14014-14024},  title = {Are Sixteen Heads Really Better than One?},  year = {2019} }
Are Rotten Apples Edible? Challenging Commonsense Inference Ability with Exceptions	2021	http://www.semanticscholar.org/paper/fd074eaa6b2441347238df28462b5db39666c327	It is shown that language models in the BERT family experience a steep drop in performance on prompts that require them to pick answers which require reasoning about context, suggesting a need for future work in developing and analyzing frameworks similar to WINOVENTI that are tuned to model-specific weaknesses.	maybe	8	Previous studies have argued that pre-trained language models encode commonsense relational knowledge (e.g. that apples are edible). However, simultaneous work has revealed that such models are often insensitive to context, even ignoring overt contextual cues such as negations. In this paper, we investigate whether masked language models (the BERT family) can move beyond naive associative biases (e.g., apple → edible) when the context warrants (e.g. ranking inedible higher when presented with the information that the apple is rotten). We introduce the WINOVENTI procedure, which adversarially exploits generic associations in masked language models to create model-specific Winograd-style entailment schemas. Using our constructed WINOVENTI challenges set of over 2, 000 schemas, we show that language models in the BERT family experience a steep drop in performance on prompts that require them to pick answers which require reasoning about context (e.g., from 89.8% to 18.4% for BERTLARGE). We present evidence that language models exhibit different associative biases, suggesting a need for future work in developing and analyzing frameworks similar to WINOVENTI that are tuned to model-specific weaknesses.	fd074eaa6b2441347238df28462b5db39666c327	@['JournalArticle']{do-pavlick-2021-are,  author = {Nam Do and Ellie Pavlick},  booktitle = {Findings},  pages = {2061-2073},  title = {Are Rotten Apples Edible? Challenging Commonsense Inference Ability with Exceptions},  year = {2021} }
Are Representations Built from the Ground Up? An Empirical Examination of Local Composition in Language Models	2022	http://www.semanticscholar.org/paper/ee5895aa70bb4dbbdda256e23cada2d2ad1b15ae		maybe	0	Compositionality, the phenomenon where the meaning of a phrase can be derived from its constituent parts, is a hallmark of human language. At the same time, many phrases are non-compositional, carrying a meaning beyond that of each part in isolation. Representing both of these types of phrases is critical for language understanding, but it is an open question whether modern language models (LMs) learn to do so; in this work we examine this question. We first formulate a problem of predicting the LM-internal representations of longer phrases given those of their constituents. We find that the representation of a parent phrase can be predicted with some accuracy given an affine transformation of its children. While we would expect the predictive accuracy to correlate with human judgments of semantic compositionality, we find this is largely not the case, indicating that LMs may not accurately distinguish between compositional and non-compositional phrases. We perform a variety of analyses, shedding light on when different varieties of LMs do and do not generate compositional representations, and discuss implications for future modeling work.	ee5895aa70bb4dbbdda256e23cada2d2ad1b15ae	@['JournalArticle', 'Conference']{liu-neubig-2022-are,  author = {Emmy Liu and Graham Neubig},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Are representations built from the ground up? An empirical examination of local composition in language models},  volume = {abs/2210.03575},  year = {2022} }
Are Prompt-based Models Clueless?	2022	http://www.semanticscholar.org/paper/5cac32f85cb25299122c39b8b73eb01fdee710b6	Analyzing few-shot prompt- based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues, and while the models perform well on instances with superficial cue, they often underperform or only marginally outperform random accuracy on instances without superficial cues.	maybe	3	Finetuning large pre-trained language models with a task-specific head has advanced the state-of-the-art on many natural language understanding benchmarks. However, models with a task-specific head require a lot of training data, making them susceptible to learning and exploiting dataset-specific superficial cues that do not generalize to other datasets.Prompting has reduced the data requirement by reusing the language model head and formatting the task input to match the pre-training objective. Therefore, it is expected that few-shot prompt-based models do not exploit superficial cues.This paper presents an empirical examination of whether few-shot prompt-based models also exploit superficial cues.Analyzing few-shot prompt-based models on MNLI, SNLI, HANS, and COPA has revealed that prompt-based models also exploit superficial cues. While the models perform well on instances with superficial cues, they often underperform or only marginally outperform random accuracy on instances without superficial cues.	5cac32f85cb25299122c39b8b73eb01fdee710b6	@['JournalArticle', 'Conference']{kavumba-etal-2022-are,  author = {Pride Kavumba and Ryo Takahashi and Yusuke Oda},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {2333-2352},  title = {Are Prompt-based Models Clueless?},  year = {2022} }
Are Pretrained Language Models Symbolic Reasoners over Knowledge?	2020	http://www.semanticscholar.org/paper/6f2b90ee5a0feea87264148c25a874f84bae20a0	This is the first study that investigates the causal relation between facts present in training and facts learned by the PLM, and shows that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning.	maybe	36	How can pretrained language models (PLMs) learn factual knowledge from the training set? We investigate the two most important mechanisms: reasoning and memorization. Prior work has attempted to quantify the number of facts PLMs learn, but we present, using synthetic data, the first study that investigates the causal relation between facts present in training and facts learned by the PLM. For reasoning, we show that PLMs seem to learn to apply some symbolic reasoning rules correctly but struggle with others, including two-hop reasoning. Further analysis suggests that even the application of learned reasoning rules is flawed. For memorization, we identify schema conformity (facts systematically supported by other facts) and frequency as key factors for its success.	6f2b90ee5a0feea87264148c25a874f84bae20a0	@['JournalArticle']{kassner-etal-2020-are,  author = {Nora Kassner and Benno Krojer and Hinrich Schütze},  booktitle = {Conference on Computational Natural Language Learning},  pages = {552-564},  title = {Are Pretrained Language Models Symbolic Reasoners over Knowledge?},  year = {2020} }
Are Pre-trained Language Models Knowledgeable to Ground Open Domain Dialogues?	2020	http://www.semanticscholar.org/paper/9d57f98aca4c9acbd624eaad8fca2f1c5893e4f1	By fine-tuning with a few dialogues containing knowledge, the pre-trained language models can outperform the state-of-the-art model that requires external knowledge in automatic evaluation and human judgment, suggesting a positive answer to the question raised.	maybe	4	We study knowledge-grounded dialogue generation with pre-trained language models. Instead of pursuing new state-of-the-art on benchmarks, we try to understand if the knowledge stored in parameters of the pre-trained models is already enough to ground open domain dialogues, and thus allows us to get rid of the dependency on external knowledge sources in generation. Through extensive experiments on benchmarks, we find that by fine-tuning with a few dialogues containing knowledge, the pre-trained language models can outperform the state-of-the-art model that requires external knowledge in automatic evaluation and human judgment, suggesting a positive answer to the question we raised.	9d57f98aca4c9acbd624eaad8fca2f1c5893e4f1	@['JournalArticle']{zhao-etal-2020-are,  author = {Yufan Zhao and Wei Wu and Can Xu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Are Pre-trained Language Models Knowledgeable to Ground Open Domain Dialogues?},  volume = {abs/2011.09708},  year = {2020} }
Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction	2020	https://www.semanticscholar.org/paper/7cf8510d5905bd8a63f1e098e05ab591d689e0fd	The proposed method provides an effective way of extracting constituency trees from the pre-trained LMs without training, and reports intriguing findings in the induced trees, including the fact that pre- trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.	seed	59	With the recent success and popularity of pre-trained language models (LMs) in natural language processing, there has been a rise in efforts to understand their inner workings. In line with such interest, we propose a novel method that assists us in investigating the extent to which pre-trained LMs capture the syntactic notion of constituency. Our method provides an effective way of extracting constituency trees from the pre-trained LMs without training. In addition, we report intriguing findings in the induced trees, including the fact that pre-trained LMs outperform other approaches in correctly demarcating adverb phrases in sentences.	7cf8510d5905bd8a63f1e098e05ab591d689e0fd	@['JournalArticle']{kim-etal-2020-are,  author = {Taeuk Kim and Jihun Choi and Daniel Edmiston and Sang-goo Lee},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Are Pre-trained Language Models Aware of Phrases? Simple but Strong Baselines for Grammar Induction},  volume = {abs/2002.00737},  year = {2020} }
Are NLP Models really able to Solve Simple Math Word Problems?	2021	http://www.semanticscholar.org/paper/13c4e5a6122f3fa2663f63e49537091da6532f35	It is shown that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs, and models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy.	maybe	66	The problem of designing NLP solvers for math word problems (MWP) has seen sustained research activity and steady gains in the test accuracy. Since existing solvers achieve high performance on the benchmark datasets for elementary level MWPs containing one-unknown arithmetic word problems, such problems are often considered “solved” with the bulk of research attention moving to more complex MWPs. In this paper, we restrict our attention to English MWPs taught in grades four and lower. We provide strong evidence that the existing MWP solvers rely on shallow heuristics to achieve high performance on the benchmark datasets. To this end, we show that MWP solvers that do not have access to the question asked in the MWP can still solve a large fraction of MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve surprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP, created by applying carefully chosen variations over examples sampled from existing datasets. The best accuracy achieved by state-of-the-art models is substantially lower on SVAMP, thus showing that much remains to be done even for the simplest of the MWPs.	13c4e5a6122f3fa2663f63e49537091da6532f35	@['JournalArticle', 'Conference']{patel-etal-2021-are,  author = {Arkil Patel and S. Bhattamishra and Navin Goyal},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {2080-2094},  title = {Are NLP Models really able to Solve Simple Math Word Problems?},  year = {2021} }
Are Neural Networks Extracting Linguistic Properties or Memorizing Training Data? An Observation with a Multilingual Probe for Predicting Tense	2021	http://www.semanticscholar.org/paper/a9423f8152e72b964ceadda9f7674a9aaebfea41	It is shown that while French tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties.	maybe	2	We evaluate the ability of Bert embeddings to represent tense information, taking French and Chinese as a case study. In French, the tense information is expressed by verb morphology and can be captured by simple surface information. On the contrary, tense interpretation in Chinese is driven by abstract, lexical, syntactic and even pragmatic information. We show that while French tenses can easily be predicted from sentence representations, results drop sharply for Chinese, which suggests that Bert is more likely to memorize shallow patterns from the training data rather than uncover abstract properties.	a9423f8152e72b964ceadda9f7674a9aaebfea41	@['JournalArticle', 'Conference']{li-wisniewski-2021-are,  author = {Bingzhi Li and Guillaume Wisniewski},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {3080-3089},  title = {Are Neural Networks Extracting Linguistic Properties or Memorizing Training Data? An Observation with a Multilingual Probe for Predicting Tense},  year = {2021} }
Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition	2020	http://www.semanticscholar.org/paper/21c6b8fa5ed28e0c977e96983f0897830d742375	It is found that BERT learns to draw pragmatic inferences, and NLI training encourages models to learn some, but not all, pragmaticinferences.	maybe	52	Natural language inference (NLI) is an increasingly important task for natural language understanding, which requires one to infer whether a sentence entails another. However, the ability of NLI models to make pragmatic inferences remains understudied. We create an IMPlicature and PRESupposition diagnostic dataset (IMPPRES), consisting of 32K semi-automatically generated sentence pairs illustrating well-studied pragmatic inference types. We use IMPPRES to evaluate whether BERT, InferSent, and BOW NLI models trained on MultiNLI (Williams et al., 2018) learn to make pragmatic inferences. Although MultiNLI appears to contain very few pairs illustrating these inference types, we find that BERT learns to draw pragmatic inferences. It reliably treats scalar implicatures triggered by “some” as entailments. For some presupposition triggers like “only”, BERT reliably recognizes the presupposition as an entailment, even when the trigger is embedded under an entailment canceling operator like negation. BOW and InferSent show weaker evidence of pragmatic reasoning. We conclude that NLI training encourages models to learn some, but not all, pragmatic inferences.	21c6b8fa5ed28e0c977e96983f0897830d742375	@['JournalArticle', 'Conference']{jeretic-etal-2020-are,  author = {Paloma Jeretic and Alex Warstadt and Suvrat Bhooshan and Adina Williams},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {8690-8705},  title = {Are Natural Language Inference Models IMPPRESsive? Learning IMPlicature and PRESupposition},  year = {2020} }
Are Large Pre-Trained Language Models Leaking Your Personal Information?	2022	http://www.semanticscholar.org/paper/77d25889a59041629e8b2a81aad56544144c6878	It is shown that PLMs do leak personal information due to memorization, but the risk of personal information being extracted by attackers is low because the models are weak at associating the personal information with its owner.	maybe	3	Large Pre-Trained Language Models (PLMs) have facilitated and dominated many NLP tasks in recent years. However, despite the great success of PLMs, there are also privacy concerns brought with PLMs. For example, recent studies show that PLMs memorize a lot of training data, including sensitive information, while the information may be leaked unintentionally and be utilized by malicious attackers. In this paper, we propose to measure whether PLMs are prone to leaking personal information. Speciﬁcally, we attempt to query PLMs for email addresses with contexts of the email address or prompts containing the owner’s name. We ﬁnd that PLMs do leak personal information due to memorization. However, the risk of speciﬁc personal information being extracted by attackers is low because the models are weak at associating the personal information with its owner. We hope this work could help the community to better understand the privacy risk of PLMs and bring new insights to make PLMs safe.	77d25889a59041629e8b2a81aad56544144c6878	@['JournalArticle']{huang-etal-2022-are,  author = {Jie Huang and Hanyin Shao and K. Chang},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Are Large Pre-Trained Language Models Leaking Your Personal Information?},  volume = {abs/2205.12628},  year = {2022} }
Are Language Models Worse than Humans at Following Prompts? It's Complicated	2023	http://www.semanticscholar.org/paper/b7c4677cc6950d556f8d58084f87b2cb9cfe29b8		maybe	0	Prompts have been the center of progress in ad-vancing language models’ zero-shot and few-shot performance. However, recent work ﬁnds that models can perform surprisingly well when given intentionally irrelevant or misleading prompts. Such results may be interpreted as evidence that model behavior is not “hu-man like”. In this study, we challenge a cen-tral assumption in such work: that humans would perform badly when given pathological instructions. We ﬁnd that humans are able to reliably ignore irrelevant instructions and thus, like models, perform well on the underlying task despite an apparent lack of signal regarding the task they are being asked to do. However, when given deliberately misleading instructions, humans follow the instructions faithfully, whereas models do not. Thus, our conclusion is mixed with respect to prior work. We argue against the earlier claim that high performance with irrelevant prompts consti-tutes evidence against models’ instruction understanding, but we reinforce the claim that models’ failure to follow misleading instructions raises concerns. More broadly, we cau-tion that future research should not idealize human behaviors as a monolith and should not train or evaluate models to mimic assumptions about these behaviors without ﬁrst validating humans’ behaviors empirically.	b7c4677cc6950d556f8d58084f87b2cb9cfe29b8	@['JournalArticle']{webson-etal-2023-are,  author = {Albert Webson and Alyssa Marie Loo and Qinan Yu and Elizabeth-Jane Pavlick},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Are Language Models Worse than Humans at Following Prompts? It's Complicated},  volume = {abs/2301.07085},  year = {2023} }
Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations	2022	http://www.semanticscholar.org/paper/0040dac7a1bf7a1eeb01c86ddb993f331f35b158	The connection between explainability and sample hardness is studied by collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge and showing that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.	maybe	0	Recent work on explainable NLP has shown that few-shot prompting can enable large pre-trained language models (LLMs) to generate grammatical and factual natural language explanations for data labels. In this work, we study the connection between explainability and sample hardness by investigating the following research question – “Are LLMs and humans equally good at explaining data labels for both easy and hard samples?” We answer this question by first collecting human-written explanations in the form of generalizable commonsense rules on the task of Winograd Schema Challenge (Winogrande dataset). We compare these explanations with those generated by GPT-3 while varying the hardness of the test samples as well as the in-context samples. We observe that (1) GPT-3 explanations are as grammatical as human explanations regardless of the hardness of the test samples, (2) for easy examples, GPT-3 generates highly supportive explanations but human explanations are more generalizable, and (3) for hard examples, human explanations are significantly better than GPT-3 explanations both in terms of label-supportiveness and generalizability judgements. We also find that hardness of the in-context examples impacts the quality of GPT-3 explanations. Finally, we show that the supportiveness and generalizability aspects of human explanations are also impacted by sample hardness, although by a much smaller margin than models.	0040dac7a1bf7a1eeb01c86ddb993f331f35b158	@['JournalArticle', 'Conference']{saha-etal-2022-are,  author = {Swarnadeep Saha and Peter Hase and Nazneen Rajani and Mohit Bansal},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Are Hard Examples also Harder to Explain? A Study with Human and Model-Generated Explanations},  volume = {abs/2211.07517},  year = {2022} }
Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens	2022	http://www.semanticscholar.org/paper/b135144dd8853b7c515f15da1ce943a5be789c1e	This work formalizes this distinction using a causal model and probabilities of necessity and sufﬁciency, which delineates the causal re-lations between a feature and a label, and shows that this distinction helps explain results of existing debiasing methods on different spurious features, and demystifies surprising results such as the encoding of spurious features in model representations afterdebiasing.	maybe	1	The term ‘spurious correlations’ has been used in NLP to informally denote any undesirable feature-label correlations. However, a correlation can be undesirable because (i) the feature is irrelevant to the label (e.g. punctuation in a review), or (ii) the feature’s effect on the label depends on the context (e.g. negation words in a review), which is ubiquitous in language tasks. In case (i), we want the model to be invariant to the feature, which is neither necessary nor sufficient for prediction. But in case (ii), even an ideal model (e.g. humans) must rely on the feature, since it is necessary (but not sufficient) for prediction. Therefore, a more fine-grained treatment of spurious features is needed to specify the desired model behavior. We formalize this distinction using a causal model and probabilities of necessity and sufficiency, which delineates the causal relations between a feature and a label. We then show that this distinction helps explain results of existing debiasing methods on different spurious features, and demystifies surprising results such as the encoding of spurious features in model representations after debiasing.	b135144dd8853b7c515f15da1ce943a5be789c1e	@['JournalArticle', 'Conference', 'Review']{joshi-etal-2022-are,  author = {Nitish Joshi and X. Pan and Hengxing He},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Are All Spurious Features in Natural Language Alike? An Analysis through a Causal Lens},  volume = {abs/2210.14011},  year = {2022} }
Are All Neurons Created Equal? Interpreting and Controlling BERT through Individual Neurons	2021	http://www.semanticscholar.org/paper/73886d45bd5a4508768218fc451de2173f543938	It is shown that encoded information and used information are not always the same, and that individual neurons can be used to control the model’s output, to some extent, and two recent ranking methods are compared and a novel one is introduced.	yes	0	While many studies have shown that linguistic information is encoded in hidden word representations, few have studied individual neurons, to show how and in which neurons it is encoded. Among these, the common approach is to use an external probe to rank neurons according to their relevance to some linguistic attribute, and to evaluate the obtained ranking using the same probe that produced it. We show that this methodology confounds distinct factors—probe quality and ranking quality—and thus we separate them. We compare two recent ranking methods and a novel one we introduce, both by probing and by causal interventions, where we modify the representations and observe the effect on the model’s output. We show that encoded information and used information are not always the same, and that individual neurons can be used to control the model’s output, to some extent. Our method can be used to identify how certain information is encoded, and how to manipulate it for debugging purposes. 1	73886d45bd5a4508768218fc451de2173f543938	@None{antverg-2021-are,  author = {Omer Antverg},  title = {Are All Neurons Created Equal? Interpreting and Controlling BERT through Individual Neurons},  year = {2021} }
Approximating How Single Head Attention Learns	2021	http://www.semanticscholar.org/paper/4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506	This work approximate model training as a two stage process: early on in training when the attention weights are uniform, the model learns to translate individual input word i to o if they cooccur frequently, and later, while the correct output is o because it knows i translates to o.	yes	4	Why do models often attend to salient words, and how does this evolve throughout training? We approximate model training as a two stage process: early on in training when the attention weights are uniform, the model learns to translate individual input word i to o if they cooccur frequently. Later, the model learns to attend to i while the correct output is o because it knows i translates to o. To formalize, we define a model property, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that i translates to o), and claim that it drives the learning of the attention. This claim is supported by the fact that before the attention mechanism is learned, KTIW can be learned from word co-occurrence statistics, but not the other way around. Particularly, we can construct a training distribution that makes KTIW hard to learn, the learning of the attention fails, and the model cannot even learn the simple task of copying the input words to the output. Our approximation explains why models sometimes attend to salient words, and inspires a toy example where a multi-head attention model can overcome the above hard training distribution by improving learning dynamics rather than expressiveness. We end by discussing the limitation of our approximation framework and suggest future directions.	4a5c7c2afeeadbc4f7c0a9101fe6ed5d8f624506	@['JournalArticle']{snell-etal-2021-approximating,  author = {Charles Burton Snell and Ruiqi Zhong and D. Klein and J. Steinhardt},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Approximating How Single Head Attention Learns},  volume = {abs/2103.07601},  year = {2021} }
AND does not mean OR: Using Formal Languages to Study Language Models’ Representations	2021	http://www.semanticscholar.org/paper/7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8	None of the simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.	maybe	7	A current open question in natural language processing is to what extent language models, which are trained with access only to the form of language, are able to capture the meaning of language. This question is challenging to answer in general, as there is no clear line between meaning and form, but rather meaning constrains form in consistent ways. The goal of this study is to offer insights into a narrower but critical subquestion: Under what conditions should we expect that meaning and form covary sufficiently, such that a language model with access only to form might nonetheless succeed in emulating meaning? Focusing on several formal languages (propositional logic and a set of programming languages), we generate training corpora using a variety of motivated constraints, and measure a distributional language model’s ability to differentiate logical symbols (AND, OR, and NOT). Our findings are largely negative: none of our simulated training corpora result in models which definitively differentiate meaningfully different symbols (e.g., AND vs. OR), suggesting a limitation to the types of semantic signals that current models are able to exploit.	7ddb18fc67f13ff8ea5467bc04eb41666f8e7cb8	@['JournalArticle', 'Conference']{traylor-etal-2021-and,  author = {Aaron Traylor and Roman Feiman and Elizabeth-Jane Pavlick},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {158-167},  title = {AND does not mean OR: Using Formal Languages to Study Language Models’ Representations},  year = {2021} }
Analyzing Transformers in Embedding Space	2022	http://www.semanticscholar.org/paper/c7fa5c2172a4624d6baa91e66344e4520d3028ad	This work presents a conceptual framework where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on, and shows that at least in part, it can abstract away model speciﬁcs and understand Transformers in theembedding space.	yes	2	Understanding Transformer-based models has attracted signiﬁcant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that an input-independent approach, where parameters are interpreted directly without a forward/back-ward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a conceptual framework where all parameters of a trained Transformer are interpreted by projecting them into the embedding space , that is, the space of vocabulary items they operate on. Focusing mostly on GPT-2 for this paper, we provide di-verse evidence to support our argument. First, an empirical analysis showing that parameters of both pretrained and ﬁne-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) construct-ing a classiﬁer without training by “translat-ing” the parameters of a ﬁne-tuned classiﬁer to parameters of a different model that was only pretrained. Overall, our ﬁndings show that at least in part, we can abstract away model speciﬁcs and understand Transformers in the embedding space.	c7fa5c2172a4624d6baa91e66344e4520d3028ad	@['JournalArticle']{dar-etal-2022-analyzing,  author = {Guy Dar and Mor Geva and Ankit Gupta and Jonathan Berant},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Analyzing Transformers in Embedding Space},  volume = {abs/2209.02535},  year = {2022} }
Analyzing the Variation Property of Contextualized Word Representations	2019	http://www.semanticscholar.org/paper/fff9d7e9cc71d6ad7217dad9e5f4994c1c155e9f	The variation property of word representations produced by the ELMo encoder is explored and it is found that lexical category, word position, the diversity of the contexts, and the Diversity of the word senses are the features most closely associated with variation.	maybe	0		fff9d7e9cc71d6ad7217dad9e5f4994c1c155e9f	@['JournalArticle', 'Conference']{mizuki-okazaki-2019-analyzing,  author = {Sakae Mizuki and Naoaki Okazaki},  booktitle = {Australasian Conference on Artificial Intelligence},  pages = {393-405},  title = {Analyzing the Variation Property of Contextualized Word Representations},  year = {2019} }
Analyzing the Structure of Attention in a Transformer Language Model	2019	https://www.semanticscholar.org/paper/a039ea239e37f53a2cb60c68e0a1967994353166	It is found that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers, and the deepest layers of the model capture the most distant relationships.	seed	181	The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.	a039ea239e37f53a2cb60c68e0a1967994353166	@['JournalArticle']{vig-belinkov-2019-analyzing,  author = {Jesse Vig and Yonatan Belinkov},  booktitle = {BlackboxNLP@ACL},  pages = {63-76},  title = {Analyzing the Structure of Attention in a Transformer Language Model},  year = {2019} }
Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models	2022	http://www.semanticscholar.org/paper/0b4f0f6b04476c4f2cdfeabf99d4b835227f0bb2	This work investigates when multilingual pretraining models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks.	maybe	5	The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.	0b4f0f6b04476c4f2cdfeabf99d4b835227f0bb2	@['JournalArticle', 'Conference']{blevins-etal-2022-analyzing,  author = {Terra Blevins and Hila Gonen and Luke Zettlemoyer},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models},  volume = {abs/2205.11758},  year = {2022} }
Analyzing the Limits of Self-Supervision in Handling Bias in Language	2021	http://www.semanticscholar.org/paper/fb2e38502ac66ae09b58e87d7ef25c60e7d49969	This paper defines and comprehensively evaluates how well language models capture the semantics of four tasks for bias: diagnosis, identification, extraction and rephrasing, and indicates that language models are capable of performing these tasks to widely varying degrees across different bias dimensions, such as gender and political affiliation.	maybe	0	Warning: This paper contains examples that may be offensive or upsetting. Prompting inputs with natural language task descriptions has emerged as a popular mechanism to elicit reasonably accurate outputs from large-scale generative language models with little to no in-context supervision. This also helps gain insight into how well language models capture the semantics of a wide range of downstream tasks purely from self-supervised pre-training on massive corpora of unlabeled text. Such models have naturally also been exposed to a lot of undesirable content like racist and sexist language and there is limited work on awareness of models along these dimensions. In this paper, we define and comprehensively evaluate how well such language models capture the semantics of four tasks for bias: diagnosis , identification , extraction and rephrasing . We define three broad classes of task descriptions for these tasks: statement , question , and completion , with numerous lexical variants within each class. We study the efficacy of prompting for each task using these classes and the null task description across several decoding methods and few-shot examples. Our analyses indicate that language models are capable of performing these tasks to widely varying degrees across different bias dimensions, such as gender and political affiliation. We believe our work is an important step towards unbiased language models by quantifying the limits of current self-supervision objectives at accomplishing such sociologically challenging tasks.	fb2e38502ac66ae09b58e87d7ef25c60e7d49969	@['JournalArticle']{bauer-etal-2021-analyzing,  author = {Lisa Bauer and Karthik Gopalakrishnan and Spandana Gella and Yang Liu and Mohit Bansal and Dilek Z. Hakkani-Tür},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Analyzing the Limits of Self-Supervision in Handling Bias in Language},  volume = {abs/2112.08637},  year = {2021} }
Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models	2021	http://www.semanticscholar.org/paper/291a00d8433fecd2dd10f7f13b62dae8ce500043	This work focuses on the transformer encoder-decoder model for the open-domain dialogue response generation task, and finds that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining.	maybe	13	In this work, we study how the finetuning stage in the pretrain-finetune framework changes the behavior of a pretrained neural language generator. We focus on the transformer encoder-decoder model for the open-domain dialogue response generation task. Our major finding is that after standard finetuning, the model forgets some of the important language generation skills acquired during large-scale pretraining. We demonstrate the forgetting phenomenon through a set of detailed behavior analysis from the perspectives of knowledge transfer, context sensitivity, and function space projection. As a preliminary attempt to alleviate the forgetting problem, we propose an intuitive finetuning strategy named “mix-review”. We find that mix-review effectively regularizes the finetuning process, and the forgetting problem is alleviated to some extent. Finally, we discuss interesting behavior of the resulting dialogue model and its implications.	291a00d8433fecd2dd10f7f13b62dae8ce500043	@['JournalArticle', 'Conference', 'Review']{ott-2021-analyzing,  author = {Myle Ott},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  pages = {1121-1133},  title = {Analyzing the Forgetting Problem in Pretrain-Finetuning of Open-domain Dialogue Response Models},  year = {2021} }
Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer	2021	http://www.semanticscholar.org/paper/33502e2cb9a059920f13daf73210c44269e9016a	This work systematically analyzes the robustness of representations layer by layer by analyzing the encoded syntactic and semantic information using diagnostic probes and finds that similar layers have similar amounts of linguistic information for data from an unseen domain.	maybe	3	The robustness of pretrained language models(PLMs) is generally measured using performance drops on two or more domains. However, we do not yet understand the inherent robustness achieved by contributions from different layers of a PLM. We systematically analyze the robustness of these representations layer by layer from two perspectives. First, we measure the robustness of representations by using domain divergence between two domains. We find that i) Domain variance increases from the lower to the upper layers for vanilla PLMs; ii) Models continuously pretrained on domain-specific data (DAPT)(Gururangan et al., 2020) exhibit more variance than their pretrained PLM counterparts; and that iii) Distilled models (e.g., DistilBERT) also show greater domain variance. Second, we investigate the robustness of representations by analyzing the encoded syntactic and semantic information using diagnostic probes. We find that similar layers have similar amounts of linguistic information for data from an unseen domain.	33502e2cb9a059920f13daf73210c44269e9016a	@None{kashyap-etal-2021-analyzing,  author = {Abhinav Ramesh Kashyap and Laiba Mehnaz and Bhavitvya Malik and Abdul Waheed and Devamanyu Hazarika and Min-Yen Kan and R. Shah},  booktitle = {ADAPTNLP},  title = {Analyzing the Domain Robustness of Pretrained Language Models, Layer by Layer},  year = {2021} }
Analyzing Text Representations under Tight Annotation Budgets: Measuring Structural Alignment	2022	http://www.semanticscholar.org/paper/017fc71c1abad32bc3337a53b40a84e36afc7a99	A metric is proposed that measures the extent to which a given representation is structurally aligned with a task and shows that an efﬁcient representation for a task is a representation that induces a good alignment between latent input structure and class structure.	maybe	0	Annotating large collections of textual data can be time consuming and expensive. That is why the ability to train models with limited annotation budgets is of great importance. In this context, it has been shown that under tight annotation budgets the choice of data representation is key. The goal of this paper is to better understand why this is so. With this goal in mind, we propose a metric that measures the extent to which a given representation is structurally aligned with a task. We conduct experiments on several text classiﬁcation datasets testing a variety of models and representations. Using our proposed metric we show that an efﬁcient representation for a task (i.e. one that enables learning from few samples) is a representation that induces a good alignment between latent input structure and class structure.	017fc71c1abad32bc3337a53b40a84e36afc7a99	@['JournalArticle']{guti'errez-etal-2022-analyzing,  author = {C'esar Gonz'alez-Guti'errez and Audi Primadhanty and Francesco Cazzaro and A. Quattoni},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Analyzing Text Representations under Tight Annotation Budgets: Measuring Structural Alignment},  volume = {abs/2210.05721},  year = {2022} }
Analyzing Sentence Fusion in Abstractive Summarization	2019	http://www.semanticscholar.org/paper/96bd3982f8c6eff4b4b48cd2aedf0adbe304bb72	This paper analyzes the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion, and reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.	maybe	33	While recent work in abstractive summarization has resulted in higher scores in automatic metrics, there is little understanding on how these systems combine information taken from multiple document sentences. In this paper, we analyze the outputs of five state-of-the-art abstractive summarizers, focusing on summary sentences that are formed by sentence fusion. We ask assessors to judge the grammaticality, faithfulness, and method of fusion for summary sentences. Our analysis reveals that system sentences are mostly grammatical, but often fail to remain faithful to the original article.	96bd3982f8c6eff4b4b48cd2aedf0adbe304bb72	@['JournalArticle', 'Conference']{lebanoff-etal-2019-analyzing,  author = {Logan Lebanoff and John Muchovej and Franck Dernoncourt and Doo Soon Kim and Seokhwan Kim and W. Chang and Fei Liu},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Analyzing Sentence Fusion in Abstractive Summarization},  volume = {abs/1910.00203},  year = {2019} }
Analyzing Semantic Faithfulness of Language Models via Input Intervention on Conversational Question Answering	2022	http://www.semanticscholar.org/paper/b742233b83405d303f61b3378f70475e4efdf75b	This paper considers three transformer models, BERT, RoBERTa, and XLNet, in both small and large version, and investigates how faithful their representations are with respect to the semantic content of texts, and proposes an intervention-based training regime that can mitigate the undesirable effects for deletion intervention.	maybe	0	Transformer-based language models have been shown to be highly effective for several NLP tasks. In this paper, we consider three transformer models, BERT, RoBERTa, and XLNet, in both small and large version, and investigate how faithful their representations are with respect to the semantic content of texts. We formalize a notion of semantic faithfulness, in which the semantic content of a text should causally ﬁgure in a model’s inferences in question answering. We then test this notion by observing a model’s behavior on answering questions about a story after performing two novel semantic interventions—deletion intervention and negation intervention. While transformer models achieve high performance on standard question answering tasks, we show that they fail to be semantically faithful once we perform these interventions for a signiﬁcant number of cases ( ∼ 50% for deletion intervention, and ∼ 20% drop in accuracy for negation intervention). We then propose an intervention-based training regime that can mitigate the undesirable effects for deletion intervention by a signiﬁcant margin (from ∼ 50% to ∼ 6% ). We analyze the inner-workings of the models to better understand the effectiveness of intervention-based training for deletion intervention. But we show that this training does not attenuate other aspects of semantic unfaithfulness such as the models’ inability to deal with negation intervention or to capture the predicate-argument structure of texts. We also test InstructGPT, via prompting, for its ability to handle the two interventions and to capture predicate-argument structure. While InstructGPT models do achieve very high performance on predicate-argument structure task, they fail to respond adequately to our deletion and negation interventions.	b742233b83405d303f61b3378f70475e4efdf75b	@['JournalArticle']{chaturvedi-etal-2022-analyzing,  author = {Akshay Chaturvedi and Swarnadeep Bhar and Soumadeep Saha and U. Garain and Nicholas M. Asher},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Analyzing Semantic Faithfulness of Language Models via Input Intervention on Conversational Question Answering},  volume = {abs/2212.10696},  year = {2022} }
Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned	2019	https://www.semanticscholar.org/paper/07a64686ce8e43ac475a8d820a8a9f1d87989583	It is found that the most important and confident heads play consistent and often linguistically-interpretable roles and when pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, it is observed that specialized heads are last to be pruned.	seed	554	Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads to the overall performance of the model and analyze the roles played by them in the encoder. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.	07a64686ce8e43ac475a8d820a8a9f1d87989583	@['JournalArticle', 'Conference']{voita-etal-2019-analyzing,  author = {Elena Voita and David Talbot and F. Moiseev and Rico Sennrich and Ivan Titov},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},  volume = {abs/1905.09418},  year = {2019} }
Analyzing machine-learned representations: A natural language case study	2020	http://www.semanticscholar.org/paper/d4fc020db15584ea040162a1afb6abc81aa6c7e4	representations of sentences in one such artificial system for natural language processing are studied to reveal parallels to the analogous representations in people, which suggest new ways to understand psychological phenomena in humans and informs best strategies for building artificial intelligence with human-like language understanding.	maybe	5	As modern deep networks become more complex, and get closer to human-like capabilities in certain domains, the question arises as to how the representations and decision rules they learn compare to the ones in humans. In this work, we study representations of sentences in one such artificial system for natural language processing. We first present a diagnostic test dataset to examine the degree of abstract composable structure represented. Analyzing performance on these diagnostic tests indicates a lack of systematicity in representations and decision rules, and reveals a set of heuristic strategies. We then investigate the effect of training distribution on learning these heuristic strategies, and we study changes in these representations with various augmentations to the training set. Our results reveal parallels to the analogous representations in people. We find that these systems can learn abstract rules and generalize them to new contexts under certain circumstances-similar to human zero-shot reasoning. However, we also note some shortcomings in this generalization behavior-similar to human judgment errors like belief bias. Studying these parallels suggests new ways to understand psychological phenomena in humans as well as informs best strategies for building artificial intelligence with human-like language understanding.	d4fc020db15584ea040162a1afb6abc81aa6c7e4	@['JournalArticle']{dasgupta-etal-2019-analyzing,  author = {I. Dasgupta and Demi Guo and S. Gershman and Noah D. Goodman},  booktitle = {Cognitive Sciences},  journal = {Cognitive science},  pages = {           e12925         },  title = {Analyzing machine-learned representations: A natural language case study},  volume = {44 12},  year = {2019} }
Analyzing Leakage of Personally Identifiable Information in Language Models	2023	https://www.semanticscholar.org/paper/a9e4d604d7a2bb4e0a3ddd5222d8f8bc7edda532	A taxonomy of PII leakage in LMs is proposed, novel attacks that can extract up to 10 × more PII sequences as existing attacks are proposed, and a subtle connection between record-level membership inference and PII reconstruction is shown.	maybe	0	—Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identiﬁable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufﬁcient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence- or user-level privacy, prevent PII disclosure. In this work, we propose (i) a taxonomy of PII leakage in LMs, (ii) metrics to quantify PII leakage, and (iii) attacks showing that PII leakage is a threat in practice. Our taxonomy provides rigorous game-based deﬁnitions for PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate attacks against GPT-2 models ﬁne-tuned on three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10 × more PII sequences as existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction.	a9e4d604d7a2bb4e0a3ddd5222d8f8bc7edda532	@None{lukas-etal-2023-analyzing,  author = {Nils Lukas and A. Salem and Robert Sim and Shruti Tople and L. Wutschitz and Santiago Zanella-B'eguelin},  title = {Analyzing Leakage of Personally Identifiable Information in Language Models},  year = {2023} }
Analyzing Individual Neurons in Pre-trained Language Models	2020	http://www.semanticscholar.org/paper/9f0272bb258506fdc0ee7d8951593914d4f9c39d	Small subsets of neurons are found to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax.	yes	55	While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.	9f0272bb258506fdc0ee7d8951593914d4f9c39d	@['JournalArticle', 'Conference']{durrani-etal-2020-analyzing,  author = {Nadir Durrani and Hassan Sajjad and Fahim Dalvi and Yonatan Belinkov},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Analyzing Individual Neurons in Pre-trained Language Models},  volume = {abs/2010.02695},  year = {2020} }
Analyzing How BERT Performs Entity Matching	2022	https://www.semanticscholar.org/paper/2111bb95cca973759ff32eb5f6b481b531055c95	The main findings are the fine-tuning process applied to the EM task mainly modifies the last layers of the BERT components, but in a different way on tokens belonging to descriptions of matching / non-matching entities, and the special structure of the EM datasets, where records are pairs of entity descriptions is recognized by BERT.	maybe	1	State-of-the-art Entity Matching (EM) approaches rely on transformer architectures, such as BERT , for generating highly contex-tualized embeddings of terms. The embeddings are then used to predict whether pairs of entity descriptions refer to the same real-world entity. BERT-based EM models demonstrated to be effective, but act as black-boxes for the users, who have limited insight into the motivations behind their decisions. In this paper, we perform a multi-facet analysis of the components of pre-trained and fine-tuned BERT architectures applied to an EM task. The main findings resulting from our extensive experimental evaluation are (1) the fine-tuning process applied to the EM task mainly modifies the last layers of the BERT components, but in a different way on tokens belonging to descriptions of matching / non-matching entities; (2) the special structure of the EM datasets, where records are pairs of entity descriptions is recognized by BERT; (3) the pair-wise semantic similarity of tokens is not a key knowledge exploited by BERT-based EM models.	2111bb95cca973759ff32eb5f6b481b531055c95	@['JournalArticle']{paganelli-etal-2022-analyzing,  author = {Matteo Paganelli and Francesco Del Buono and Andrea Baraldi and F. Guerra},  booktitle = {Proceedings of the VLDB Endowment},  journal = {Proc. VLDB Endow.},  pages = {1726-1738},  title = {Analyzing How BERT Performs Entity Matching},  volume = {15},  year = {2022} }
Analyzing Encoded Concepts in Transformer Language Models	2022	http://www.semanticscholar.org/paper/a97da58551262ce74a06367f996f806faf04392c	A novel framework ConceptX is proposed, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models, which uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts.	maybe	6	We propose a novel framework ConceptX, to analyze how latent concepts are encoded in representations learned within pre-trained lan-guage models. It uses clustering to discover the encoded concepts and explains them by aligning with a large set of human-defined concepts. Our analysis on seven transformer language models reveal interesting insights: i) the latent space within the learned representations overlap with different linguistic concepts to a varying degree, ii) the lower layers in the model are dominated by lexical concepts (e.g., affixation) and linguistic ontologies (e.g. Word-Net), whereas the core-linguistic concepts (e.g., morphology, syntactic relations) are better represented in the middle and higher layers, iii) some encoded concepts are multi-faceted and cannot be adequately explained using the existing human-defined concepts.	a97da58551262ce74a06367f996f806faf04392c	@['JournalArticle', 'Conference']{sajjad-etal-2022-analyzing,  author = {Hassan Sajjad and Nadir Durrani and Fahim Dalvi and Firoj Alam and Abdul Rafae Khan and Jia Xu},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {3082-3101},  title = {Analyzing Encoded Concepts in Transformer Language Models},  year = {2022} }
Analyzing Commonsense Emergence in Few-shot Knowledge Models	2021	http://www.semanticscholar.org/paper/8bdba45e46471ce23ac2dde24c849623997daaa7	The results show that commonsense knowledge models can rapidly adapt from limited examples, indicating that KG fine-tuning serves to learn an interface to encoded knowledge learned during pretraining.	yes	12	Recently, commonsense knowledge models — pretrained language models (LM) finetuned on knowledge graph (KG) tuples — showed that considerable amounts of commonsense knowledge can be encoded in the parameters of large language models [Bosselut et al., 2019]. However, as parallel studies show that LMs are poor hypothesizers of declarative commonsense relationships [Petroni et al., 2019] on their own, it remains unclear whether this knowledge is learned during pretraining or from fine-tuning on KG examples. To investigate this question, we train commonsense knowledge models in few-shot settings to study the emergence of their commonsense representation abilities. Our results show that commonsense knowledge models can rapidly adapt from limited examples, indicating that KG fine-tuning serves to learn an interface to encoded knowledge learned during pretraining. Importantly, our analysis of absolute, angular, and distributional parameter changes during few-shot fine-tuning provides novel insights into how this interface is learned.	8bdba45e46471ce23ac2dde24c849623997daaa7	@['JournalArticle']{da-etal-2021-analyzing,  author = {Jeff Da and Ronan Le Bras and Ximing Lu and Yejin Choi and Antoine Bosselut},  booktitle = {Conference on Automated Knowledge Base Construction},  title = {Analyzing Commonsense Emergence in Few-shot Knowledge Models},  year = {2021} }
Analyzing Biases to Spurious Correlations in Text Classification Tasks	2022	http://www.semanticscholar.org/paper/909f63e1ddbc72c97f310a8a6cc2d12e5bcb84c1	It is shown that even when only ‘stop’ words are available at the input stage, it is possible to predict the class significantly better than random, and real spurious correlations can be exploited by current state-of-the-art deep-learning systems.	maybe	0	Machine learning systems have shown impressive performance across a range of natural language tasks. However, it has been hypothesized that these systems are prone to learning spurious correlations that may be present in the training data. Though these correlations will not impact in-domain performance, they are unlikely to generalize well to out-of-domain data, limiting the applicability of systems. This work examines this phenomenon on text classification tasks. Rather than artificially injecting features into the data, we demonstrate that real spurious correlations can be exploited by current state-of-the-art deep-learning systems. Specifically, we show that even when only ‘stop’ words are available at the input stage, it is possible to predict the class significantly better than random. Though it is shown that these stop words are not required for good in-domain performance, they can degrade the ability of the system to generalize well to out-of-domain data.	909f63e1ddbc72c97f310a8a6cc2d12e5bcb84c1	@['JournalArticle']{liusie-etal-2022-analyzing,  author = {Adian Liusie and Vatsal Raina and Vyas Raina and M. Gales},  booktitle = {AACL},  pages = {78-84},  title = {Analyzing Biases to Spurious Correlations in Text Classification Tasks},  year = {2022} }
Analyzing BERT’s Knowledge of Hypernymy via Prompting	2021	https://www.semanticscholar.org/paper/4ab9f5c94ad003e774c0994011214a5721461cbe	This paper examines BERT’s knowledge of lexical semantic relations, focusing on hypernymy, the “is-a” relation that relates a word to a superordinate category, and finds that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernym with up to 57% accuracy.	maybe	7	The high performance of large pretrained language models (LLMs) such as BERT on NLP tasks has prompted questions about BERT’s linguistic capabilities, and how they differ from humans’. In this paper, we approach this question by examining BERT’s knowledge of lexical semantic relations. We focus on hypernymy, the “is-a” relation that relates a word to a superordinate category. We use a prompting methodology to simply ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57% accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT’s predictions and performance on a dataset containing uncommon hyponyms and hypernyms indicate that its knowledge of hypernymy is still limited.	4ab9f5c94ad003e774c0994011214a5721461cbe	@['JournalArticle']{hanna-mareček-2021-analyzing,  author = {Michael Hanna and D. Mareček},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {275-282},  title = {Analyzing BERT’s Knowledge of Hypernymy via Prompting},  year = {2021} }
Analyzing and Controlling Inter-Head Diversity in Multi-Head Attention	2021	http://www.semanticscholar.org/paper/2080b0051961bbe6a2508054b3699d43209182ea	This paper quantitatively analyze inter-head diversity of multi-head attention by applying recently developed similarity measures between two deep representations: Singular Vector Canonical Correlation Analysis (SVCCA) and Centered Kernel Alignment (CKA) and empirically show that multi- head attention does diversify representation subspaces of each head as the number of heads increases.	maybe	4	Multi-head attention, a powerful strategy for Transformer, is assumed to utilize information from diverse representation subspaces. However, measuring diversity between heads’ representations or exploiting the diversity has been rarely studied. In this paper, we quantitatively analyze inter-head diversity of multi-head attention by applying recently developed similarity measures between two deep representations: Singular Vector Canonical Correlation Analysis (SVCCA) and Centered Kernel Alignment (CKA). By doing so, we empirically show that multi-head attention does diversify representation subspaces of each head as the number of heads increases. Based on our analysis, we hypothesize that there exists an optimal inter-head diversity with which a model can achieve better performance. To examine our hypothesis, we deeply inspect three techniques to control the inter-head diversity; (1) Hilbert-Schmidt Independence Criterion regularizer among representation subspaces, (2) Orthogonality regularizer, and (3) Drophead as zero-outing each head randomly in every training step. In our experiments on various machine translation and language modeling tasks, we show that controlling inter-head diversity leads to the best performance among baselines.	2080b0051961bbe6a2508054b3699d43209182ea	@None{yun-etal-2021-analyzing,  author = {Hyeongu Yun and Taegwan Kang and Kyomin Jung},  booktitle = {Applied Sciences},  journal = {Applied Sciences},  title = {Analyzing and Controlling Inter-Head Diversity in Multi-Head Attention},  year = {2021} }
Analysis of the Semantic Vector Space Induced by a Neural Language Model and a Corpus	2022	http://www.semanticscholar.org/paper/52f9287b7db62a6daaa082d6f01c4d21ca980a53	This article provides a quantitative analysis of the semantic vector space induced by the XLM-RoBERTa model and the Wikicorpus and proposes a score called Cluster Dispersion Score which reflects how disperse is the collection of clusters.	maybe	0	Although contextual word representations produced by transformer-based language models (e.g., BERT) have proven to be very successful in different kinds of NLP tasks, there is still little knowledge about how these contextual embeddings are connected to word meanings or semantic features. In this article, we provide a quantitative analysis of the semantic vector space induced by the XLM-RoBERTa model and the Wikicorpus. We study the geometric properties of vector embeddings of selected words. We use HDBSCAN clustering algorithm and propose a score called Cluster Dispersion Score which reflects how disperse is the collection of clusters. Our analysis shows that the number of meanings of a word is not directly correlated with the dispersion of embeddings of this word in the semantic vector space induced by the language model and a corpus. Some observations about the division of clusters of embeddings for several selected words are provided.	52f9287b7db62a6daaa082d6f01c4d21ca980a53	@['JournalArticle']{chen-etal-2022-analysis,  author = {Xinying Chen and Jan Hula and A. Dvořák},  booktitle = {Conference on Theory and Practice of Information Technologies},  pages = {103-110},  title = {Analysis of the Semantic Vector Space Induced by a Neural Language Model and a Corpus},  year = {2022} }
Analysis Methods in Neural Language Processing: A Survey	2018	https://www.semanticscholar.org/paper/668f42a4d4094f0a66d402a16087e14269b31a1f	Analysis methods in neural language processing are reviewed, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.	seed	360	The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.	668f42a4d4094f0a66d402a16087e14269b31a1f	@['JournalArticle', 'Review']{belinkov-glass-2018-analysis,  author = {Yonatan Belinkov and James R. Glass},  booktitle = {International Conference on Topology, Algebra and Categories in Logic},  journal = {Transactions of the Association for Computational Linguistics},  pages = {49-72},  title = {Analysis Methods in Neural Language Processing: A Survey},  volume = {7},  year = {2018} }
Analysis and Prediction of NLP Models via Task Embeddings	2021	http://www.semanticscholar.org/paper/94ca89b7df79b2e7e795a36da0cda0bf35bff64a	This paper fits a single transformer to all MetaEval tasks jointly while conditioning it on learned embeddings, enabling a novel analysis of the space of tasks and shows that task aspects can be mapped to task embeddlings for new tasks without using any annotated examples.	maybe	2	Task embeddings are low-dimensional representations that are trained to capture task properties. In this paper, we propose MetaEval, a collection of 101 NLP tasks. We fit a single transformer to all MetaEval tasks jointly while conditioning it on learned embeddings. The resulting task embeddings enable a novel analysis of the space of tasks. We then show that task aspects can be mapped to task embeddings for new tasks without using any annotated examples. Predicted embeddings can modulate the encoder for zero-shot inference and outperform a zero-shot baseline on GLUE tasks. The provided multitask setup can function as a benchmark for future transfer learning research.	94ca89b7df79b2e7e795a36da0cda0bf35bff64a	@['JournalArticle']{sileo-moens-2021-analysis,  author = {Damien Sileo and Marie-Francine Moens},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {633-647},  title = {Analysis and Prediction of NLP Models via Task Embeddings},  year = {2021} }
Analysis and Evaluation of Language Models for Word Sense Disambiguation	2021	http://www.semanticscholar.org/paper/2d22b0b3339b302d25f5d683950459d2fdfc34bd	An in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity reveals that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense.	maybe	25	Abstract Transformer-based language models have taken many fields in NLP by storm. BERT and its derivatives dominate most of the existing evaluation benchmarks, including those for Word Sense Disambiguation (WSD), thanks to their ability in capturing context-sensitive semantic nuances. However, there is still little knowledge about their capabilities and potential limitations in encoding and recovering word senses. In this article, we provide an in-depth quantitative and qualitative analysis of the celebrated BERT model with respect to lexical ambiguity. One of the main conclusions of our analysis is that BERT can accurately capture high-level sense distinctions, even when a limited number of examples is available for each word sense. Our analysis also reveals that in some cases language models come close to solving coarse-grained noun disambiguation under ideal conditions in terms of availability of training data and computing resources. However, this scenario rarely occurs in real-world settings and, hence, many practical challenges remain even in the coarse-grained setting. We also perform an in-depth comparison of the two main language model-based WSD strategies, namely, fine-tuning and feature extraction, finding that the latter approach is more robust with respect to sense bias and it can better exploit limited available training data. In fact, the simple feature extraction strategy of averaging contextualized embeddings proves robust even using only three training sentences per word sense, with minimal improvements obtained by increasing the size of this training data.	2d22b0b3339b302d25f5d683950459d2fdfc34bd	@['JournalArticle']{loureiro-etal-2021-analysis,  author = {Daniel Loureiro and Kiamehr Rezaee and Mohammad Taher Pilehvar and José Camacho-Collados},  booktitle = {International Conference on Computational Logic},  journal = {Computational Linguistics},  pages = {387-443},  title = {Analysis and Evaluation of Language Models for Word Sense Disambiguation},  volume = {47},  year = {2021} }
Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models	2020	http://www.semanticscholar.org/paper/5a0dfe2f44aa91478fb84864c565e9902a9344da	Comparisons are made between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent and create locally-optimal approximations for the intermediate representations from the language model.	maybe	2	Researchers have recently demonstrated that tying the neural weights between the input look-up table and the output classification layer can improve training and lower perplexity on sequence learning tasks such as language modelling. Such a procedure is possible due to the design of the softmax classification layer, which previous work has shown to comprise a viable set of semantic representations for the model vocabulary, and these these output embeddings are known to perform well on word similarity benchmarks. In this paper, we make meaningful comparisons between the input and output embeddings and other SOTA distributional models to gain a better understanding of the types of information they represent. We also construct a new set of word embeddings using the output embeddings to create locally-optimal approximations for the intermediate representations from the language model. These locally-optimal embeddings demonstrate excellent performance across all our evaluations.	5a0dfe2f44aa91478fb84864c565e9902a9344da	@['JournalArticle']{derby-etal-2020-analysing,  author = {Steven Derby and Paul Miller and Barry Devereux},  booktitle = {Conference on Computational Natural Language Learning},  pages = {442-454},  title = {Analysing Word Representation from the Input and Output Embeddings in Neural Network Language Models},  year = {2020} }
Analysing Lexical Semantic Change with Contextualised Word Representations	2020	http://www.semanticscholar.org/paper/328669ab500649ef14bc98ab59b78c483dbd3db4	This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations, a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics.	maybe	94	This paper presents the first unsupervised approach to lexical semantic change that makes use of contextualised word representations. We propose a novel method that exploits the BERT neural language model to obtain representations of word usages, clusters these representations into usage types, and measures change along time with three proposed metrics. We create a new evaluation dataset and show that the model representations and the detected semantic shifts are positively correlated with human judgements. Our extensive qualitative analysis demonstrates that our method captures a variety of synchronic and diachronic linguistic phenomena. We expect our work to inspire further research in this direction.	328669ab500649ef14bc98ab59b78c483dbd3db4	@['JournalArticle', 'Conference']{giulianelli-etal-2020-analysing,  author = {Mario Giulianelli and Marco Del Tredici and Raquel Fern'andez},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3960-3973},  title = {Analysing Lexical Semantic Change with Contextualised Word Representations},  year = {2020} }
Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT	2022	http://www.semanticscholar.org/paper/e8795958aad9c8514d5d7d28b022d42ca4a3a243	It is feasible to prompt InstructGPT to generate meaningful analogies and the best prompts tend to be precise imperative statements especially with a low temperature setting, and the quality of generations varies substantially by model size.	maybe	1	We propose a novel application of prompting Pre-trained Language Models (PLMs) to generate analogies and study how to design effective prompts for two task settings: generating a source concept analogous to a given target concept (aka Analogous Concept Generation or ACG ), and generating an explanation of the similarity between a given pair of target concept and source concept (aka Analogous Explanation Generation or AEG ). We found that it is feasible to prompt InstructGPT to generate meaningful analogies and the best prompts tend to be precise imperative statements especially with a low temperature setting. We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to cer-tain variations (e.g., questions vs. imperative statements). Further, we conducted human evaluation on 1.4k of the generated analogies and found that the quality of generations varies substantially by model size. The largest InstructGPT model can achieve human-level performance at generating meaningful analogies for a given target while there is still room for improvement on the AEG task. 1	e8795958aad9c8514d5d7d28b022d42ca4a3a243	@['JournalArticle']{bhavya-etal-2022-analogy,  author = {Bhavya and Jinjun Xiong and ChengXiang Zhai},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT},  volume = {abs/2210.04186},  year = {2022} }
AnaLog: Testing Analytical and Deductive Logic Learnability in Language Models	2022	http://www.semanticscholar.org/paper/41237c1350eb2371363af543a6c6a30db9d09197	The best performing language model is closely analysed and it is shown that while it performs more consistently than other language models across logical connectives and reasoning domains, it still is sensitive to lexical and syntactic variations in the realisation of logical statements.	yes	0	We investigate the extent to which pre-trained language models acquire analytical and deductive logical reasoning capabilities as a side effect of learning word prediction. We present AnaLog, a natural language inference task designed to probe models for these capabilities, controlling for different invalid heuristics the models may adopt instead of learning the desired generalisations. We test four languagemodels on AnaLog, finding that they have all learned, to a different extent, to encode information that is predictive of entailment beyond shallow heuristics such as lexical overlap and grammaticality. We closely analyse the best performing language model and show that while it performs more consistently than other language models across logical connectives and reasoning domains, it still is sensitive to lexical and syntactic variations in the realisation of logical statements.	41237c1350eb2371363af543a6c6a30db9d09197	@['JournalArticle']{ryb-etal-2022-analog:,  author = {Samuel Ryb and Mario Giulianelli and Arabella J. Sinclair and Raquel Fernández},  booktitle = {STARSEM},  pages = {55-68},  title = {AnaLog: Testing Analytical and Deductive Logic Learnability in Language Models},  year = {2022} }
An Interpretability Illusion for BERT	2021	http://www.semanticscholar.org/paper/9b9dc2b3d95d2f4e4269a9818c14c70c1f801384	An “interpretability illusion” that arises when analyzing the BERT model is described and a taxonomy of model-learned concepts is provided and methodological implications for interpretability research are discussed, especially the importance of testing hypotheses on multiple data sets.	maybe	10	We describe an “interpretability illusion” that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT’s embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.	9b9dc2b3d95d2f4e4269a9818c14c70c1f801384	@['JournalArticle']{bolukbasi-etal-2021-an,  author = {Tolga Bolukbasi and Adam Pearce and Ann Yuan and Andy Coenen and Emily Reif and Fernanda Vi'egas and M. Wattenberg},  booktitle = {ArXiv},  journal = {ArXiv},  title = {An Interpretability Illusion for BERT},  volume = {abs/2104.07143},  year = {2021} }
An Interpretability Evaluation Benchmark for Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/0a9df881784009cbb8efcd037d82ae222440aade	A novel evaluation benchmark providing with both English and Chinese annotated data that tests LMs abilities in multiple dimensions, i.e., grammar, semantics, knowledge, reasoning and computation, and contains perturbed instances for each original instance so as to use the rationale consistency under perturbations as the metric for faithfulness, a perspective of interpretability.	maybe	0	While pre-trained language models (LMs) have brought great improvements in many NLP tasks, there is increasing attention to ex-plore capabilities of LMs and interpret their predictions. However, existing works usually focus only on a certain capability with some downstream tasks. There is a lack of datasets for directly evaluating the masked word prediction performance and the interpretability of pre-trained LMs. To ﬁll in the gap, we propose a novel evaluation benchmark providing with both English and Chinese annotated data. It tests LMs abilities in multiple dimensions, i.e., grammar, semantics, knowledge, reasoning and computation. In addition, it provides carefully annotated token-level rationales that satisfy sufﬁciency and compactness. It contains perturbed instances for each original instance, so as to use the rationale consistency under perturbations as the metric for faithfulness, a perspective of interpretability. We conduct experiments on several widely-used pretrained LMs. The results show that they perform very poorly on the dimensions of knowledge and computation. And their plausibility in all dimensions is far from satisfactory, especially when the rationale is short. In addition, the pre-trained LMs we evaluated are not robust on syntax-aware data. We will release this evaluation benchmark at http://xyz , and hope it can facilitate the research progress of pre-trained LMs.	0a9df881784009cbb8efcd037d82ae222440aade	@['JournalArticle']{shen-etal-2022-an,  author = {Ya-Ming Shen and Lijie Wang and Ying Chen and Xinyan Xiao and Jing Liu and Hua Wu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {An Interpretability Evaluation Benchmark for Pre-trained Language Models},  volume = {abs/2207.13948},  year = {2022} }
An Information Theoretic View on Selecting Linguistic Probes	2020	http://www.semanticscholar.org/paper/5fd2228acb219dcf477d080eb90fed5dcc918bf7	It is found that the methods to construct and select good probes proposed by the two papers, *control task* and *control function* are equivalent -- the errors of their approaches are identical (modulo irrelevant terms).	maybe	11	There is increasing interest in assessing the linguistic knowledge encoded in neural representations. A popular approach is to attach a diagnostic classifier -- or "probe" -- to perform supervised classification from internal representations. However, how to select a good probe is in debate. Hewitt and Liang (2019) showed that a high performance on diagnostic classification itself is insufficient, because it can be attributed to either "the representation being rich in knowledge", or "the probe learning the task", which Pimentel et al. (2020) challenged. We show this dichotomy is valid information-theoretically. In addition, we find that the methods to construct and select good probes proposed by the two papers, *control task* (Hewitt and Liang, 2019) and *control function* (Pimentel et al., 2020), are equivalent -- the errors of their approaches are identical (modulo irrelevant terms). Empirically, these two selection criteria lead to results that highly agree with each other.	5fd2228acb219dcf477d080eb90fed5dcc918bf7	@['JournalArticle', 'Conference']{zhu-rudzicz-2020-an,  author = {Zining Zhu and F. Rudzicz},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {An Information Theoretic View on Selecting Linguistic Probes},  volume = {abs/2009.07364},  year = {2020} }
An Exploratory Study on Code Attention in BERT	2022	http://www.semanticscholar.org/paper/7df670785e59fa484f358198493a105b5f43ef3e	The attention behavior of PLM on code is investigated and it is shown that BERT pays more attention to syntactic entities, specifically identifiers and separators, in contrast to the most attended token [CLS] in NLP.	maybe	2	Many recent models in software engineering introduced deep neural models based on the Transformer architecture or use transformer-based Pre-trained Language Models (PLM) trained on code. Although these models achieve the state of the arts results in many downstream tasks such as code summarization and bug detection, they are based on Transformer and PLM, which are mainly studied in the Natural Language Processing (NLP) field. The current studies rely on the reasoning and practices from NLP for these models in code, despite the differences between natural languages and programming languages. There is also limited literature on explaining how code is modeled. Here, we investigate the attention behavior of PLM on code and compare it with natural language. We pre-trained BERT, a Transformer based PLM, on code and explored what kind of information it learns, both semantic and syntactic. We run several experiments to analyze the attention values of code constructs on each other and what BERT learns in each layer. Our analyses show that BERT pays more attention to syntactic entities, specifically identifiers and separators, in contrast to the most attended token [CLS] in NLP. This observation motivated us to leverage identifiers to represent the code sequence instead of the [CLS] token when used for code clone detection. Our results show that employing embeddings from identifiers increases the performance of BERT by 605% and 4% F1-score in its lower layers and the upper layers, respectively. When identifiers' embeddings are used in CodeBERT, a code-based PLM, the performance is improved by 21-24% in the F1-score of clone detection. The findings can benefit the research community by using code-specific representations instead of applying the common embeddings used in NLP, and open new directions for developing smaller models with similar performance.	7df670785e59fa484f358198493a105b5f43ef3e	@['JournalArticle', 'Book', 'Conference']{sharma-etal-2022-an,  author = {Rishab Sharma and Fuxiang Chen and Fatemeh Fard and David Lo},  booktitle = {IEEE International Conference on Program Comprehension},  journal = {2022 IEEE/ACM 30th International Conference on Program Comprehension (ICPC)},  pages = {437-448},  title = {An Exploratory Study on Code Attention in BERT},  year = {2022} }
An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models	2021	http://www.semanticscholar.org/paper/de6807676d8171472ed6cf421c4e4ed3cbb47699	An empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation, Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias is performed, finding self-debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks.	maybe	25	Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model’s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.	de6807676d8171472ed6cf421c4e4ed3cbb47699	@['JournalArticle', 'Conference', 'Review']{meade-etal-2021-an,  author = {Nicholas Meade and Elinor Poole-Dayan and Siva Reddy},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1878-1898},  title = {An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models},  year = {2021} }
An Empirical Study of Transformer Language Models on BLiMP and Reading Time Prediction	2021	http://www.semanticscholar.org/paper/7a1e968cbc9e37ac4ff013a4068e2ec8c1531f14	This work trains a suite of smallscale Transformer language models, each differing from each other with minimal change in, e.g., training objective or data, and analyzes how those small changes during training affect the acquisition of linguistic knowledge in the resulting models.	maybe	0	Recent progress in large pretrained language models has led to a growth of analyses examining the linguistic knowledge encoded in these models. Due to limited computation, the analyses are mostly conducted on released language model checkpoints, which makes it difficult to study how various factors during training affect the acquired linguistic knowledge. In this work, we train a suite of smallscale Transformer language models, each differing from each other with minimal change in, e.g., training objective or data, and analyze how those small changes during training affect the acquisition of linguistic knowledge in the resulting models. We first evaluate the models on BLiMP, a targeted evaluation benchmark of multiple English linguistic phenomena. Results show that modifying the training objective (e.g. use focal loss or add auxiliary loss) does not bring significant gains on average, but shows promising improvement on several subcategories, e.g., detection of adjunct island and correct scope of negative polarity items. We also evaluate the models on a human reading time prediction task. Experimental results demonstrate that none of the models, including strong baselines such as GPT-2 and GPT-3, can predict the correct range of human reading time for the garden path sentences, implying models do not have human-like generalization on syntactically ambiguous sentences.	7a1e968cbc9e37ac4ff013a4068e2ec8c1531f14	@None{sun-2021-an,  author = {Simeng Sun},  title = {An Empirical Study of Transformer Language Models on BLiMP and Reading Time Prediction},  year = {2021} }
An Empirical Study of Memorization in NLP	2022	http://www.semanticscholar.org/paper/51cb3d853cbceb1fc0d55bfc416435fec6e0a03d	It is demonstrated that top-ranked memorized training instances are likely atypical, and removing the top-memorization training instances leads to a more serious drop in test accuracy compared with removing training instances randomly, and an attribution method is developed to better understand why a training instance is memorized.	maybe	2	A recent study by Feldman (2020) proposed a long-tail theory to explain the memorization behavior of deep learning models. However, memorization has not been empirically verified in the context of NLP, a gap addressed by this work. In this paper, we use three different NLP tasks to check if the long-tail theory holds. Our experiments demonstrate that top-ranked memorized training instances are likely atypical, and removing the top-memorized training instances leads to a more serious drop in test accuracy compared with removing training instances randomly. Furthermore, we develop an attribution method to better understand why a training instance is memorized. We empirically show that our memorization attribution method is faithful, and share our interesting finding that the top-memorized parts of a training instance tend to be features negatively correlated with the class label.	51cb3d853cbceb1fc0d55bfc416435fec6e0a03d	@['JournalArticle', 'Conference']{zheng-jiang-2022-an,  author = {Xiaosen Zheng and Jing Jiang},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {An Empirical Study of Memorization in NLP},  volume = {abs/2203.12171},  year = {2022} }
An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks	2022	http://www.semanticscholar.org/paper/511e69bfa4213442e0afa35d7bbc503aa5d93405	Empirical study of replacing parsed graphs or trees with trivial ones for tasks in the GLUE benchmark reveals that the gains might not be signiﬁcantly attributed to explicit linguistic priors but rather to more feature interactions brought by fusion layers.	maybe	0	Though linguistic knowledge emerges during large-scale language model pretraining, recent work attempt to explicitly incorporate human-defined linguistic priors into task-specific fine-tuning. Infusing language models with syntactic or semantic knowledge from parsers has shown improvements on many language understanding tasks. To further investigate the effectiveness of structural linguistic priors, we conduct empirical study of replacing parsed graphs or trees with trivial ones (rarely carrying linguistic knowledge e.g., balanced tree) for tasks in the GLUE benchmark. Encoding with trivial graphs achieves competitive or even better performance in fully-supervised and few-shot settings. It reveals that the gains might not be significantly attributed to explicit linguistic priors but rather to more feature interactions brought by fusion layers. Hence we call for attention to using trivial graphs as necessary baselines to design advanced knowledge fusion methods in the future.	511e69bfa4213442e0afa35d7bbc503aa5d93405	@['JournalArticle', 'Conference']{yu-etal-2022-an,  author = {Changlong Yu and Tianyi Xiao and Lingpeng Kong and Yangqiu Song and Wilfred Ng},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {An Empirical Revisiting of Linguistic Knowledge Fusion in Language Understanding Tasks},  volume = {abs/2210.13002},  year = {2022} }
An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs	2022	http://www.semanticscholar.org/paper/651ae53112e73b02440773727b68cedbf8322705	The effect of different synthetic datasets on language models with various architectures and sizes is studied to show that encoder-decoder models beneﬁt from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance.	maybe	1	Self-supervision based on the information extracted from large knowledge graphs has been shown to improve the generalization of language models, in zero-shot evaluation on various downstream language reasoning tasks. Since these improvements are reported in ag-gregate, however, little is known about (i) how to select the appropriate knowledge for solid performance across tasks, (ii) how to combine this knowledge with neural language models, and (iii) how these pairings affect granular task performance. In this paper, we study the effect of knowledge sampling strategies and sizes that can be used to generate synthetic data for adapting language models. We study the effect of different synthetic datasets on language models with various architectures and sizes. The resulting models are evalu-ated against four task properties: domain overlap, answer similarity, vocabulary overlap, and answer length. Our experiments show that encoder-decoder models beneﬁt from more data to learn from, whereas sampling strategies that balance across different aspects yield best performance. Most of the improvement occurs on questions with short answers and dissimilar answer candidates, which corresponds to the characteristics of the data used for pretraining.	651ae53112e73b02440773727b68cedbf8322705	@['JournalArticle']{zhang-etal-2022-an,  author = {Jiarui Zhang and Filip Ilievski and Kaixin Ma and Jonathan M Francis and A. Oltramari},  booktitle = {ArXiv},  journal = {ArXiv},  title = {An Empirical Investigation of Commonsense Self-Supervision with Knowledge Graphs},  volume = {abs/2205.10661},  year = {2022} }
An Empirical Comparison of Instance Attribution Methods for NLP	2021	http://www.semanticscholar.org/paper/b2ad1812d0d8f0f1eb19c17e0bccfce4399dd575	It is found that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods.	maybe	18	Widespread adoption of deep models has motivated a pressing need for approaches to interpret network outputs and to facilitate model debugging. Instance attribution methods constitute one means of accomplishing these goals by retrieving training instances that (may have) led to a particular prediction. Influence functions (IF; Koh and Liang 2017) provide machinery for doing this by quantifying the effect that perturbing individual train instances would have on a specific test prediction. However, even approximating the IF is computationally expensive, to the degree that may be prohibitive in many cases. Might simpler approaches (e.g., retrieving train examples most similar to a given test point) perform comparably? In this work, we evaluate the degree to which different potential instance attribution agree with respect to the importance of training samples. We find that simple retrieval methods yield training instances that differ from those identified via gradient-based methods (such as IFs), but that nonetheless exhibit desirable characteristics similar to more complex attribution methods. Code for all methods and experiments in this paper is available at: https://github.com/successar/instance_attributions_NLP.	b2ad1812d0d8f0f1eb19c17e0bccfce4399dd575	@['JournalArticle', 'Conference']{pezeshkpour-etal-2021-an,  author = {Pouya Pezeshkpour and Sarthak Jain and Byron C. Wallace and Sameer Singh},  booktitle = {North American Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {An Empirical Comparison of Instance Attribution Methods for NLP},  volume = {abs/2104.04128},  year = {2021} }
An Attention Matrix for Every Decision: Faithfulness-based Arbitration Among Multiple Attention-Based Interpretations of Transformers in Text Classification	2022	http://www.semanticscholar.org/paper/608244ccc7cdae31d63c533d2086747ee0f5b16a		maybe			608244ccc7cdae31d63c533d2086747ee0f5b16a	
An Approximate Perspective on Word Prediction in Context: Ontological Semantics Meets BERT	2021	http://www.semanticscholar.org/paper/4879c46415d5e7d8099e497398bc4d7b30664174	An analysis of a large neural network model – BERT, by placing its word prediction in context capability under the framework of Ontological Semantics, revealing the degree to which each output satisfies the sentences constraints.	maybe	0		4879c46415d5e7d8099e497398bc4d7b30664174	@['JournalArticle']{misra-rayz-2021-an,  author = {Kanishka Misra and J. Rayz},  booktitle = {Annual Conference on the North American Fuzzy Information Processing Society},  pages = {157-169},  title = {An Approximate Perspective on Word Prediction in Context: Ontological Semantics Meets BERT},  year = {2021} }
An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models	2020	http://www.semanticscholar.org/paper/320df925e7f83412b6e6b947bc9c9227aa9be46e	This paper demonstrates that appropriately using negative examples about particular constructions will boost the model’s robustness on them in English, with a negligible loss of perplexity, and can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.	maybe	11	We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as barks in *The dogs barks. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model’s robustness on them in English, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. We then provide a detailed analysis of the trained models. One of our findings is the difficulty of object-relative clauses for RNNs. We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions.	320df925e7f83412b6e6b947bc9c9227aa9be46e	@['JournalArticle', 'Conference']{noji-takamura-2020-an,  author = {Hiroshi Noji and Hiroya Takamura},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {3375-3385},  title = {An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models},  year = {2020} }
An Analysis of Social Biases Present in BERT Variants Across Multiple Languages	2022	http://www.semanticscholar.org/paper/67817c0005864e9f251f14deb6c77d4724e31927	This paper proposes a template-based method to measure any kind of bias, based on sentence pseudo-likelihood, that can handle morphologically complex languages with gender-based adjective declensions and concludes that current methods of probing for bias are highly language-dependent.	maybe	0	Although large pre-trained language models have achieved great success in many NLP tasks, it has been shown that they reﬂect human biases from their pre-training corpora. This bias may lead to undesirable outcomes when these models are applied in real-world settings. In this paper, we investigate the bias present in monolingual BERT models across a diverse set of languages (English, Greek, and Persian). While recent research has mostly focused on gender-related biases, we analyze religious and ethnic biases as well and propose a template-based method to measure any kind of bias, based on sentence pseudo-likelihood, that can handle morphologically complex languages with gender-based adjective declensions. We analyze each monolingual model via this method and visualize cultural similarities and differences across different dimensions of bias. Ultimately, we conclude that current methods of probing for bias are highly language-dependent, necessitating cultural insights regarding the unique ways bias is expressed in each language and culture (e.g. through coded language, synecdoche, and other similar linguistic concepts). We also hypothesize that higher measured social biases in the non-English BERT models correlate with user-generated content in their training.	67817c0005864e9f251f14deb6c77d4724e31927	@['JournalArticle']{milios-behnamghader-2022-an,  author = {Aristides Milios and Parishad BehnamGhader},  booktitle = {ArXiv},  journal = {ArXiv},  title = {An Analysis of Social Biases Present in BERT Variants Across Multiple Languages},  volume = {abs/2211.14402},  year = {2022} }
An Analysis of Negation in Natural Language Understanding Corpora	2022	http://www.semanticscholar.org/paper/b9ffb9ccc000ccc5b74d1e9eaeafbcd5fe86e78d	This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks and concludes that new corpora accounting for negation are needed to solve natural languageUnderstanding tasks when negation is present.	maybe	5	This paper analyzes negation in eight popular corpora spanning six natural language understanding tasks. We show that these corpora have few negations compared to general-purpose English, and that the few negations in them are often unimportant. Indeed, one can often ignore negations and still make the right predictions. Additionally, experimental results show that state-of-the-art transformers trained with these corpora obtain substantially worse results with instances that contain negation, especially if the negations are important. We conclude that new corpora accounting for negation are needed to solve natural language understanding tasks when negation is present.	b9ffb9ccc000ccc5b74d1e9eaeafbcd5fe86e78d	@['JournalArticle', 'Conference']{hossain-etal-2022-an,  author = {Md Mosharaf Hossain and Dhivya Chinnappa and Eduardo Blanco},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {An Analysis of Negation in Natural Language Understanding Corpora},  volume = {abs/2203.08929},  year = {2022} }
An Analysis of Natural Language Inference Benchmarks through the Lens of Negation	2020	http://www.semanticscholar.org/paper/2bc77cdfa44bdea93cee56f8a4eaae60ddc195e1	A new benchmark for natural language inference in which negation plays a critical role is presented, and it is shown that state-of-the-art transformers struggle making inference judgments with the new pairs.	maybe	43	Negation is underrepresented in existing natural language inference benchmarks. Additionally, one can often ignore the few negations in existing benchmarks and still make the right inference judgments. In this paper, we present a new benchmark for natural language inference in which negation plays a critical role. We also show that state-of-the-art transformers struggle making inference judgments with the new pairs.	2bc77cdfa44bdea93cee56f8a4eaae60ddc195e1	@['JournalArticle', 'Conference']{hossain-etal-2020-an,  author = {Md Mosharaf Hossain and Venelin Kovatchev and Pranoy Dutta and T. Kao and Elizabeth Wei and Eduardo Blanco},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {9106-9118},  title = {An Analysis of Natural Language Inference Benchmarks through the Lens of Negation},  year = {2020} }
An Analysis of Encoder Representations in Transformer-Based Machine Translation	2018	https://www.semanticscholar.org/paper/94238dead40b12735d79ed63e29ead70730261a2	This work investigates the information that is learned by the attention mechanism in Transformer models with different translation quality, and sheds light on the relative strengths and weaknesses of the various encoder representations.	seed	206	The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the Transformer is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.	94238dead40b12735d79ed63e29ead70730261a2	@['JournalArticle']{raganato-tiedemann-2018-an,  author = {Alessandro Raganato and J. Tiedemann},  booktitle = {BlackboxNLP@EMNLP},  pages = {287-297},  title = {An Analysis of Encoder Representations in Transformer-Based Machine Translation},  year = {2018} }
An AI ethics ‘David and Goliath’: value conflicts between large tech companies and their employees	2022	http://www.semanticscholar.org/paper/2858c60318bde731ac4dc1ceddbe69eaf3906e18		maybe	1		2858c60318bde731ac4dc1ceddbe69eaf3906e18	@['Review']{ryan-etal-2022-an,  author = {M. Ryan and E. Christodoulou and Josephina Antoniou and Kalypso Iordanou},  booktitle = {AI &amp; SOCIETY},  journal = {AI &amp; SOCIETY},  title = {An AI ethics ‘David and Goliath’: value conflicts between large tech companies and their employees},  year = {2022} }
Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals	2020	https://www.semanticscholar.org/paper/1d7f3297924a9dd90cfc0df522ebe9138c28b46f	The inability to infer behavioral conclusions from probing results is pointed out, and an alternative method that focuses on how the information is being used is offered, rather than on what information is encoded is offered.	yes	101	Abstract A growing body of work makes use of probing in order to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results, and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, for example, is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.1	1d7f3297924a9dd90cfc0df522ebe9138c28b46f	@['JournalArticle']{elazar-etal-2020-amnesic,  author = {Yanai Elazar and Shauli Ravfogel and Alon Jacovi and Yoav Goldberg},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {160-175},  title = {Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals},  volume = {9},  year = {2020} }
AmbiCoref: Evaluating Human and Model Sensitivity to Ambiguous Coreference	2023	https://www.semanticscholar.org/paper/0de92517f5a2bb2fd0944d1d526ad71abfe690a6		maybe	0	Given a sentence “Abby told Brittney that she upset Courtney”, one would struggle to understand who “she” refers to, and ask for clariﬁcation. However, if the word “upset” were replaced with “hugged”, “she” unam-biguously refers to Abby. We study if mod-ern co-reference resolution models are sensitive to such pronominal ambiguity. To this end, we construct A MBI C OREF , a diagnostic corpus of minimal sentence pairs with ambiguous and unambiguous referents. Our examples generalize psycholinguistic studies of human perception of ambiguity around particular arrangements of verbs and their arguments. Analysis shows that (1) humans are less sure of referents in ambiguous AmbiCoref examples than unambiguous ones, and (2) most coreference models show little difference in output between ambiguous and unambiguous pairs. We release A MBI C OREF as a diagnostic corpus for testing whether models treat ambiguity similarly to humans. 1	0de92517f5a2bb2fd0944d1d526ad71abfe690a6	@None{yuan-etal-2023-ambicoref:,  author = {Yuewei Yuan and Chaitanya Malaviya and Mark Yatskar},  title = {AmbiCoref: Evaluating Human and Model Sensitivity to Ambiguous Coreference},  year = {2023} }
Alleviating the Knowledge-Language Inconsistency: A Study for Deep Commonsense Knowledge	2021	http://www.semanticscholar.org/paper/03db991698d95a54fe229ff2f10fe22f58e8ac34	It is shown that deep commonsense knowledge occupies a significant part of Commonsense knowledge, while the conventional methods based on pre-trained language models fail to capture it effectively, and a novel method is proposed to mine the deep commonsens knowledge from raw text that is exactly language expression, alleviating the reliance of conventional methods on the triple representation form.	maybe	0	Knowledge facts are typically represented by relational triples, while we observe that some commonsense facts are represented by triples whose forms are inconsistent with the corresponding language expressions. For commonsense mining tasks, this inconsistency raises a challenge for the prevailing methods using pre-trained language models that learn the expression of language. However, there are few studies which focus on this inconsistency issue. To fill this empty, in this paper, we term the commonsense knowledge whose triple form is heavily inconsistent with the language expression as deep commonsense knowledge and first conduct extensive exploratory experiments to study deep commonsense knowledge. We show that deep commonsense knowledge occupies a significant part of commonsense knowledge, while the conventional methods based on pre-trained language models fail to capture it effectively. We further propose a novel method to mine the deep commonsense knowledge from raw text that is exactly language expression, alleviating the reliance of conventional methods on the triple representation form. Experiments demonstrate that our proposed method substantially improves the performance in mining deep commonsense knowledge.	03db991698d95a54fe229ff2f10fe22f58e8ac34	@['JournalArticle']{zhang-etal-2021-alleviating,  author = {Yi Zhang and Lei Li and Yunfang Wu and Qi Su and Xu Sun},  booktitle = {IEEE/ACM Transactions on Audio Speech and Language Processing},  journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},  pages = {594-604},  title = {Alleviating the Knowledge-Language Inconsistency: A Study for Deep Commonsense Knowledge},  volume = {30},  year = {2021} }
All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text	2021	http://www.semanticscholar.org/paper/a16ae67070de155789a871cb27ecbf9eaa98b379	The role untrained human evaluations play in NLG evaluation is examined and three approaches for quickly training evaluators to better identify GPT3-authored text are explored and it is found that while evaluation accuracy improved up to 55%, it did not significantly improve across the three domains.	maybe	78	Human evaluations are typically considered the gold standard in natural language generation, but as models’ fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts’ ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators’ accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models.	a16ae67070de155789a871cb27ecbf9eaa98b379	@['JournalArticle', 'Conference']{clark-etal-2021-all,  author = {Elizabeth Clark and Tal August and Sofia Serrano and Nikita Haduong and Suchin Gururangan and Noah A. Smith},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7282-7296},  title = {All That’s ‘Human’ Is Not Gold: Evaluating Human Evaluation of Generated Text},  year = {2021} }
ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns’ Semantic Properties and their Prototypicality	2021	https://www.semanticscholar.org/paper/a58d790d1ee6e08786cd44e75c3544c18ede4f63	This study probes BERT for the properties of English nouns as expressed by adjectives that do not restrict the reference scope of the noun they modify, but instead emphasise some inherent aspect (“red strawberry”) and shows that the model has marginal knowledge of these features and their prevalence as expressed in datasets.	maybe	5	Large scale language models encode rich commonsense knowledge acquired through exposure to massive data during pre-training, but their understanding of entities and their semantic properties is unclear. We probe BERT (Devlin et al., 2019) for the properties of English nouns as expressed by adjectives that do not restrict the reference scope of the noun they modify (as in “red car”), but instead emphasise some inherent aspect (“red strawberry”). We base our study on psycholinguistics datasets that capture the association strength between nouns and their semantic features. We probe BERT using cloze tasks and in a classification setting, and show that the model has marginal knowledge of these features and their prevalence as expressed in these datasets. We discuss factors that make evaluation challenging and impede drawing general conclusions about the models’ knowledge of noun properties. Finally, we show that when tested in a fine-tuning setting addressing entailment, BERT successfully leverages the information needed for reasoning about the meaning of adjective-noun constructions outperforming previous methods.	a58d790d1ee6e08786cd44e75c3544c18ede4f63	@['JournalArticle']{apidianaki-soler-2021-all,  author = {Marianna Apidianaki and Aina Garí Soler},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {79-94},  title = {ALL Dolphins Are Intelligent and SOME Are Friendly: Probing BERT for Nouns’ Semantic Properties and their Prototypicality},  year = {2021} }
All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality	2021	http://www.semanticscholar.org/paper/51b0c571d89bd2d39a194f60f91f0a03d74574b5	It is argued that accounting forRogue dimensions is essential for any similarity-based analysis of contextual language models and simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality.	yes	22	Similarity measures are a vital tool for understanding how language models represent and process language. Standard representational similarity measures such as cosine similarity and Euclidean distance have been successfully used in static word embedding models to understand how words cluster in semantic space. Recently, these measures have been applied to embeddings from contextualized models such as BERT and GPT-2. In this work, we call into question the informativity of such measures for contextualized language models. We find that a small number of rogue dimensions, often just 1-3, dominate these measures. Moreover, we find a striking mismatch between the dimensions that dominate similarity measures and those which are important to the behavior of the model. We show that simple postprocessing techniques such as standardization are able to correct for rogue dimensions and reveal underlying representational quality. We argue that accounting for rogue dimensions is essential for any similarity-based analysis of contextual language models.	51b0c571d89bd2d39a194f60f91f0a03d74574b5	@['JournalArticle', 'Conference']{timkey-schijndel-2021-all,  author = {William Timkey and Marten van Schijndel},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality},  volume = {abs/2109.04404},  year = {2021} }
Aligning Faithful Interpretations with their Social Attribution	2020	http://www.semanticscholar.org/paper/86471bf927401bf88af83626797228c2bf10a282	It is found that the requirement of model interpretations to be faithful is vague and incomplete, and faithfulness is reformulated as an accurate attribution of causality to the model, and aligned faithfulness: faithful causal chains that are aligned with their expected social behavior is introduced.	yes	64	Abstract We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations.	86471bf927401bf88af83626797228c2bf10a282	@['JournalArticle']{jacovi-goldberg-2020-aligning,  author = {Alon Jacovi and Yoav Goldberg},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {294-310},  title = {Aligning Faithful Interpretations with their Social Attribution},  volume = {9},  year = {2020} }
AI Personification: Estimating the Personality of Language Models	2022	https://www.semanticscholar.org/paper/cfd238f54fa9497ecaaed35d341e0c000632948d	This work explores the personality traits of several large-scale language models designed for open-ended text generation and the datasets used for training them and develops robust methods that quantify the personality trait of these models and their underlying datasets.	maybe	4	Technology for open-ended language generation, a key application of artiﬁcial intelligence, has advanced to a great extent in recent years. Large-scale language models, which are trained on large corpora of text, are being used in a wide range of applications everywhere, from virtual assistants to conversational bots. While these language models output ﬂuent text, existing research shows that these models can and do capture human biases. Many of these biases, especially those that could potentially cause harm, are being well investigated. On the other hand, studies that infer and change personality traits inherited by these models have been scarce or non-existent. In this work, we explore the personality traits of several large-scale language models designed for open-ended text generation and the datasets used for training them. Our work builds on the popular Big Five factors and develops robust methods that quantify the personality traits of these models and their underlying datasets. In particular, we trigger the models with a questionnaire designed for personality assessment and subsequently classify the text responses into quantiﬁable traits using a Zero-shot classiﬁer. Our classiﬁcation sheds light on an important anthropomorphic element found in such AI models and can help stakeholders decide how they should be applied and how society could perceive them. We augment our analysis by studying approaches that can alter these personalities.	cfd238f54fa9497ecaaed35d341e0c000632948d	@['JournalArticle']{karra-etal-2022-ai,  author = {Saketh Reddy Karra and Son Nguyen and Theja Tulabandhula},  booktitle = {ArXiv},  journal = {ArXiv},  title = {AI Personification: Estimating the Personality of Language Models},  volume = {abs/2204.12000},  year = {2022} }
AI model GPT-3 (dis)informs us better than humans	2023	https://www.semanticscholar.org/paper/5575e53739ef0c48c9d6b7cc2146ff89516d0199	This paper evaluates whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determines whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3.	yes	0	Artificial intelligence is changing the way we create and evaluate information, and this is happening during an infodemic, which has been having dramatic effects on global health. In this paper we evaluate whether recruited individuals can distinguish disinformation from accurate information, structured in the form of tweets, and determine whether a tweet is organic or synthetic, i.e., whether it has been written by a Twitter user or by the AI model GPT-3. Our results show that GPT-3 is a double-edge sword, which, in comparison with humans, can produce accurate information that is easier to understand, but can also produce more compelling disinformation. We also show that humans cannot distinguish tweets generated by GPT-3 and real Twitter users. Starting from our results, we reflect on the dangers of AI for disinformation, and on how we can improve information campaigns to benefit global health.	5575e53739ef0c48c9d6b7cc2146ff89516d0199	@['JournalArticle']{spitale-etal-2023-ai,  author = {Giovanni Spitale and N. Biller-Andorno and Federico Germani},  booktitle = {ArXiv},  journal = {ArXiv},  title = {AI model GPT-3 (dis)informs us better than humans},  volume = {abs/2301.11924},  year = {2023} }
Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding	2020	http://www.semanticscholar.org/paper/78db1529bd67ef885fe550ab3ed7f965067e8928	The Adversarial Watermarking Transformer (AWT) is introduced with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message.	maybe	14	Recent advances in natural language generation have introduced powerful language models with high-quality output text. However, this raises concerns about the potential misuse of such models for malicious purposes. In this paper, we study natural language watermarking as a defense to help better mark and trace the provenance of text. We introduce the Adversarial Watermarking Transformer (AWT) with a jointly trained encoder-decoder and adversarial training that, given an input text and a binary message, generates an output text that is unobtrusively encoded with the given message. We further study different training and inference strategies to achieve minimal changes to the semantics and correctness of the input text.AWT is the first end-to-end model to hide data in text by automatically learning -without ground truth- word substitutions along with their locations in order to encode the message. We empirically show that our model is effective in largely preserving text utility and decoding the watermark while hiding its presence against adversaries. Additionally, we demonstrate that our method is robust against a range of attacks.	78db1529bd67ef885fe550ab3ed7f965067e8928	@['JournalArticle']{abdelnabi-fritz-2020-adversarial,  author = {Sahar Abdelnabi and Mario Fritz},  booktitle = {IEEE Symposium on Security and Privacy},  journal = {2021 IEEE Symposium on Security and Privacy (SP)},  pages = {121-140},  title = {Adversarial Watermarking Transformer: Towards Tracing Text Provenance with Data Hiding},  year = {2020} }
Adversarial Semantic Collisions	2020	http://www.semanticscholar.org/paper/aeb4478619461ca25592e6d692f3591ec8c4091b		maybe	16	We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts-- including paraphrase identification, document retrieval, response suggestion, and extractive summarization-- are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert.	aeb4478619461ca25592e6d692f3591ec8c4091b	@['JournalArticle', 'Conference']{song-etal-2020-adversarial,  author = {Congzheng Song and Alexander M. Rush and Vitaly Shmatikov},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4198-4210},  title = {Adversarial Semantic Collisions},  year = {2020} }
Addressing Token Uniformity in Transformers via Singular Value Transformation	2022	http://www.semanticscholar.org/paper/4cf7889c0fc5e181c20e64a4b26cb08ce25e7b45	This paper proposes to use the distribution of singular values of outputs of each transformer layer to characterise the phenomenon of token uniformity and empirically illustrate that a less skewed singular value distribution can alleviate the ‘token uniformity’ problem.	maybe	0	Token uniformity is commonly observed in transformer-based models, in which different tokens share a large proportion of similar information after going through stacked multiple self-attention layers in a transformer. In this paper, we propose to use the distribution of singular values of outputs of each transformer layer to characterise the phenomenon of token uniformity and empirically illustrate that a less skewed singular value distribution can alleviate the ‘token uniformity’ problem. Base on our observations, we define several desirable properties of singular value distributions and propose a novel transformation function for updating the singular values. We show that apart from alleviating token uniformity, the transformation function should preserve the local neighbourhood structure in the original embedding space. Our proposed singular value transformation function is applied to a range of transformer-based language models such as BERT, ALBERT, RoBERTa and DistilBERT, and improved performance is observed in semantic textual similarity evaluation and a range of GLUE tasks. Our source code is available at https:// github.com/hanqi-qi/tokenUni.git .	4cf7889c0fc5e181c20e64a4b26cb08ce25e7b45	@['JournalArticle', 'Conference']{yan-etal-2022-addressing,  author = {Hanqi Yan and Lin Gui and Wenjie Li and Yulan He},  booktitle = {Conference on Uncertainty in Artificial Intelligence},  journal = {ArXiv},  title = {Addressing Token Uniformity in Transformers via Singular Value Transformation},  volume = {abs/2208.11790},  year = {2022} }
Active Example Selection for In-Context Learning	2022	http://www.semanticscholar.org/paper/b8bd29a6104d26a16687400049a4e7e026ae6258	It is demonstrated that in-context learning performance can be highly unstable across samples of examples, indicating the id-iosyncrasies of how language models acquire information.	maybe	3	With a handful of demonstration examples, large-scale language models demonstrate strong capability to perform various tasks by in-context learning from these examples, without any fine-tuning. We demonstrate that in-context learning performance can be highly unstable across samples of examples, indicating the idiosyncrasies of how language models acquire information. We formulate example selection for in-context learning as a sequential decision problem, and propose a reinforcement learning algorithm for identifying generalizable policies to select demonstration examples. For GPT-2, our learned policies demonstrate strong abilities of generalizing to unseen tasks in training, with a 5.8% improvement on average. Examples selected from our learned policies can even achieve a small improvement on GPT-3 Ada. However, the improvement diminishes on larger GPT-3 models, suggesting emerging capabilities of large language models.	b8bd29a6104d26a16687400049a4e7e026ae6258	@['JournalArticle', 'Conference']{zhang-etal-2022-active,  author = {Yiming Zhang and Shi Feng and Chenhao Tan},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {Active Example Selection for In-Context Learning},  volume = {abs/2211.04486},  year = {2022} }
Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models	2021	http://www.semanticscholar.org/paper/f640ef82eef63f16e9e732409e72aec638a53a3d	It is confirmed that current PLMs are prone to generate inconsistent predictions even for semantically identical inputs, and observed that multi-task training with paraphrase identification tasks is of benefit to improve consistency, increasing the consistency by 13% on average.	yes	5	Consistency, which refers to the capability of generating the same predictions for semantically similar contexts, is a highly desirable property for a sound language understanding model. Although recent pretrained language models (PLMs) deliver outstanding performance in various downstream tasks, they should exhibit consistent behaviour provided the models truly understand language. In this paper, we propose a simple framework named consistency analysis on language understanding models (CALUM) to evaluate model’s lower-bound consistency ability. Through experiments, we confirmed that current PLMs are prone to generate inconsistent predictions even for semantically identical inputs. We also observed that multi-task training with paraphrase identification tasks is of benefit to improve consistency, increasing the consistency by 13% on average.	f640ef82eef63f16e9e732409e72aec638a53a3d	@['JournalArticle']{jang-etal-2021-accurate,  author = {Myeongjun Jang and D. Kwon and Thomas Lukasiewicz},  booktitle = {ArXiv},  journal = {ArXiv},  title = {Accurate, yet inconsistent? Consistency Analysis on Language Understanding Models},  volume = {abs/2108.06665},  year = {2021} }
Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention	2021	http://www.semanticscholar.org/paper/da238bb002e0cca26e6827e6914e0d4ecb099442	It is shown that surprisal of the verb or reflexive pronoun predicts facilitatory interference effects in ungrammatical sentences, where a distractor noun that matches in number with theverb or pronouns leads to faster reading times, despite the distractor not participating in the agreement relation.	maybe	4	We advance a novel explanation of similarity-based interference effects in subject-verb and reflexive pronoun agreement processing, grounded in surprisal values computed from a pretrained large-scale Transformer model, GPT-2. Specifically, we show that surprisal of the verb or reflexive pronoun predicts facilitatory interference effects in ungrammatical sentences, where a distractor noun that matches in number with the verb or pronouns leads to faster reading times, despite the distractor not participating in the agreement relation. We review the human empirical evidence for such effects, including recent meta-analyses and large-scale studies. We also show that attention patterns (indexed by entropy and other measures) in the Transformer show patterns of diffuse attention in the presence of similar distractors, consistent with cue-based retrieval models of parsing. But in contrast to these models, the attentional cues and memory representations are learned entirely from the simple self-supervised task of predicting the next word.	da238bb002e0cca26e6827e6914e0d4ecb099442	@['JournalArticle', 'Review']{ryu-lewis-2021-accounting,  author = {S. Ryu and Richard L. Lewis},  booktitle = {Workshop on Cognitive Modeling and Computational Linguistics},  journal = {ArXiv},  title = {Accounting for Agreement Phenomena in Sentence Comprehension with Transformer Language Models: Effects of Similarity-based Interference on Surprisal and Attention},  volume = {abs/2104.12874},  year = {2021} }
Abstraction not Memory: BERT and the English Article System	2022	http://www.semanticscholar.org/paper/a8d5fd996cfdb7dc2202a0df990b53e9b0ffece9		maybe	0	Article prediction is a task that has long defied accurate linguistic description. As such, this task is ideally suited to evaluate models on their ability to emulate native-speaker intuition. To this end, we compare the performance of native English speakers and pre-trained models on the task of article prediction set up as a three way choice (a/an, the, zero). Our experiments with BERT show that BERT outperforms humans on this task across all articles. In particular, BERT is far superior to humans at detecting the zero article, possibly because we insert them using rules that the deep neural model can easily pick up. More interestingly, we find that BERT tends to agree more with annotators than with the corpus when inter-annotator agreement is high but switches to agreeing more with the corpus as inter-annotator agreement drops. We contend that this alignment with annotators, despite being trained on the corpus, suggests that BERT is not memorising article use, but captures a high level generalisation of article use akin to human intuition.	a8d5fd996cfdb7dc2202a0df990b53e9b0ffece9	@['JournalArticle', 'Conference']{madabushi-etal-2022-abstraction,  author = {Harish Tayyar Madabushi and Dagmar Divjak and P. Milin},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {924-931},  title = {Abstraction not Memory: BERT and the English Article System},  year = {2022} }
About Time: Do Transformers Learn Temporal Verbal Aspect?	2022	http://www.semanticscholar.org/paper/201d7a247f7bc4b4c7826edd2c8cbe31199b3e62		yes	2	Aspect is a linguistic concept that describes how an action, event, or state of a verb phrase is situated in time. In this paper, we explore whether different transformer models are capable of identifying aspectual features. We focus on two specific aspectual features: telicity and duration. Telicity marks whether the verb’s action or state has an endpoint or not (telic/atelic), and duration denotes whether a verb expresses an action (dynamic) or a state (stative). These features are integral to the interpretation of natural language, but also hard to annotate and identify with NLP methods. We perform experiments in English and French, and our results show that transformer models adequately capture information on telicity and duration in their vectors, even in their non-finetuned forms, but are somewhat biased with regard to verb tense and word order.	201d7a247f7bc4b4c7826edd2c8cbe31199b3e62	@None{metheniti-etal-2022-about,  author = {Eleni (Lena) Metheniti and Tim Van de Cruys and Nabil Hathout},  booktitle = {Workshop on Cognitive Modeling and Computational Linguistics},  journal = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},  title = {About Time: Do Transformers Learn Temporal Verbal Aspect?},  year = {2022} }
Abductive Commonsense Reasoning	2019	http://www.semanticscholar.org/paper/a550f576ff20b8cce98f3ddad0043d3783fbc9b4	This study introduces a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations, and conceptualizes two new tasks -- Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and Abduction NLG: a conditional generation task for explaining given observations in natural language.	maybe	247	Abductive reasoning is inference to the most plausible explanation. For example, if Jenny finds her house in a mess when she returns from work, and remembers that she left a window open, she can hypothesize that a thief broke into her house and caused the mess, as the most plausible explanation. While abduction has long been considered to be at the core of how people interpret and read between the lines in natural language (Hobbs et al., 1988), there has been relatively little research in support of abductive natural language inference and generation. We present the first study that investigates the viability of language-based abductive reasoning. We introduce a challenge dataset, ART, that consists of over 20k commonsense narrative contexts and 200k explanations. Based on this dataset, we conceptualize two new tasks -- (i) Abductive NLI: a multiple-choice question answering task for choosing the more likely explanation, and (ii) Abductive NLG: a conditional generation task for explaining given observations in natural language. On Abductive NLI, the best model achieves 68.9% accuracy, well below human performance of 91.4%. On Abductive NLG, the current best language generators struggle even more, as they lack reasoning capabilities that are trivial for humans. Our analysis leads to new insights into the types of reasoning that deep pre-trained language models fail to perform--despite their strong performance on the related but more narrowly defined task of entailment NLI--pointing to interesting avenues for future research.	a550f576ff20b8cce98f3ddad0043d3783fbc9b4	@['JournalArticle']{bhagavatula-etal-2019-abductive,  author = {Chandra Bhagavatula and Ronan Le Bras and Chaitanya Malaviya and Keisuke Sakaguchi and Ari Holtzman and Hannah Rashkin and Doug Downey and S. Yih and Yejin Choi},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {Abductive Commonsense Reasoning},  volume = {abs/1908.05739},  year = {2019} }
A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/37e8151365c93578e6d645b27377ebb0414b22ed	This paper conducts large-scale experiments with the pre-trained BERT model on three natural language understanding (NLU) tasks, showing that BERT does contain sparse and robust subnetworks (SRNets) within certain sparsity constraint and exploring the upper bound of SRNets by making use of the OOD information, which reveals that there exist sparse and almost unbiased subnets.	maybe	0	Despite the remarkable success of pre-trained language models (PLMs), they still face two challenges: First, large-scale PLMs are inefﬁcient in terms of memory footprint and computation. Second, on the downstream tasks, PLMs tend to rely on the dataset bias and struggle to generalize to out-of-distribution (OOD) data. In response to the efﬁciency problem, recent studies show that dense PLMs can be replaced with sparse subnetworks without hurting the performance. Such subnetworks can be found in three scenarios: 1) the ﬁne-tuned PLMs, 2) the raw PLMs and then ﬁne-tuned in isolation, and even inside 3) PLMs without any parameter ﬁne-tuning. However, these results are only obtained in the in-distribution (ID) setting. In this paper, we extend the study on PLMs subnetworks to the OOD setting, investigating whether sparsity and robustness to dataset bias can be achieved simultaneously. To this end, we conduct extensive experiments with the pre-trained BERT model on three natural language understanding (NLU) tasks. Our results demonstrate that sparse and robust subnetworks (SRNets) can consistently be found in BERT , across the aforementioned three scenarios, using different training and compression methods. Furthermore, we explore the upper bound of SRNets using the OOD information and show that there exist sparse and almost unbiased BERT subnetworks . Finally, we present 1) an analytical study that provides insights on how to promote the efﬁciency of SRNets searching process and 2) a solution to improve subnetworks’ performance at high sparsity. The code is available at https://github.com/llyx97/sparse-and-robust-PLM . PLMs, the problems of robustness and efﬁciency simultaneously. by this, we the study on PLM subnetwork to the OOD scenario, investigating whether there exist PLM subnetworks that are both sparse and robust against dataset bias? To this question, we conduct large-scale experiments with the pre-trained BERT model on three natural language understanding (NLU) tasks that are widely-studied in the question of dataset bias. We consider a variety of setups including the three pruning and ﬁne-tuning paradigms, standard and debiasing training objectives, different model pruning methods, and different variants of PLMs from the BERT family. Our results show that BERT does contain sparse and robust subnetworks (SRNets) within certain sparsity constraint (e.g., less than 70%), giving afﬁrmative answer to the above question. Compared with a standard ﬁne-tuned SRNets exhibit comparable ID performance and remarkable OOD improvement. When it to BERT model ﬁne-tuned with debiasing method, SRNets can preserve the full model’s ID and OOD performance with much fewer parameters. On this basis, we further explore the upper bound of SRNets by making use of the OOD information, which reveals that there exist sparse and almost unbiased subnetworks, even in a standard ﬁne-tuned BERT that is biased . Regardless of the intriguing properties of SRNets, we ﬁnd that the subnetwork searching process still have room for improvement, based on some observations from the above experiments. First, we study the timing to start searching SRNets during full BERT ﬁne-tuning, and ﬁnd that the entire training and searching cost can be reduced from this perspective. Second, we reﬁne the mask training method with gradual sparsity increase, which is quite effective in identifying SRNets at high sparsity.	37e8151365c93578e6d645b27377ebb0414b22ed	@['JournalArticle']{liu-etal-2022-a,  author = {Yuanxin Liu and Fandong Meng and Zheng Lin and JiangNan Li and Peng Fu and Yanan Cao and Weiping Wang and Jie Zhou},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Win-win Deal: Towards Sparse and Robust Pre-trained Language Models},  volume = {abs/2210.05211},  year = {2022} }
A Watermark for Large Language Models	2023	https://www.semanticscholar.org/paper/de352871a2fb75fc49e4c469d21e92c33701b28d	A statistical test for detecting the watermark with interpretable p-values is proposed, and an information-theoretic framework for analyzing the sensitivity of the watermarks is derived.	maybe	2	Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of “whitelist” tokens before a word is generated, and then softly promoting use of whitelist tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multibillion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.	de352871a2fb75fc49e4c469d21e92c33701b28d	@['JournalArticle']{kirchenbauer-etal-2023-a,  author = {John Kirchenbauer and Jonas Geiping and Yuxin Wen and Jonathan Katz and Ian Miers and T. Goldstein},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Watermark for Large Language Models},  volume = {abs/2301.10226},  year = {2023} }
A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks	2022	http://www.semanticscholar.org/paper/e466852cfeb09941b18d9510de7c4e87e01405bf	Three practical scenarios in which attackers release datasets, pre-trained models, and ﬁne-tuned models respectively are categorized, then their unique evaluation methodologies are discussed, and an open-source toolkit is developed to foster the implementations and evaluations of textual backdoor learning.	maybe	2	Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predeﬁned triggers. As various attack and defense models have been proposed, it is of great signiﬁcance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires speciﬁc evaluation protocols; (2) The evaluation metrics only consider whether the attacks could ﬂip the models’ predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and ﬁne-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor 3 to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.	e466852cfeb09941b18d9510de7c4e87e01405bf	@['JournalArticle']{cui-etal-2022-a,  author = {Ganqu Cui and Lifan Yuan and Bingxiang He and Yangyi Chen and Zhiyuan Liu and Maosong Sun},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks},  volume = {abs/2206.08514},  year = {2022} }
A Theoretical Framework for AI Models Explainability	2022	http://www.semanticscholar.org/paper/aa4f29fa03b8b1fd083786aa08fb3307a149fb40	This work proposes a novelition of explanation that is a synthesis of what can be found in the literature and simplifies how these properties are operationalized and it provides new insight into common explanation methods that are analyzed as case studies.	maybe	0	EXplainable Artiﬁcial Intelligence (XAI) is a vi-brant research topic in the artiﬁcial intelligence community, with growing interest across methods and domains. Much has been written about the subject, yet XAI still lacks shared terminology and a framework capable of providing structural sound-ness to explanations. In our work, we address these issues by proposing a novel deﬁnition of explanation that is a synthesis of what can be found in the literature. We recognize that explanations are not atomic but the combination of evidence stemming from the model and its input-output mapping, and the human interpretation of this evidence. Furthermore, we ﬁt explanations into the properties of faithfulness (i.e., the explanation being a true description of the model’s inner workings and decision-making process) and plausibility (i.e., how much the explanation looks convincing to the user). Using our proposed theoretical framework simpliﬁes how these properties are operationalized and it provides new insight into common explanation methods that we analyze as case studies.	aa4f29fa03b8b1fd083786aa08fb3307a149fb40	@['JournalArticle']{rizzo-etal-2022-a,  author = {Matteo Rizzo and Alberto Veneri and A. Albarelli and C. Lucchese and C. Conati},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Theoretical Framework for AI Models Explainability},  volume = {abs/2212.14447},  year = {2022} }
A Tale of a Probe and a Parser	2020	http://www.semanticscholar.org/paper/f7ea2edb8df178d4200029be218507ed8170dfa3	To explore whether syntactic probes would do better to make use of existing techniques, this work compares the structural probe to a more traditional parser with an identical lightweight parameterisation.	maybe	40	Measuring what linguistic information is encoded in neural models of language has become popular in NLP. Researchers approach this enterprise by training “probes”—supervised models designed to extract linguistic structure from another model’s output. One such probe is the structural probe (Hewitt and Manning, 2019), designed to quantify the extent to which syntactic information is encoded in contextualised word representations. The structural probe has a novel design, unattested in the parsing literature, the precise benefit of which is not immediately obvious. To explore whether syntactic probes would do better to make use of existing techniques, we compare the structural probe to a more traditional parser with an identical lightweight parameterisation. The parser outperforms structural probe on UUAS in seven of nine analysed languages, often by a substantial amount (e.g. by 11.1 points in English). Under a second less common metric, however, there is the opposite trend—the structural probe outperforms the parser. This begs the question: which metric should we prefer?	f7ea2edb8df178d4200029be218507ed8170dfa3	@['JournalArticle', 'Conference']{maudslay-etal-2020-a,  author = {R. Maudslay and Josef Valvoda and Tiago Pimentel and Adina Williams and Ryan Cotterell},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {7389-7395},  title = {A Tale of a Probe and a Parser},  year = {2020} }
A Systematic Review of Reproducibility Research in Natural Language Processing	2021	http://www.semanticscholar.org/paper/59e7ed6132ce9992a6790a0a179b9eed73959780	This work aims to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP.	maybe	19	Against the background of what has been termed a reproducibility crisis in science, the NLP field is becoming increasingly interested in, and conscientious about, the reproducibility of its results. The past few years have seen an impressive range of new initiatives, events and active research in the area. However, the field is far from reaching a consensus about how reproducibility should be defined, measured and addressed, with diversity of views currently increasing rather than converging. With this focused contribution, we aim to provide a wide-angle, and as near as possible complete, snapshot of current work on reproducibility in NLP,	59e7ed6132ce9992a6790a0a179b9eed73959780	@['JournalArticle', 'Conference', 'Review']{belz-etal-2021-a,  author = {Anya Belz and Shubham Agarwal and Anastasia Shimorina and Ehud Reiter},  booktitle = {Conference of the European Chapter of the Association for Computational Linguistics},  journal = {ArXiv},  title = {A Systematic Review of Reproducibility Research in Natural Language Processing},  volume = {abs/2103.07929},  year = {2021} }
A Systematic Investigation of Commonsense Understanding in Large Language Models	2021	http://www.semanticscholar.org/paper/d52302d302499cfd3430508b61b0f50fe7fce90c	It is found that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in the authors' benchmarks, and that leveraging explicit commonsense knowledge does not yield substantial improvement.	yes	5	Large language models (e.g., Brown et al., 2020; Patwary et al., 2021) have shown impressive performance on many natural language processing (NLP) tasks in a zero-shot setting. We ask whether these models exhibit commonsense understanding – a critical component of NLP applications – by evaluating models against four commonsense benchmarks. We find that the impressive zeroshot performance of large language models is mostly due to existence of dataset bias in our benchmarks. We also show that the zeroshot performance is sensitive to the choice of hyper-parameters and similarity of the benchmark to the pre-training datasets. Moreover, we did not observe substantial improvements when evaluating models in a few-shot setting. Finally, in contrast to previous work, we find that leveraging explicit commonsense knowledge does not yield substantial improvement.	d52302d302499cfd3430508b61b0f50fe7fce90c	@['JournalArticle']{li-etal-2021-a,  author = {Xiang Li and A. Kuncoro and Cyprien de Masson d'Autume and P. Blunsom and Aida Nematzadeh},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Systematic Investigation of Commonsense Understanding in Large Language Models},  volume = {abs/2111.00607},  year = {2021} }
A Systematic Assessment of Syntactic Generalization in Neural Language Models	2020	http://www.semanticscholar.org/paper/427973cbf535187c95cd174adce64c20292a0c78	A systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites finds substantial differences in syntactic generalization performance by model architecture.	yes	99	While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M-40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.	427973cbf535187c95cd174adce64c20292a0c78	@['JournalArticle', 'Conference']{hu-etal-2020-a,  author = {Jennifer Hu and Jon Gauthier and Peng Qian and Ethan Gotlieb Wilcox and R. Levy},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},  volume = {abs/2005.03692},  year = {2020} }
A Systematic Analysis of Morphological Content in BERT Models for Multiple Languages	2020	http://www.semanticscholar.org/paper/854fa0e723c5933c62eeddb2a8b1dbee08f0d679	The experiments contained herein show that transformer architectures largely partition their embedding space into convex sub-regions highly correlated with morphological feature value, and the contextualized nature of transformer embeddings allows models to distinguish ambiguous morphological forms in many, but not all cases.	maybe	22	This work describes experiments which probe the hidden representations of several BERT-style models for morphological content. The goal is to examine the extent to which discrete linguistic structure, in the form of morphological features and feature values, presents itself in the vector representations and attention distributions of pre-trained language models for five European languages. The experiments contained herein show that (i) Transformer architectures largely partition their embedding space into convex sub-regions highly correlated with morphological feature value, (ii) the contextualized nature of transformer embeddings allows models to distinguish ambiguous morphological forms in many, but not all cases, and (iii) very specific attention head/layer combinations appear to hone in on subject-verb agreement.	854fa0e723c5933c62eeddb2a8b1dbee08f0d679	@['JournalArticle']{edmiston-2020-a,  author = {Daniel Edmiston},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Systematic Analysis of Morphological Content in BERT Models for Multiple Languages},  volume = {abs/2004.03032},  year = {2020} }
A Survey of the State of Explainable AI for Natural Language Processing	2020	http://www.semanticscholar.org/paper/4b322cf280f459deb6d9e2eb2430d1a28141934c	The operations and explainability techniques currently available for generating explanations for NLP model predictions are detailed to serve as a resource for model developers in the community and to point out the current gaps.	maybe	134	Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.	4b322cf280f459deb6d9e2eb2430d1a28141934c	@['JournalArticle', 'Review']{danilevsky-etal-2020-a,  author = {Marina Danilevsky and Kun Qian and R. Aharonov and Yannis Katsis and B. Kawas and P. Sen},  booktitle = {AACL},  pages = {447-459},  title = {A Survey of the State of Explainable AI for Natural Language Processing},  year = {2020} }
A Survey of Pretrained Language Models Based Text Generation	2022	http://www.semanticscholar.org/paper/b42e3a759348f27cca2f918a6bd0b139a5312e44	This survey presents the recent advances achieved in the topic of PLMs for text generation and introduces three key points of applying PLMs to text generation: how to encode the input data as representations preserving input semantics which can be fused into PLMs.	maybe	12	Text Generation aims to produce plausible and readable text in human language from input data. The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). Grounding text generation on PLMs is seen as a promising direction in both academia and industry. In this survey, we present the recent advances achieved in the topic of PLMs for text generation. In detail, we begin with introducing three key points of applying PLMs to text generation: 1) how to encode the input data as representations preserving input semantics which can be fused into PLMs; 2) how to design a universal and performant architecture of PLMs served as generation models; and 3) how to optimize PLMs given the reference text and ensure the generated text satisfying special text properties. Then, we figure out several challenges and future directions within each key point. Next, we present a summary of various useful resources and typical text generation applications to work with PLMs. Finally, we conclude and summarize the contribution of this survey.	b42e3a759348f27cca2f918a6bd0b139a5312e44	@['JournalArticle', 'Review']{li-etal-2022-a,  author = {Junyi Li and Tianyi Tang and Wayne Xin Zhao and J. Nie and Ji-rong Wen},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Survey of Pretrained Language Models Based Text Generation},  volume = {abs/2201.05273},  year = {2022} }
A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/723fcade538f71df5fe5d1cde279686240f97b9f	This is the first survey paper to summarize CTG techniques from the perspective of PLMs, and it is hoped it can help researchers in related fields to quickly track the academic frontier, providing them with a landscape of the area and a roadmap for future research.	maybe	25	Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize CTG techniques from the perspective of PLMs. We hope it can help researchers in related fields to quickly track the academic frontier, providing them with a landscape of the area and a roadmap for future research.	723fcade538f71df5fe5d1cde279686240f97b9f	@['JournalArticle', 'Review']{zhang-etal-2022-a,  author = {Han Zhang and Haolin Song and Shaoyu Li and Ming Zhou and Dawei Song},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models},  volume = {abs/2201.05337},  year = {2022} }
A Survey for In-context Learning	2022	http://www.semanticscholar.org/paper/8aa98fbfb6f1e979dead13ce24075503fe47658e	The progress, challenges, and future work in ICL are summarized and a formal definition of ICL is presented and its correlation to related studies are clarified and potential directions for further research are provided.	maybe	0	With the increasing ability of large language models (LLMs), in-context learning (ICL) has become a new paradigm for natural language processing (NLP), where LLMs make predictions only based on contexts augmented with a few training examples. It has been a new trend exploring ICL to evaluate and extrapolate the ability of LLMs. In this paper, we aim to survey and summarize the progress, challenges, and future work in ICL. We ﬁrst present a formal deﬁnition of ICL and clarify its correlation to related studies. Then, we organize and discuss advanced techniques of ICL, including training strategies, prompting strategies, and so on. Finally, we present the challenges of ICL and provide potential directions for further research. We hope our work can encourage more research on uncovering how ICL works and improving ICL in future work. 1	8aa98fbfb6f1e979dead13ce24075503fe47658e	@['JournalArticle', 'Review']{dong-etal-2022-a,  author = {Qingxiu Dong and Lei Li and Damai Dai and Ce Zheng and Zhiyong Wu and Baobao Chang and Xu Sun and Jingjing Xu and Zhifang Sui},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Survey for In-context Learning},  volume = {abs/2301.00234},  year = {2022} }
A Study of the Attention Abnormality in Trojaned BERTs	2022	http://www.semanticscholar.org/paper/2b843ad9fa9baa7ac0e0ef43fa5e97533e6210ce	This paper is the first to analyze the Trojan mechanism and develop a Trojan detector based on the transformer’s attention, and proposes an attention-based Trojan detector to distinguish Trojaned models from clean ones.	maybe	4	Trojan attacks raise serious security concerns. In this paper, we investigate the underlying mechanism of Trojaned BERT models. We observe the attention focus drifting behavior of Trojaned models, i.e., when encountering an poisoned input, the trigger token hijacks the attention focus regardless of the context. We provide a thorough qualitative and quantitative analysis of this phenomenon, revealing insights into the Trojan mechanism. Based on the observation, we propose an attention-based Trojan detector to distinguish Trojaned models from clean ones. To the best of our knowledge, we are the first to analyze the Trojan mechanism and develop a Trojan detector based on the transformer’s attention.	2b843ad9fa9baa7ac0e0ef43fa5e97533e6210ce	@['JournalArticle', 'Conference']{lyu-etal-2022-a,  author = {Weimin Lyu and Songzhu Zheng and Teng Ma and Chao Chen},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4727-4741},  title = {A Study of the Attention Abnormality in Trojaned BERTs},  year = {2022} }
A Study of Implicit Bias in Pretrained Language Models against People with Disabilities	2022	http://www.semanticscholar.org/paper/c3765993112289a0355989f87842a5007425bcea	Using perturbation sensitivity analysis, an assortment of popular word embedding-based and transformer-based PLMs are tested and shown to have significant biases against people with disabilities, demonstrating how models trained on large corpora widely favor ableist language.	maybe	1	Pretrained language models (PLMs) have been shown to exhibit sociodemographic biases, such as against gender and race, raising concerns of downstream biases in language technologies. However, PLMs’ biases against people with disabilities (PWDs) have received little attention, in spite of their potential to cause similar harms. Using perturbation sensitivity analysis, we test an assortment of popular word embedding-based and transformer-based PLMs and show significant biases against PWDs in all of them. The results demonstrate how models trained on large corpora widely favor ableist language.	c3765993112289a0355989f87842a5007425bcea	@['JournalArticle', 'Conference']{venkit-etal-2022-a,  author = {Pranav Venkit and Mukund Srinath and Shomir Wilson},  booktitle = {International Conference on Computational Linguistics},  pages = {1324-1332},  title = {A Study of Implicit Bias in Pretrained Language Models against People with Disabilities},  year = {2022} }
A study of BERT’s processing of negations to determine sentiment	2021	http://www.semanticscholar.org/paper/866720f5a5f9b8c735300d191eff025741e78ece	Best-practice methods for training BERT to deal better with negations in sentiment analysis can be formulated.	maybe	1	The question considered in this paper is: can BERT e↵ectively distinguish the meaning of the following two sentences: ’BERT is capable of understanding negations ’ and ’BERT is not capable of understanding negations’? This work aims to fulfill the gap in the knowledge about BERT’s capacity to handle negations. The specific task under examination is sentiment analysis, where erroneous understanding of negations directly a↵ects the model’s performance by wrongly switching polarity of the detected sentiments. In order to determine what BERT ’understands’ from negated text, a model was trained and tested by using adversarial conditions. With four distinct configurations, handling negations was studied by interchanging negated sentences during training and testing. The results exposed that in three out of four cases, the BERT’s propensity to deal with negations by memorizing information in the large number of connections used by the model, instead of truly understanding the linguistic mechanism of negations. In the remaining case, the model’s performance suggested taking decisions based on random features without exposing clear reasoning. Based on these insights, best-practice methods for training BERT to deal better with negations in sentiment analysis can be formulated.	866720f5a5f9b8c735300d191eff025741e78ece	@None{tejada-etal-2021-a,  author = {Giorgia Nidia Carranza Tejada and J. Scholtes and Gerasimos and Spanakis},  title = {A study of BERT’s processing of negations to determine sentiment},  year = {2021} }
A Structural Probe for Finding Syntax in Word Representations	2019	https://www.semanticscholar.org/paper/455a8838cde44f288d456d01c76ede95b56dc675	A structural probe is proposed, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space, and shows that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax Trees are embedded implicitly in deep models’ vector geometry.	seed	712	Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network’s word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models’ vector geometry.	455a8838cde44f288d456d01c76ede95b56dc675	@['JournalArticle', 'Conference']{hewitt-manning-2019-a,  author = {John Hewitt and Christopher D. Manning},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {4129-4138},  title = {A Structural Probe for Finding Syntax in Word Representations},  year = {2019} }
A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing	2022	http://www.semanticscholar.org/paper/b5dca588dc5006feef49781ac82763b40788ce04	It is argued that the community should stop using rank correlation as an evaluation metric for attention-based explanations and instead test various explanation methods and employ a human-in-the-loop process to determine if the explanations align with human intuition for the particular use case at hand.	maybe	1	. There has been signiﬁcant debate in the NLP community about whether or not attention weights can be used as an explanation – a mechanism for interpreting how important each input token is for a particular prediction. The validity of “attention as explanation” has so far been evaluated by computing the rank correlation between attention-based explanations and existing feature attribution expla- nations using LSTM-based models. In our work, we (i) compare the rank correlation between ﬁve more recent feature attribution methods and two attention-based methods, on two types of NLP tasks, and (ii) extend this analysis to also include transformer-based models. We ﬁnd that attention-based explanations do not corre- late strongly with any recent feature attribution methods, regardless of the model or task. Furthermore, we ﬁnd that none of the tested explanations correlate strongly with one another for the transformer-based model, leading us to question the underlying assumption that we should measure the validity of attention-based explana- tions based on how well they correlate with existing feature attribution explanation methods. After conducting experiments on ﬁve datasets using two different models, we argue that the community should stop using rank correlation as an evaluation metric for attention-based explanations. We suggest that researchers and practition- ers should instead test various explanation methods and employ a human-in-the-loop process to determine if the explanations align with human intuition for the particular use case at hand.	b5dca588dc5006feef49781ac82763b40788ce04	@['JournalArticle']{neely-etal-2022-a,  author = {Michael Neely and Stefan F. Schouten and Maurits Bleeker and Ana Lucic},  booktitle = {HHAI},  pages = {60-78},  title = {A Song of (Dis)agreement: Evaluating the Evaluation of Explainable Artificial Intelligence in Natural Language Processing},  year = {2022} }
A Simple But Effective Bert Model for Dialog State Tracking on Resource-Limited Systems	2019	http://www.semanticscholar.org/paper/ec24a0731fb437323e90680e803533326b05c861	This work proposes a simple but effective DST model based on BERT that can operate in situations where the domain ontology may change dynamically, and makes the model small and fast enough for resource-restricted systems.	maybe	18	In a task-oriented dialog system, the goal of dialog state tracking (DST) is to monitor the state of the conversation from the dialog history. Recently, many deep learning based methods have been proposed for the task. Despite their impressive performance, current neural architectures for DST are typically heavily-engineered and conceptually complex, making it difficult to implement, debug, and maintain them in a production setting. In this work, we propose a simple but effective DST model based on BERT. In addition to its simplicity, our approach also has a number of other advantages: (a) the number of parameters does not grow with the ontology size (b) the model can operate in situations where the domain ontology may change dynamically. Experimental results demonstrate that our BERT-based model outperforms previous methods by a large margin, achieving new state-of-the-art results on the standard WoZ 2.0 dataset 1. Finally, to make the model small and fast enough for resource-restricted systems, we apply the knowledge distillation method to compress our model. The final compressed model achieves comparable results with the original model while being 8x smaller and 7x faster.	ec24a0731fb437323e90680e803533326b05c861	@['JournalArticle', 'Conference']{lai-etal-2019-a,  author = {T. Lai and Quan Hung Tran and Trung Bui and D. Kihara},  booktitle = {IEEE International Conference on Acoustics, Speech, and Signal Processing},  journal = {ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},  pages = {8034-8038},  title = {A Simple But Effective Bert Model for Dialog State Tracking on Resource-Limited Systems},  year = {2019} }
A S URVEY ON GPT-3	2022	http://www.semanticscholar.org/paper/c4f0e7c0d5b297e4c8bbdf6434ab73276b34005b	Both academic and commercial efforts applying GPT-3 in diverse domains such as developing conversational AI chatbots, software development, creative work, domain knowledge, and business productivity are surveyed.	maybe	0	This paper provides an introductory survey to GPT-3. We cover some of the historical development behind this technology, some of the key features of GPT-3, and discuss the machine learning model and the datasets used. We survey both academic and commercial efforts applying GPT-3 in diverse domains such as developing conversational AI chatbots, software development, creative work, domain knowledge, and business productivity. We discuss some of the challenges that GPT-3 faces such as the problems of training complexity, bias, and hallucination/incorrect answers. We also discuss the future research opportunities in this area.	c4f0e7c0d5b297e4c8bbdf6434ab73276b34005b	@['Review']{zong-krishnamachari-2022-a,  author = {M. Zong and Bhaskar Krishnamachari},  title = {A S URVEY ON GPT-3},  year = {2022} }
A Review on Language Models as Knowledge Bases	2022	http://www.semanticscholar.org/paper/cd471b5ef162906ef3d9a84398b3f98e9ee4bf56	This paper presents a set of aspects that it is deemed an LM should have to fully act as a KB, and reviews the recent literature with respect to those aspects.	maybe	12	Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufﬁciently large (web) corpus will encode a signiﬁcant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a ma-jor advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem an LM should have to fully act as a KB, and review the recent literature with respect to those aspects. 1	cd471b5ef162906ef3d9a84398b3f98e9ee4bf56	@['JournalArticle', 'Review']{alkhamissi-etal-2022-a,  author = {Badr AlKhamissi and Millicent Li and Asli Celikyilmaz and Mona T. Diab and Marjan Ghazvininejad},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Review on Language Models as Knowledge Bases},  volume = {abs/2204.06031},  year = {2022} }
A Report on the FigLang 2022 Shared Task on Understanding Figurative Language	2022	https://www.semanticscholar.org/paper/1a8ed324961a01c83e505f47da7112b443c4fd8b	The shared task challenged participants to build models that are able to not only predict the right label for a figurative NLI instance, but also generate a convincing free-text explanation.	maybe	0	We present the results of the Shared Task on Understanding Figurative Language that we conducted as a part of the 3rd Workshop on Figurative Language Processing (FigLang 2022) at EMNLP 2022. The shared task is based on the FLUTE dataset (Chakrabarty et al., 2022), which consists of NLI pairs containing figurative language along with free text explanations for each NLI instance. The task challenged participants to build models that are able to not only predict the right label for a figurative NLI instance, but also generate a convincing free-text explanation. The participants were able to significantly improve upon provided baselines in both automatic and human evaluation settings. We further summarize the submitted systems and discuss the evaluation results.	1a8ed324961a01c83e505f47da7112b443c4fd8b	@None{saakyan-etal-2022-a,  author = {A. Saakyan and Tuhin Chakrabarty and Debanjan Ghosh and S. Muresan},  booktitle = {FLP},  title = {A Report on the FigLang 2022 Shared Task on Understanding Figurative Language},  year = {2022} }
A Redhead Walks into a Bar: Experiences of Writing Fiction with Artificial Intelligence	2022	http://www.semanticscholar.org/paper/6d6ec79cd196b74d857c6426c1df1e94ede4be13		yes	0	Human creativity has been often aided and supported by artificial tools, spanning traditional tools such as ideation cards, pens, and paper, to computed and software. Tools for creativity are increasingly using artificial intelligence to not only support the creative process, but also to act upon the creation with a higher level of agency. This paper focuses on writing fiction as a creative activity and explores human-AI co-writing through a research product, which employs a natural language processing model, the Generative Pre-trained Transformer 3 (GPT-3), to assist the co-authoring of narrative fiction. We report on two progressive – not comparative – autoethnographic studies to attain our own creative practices in light of our engagement with the research product: (1) a co-writing activity initiated by basic textual prompts using basic elements of narrative and (2) a co-writing activity initiated by more advanced textual prompts using elements of narrative, including dialects and metaphors undertaken by one of the authors of this paper who has doctoral training in literature. In both studies, we quickly came up against the limitations of the system; then, we repositioned our goals and practices to maximize our chances of success. As a result, we discovered not only limitations but also hidden capabilities, which not only altered our creative practices and outcomes, but which began to change the ways we were relating to the AI as collaborator.	6d6ec79cd196b74d857c6426c1df1e94ede4be13	@['Book', 'JournalArticle']{ghajargar-etal-2022-a,  author = {Maliheh Ghajargar and Jeffrey Bardzell and Love Lagerkvist},  booktitle = {International Conference on Entertainment and Media in the Ubiquitous Era},  journal = {Proceedings of the 25th International Academic Mindtrek Conference},  title = {A Redhead Walks into a Bar: Experiences of Writing Fiction with Artificial Intelligence},  year = {2022} }
A Recipe for Arbitrary Text Style Transfer with Large Language Models	2021	http://www.semanticscholar.org/paper/7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd	Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as ‘make this melodramatic’ or ‘insert a metaphor’.	maybe	35	In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as ‘make this melodramatic’ or ‘insert a metaphor.’	7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd	@['JournalArticle', 'Conference']{reif-etal-2021-a,  author = {Emily Reif and Daphne Ippolito and Ann Yuan and Andy Coenen and Chris Callison-Burch and Jason Wei},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {A Recipe for Arbitrary Text Style Transfer with Large Language Models},  volume = {abs/2109.03910},  year = {2021} }
A Property Induction Framework for Neural Language Models	2022	http://www.semanticscholar.org/paper/1644147ac8dd06cfbfc3a0f87739d0c04e5d4681	A framework that uses neural-network language models (LMs) to perform property induction—a task in which humans generalize novel property knowledge from one or more concepts to others, suggesting the presence of a taxonomic bias in their representations is presented.	maybe	2	To what extent can experience from language contribute to our conceptual knowledge? Computational explorations of this question have shed light on the ability of powerful neural lan- guage models (LMs)—informed solely through text input— to encode and elicit information about concepts and proper- ties. To extend this line of research, we present a framework that uses neural-network language models (LMs) to perform property induction—a task in which humans generalize novel property knowledge ( has sesamoid bones ) from one or more concepts ( robins ) to others ( sparrows, canaries ). Patterns of property induction observed in humans have shed consider- able light on the nature and organization of human conceptual knowledge. Inspired by this insight, we use our framework to explore the property inductions of LMs, and ﬁnd that they show an inductive preference to generalize novel properties on the basis of category membership, suggesting the presence of a taxonomic bias in their representations.	1644147ac8dd06cfbfc3a0f87739d0c04e5d4681	@['JournalArticle']{misra-etal-2022-a,  author = {Kanishka Misra and J. Rayz and Allyson Ettinger},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Property Induction Framework for Neural Language Models},  volume = {abs/2205.06910},  year = {2022} }
A Primer in BERTology: What We Know About How BERT Works	2020	https://www.semanticscholar.org/paper/bd20069f5cac3e63083ecf6479abc1799db33ce0	This paper is the first survey of over 150 studies of the popular BERT model, reviewing the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression.	seed	756	Abstract Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.	bd20069f5cac3e63083ecf6479abc1799db33ce0	@['JournalArticle', 'Review']{rogers-etal-2020-a,  author = {Anna Rogers and Olga Kovaleva and Anna Rumshisky},  booktitle = {Transactions of the Association for Computational Linguistics},  journal = {Transactions of the Association for Computational Linguistics},  pages = {842-866},  title = {A Primer in BERTology: What We Know About How BERT Works},  volume = {8},  year = {2020} }
A Pragmatics-Centered Evaluation Framework for Natural Language Understanding	2022	http://www.semanticscholar.org/paper/a132268436ce0a8466e6f3920533e07c984dd5e6	It is shown that natural language inference, a widely used pretraining task, does not result in genuinely universal representations, which presents a new challenge for multi-task learning.	maybe	1	New models for natural language understanding have recently made an unparalleled amount of progress, which has led some researchers to suggest that the models induce universal text representations. However, current benchmarks are predominantly targeting semantic phenomena; we make the case that pragmatics needs to take center stage in the evaluation of natural language understanding. We introduce PragmEval, a new benchmark for the evaluation of natural language understanding, that unites 11 pragmatics-focused evaluation datasets for English. PragmEval can be used as supplementary training data in a multi-task learning setup, and is publicly available, alongside the code for gathering and preprocessing the datasets. Using our evaluation suite, we show that natural language inference, a widely used pretraining task, does not result in genuinely universal representations, which presents a new challenge for multi-task learning.	a132268436ce0a8466e6f3920533e07c984dd5e6	@['JournalArticle']{sileo-etal-2019-a,  author = {Damien Sileo and Philippe Muller and T. V. D. Cruys and Camille Pradel},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {2382-2394},  title = {A Pragmatics-Centered Evaluation Framework for Natural Language Understanding},  year = {2019} }
A Peek Into the Memory of T5: Investigating the Factual Knowledge Memory in a Closed-Book QA Setting and Finding Responsible Parts	2022	http://www.semanticscholar.org/paper/848c26b25cf199ea13ebdd632fd108bbd816aa0a	It is demonstrated that some model parts are more important than others in retaining knowledge through a series of pruning experiments and the roles of encoder and decoder in a closed-book setting are investigated.	maybe	0	Recent research shows that Transformer-based language models (LMs) store con-siderable factual knowledge from the unstructured text datasets on which they are pre-trained. The existence and amount of such knowledge have been investigated by probing pre-trained Transformers to answer questions without accessing any external context or knowledge (also called closed-book question answering (QA)). However, this factual knowledge is spread over the parameters inexplicably. The parts of the model most responsible for ﬁnding an answer only from a question are unclear. This study aims to understand which parts are responsible for the Transformer-based T5 reaching an answer in a closed-book QA setting. Furthermore, we introduce a head importance scoring method and compare it with other methods on three datasets. We investigate important parts by looking inside the attention heads in a novel manner. We also investigate why some heads are more critical than others and suggest a good identiﬁcation approach. We demonstrate that some model parts are more important than others in retaining knowledge through a series of pruning experiments. We also investigate the roles of encoder and decoder in a closed-book setting.	848c26b25cf199ea13ebdd632fd108bbd816aa0a	@None{alkhaldi-etal-2022-a,  author = {Tareq Alkhaldi and Chenhui Chu and S. Kurohashi},  booktitle = {Journal of Natural Language Processing},  journal = {Journal of Natural Language Processing},  title = {A Peek Into the Memory of T5: Investigating the Factual Knowledge Memory in a Closed-Book QA Setting and Finding Responsible Parts},  year = {2022} }
A Paradigm Shift from “Human Writing” to “Machine Generation” in Personality Test Development: an Application of State-of-the-Art Natural Language Processing	2022	http://www.semanticscholar.org/paper/5cd331535ab747dc40c874b421040b223057cab3		maybe	0		5cd331535ab747dc40c874b421040b223057cab3	@None{lee-etal-2022-a,  author = {Philseok Lee and Shea Fyffe and Mi-Kyung Son and Zihao Jia and Ziyu Yao},  booktitle = {Journal of business and psychology},  journal = {Journal of Business and Psychology},  pages = {163-190},  title = {A Paradigm Shift from “Human Writing” to “Machine Generation” in Personality Test Development: an Application of State-of-the-Art Natural Language Processing},  volume = {38},  year = {2022} }
A Novel Deep Learning Approach Using Contextual Embeddings for Toponym Resolution	2021	http://www.semanticscholar.org/paper/6247e711e2c9a9e743bfd78eac6e1393ee1f9374	This article describes a novel approach for toponym resolution with deep neural networks that does not involve matching references in the text against entries in a gazetteer, instead directly predicting geo-spatial coordinates.	maybe	3	This article describes a novel approach for toponym resolution with deep neural networks. The proposed approach does not involve matching references in the text against entries in a gazetteer, instead directly predicting geo-spatial coordinates. Multiple inputs are considered in the neural network architecture (e.g., the surrounding words are considered in combination with the toponym to disambiguate), using pre-trained contextual word embeddings (i.e., ELMo or BERT) as well as bi-directional Long Short-Term Memory units, which are both regularly used for modeling textual data. The intermediate representations are then used to predict a probability distribution over possible geo-spatial regions, and finally to predict the coordinates for the input toponym. The proposed model was tested on three datasets used on previous toponym resolution studies, specifically the (i) War of the Rebellion, (ii) Local–Global Lexicon, and (iii) SpatialML corpora. Moreover, we evaluated the effect of using (i) geophysical terrain properties as external information, including information on elevation or terrain development, among others, and (ii) additional data collected from Wikipedia articles, to further help with the training of the model. The obtained results show improvements using the proposed method, when compared to previous approaches, and specifically when BERT embeddings and additional data are involved.	6247e711e2c9a9e743bfd78eac6e1393ee1f9374	@['JournalArticle']{cardoso-etal-2021-a,  author = {Ana Bárbara Cardoso and Bruno Martins and J. Estima},  booktitle = {ISPRS Int. J. Geo Inf.},  journal = {ISPRS Int. J. Geo Inf.},  pages = {28},  title = {A Novel Deep Learning Approach Using Contextual Embeddings for Toponym Resolution},  volume = {11},  year = {2021} }
A Non-Linear Structural Probe	2021	http://www.semanticscholar.org/paper/380ed27cad8475c70eff7508f0ef4bc962fad0d3	This work examines the case of a structural probe (Hewitt and Manning, 2019), and finds that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly.	maybe	11	Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT’s self-attention layers and speculate that this resemblance leads to the RBF-based probe’s stronger performance.	380ed27cad8475c70eff7508f0ef4bc962fad0d3	@['JournalArticle', 'Conference']{white-etal-2021-a,  author = {Jennifer C. White and Tiago Pimentel and Naomi Saphra and Ryan Cotterell},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {132-138},  title = {A Non-Linear Structural Probe},  year = {2021} }
A Neural Scaling Law from the Dimension of the Data Manifold	2020	http://www.semanticscholar.org/paper/2c5a8950cf0a13e229ad19093ba064495fda8de7	This work confirms the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, and test the theory with CNN image classifiers on several datasets and with GPT-type language models.	maybe	28	When data is plentiful, the loss achieved by well-trained neural networks scales as a power-law $L \propto N^{-\alpha}$ in the number of network parameters $N$. This empirical scaling law holds for a wide variety of data modalities, and may persist over many orders of magnitude. The scaling law can be explained if neural models are effectively just performing regression on a data manifold of intrinsic dimension $d$. This simple theory predicts that the scaling exponents $\alpha \approx 4/d$ for cross-entropy and mean-squared error losses. We confirm the theory by independently measuring the intrinsic dimension and the scaling exponents in a teacher/student framework, where we can study a variety of $d$ and $\alpha$ by dialing the properties of random teacher networks. We also test the theory with CNN image classifiers on several datasets and with GPT-type language models.	2c5a8950cf0a13e229ad19093ba064495fda8de7	@['JournalArticle']{sharma-kaplan-2020-a,  author = {Utkarsh Sharma and J. Kaplan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Neural Scaling Law from the Dimension of the Data Manifold},  volume = {abs/2004.10802},  year = {2020} }
A Natural Bias for Language Generation Models	2022	http://www.semanticscholar.org/paper/1fa6f4ba48cbd85fc6dbb8d1c3300760419151a8	Here, it is shown that one can effectively endow a standard neural language generation model with a separate module that reﬂects unigram frequency statistics as prior knowledge, encouraging the model to specialise in non-frequency-related aspects of language.	maybe	0	After just a few hundred training updates, a standard probabilistic model for language generation has likely not yet learnt many semantic or syntactic rules of natural language, which inherently makes it difﬁcult to estimate the right probability distribution over next tokens. Yet around this point, these models have identiﬁed a simple, loss-minimising behaviour: to output the unigram distribution of the target training corpus. The use of such a crude heuristic raises the question: Rather than wasting precious compute resources and model capacity for learning this strategy at early training stages, can we initialise our models with this behaviour? Here, we show that we can effectively endow our model with a separate module that reﬂects unigram frequency statistics as prior knowledge . Standard neural language generation architectures offer a natural opportunity for implementing this idea: by initialising the bias term in a model’s ﬁnal linear layer with the log-unigram distribution. Experiments in neural machine translation demonstrate that this simple technique: (i) improves learning efﬁciency; (ii) achieves better overall performance; and (iii) appears to disentangle strong frequency effects, encouraging the model to specialise in non-frequency-related aspects of language.	1fa6f4ba48cbd85fc6dbb8d1c3300760419151a8	@['JournalArticle']{meister-etal-2022-a,  author = {Clara Meister and Wojciech Stokowiec and Tiago Pimentel and Lei Yu and Laura Rimell and A. Kuncoro},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Natural Bias for Language Generation Models},  volume = {abs/2212.09686},  year = {2022} }
A Mutual Information Maximization Perspective of Language Representation Learning	2019	https://www.semanticscholar.org/paper/b04889922aae7f799affb2ae6508bc5f5c989567	This work shows state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence).	seed	107	We show state-of-the-art word representation learning methods maximize an objective function that is a lower bound on the mutual information between different parts of a word sequence (i.e., a sentence). Our formulation provides an alternative perspective that unifies classical word embedding models (e.g., Skip-gram) and modern contextual embeddings (e.g., BERT, XLNet). In addition to enhancing our theoretical understanding of these methods, our derivation leads to a principled framework that can be used to construct new self-supervised tasks. We provide an example by drawing inspirations from related methods based on mutual information maximization that have been successful in computer vision, and introduce a simple self-supervised objective that maximizes the mutual information between a global sentence representation and n-grams in the sentence. Our analysis offers a holistic view of representation learning methods to transfer knowledge and translate progress across multiple domains (e.g., natural language processing, computer vision, audio processing).	b04889922aae7f799affb2ae6508bc5f5c989567	@['JournalArticle']{kong-etal-2019-a,  author = {Lingpeng Kong and Cyprien de Masson d'Autume and Wang Ling and Lei Yu and Zihang Dai and Dani Yogatama},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {A Mutual Information Maximization Perspective of Language Representation Learning},  volume = {abs/1910.08350},  year = {2019} }
A Multiscale Visualization of Attention in the Transformer Model	2019	http://www.semanticscholar.org/paper/0de0a44b859a3719d11834479112314b4caba669	An open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism in the Transformer model is introduced.	maybe	326	The Transformer is a sequence model that forgoes traditional recurrent architectures in favor of a fully attention-based approach. Besides improving performance, an advantage of using attention is that it can also help to interpret a model by showing how the model assigns weight to different input elements. However, the multi-layer, multi-head attention mechanism in the Transformer model can be difficult to decipher. To make the model more accessible, we introduce an open-source tool that visualizes attention at multiple scales, each of which provides a unique perspective on the attention mechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three example use cases: detecting model bias, locating relevant attention heads, and linking neurons to model behavior.	0de0a44b859a3719d11834479112314b4caba669	@['JournalArticle', 'Conference']{vig-2019-a,  author = {Jesse Vig},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  journal = {ArXiv},  title = {A Multiscale Visualization of Attention in the Transformer Model},  volume = {abs/1906.05714},  year = {2019} }
A multilabel approach to morphosyntactic probing	2021	http://www.semanticscholar.org/paper/b4ab9b198ed140928751143af68c6b45adf396ca	It is shown that multilingual BERT renders many morphosyntactic features easily and simultaneously extractable (e.g., gender, grammatical case, pronominal type) and has the added benefit of revealing the linguistic properties that language models recognize as being shared across languages.	maybe	2	We propose using a multilabel probing task to assess the morphosyntactic representations of multilingual word embeddings. This tweak on canonical probing makes it easy to explore morphosyntactic representations, both holistically and at the level of individual features (e.g., gender, number, case), and leads more naturally to the study of how language models handle co-occurring features (e.g., agreement phenomena). We demonstrate this task with multilingual BERT (Devlin et al., 2018), training probes for seven typologically diverse languages: Afrikaans, Croatian, Finnish, Hebrew, Korean, Spanish, and Turkish. Through this simple but robust paradigm, we verify that multilingual BERT renders many morphosyntactic features simultaneously extractable. We further evaluate the probes on six held-out languages: Arabic, Chinese, Marathi, Slovenian, Tagalog, and Yorùbá. This zero-shot style of probing has the added beneﬁt of revealing which cross-linguistic properties a language model recognizes as being shared by multiple languages.	b4ab9b198ed140928751143af68c6b45adf396ca	@['JournalArticle', 'Conference']{shapiro-etal-2021-a,  author = {Naomi Tachikawa Shapiro and Amandalynne Paullada and Shane Steinert-Threlkeld},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {4486-4524},  title = {A multilabel approach to morphosyntactic probing},  year = {2021} }
A Matter of Framing: The Impact of Linguistic Formalism on Probing Results	2020	https://www.semanticscholar.org/paper/a160dbe78b0546679ec8a3140b3cf4614e3cc485	An in-depth cross-formalism layer probing study in role semantics finds linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrates that layer probing can detect subtle differences between the implementations of the same linguistic formalism.	seed	15	Deep pre-trained contextualized encoders like BERT (Delvin et al., 2019) demonstrate remarkable performance on a range of downstream tasks. A recent line of research in probing investigates the linguistic knowledge implicitly learned by these models during pre-training. While most work in probing operates on the task level, linguistic tasks are rarely uniform and can be represented in a variety of formalisms. Any linguistics-based probing study thereby inevitably commits to the formalism used to annotate the underlying data. Can the choice of formalism affect probing results? To investigate, we conduct an in-depth cross-formalism layer probing study in role semantics. We find linguistically meaningful differences in the encoding of semantic role- and proto-role information by BERT depending on the formalism and demonstrate that layer probing can detect subtle differences between the implementations of the same linguistic formalism. Our results suggest that linguistic formalism is an important dimension in probing studies, along with the commonly used cross-task and cross-lingual experimental settings.	a160dbe78b0546679ec8a3140b3cf4614e3cc485	@['JournalArticle', 'Conference']{kuznetsov-gurevych-2020-a,  author = {Ilia Kuznetsov and Iryna Gurevych},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {A Matter of Framing: The Impact of Linguistic Formalism on Probing Results},  volume = {abs/2004.14999},  year = {2020} }
A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks	2020	http://www.semanticscholar.org/paper/01400290c7db96c4d665d1c29519c42ba47401e0	This paper hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task, and analyzes properties of the cross-entropy objective to show that $\epsilon$-optimal language models in cross-ENTropy (log-perplexity) learn features that are $\mathcal{O}(\sqrt{\ep silon})$-good on natural linear classification tasks, demonstrating	maybe	39	Autoregressive language models pretrained on large corpora have been successful at solving downstream tasks, even with zero-shot usage. However, there is little theoretical justification for their success. This paper considers the following questions: (1) Why should learning the distribution of natural language help with downstream classification tasks? (2) Why do features learned using language modeling help solve downstream tasks with linear classifiers? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as next word prediction tasks, thus making language modeling a meaningful pretraining task. For (2), we analyze properties of the cross-entropy objective to show that $\epsilon$-optimal language models in cross-entropy (log-perplexity) learn features that are $\mathcal{O}(\sqrt{\epsilon})$-good on natural linear classification tasks, thus demonstrating mathematically that doing well on language modeling can be beneficial for downstream tasks. We perform experiments to verify assumptions and validate theoretical results. Our theoretical insights motivate a simple alternative to the cross-entropy objective that performs well on some linear classification tasks.	01400290c7db96c4d665d1c29519c42ba47401e0	@['JournalArticle']{saunshi-etal-2020-a,  author = {Nikunj Saunshi and Sadhika Malladi and Sanjeev Arora},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {A Mathematical Exploration of Why Language Models Help Solve Downstream Tasks},  volume = {abs/2010.03648},  year = {2020} }
A Latent-Variable Model for Intrinsic Probing	2022	http://www.semanticscholar.org/paper/90c6a48dd32f91b08fdf5a0cf9a37a54799229b1	This work proposes a novel latent-variable formulation for con-structing intrinsic probes and derives a tractable variational approximation to the log-likelihood and empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax.	maybe	2	The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. In-deed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization. In this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute, but also to pinpoint where this attribute is encoded. We propose a novel latent-variable formulation for con-structing intrinsic probes and derive a tractable variational approximation to the log-likelihood. Our results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature. Finally, we ﬁnd empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax.	90c6a48dd32f91b08fdf5a0cf9a37a54799229b1	@['JournalArticle']{stańczak-etal-2022-a,  author = {Karolina Stańczak and Lucas Torroba Hennigen and Adina Williams and Ryan Cotterell and Isabelle Augenstein},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Latent-Variable Model for Intrinsic Probing},  volume = {abs/2201.08214},  year = {2022} }
A Large Harvested Corpus of Location Metonymy	2020	http://www.semanticscholar.org/paper/05b5752fea62e8245d9fa25c69ef171ed3fb1e07	This work proposes a new, labelled, high-quality corpus of location metonymy called WiMCor, which is large in size and has high coverage, and uses different labels of varying granularity to annotate the corpus.	maybe	3	Metonymy is a figure of speech in which an entity is referred to by another related entity. The existing datasets of metonymy are either too small in size or lack sufficient coverage. We propose a new, labelled, high-quality corpus of location metonymy called WiMCor, which is large in size and has high coverage. The corpus is harvested semi-automatically from English Wikipedia. We use different labels of varying granularity to annotate the corpus. The corpus can directly be used for training and evaluating automatic metonymy resolution systems. We construct benchmarks for metonymy resolution, and evaluate baseline methods using the new corpus.	05b5752fea62e8245d9fa25c69ef171ed3fb1e07	@['JournalArticle']{mathews-strube-2020-a,  author = {Kevin Alex Mathews and M. Strube},  booktitle = {International Conference on Language Resources and Evaluation},  pages = {5678-5687},  title = {A Large Harvested Corpus of Location Metonymy},  year = {2020} }
A howling success or a working sea? Testing what BERT knows about metaphors	2021	https://www.semanticscholar.org/paper/18e083228bce6f0f5447316078980f9ac567886e		maybe	2	Metaphor is a widespread linguistic and cognitive phenomenon that is ruled by mechanisms which have received attention in the literature. Transformer Language Models such as BERT have brought improvements in metaphor-related tasks. However, they have been used only in application contexts, while their knowledge of the phenomenon has not been analyzed. To test what BERT knows about metaphors, we challenge it on a new dataset that we designed to test various aspects of this phenomenon such as variations in linguistic structure, variations in conventionality, the boundaries of the plausibility of a metaphor and the interpretations that we attribute to metaphoric expressions. Results bring out some tendencies that suggest that the model can reproduce some human intuitions about metaphors.	18e083228bce6f0f5447316078980f9ac567886e	@['JournalArticle']{pedinotti-etal-2021-a,  author = {Paolo Pedinotti and Eliana Di Palma and Ludovica Cerini and Alessandro Lenci},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  pages = {192-204},  title = {A howling success or a working sea? Testing what BERT knows about metaphors},  year = {2021} }
A hierarchy of linguistic predictions during natural language comprehension	2020	http://www.semanticscholar.org/paper/4b2d04556f7d3377998f4f236b57086a1be74f23	This work establishes that brain responses to words are modulated by ubiquitous, probabilistic predictions, and disentangle model-based predictions into distinct dimensions, revealing dissociable signatures of syntactic, phonemic and semantic predictions.	maybe	56	Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analysing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous, probabilistic predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable signatures of syntactic, phonemic and semantic predictions. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.	4b2d04556f7d3377998f4f236b57086a1be74f23	@['JournalArticle']{heilbron-etal-2020-a,  author = {Micha Heilbron and K. Armeni and J. Schoffelen and P. Hagoort and F. D. de Lange},  booktitle = {bioRxiv},  journal = {Proceedings of the National Academy of Sciences of the United States of America},  title = {A hierarchy of linguistic predictions during natural language comprehension},  volume = {119},  year = {2020} }
A Hazard Analysis Framework for Code Synthesis Large Language Models	2022	http://www.semanticscholar.org/paper/dd112d4dbd4656223770989778f39700de3052bc	This paper outlines a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically.	maybe	2	Codex, a large language model (LLM) trained on a variety of codebases, exceeds the previous state of the art in its capacity to synthesize and generate code. Although Codex provides a plethora of benefits, models that may generate code on such scale have significant limitations, alignment problems, the potential to be misused, and the possibility to increase the rate of progress in technical fields that may themselves have destabilizing impacts or have misuse potential. Yet such safety impacts are not yet known or remain to be explored. In this paper, we outline a hazard analysis framework constructed at OpenAI to uncover hazards or safety risks that the deployment of models like Codex may impose technically, socially, politically, and economically. The analysis is informed by a novel evaluation framework that determines the capacity of advanced code generation techniques against the complexity and expressivity of specification prompts, and their capability to understand and execute them relative to human ability.	dd112d4dbd4656223770989778f39700de3052bc	@['JournalArticle']{khlaaf-etal-2022-a,  author = {Heidy Khlaaf and Pamela Mishkin and Joshua Achiam and Gretchen Krueger and Miles Brundage},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Hazard Analysis Framework for Code Synthesis Large Language Models},  volume = {abs/2207.14157},  year = {2022} }
A Generator-based Method for Attacking Embedding-based Text Classifiers	2022	http://www.semanticscholar.org/paper/ed5afd71cd3e2fdcd7415de8ff966dceea3fb8fd	This paper proposes a method to attack embedding-based text classifiers, namely GenEmC, to reduce the high computational cost of rule-based replacement and shows that the average success rate and semantic-preserving output score ofGenEmC is moderately higher than those of PWWS and TextBugger.	maybe	0	This paper proposes a method to attack embedding-based text classifiers, namely GenEmC, to reduce the high computational cost of rule-based replacement. GenEmC includes three main steps including data preparation, embedding decoder training, and generator training. The embedding decoder is learned to convert embedding vectors to texts. The generator is trained by a two-term loss to generate adversarial embedding vectors from original texts. After three steps, the combination of two trained models could generate adversarial texts instantly. To demonstrate the effectiveness of GenEmC, the experiments are conducted on the IMDB dataset. The target classifiers are well-trained GRU, LSTM, and Bert. GenEmC is compared with two well-known rule-based replacement methods known as PWWS and TextBugger. The experiments have demonstrated that for 1,000 texts in test set, while the computational cost of GenEmC is an average of about 6 minutes, the others require at least 10 hours. Besides, the average success rate and semantic-preserving output score of GenEmC is moderately higher than those of PWWS and TextBugger.	ed5afd71cd3e2fdcd7415de8ff966dceea3fb8fd	@['JournalArticle', 'Conference']{kha-etal-2022-a,  author = {Do Minh Kha and D. Nguyen and Minh Le Nguyen and P. N. Hung},  booktitle = {Conference on Research, Innovation and Vision for the Future in Computing & Communication Technologies},  journal = {2022 RIVF International Conference on Computing and Communication Technologies (RIVF)},  pages = {618-623},  title = {A Generator-based Method for Attacking Embedding-based Text Classifiers},  year = {2022} }
A Generalized Lottery Ticket Hypothesis	2021	http://www.semanticscholar.org/paper/7220984a462b650a0f1e12a0954cf7cd273434aa	It is described how structured pruning methods, including pruning units or factorizing fully-connected layers into products of lowrank matrices, can be cast as particular instances of this “generalized” lottery ticket hypothesis.	maybe	3	We introduce a generalization to the lottery ticket hypothesis in which the notion of “sparsity” is relaxed by choosing an arbitrary basis in the space of parameters. We present evidence that the original results reported for the canonical basis continue to hold in this broader setting. We describe how structured pruning methods, including pruning units or factorizing fully-connected layers into products of lowrank matrices, can be cast as particular instances of this “generalized” lottery ticket hypothesis. The investigations reported here are preliminary and are provided to encourage further research along this direction.	7220984a462b650a0f1e12a0954cf7cd273434aa	@['JournalArticle']{alabdulmohsin-etal-2021-a,  author = {Ibrahim M. Alabdulmohsin and L. Markeeva and Daniel Keysers and I. Tolstikhin},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Generalized Lottery Ticket Hypothesis},  volume = {abs/2107.06825},  year = {2021} }
A Framework for Measuring Compositional Inductive Bias	2021	http://www.semanticscholar.org/paper/32f7dd1a327f0ac82ff8f3958ec94be955130bd4	A hierarchical model is proposed which might show an inductive bias towards relocatable atomic groups of tokens, thus potentially encouraging the emergence of words, and is used to compare and contrast a wide range of models.	maybe	0	We present a framework for measuring the compositional inductive bias of models in the context of emergent communications. We devise corrupted compositional grammars that probe for limitations in the compositional inductive bias of frequently used models. We use these corrupted compositional grammars to compare and contrast a wide range of models, and to compare the choice of soft, Gumbel, and discrete representations. We propose a hierarchical model which might show an inductive bias towards relocatable atomic groups of tokens, thus potentially encouraging the emergence of words. We experiment with probing for the compositional inductive bias of sender and receiver networks in isolation, and also placed end-to-end, as an auto-encoder.	32f7dd1a327f0ac82ff8f3958ec94be955130bd4	@['JournalArticle']{perkins-2021-a,  author = {Hugh Perkins},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Framework for Measuring Compositional Inductive Bias},  volume = {abs/2103.04180},  year = {2021} }
A Fine-grained Interpretability Evaluation Benchmark for Neural NLP	2022	http://www.semanticscholar.org/paper/4a3568b47c042f3059d2a11f5e12ca5ce3492d75	A novel benchmark to evaluate the interpretability of both neural models and saliency methods, which covers three representative NLP tasks: sentiment analysis, textual similarity and reading comprehension, each provided with both English and Chinese annotated data.	maybe	3	While there is increasing concern about the interpretability of neural models, the evaluation of interpretability remains an open problem, due to the lack of proper evaluation datasets and metrics. In this paper, we present a novel benchmark to evaluate the interpretability of both neural models and saliency methods. This benchmark covers three representative NLP tasks: sentiment analysis, textual similarity and reading comprehension, each provided with both English and Chinese annotated data. In order to precisely evaluate the interpretability, we provide token-level rationales that are carefully annotated to be sufficient, compact and comprehensive. We also design a new metric, i.e., the consistency between the rationales before and after perturbations, to uniformly evaluate the interpretability on different types of tasks. Based on this benchmark, we conduct experiments on three typical models with three saliency methods, and unveil their strengths and weakness in terms of interpretability. We will release this benchmark (https://www.luge.ai/#/luge/task/taskDetail?taskId=15) and hope it can facilitate the research in building trustworthy systems.	4a3568b47c042f3059d2a11f5e12ca5ce3492d75	@['JournalArticle']{wang-etal-2022-a,  author = {Lijie Wang and Yaozong Shen and Shu-ping Peng and Shuai Zhang and Xinyan Xiao and Hao Liu and Hongxuan Tang and Ying Chen and Hua Wu and Haifeng Wang},  booktitle = {Conference on Computational Natural Language Learning},  journal = {ArXiv},  title = {A Fine-grained Interpretability Evaluation Benchmark for Neural NLP},  volume = {abs/2205.11097},  year = {2022} }
A fine-grained comparison of pragmatic language understanding in humans and language models	2022	http://www.semanticscholar.org/paper/5d96a7b57f4959ebcf65eaa21b9f6004b7cdeae9		maybe	3	Pragmatics is an essential part of communication, but it remains unclear what mechanisms underlie human pragmatic communication and whether NLP systems capture pragmatic language understanding. To investigate both these questions, we perform a ﬁne-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We ﬁnd that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor the literal interpretation of an utterance over heuristic-based distractors. We also ﬁnd evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that even paradigmatic pragmatic phenomena may be solved without explicit representations of other agents’ mental states, and that artiﬁcial models can be used to gain mechanistic insights into human pragmatic processing.	5d96a7b57f4959ebcf65eaa21b9f6004b7cdeae9	@['JournalArticle']{hu-etal-2022-a,  author = {Jennifer Hu and Sammy Floyd and Olessia Jouravlev and Evelina Fedorenko and E. Gibson},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A fine-grained comparison of pragmatic language understanding in humans and language models},  volume = {abs/2212.06801},  year = {2022} }
A Few More Examples May Be Worth Billions of Parameters	2021	http://www.semanticscholar.org/paper/20e5561e3e576fbc10b0e97f8f64d8c875e17ad7	It is hypothesized that unlike open question answering, solving strategies for tasks with a more restricted output space transfer across examples, and can therefore be learned with small amounts of labeled data, the dynamics of increasing the number of model parameters versus thenumber of labeled examples across a wide variety of tasks.	maybe	4	We investigate the dynamics of increasing the number of model parameters versus the number of labeled examples across a wide variety of tasks. Our exploration reveals that while scaling parameters consistently yields performance improvements, the contribution of additional examples highly depends on the task’s format. Specifically, in open question answering tasks, enlarging the training set does not improve performance. In contrast, classification, extractive question answering, and multiple choice tasks benefit so much from additional examples that collecting a few hundred examples is often “worth” billions of parameters. We hypothesize that unlike open question answering, which involves recalling specific information, solving strategies for tasks with a more restricted output space transfer across examples, and can therefore be learned with small amounts of labeled data.1	20e5561e3e576fbc10b0e97f8f64d8c875e17ad7	@['JournalArticle']{kirstain-etal-2021-a,  author = {Yuval Kirstain and Patrick Lewis and S. Riedel and Omer Levy},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Few More Examples May Be Worth Billions of Parameters},  volume = {abs/2110.04374},  year = {2021} }
A Distributional Approach to Controlled Text Generation	2020	http://www.semanticscholar.org/paper/07fd366a8ebdefe54cdb57d87c81dcd22de25a91	This approach permits to specify, in a single formal framework, both “pointwise” and “distributional” constraints over the target LM while minimizing KL divergence from the initial LM distribution.	maybe	49	We propose a Distributional Approach for addressing Controlled Text Generation from pre-trained Language Models (LMs). This approach permits to specify, in a single formal framework, both “pointwise” and “distributional” constraints over the target LM — to our knowledge, the first model with such generality — while minimizing KL divergence from the initial LM distribution. The optimal target distribution is then uniquely determined as an explicit EBM (Energy-Based Model) representation. From that optimal representation we then train a target controlled Autoregressive LM through an adaptive distributional variant of Policy Gradient. We conduct a first set of experiments over pointwise constraints showing the advantages of our approach over a set of baselines, in terms of obtaining a controlled LM balancing constraint satisfaction with divergence from the initial LM. We then perform experiments over distributional constraints, a unique feature of our approach, demonstrating its potential as a remedy to the problem of Bias in Language Models. Through an ablation study, we show the effectiveness of our adaptive technique for obtaining faster convergence.1	07fd366a8ebdefe54cdb57d87c81dcd22de25a91	@['JournalArticle']{khalifa-etal-2020-a,  author = {Muhammad Khalifa and Hady ElSahar and Marc Dymetman},  booktitle = {International Conference on Learning Representations},  journal = {ArXiv},  title = {A Distributional Approach to Controlled Text Generation},  volume = {abs/2012.11635},  year = {2020} }
A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction	2023	https://www.semanticscholar.org/paper/9cec6ff085064ca77200a58362f7242f5881a238		maybe	0	Knowledge of syntax includes knowledge of rare, idiosyncratic constructions. LLMs must overcome frequency biases in order to master such constructions. In this study, I prompt GPT-3 to give acceptability judgments on the English-language Article + Adjective + Numeral + Noun construction (e.g., “a lovely ﬁve days”). I validate the prompt using the CoLA corpus of acceptability judgments and then zero in on the AANN construction. I compare GPT-3’s judgments to crowdsourced human judgments on a subset of sentences. GPT-3’s judgments are broadly similar to human judgments and generally align with proposed constraints in the literature but, in some cases, GPT-3’s judgments and human judgments di-verge from the literature and from each other.	9cec6ff085064ca77200a58362f7242f5881a238	@['JournalArticle']{mahowald-2023-a,  author = {Kyle Mahowald},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Discerning Several Thousand Judgments: GPT-3 Rates the Article + Adjective + Numeral + Noun Construction},  volume = {abs/2301.12564},  year = {2023} }
A critical appraisal of equity in conversational AI: Evidence from auditing GPT-3's dialogues with different publics on climate change and Black Lives Matter	2022	http://www.semanticscholar.org/paper/d149f1df6cd921e8e63aa0d7f72e7eae96076fef	An auditing study to examine how GPT-3 responded to different sub-populations on crucial science and social topics: climate change and the Black Lives Matter movement, and proposes an analytical framework for unpacking the meaning of equity in human-AI dialogues.	yes	0	Autoregressive language models, which use deep learning to produce human-like texts, have become increasingly widespread. Such models are powering popular virtual assistants in areas like smart health, finance, and autonomous driving, and facilitating the production of creative writing in domains from the entertainment industry to science communities. While the parameters of these large language models are improving, concerns persist that these models might not work equally for all subgroups in society. Despite growing discussions of AI fairness across disciplines, there is a lack of systemic metrics to assess what equity means in dialogue systems and how to engage different populations in the assessment loop. Grounded in theories of deliberative democracy and science and technology studies, this paper proposes an analytical framework for unpacking the meaning of equity in human-AI dialogues. Using this framework, we conducted an auditing study to examine how GPT-3 responded to different sub-populations on crucial science and social topics: climate change and the Black Lives Matter (BLM) movement. Our corpus consists of over 20,000 rounds of dialogues between GPT-3 and 3290 individuals who vary in gender, race and ethnicity, education level, English as a first language, and opinions toward the issues. We found a substantively worse user experience with GPT-3 among the opinion and the education minority subpopulations; however, these two groups achieved the largest knowledge gain, changing attitudes toward supporting BLM and climate change efforts after the chat. We traced these user experience divides to conversational differences and found that GPT-3 used more negative expressions when it responded to the education and opinion minority groups, compared to its responses to the majority groups. To what extent GPT-3 uses justification when responding to the minority groups is contingent on the issue. We discuss the implications of our findings for a deliberative conversational AI system that centralizes diversity, equity, and inclusion. prompts on the climate change and 1,742 prompts from the BLM into dichotomous supporting climate change/BLM or not . The prompts were randomly and evenly selected from participants’ inputs and chatbot responses. We leveraged Recurrent Neural Networks (RNN) with a gated recurrent unit (GRU) to train two classifiers to identify climate change and BLM stances respectively (69). The models showed good performance in stance detection, with an F1 score of 0.74 for climate change prompts and 0.78 for BLM prompts. We then used this stance model to calculate the probability of whether each prompt was supportive of the topic and predict the binary stance in a one-hot manner. Further details of the codebook and training processes are in SI Appendix A 3.4.	d149f1df6cd921e8e63aa0d7f72e7eae96076fef	@['JournalArticle']{chen-etal-2022-a,  author = {Kaiping Chen and Anqi Shao and Jirayu Burapacheep and Yixuan Li},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A critical appraisal of equity in conversational AI: Evidence from auditing GPT-3's dialogues with different publics on climate change and Black Lives Matter},  volume = {abs/2209.13627},  year = {2022} }
A Continuum of Generation Tasks for Investigating Length Bias and Degenerate Repetition	2022	https://www.semanticscholar.org/paper/b6d98839dfae708d845ba482665f3e6343f4f173	A new experimental framework is introduced that allows for smoothly vary task constrainedness, from MT at one end to fully open-ended generation at the other, while keeping all other aspects intact, and shows that repetition decreases smoothly with constrainedness.	maybe	0	Language models suffer from various degenerate behaviors. These differ between tasks: machine translation (MT) exhibits length bias, while tasks like story generation exhibit excessive repetition. Recent work has attributed the difference to task constrainedness, but evidence for this claim has always involved many confounding variables. To study this question directly, we introduce a new experimental framework that allows us to smoothly vary task constrainedness, from MT at one end to fully open-ended generation at the other, while keeping all other aspects fixed. We find that: (1) repetition decreases smoothly with constrainedness, explaining the difference in repetition across tasks; (2) length bias surprisingly also decreases with constrainedness, suggesting some other cause for the difference in length bias; (3) across the board, these problems affect the mode, not the whole distribution; (4) the differences cannot be attributed to a change in the entropy of the distribution, since another method of changing the entropy, label smoothing, does not produce the same effect.	b6d98839dfae708d845ba482665f3e6343f4f173	@['JournalArticle']{riley-chiang-2022-a,  author = {Darcey Riley and David Chiang},  booktitle = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},  journal = {ArXiv},  title = {A Continuum of Generation Tasks for Investigating Length Bias and Degenerate Repetition},  volume = {abs/2210.10817},  year = {2022} }
A computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface of dependency resolution and training time	2022	http://www.semanticscholar.org/paper/f9c6ad7642a0f0633374b566d88e564952597acd	The ability of Transformer models to capture subjectverb and noun-adjective agreement dependencies in Galician is explored and interesting insights into the number of training steps required by a Transformer model to solve long-distance dependencies are provided.	maybe	0	This paper explores the ability of Transformer models to capture subjectverb and noun-adjective agreement dependencies in Galician. We conduct a series of word prediction experiments in which we manipulate dependency length together with the presence of an attractor noun that acts as a lure. First, we evaluate the overall performance of the existing monolingual and multilingual models for Galician. Secondly, to observe the effects of the training process, we compare the different degrees of achievement of two monolingual BERT models at different training points. We also release their checkpoints and propose an alternative evaluation metric. Our results confirm previous findings by similar works that use the agreement prediction task and provide interesting insights into the number of training steps required by a Transformer model to solve long-distance dependencies.	f9c6ad7642a0f0633374b566d88e564952597acd	@['JournalArticle']{flores-garcia-2022-a,  author = {Iria de-Dios-Flores and Marcos Garcia},  booktitle = {Proces. del Leng. Natural},  journal = {Proces. del Leng. Natural},  pages = {15-26},  title = {A computational psycholinguistic evaluation of the syntactic abilities of Galician BERT models at the interface of dependency resolution and training time},  volume = {69},  year = {2022} }
A Comprehensive Analysis of Subword Contextual Embeddings for Languages with Rich Morphology	2021	http://www.semanticscholar.org/paper/7a5ecc6cbfb70e72fb3c3660f3d900c0643bf566		maybe	0		7a5ecc6cbfb70e72fb3c3660f3d900c0643bf566	@None{akdemir-etal-2021-a,  author = {Arda Akdemir and T. Shibuya and Tunga Güngör},  booktitle = {Advances in Intelligent Systems and Computing},  journal = {Advances in Intelligent Systems and Computing},  title = {A Comprehensive Analysis of Subword Contextual Embeddings for Languages with Rich Morphology},  year = {2021} }
A Comparison of Code Embeddings and Beyond	2021	http://www.semanticscholar.org/paper/77056c2118001e545d5e67ff6f333ab3c6c070bd	This paper systemically evaluates the performance of eight program representation learning models on three common tasks, where six models are based on abstract syntax trees and two models areBased on plain text of source code, and applies a prediction attribution technique to find what elements are captured and responsible for the predictions in each task.	maybe	1	Program representation learning is a fundamental task in software engineering applications. With the availability of “big code” and the development of deep learning techniques, various program representation learning models have been proposed to understand the semantic properties of programs and applied on different software engineering tasks. However, no previous study has comprehensively assessed the generalizability of these deep models on different tasks, so that the pros and cons of the models are unclear. In this experience paper, we try to bridge this gap by systemically evaluating the performance of eight program representation learning models on three common tasks, where six models are based on abstract syntax trees and two models are based on plain text of source code. We kindly explain the criteria for selecting the models and tasks, as well as the method for enabling end-to-end learning in each task. The results of performance evaluation show that they perform diversely in each task and the performance of the AST-based models is generally unstable over different tasks. In order to further explain the results, we apply a prediction attribution technique to find what elements are captured by the models and responsible for the predictions in each task. Based on the findings, we discuss some general principles for better capturing the information in the source code, and hope to inspire researchers to improve program representation learning methods for software engineering tasks.	77056c2118001e545d5e67ff6f333ab3c6c070bd	@['JournalArticle']{han-etal-2021-a,  author = {Siqi Han and Dongxia Wang and Wan-Mei Li and Xuesong Lu},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Comparison of Code Embeddings and Beyond},  volume = {abs/2109.07173},  year = {2021} }
A Comparative Study of Transformers on Word Sense Disambiguation	2021	http://www.semanticscholar.org/paper/f068c591f69af4a836b89624c6a8d8e2790ef37d	A comparative study and an extensive analysis of nine widely adopted Transformer models and a simple yet effective approach to WSD that uses a k -Nearest Neighbor (kNN) classification on CWEs are presented.	maybe	0		f068c591f69af4a836b89624c6a8d8e2790ef37d	@['JournalArticle']{chawla-etal-2021-a,  author = {Avi Chawla and Nidhi Mulay and Vikas Bishnoi and Gaurav Dhama and Dr. Anil Kumar Singh},  booktitle = {International Conference on Neural Information Processing},  pages = {748-756},  title = {A Comparative Study of Transformers on Word Sense Disambiguation},  year = {2021} }
A Comparative Study of Lexical Substitution Approaches based on Neural Language Models	2020	http://www.semanticscholar.org/paper/993a2a567fa14039f930295dad8e2779ef7dd2c0	A large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution shows that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly.	maybe	5	Lexical substitution in context is an extremely powerful technology that can be used as a backbone of various NLP applications, such as word sense induction, lexical relation extraction, data augmentation, etc. In this paper, we present a large-scale comparative study of popular neural language and masked language models (LMs and MLMs), such as context2vec, ELMo, BERT, XLNet, applied to the task of lexical substitution. We show that already competitive results achieved by SOTA LMs/MLMs can be further improved if information about the target word is injected properly, and compare several target injection methods. In addition, we provide analysis of the types of semantic relations between the target and substitutes generated by different models providing insights into what kind of words are really generated or given by annotators as substitutes.	993a2a567fa14039f930295dad8e2779ef7dd2c0	@['JournalArticle']{arefyev-etal-2020-a,  author = {N. Arefyev and Boris Sheludko and A. V. Podolskiy and Alexander Panchenko},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Comparative Study of Lexical Substitution Approaches based on Neural Language Models},  volume = {abs/2006.00031},  year = {2020} }
A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space	2021	http://www.semanticscholar.org/paper/36e69a05b315c7f2c51379a261b70154482a4c74	It is shown that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks and it is found that tense information in verb representations dominates sense semantics.	maybe	12	The representation degeneration problem in Contextual Word Representations (CWRs) hurts the expressiveness of the embedding space by forming an anisotropic cone where even unrelated words have excessively positive correlations. Existing techniques for tackling this issue require a learning process to re-train models with additional objectives and mostly employ a global assessment to study isotropy. Our quantitative analysis over isotropy shows that a local assessment could be more accurate due to the clustered structure of CWRs. Based on this observation, we propose a local cluster-based method to address the degeneration issue in contextual embedding spaces. We show that in clusters including punctuations and stop words, local dominant directions encode structural information, removing which can improve CWRs performance on semantic tasks. Moreover, we find that tense information in verb representations dominates sense semantics. We show that removing dominant directions of verb representations can transform the space to better suit semantic applications. Our experiments demonstrate that the proposed cluster-based method can mitigate the degeneration problem on multiple tasks.	36e69a05b315c7f2c51379a261b70154482a4c74	@['JournalArticle', 'Conference']{rajaee-pilehvar-2021-a,  author = {S. Rajaee and Mohammad Taher Pilehvar},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {575-584},  title = {A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space},  year = {2021} }
A closer look at the performance of neural language models on reflexive anaphor licensing	2020	http://www.semanticscholar.org/paper/d926c378355d54925df1909fcb71f115d34b1cc8	This work uses a new paradigm to re-evaluate the performance of a diverse set of NLMs on reflexive anaphor licensing and demonstrates the value of wellcontrolled psycholinguistic methods in gaining a fine-grained understanding of NLM learning potential.	maybe	9	An emerging line of work uses psycholinguistic methods to evaluate the syntactic generalizations acquired by neural language models (NLMs). While this approach has shown NLMs to be capable of learning a wide range of linguistic knowledge, confounds in the design of previous experiments may have obscured the potential of NLMs to learn certain grammatical phenomena. We use a new paradigm to re-evaluate the performance of a diverse set of NLMs on reflexive anaphor licensing. Under our paradigm, the models consistently show stronger evidence of learning than reported in previous work. Our approach demonstrates the value of wellcontrolled psycholinguistic methods in gaining a fine-grained understanding of NLM learning potential.	d926c378355d54925df1909fcb71f115d34b1cc8	@None{hu-etal-2020-a,  author = {Jennifer Hu and S. Y. Chen and R. Levy},  booktitle = {SCIL},  pages = {382-392},  title = {A closer look at the performance of neural language models on reflexive anaphor licensing},  volume = {3},  year = {2020} }
A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English	2020	http://www.semanticscholar.org/paper/4bda9767c11a97a3d1a83577cb8ec94f16ceccb5	This work evaluates three transformer-based language models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks, and highlights the importance of model comparison in evaluation task and building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.	maybe	3	Transformer-based language models achieve high performance on various tasks, but we still lack understanding of the kind of linguistic knowledge they learn and rely on. We evaluate three models (BERT, RoBERTa, and ALBERT), testing their grammatical and semantic knowledge by sentence-level probing, diagnostic cases, and masked prediction tasks. We focus on relative clauses (in American English) as a complex phenomenon needing contextual information and antecedent identification to be resolved. Based on a naturalistic dataset, probing shows that all three models indeed capture linguistic knowledge about grammaticality, achieving high performance.Evaluation on diagnostic cases and masked prediction tasks considering fine-grained linguistic knowledge, however, shows pronounced model-specific weaknesses especially on semantic knowledge, strongly impacting models’ performance. Our results highlight the importance of (a)model comparison in evaluation task and (b) building up claims of model performance and the linguistic knowledge they capture beyond purely probing-based evaluations.	4bda9767c11a97a3d1a83577cb8ec94f16ceccb5	@['JournalArticle', 'Conference']{mosbach-etal-2020-a,  author = {Marius Mosbach and Stefania Degaetano-Ortlieb and Marie-Pauline Krielke and Badr M. Abdullah and D. Klakow},  booktitle = {International Conference on Computational Linguistics},  pages = {771-787},  title = {A Closer Look at Linguistic Knowledge in Masked Language Models: The Case of Relative Clauses in American English},  year = {2020} }
A Closer Look at How Fine-tuning Changes BERT	2021	http://www.semanticscholar.org/paper/966a38882be844dbf7e8b15478e1bdf3c75ef8a6	It is hypothesized that fine-tuning affects classification performance by increasing the distances between examples associated with different labels and an exception to the prevailing wisdom that “fine- Tuning always improves performance” is discovered.	maybe	11	Given the prevalence of pre-trained contextualized representations in today’s NLP, there have been many efforts to understand what information they contain, and why they seem to be universally successful. The most common approach to use these representations involves fine-tuning them for an end task. Yet, how fine-tuning changes the underlying embedding space is less studied. In this work, we study the English BERT family and use two probing techniques to analyze how fine-tuning changes the space. We hypothesize that fine-tuning affects classification performance by increasing the distances between examples associated with different labels. We confirm this hypothesis with carefully designed experiments on five different NLP tasks. Via these experiments, we also discover an exception to the prevailing wisdom that “fine-tuning always improves performance”. Finally, by comparing the representations before and after fine-tuning, we discover that fine-tuning does not introduce arbitrary changes to representations; instead, it adjusts the representations to downstream tasks while largely preserving the original spatial structure of the data points.	966a38882be844dbf7e8b15478e1bdf3c75ef8a6	@['JournalArticle', 'Conference']{zhou-srikumar-2021-a,  author = {Yichu Zhou and Vivek Srikumar},  booktitle = {Annual Meeting of the Association for Computational Linguistics},  pages = {1046-1061},  title = {A Closer Look at How Fine-tuning Changes BERT},  year = {2021} }
A Close Look into the Calibration of Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/780f82a2d804c4f7ca8a4524c028755a03e6eafb	It is highlighted that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in conﬁdence, no matter the predictions are correct or not, which presents some contradiction with two established conclusions.	maybe	0	Pre-trained language models (PLMs) achieve remarkable performance on many downstream tasks, but may fail in giving reliable estimates of their predictive uncertainty. However, well-calibrated PLMs are essential for decision makers in high-risk applications since their conﬁdence scores reﬂect the true probability of the outcomes. Given the lack of a comprehensive understanding of PLMs calibration, we take a close look into this new research problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the ﬁrst question, we conduct ﬁne-grained control experiments to study the dynamic change in PLMs’ calibration performance in training. We consider six factors as control variables, including dataset difﬁculty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. In experiments, we observe a consistent change in calibration performance across six factors. We ﬁnd that PLMs don’t learn to become calibrated in training, evidenced by the continual increase in conﬁdence, no matter the predictions are correct or not. We highlight that our ﬁnding presents some contradiction with two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconﬁdence issue, in both in-distribution and various out-of-distribution settings. Besides unlearnable calibration methods (e.g., label smoothing), we adapt two recently proposed learnable methods that directly collect data to train models to have reasonable conﬁdence estimations. Also, we propose extended learnable methods based on existing ones to further improve or maintain PLMs calibration	780f82a2d804c4f7ca8a4524c028755a03e6eafb	@['JournalArticle']{chen-etal-2022-a,  author = {Yangyi Chen and Lifan Yuan and Ganqu Cui and Zhiyuan Liu and Heng Ji},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Close Look into the Calibration of Pre-trained Language Models},  volume = {abs/2211.00151},  year = {2022} }
A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models	2022	http://www.semanticscholar.org/paper/8fcefc5208a6ad49f03019f4b1caf7264b3dbac0	This work proposes a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands and math operators on the output solution, and shows that robustness does not appear to continuously improve as a function of scale, but that the recent GPT-3-Instruct achieves a dra-matic improvement in both robustness and sensitivity, compared to all other GPT variants.	maybe	2	We have recently witnessed a number of im-pressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when predicting a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of bivariate math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of scale, but that the recent LLM, GPT-3-Instruct (175B), achieves a dra-matic improvement in both robustness and sensitivity, compared to all other GPT variants. 1	8fcefc5208a6ad49f03019f4b1caf7264b3dbac0	@['JournalArticle']{stolfo-etal-2022-a,  author = {Alessandro Stolfo and Zhijing Jin and K. Shridhar and B. Schölkopf and Mrinmaya Sachan},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},  volume = {abs/2210.12023},  year = {2022} }
A Benchmark Arabic Dataset for Commonsense Explanation	2020	http://www.semanticscholar.org/paper/7b854f96f1b491719b9b8bb773804194cb5e00fb	A benchmark Arabic dataset for commonsense explanation that consists of Arabic sentences that does not make sense along with three choices to select among them the one that explains why the sentence is false.	maybe	0	Language comprehension and commonsense knowledge validation by machines are challenging tasks that are still under researched and evaluated for Arabic text. In this paper, we present a benchmark Arabic dataset for commonsense explanation. The dataset consists of Arabic sentences that does not make sense along with three choices to select among them the one that explains why the sentence is false. Furthermore, this paper presents baseline results to assist and encourage the future evaluation of research in this field. The dataset is distributed under the Creative Commons CC-BY-SA 4.0 license and can be found on GitHub.	7b854f96f1b491719b9b8bb773804194cb5e00fb	@['JournalArticle']{tawalbeh-smadi-2020-a,  author = {Saja AL-Tawalbeh and Mohammad Al-Smadi},  booktitle = {ArXiv},  journal = {ArXiv},  title = {A Benchmark Arabic Dataset for Commonsense Explanation},  volume = {abs/2012.10251},  year = {2020} }
A Bayesian Framework for Information-Theoretic Probing	2021	http://www.semanticscholar.org/paper/884c7aac3358a6f91887dd0d091759963764bedd	A new framework is proposed to measure what the authors term Bayesian mutual information, which analyses information from the perspective of Bayesian agents—allowing for more intuitive findings in scenarios with finite data.	maybe	16	Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents—allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.	884c7aac3358a6f91887dd0d091759963764bedd	@['JournalArticle', 'Conference']{pimentel-cotterell-2021-a,  author = {Tiago Pimentel and Ryan Cotterell},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {A Bayesian Framework for Information-Theoretic Probing},  volume = {abs/2109.03853},  year = {2021} }
20Q: Overlap-Free World Knowledge Benchmark for Language Models	2022	https://www.semanticscholar.org/paper/9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3	20Q is introduced, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models and shows that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities.	maybe	0	What do language models know about our world? This question is hard to answer but important to get right. To this end, we introduce 20Q, a novel benchmark using the Twenty Questions game to evaluate world knowledge and common sense of language models. Thanks to our overlap-free benchmark, language models learn the game of Twenty Questions without learning relevant knowledge for the test set. We uncover two intuitive factors influencing the world knowledge of language models: the size of the model and the topic frequency in the pre-training data. Moreover, we show that in-context learning is inefficient for evaluating language models’ world knowledge — fine-tuning is necessary to show their true capabilities. Lastly, our results show room for improvement to enhance the world knowledge and common sense of large language models. A potential solution would be to up-sample unfrequent topics in the pre-training of language models.	9cd0cb3af7c2215eae9afdcf500a2bcd5330aae3	@None{bruyn-etal-2022-20q:,  author = {Maxime De Bruyn and Ehsan Lotfi and Jeska Buhmann and Walter Daelemans},  booktitle = {IEEE Games Entertainment Media Conference},  title = {20Q: Overlap-Free World Knowledge Benchmark for Language Models},  year = {2022} }
[Rp] Learning to Deceive with Attention-Based Explanations	2021	http://www.semanticscholar.org/paper/538ef96876908f95d4e68940c7231df62c9ff0d4	This reproducibility report argues that by running their experiments, one can reproduce the results which support the claims by Pruthi et al.	maybe	0	Learning to Deceive with Attention-Based Explanations by Pruthi et al. makes two claims which we reproduce in this work. Their first claim entails that attention weights can be manipulated to shift mass away from a predefined set of impermissible tokens without significant chance in performance. Their second claim entails that these manipulated models still rely on information from the set of impermissible tokens. In this reproducibility report we argue that by running their experiments, we can reproduce the results which support the claims by Pruthi et al..	538ef96876908f95d4e68940c7231df62c9ff0d4	@None{broek-gommers-2021-[rp],  author = {Jelle van den Broek and Koen Gommers},  title = {[Rp] Learning to Deceive with Attention-Based Explanations},  year = {2021} }
[Re] Reproducing Learning to Deceive With Attention-Based Explanations	2021	http://www.semanticscholar.org/paper/ed80a8888bcd3a8adf2ca784a5699969eaa597da		maybe	0	of using classification and sequence-to-sequence (seq2seq) They examine the modelʼs use of impermissible tokens, which are tokens that can introduce bias e.g. gendered pronouns. Across multiple datasets, the show that with the impermissible tokens removed the model accuracy drops, implying their usage in prediction. And then by penalising attention to the impermissible tokens but keeping them in, they train models that hence be using the impermissible tokens, but does not being to the impermissible tokens. As the paperʼs have significant implications for the use of attention-based explanations, we seek to reproduce their results.	ed80a8888bcd3a8adf2ca784a5699969eaa597da	@None{habacker-etal-2021-[re],  author = {Rahel Habacker and A. Harrison and Mathias Parisot and Ard Snijders},  title = {[Re] Reproducing Learning to Deceive With Attention-Based Explanations},  year = {2021} }
“You Are Grounded!”: Latent Name Artifacts in Pre-trained Language Models	2020	http://www.semanticscholar.org/paper/4d96bfa3a4c283d6fcd71122897c2cb8ee1c886d	This work focuses on artifacts associated with the representation of given names, which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Donald), and suggests additional pre-training on different corpora may mitigate this bias.	maybe	32	Pre-trained language models (LMs) may perpetuate biases originating in their training corpus to downstream models. We focus on artifacts associated with the representation of given names (e.g., Donald), which, depending on the corpus, may be associated with specific entities, as indicated by next token prediction (e.g., Trump). While helpful in some contexts, grounding happens also in under-specified or inappropriate contexts. For example, endings generated for `Donald is a' substantially differ from those of other names, and often have more-than-average negative sentiment. We demonstrate the potential effect on downstream tasks with reading comprehension probes where name perturbation changes the model answers. As a silver lining, our experiments suggest that additional pre-training on different corpora may mitigate this bias.	4d96bfa3a4c283d6fcd71122897c2cb8ee1c886d	@['JournalArticle', 'Conference']{shwartz-etal-2020-“you,  author = {Vered Shwartz and Rachel Rudinger and Oyvind Tafjord},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {“You Are Grounded!”: Latent Name Artifacts in Pre-trained Language Models},  volume = {abs/2004.03012},  year = {2020} }
“Was it “stated” or was it “claimed”?: How linguistic bias affects generative language models	2021	http://www.semanticscholar.org/paper/d8d5ab10cb30034296b1bf6acd15a89b7be13a25		yes	2	People use language in subtle and nuanced ways to convey their beliefs. For instance, saying claimed instead of said casts doubt on the truthfulness of the underlying proposition, thus representing the author’s opinion on the matter. Several works have identified such linguistic classes of words that occur frequently in natural language text and are bias-inducing by virtue of their framing effects. In this paper, we test whether generative language models (including GPT-2 (CITATION) are sensitive to these linguistic framing effects. In particular, we test whether prompts that contain linguistic markers of author bias (e.g., hedges, implicatives, subjective intensifiers, assertives) influence the distribution of the generated text. Although these framing effects are subtle and stylistic, we find evidence that they lead to measurable style and topic differences in the generated text, leading to language that is, on average, more polarised and more skewed towards controversial entities and events.	d8d5ab10cb30034296b1bf6acd15a89b7be13a25	@['JournalArticle', 'Conference']{patel-pavlick-2021-“was,  author = {Roma Patel and Elizabeth-Jane Pavlick},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  pages = {10080-10095},  title = {“Was it “stated” or was it “claimed”?: How linguistic bias affects generative language models},  year = {2021} }
“Politeness, you simpleton!” retorted [MASK]: Masked prediction of literary characters	2021	http://www.semanticscholar.org/paper/c924734a41251f03242cbb6345fb046d5fe3eeb3	It is found that when continuously trained, entity embeddings do well at the masked entity prediction task, and that they encode considerable information about the traits and characteristics of the entities.	maybe	1	What is the best way to learn embeddings for entities, and what can be learned from them? We consider this question for the case of literary characters. We address the highly challenging task of guessing, from a sentence in the novel, which character is being talked about, and we probe the embeddings to see what information they encode about their literary characters. We find that when continuously trained, entity embeddings do well at the masked entity prediction task, and that they encode considerable information about the traits and characteristics of the entities.	c924734a41251f03242cbb6345fb046d5fe3eeb3	@None{holgate-erk-2021-“politeness,  author = {Eric Holgate and K. Erk},  booktitle = {International Conference on Computational Semantics},  title = {“Politeness, you simpleton!” retorted [MASK]: Masked prediction of literary characters},  year = {2021} }
“No, They Did Not”: Dialogue Response Dynamics in Pre-trained Language Models	2022	http://www.semanticscholar.org/paper/a861f5aeef16dd697fed6e9f7e690869aac6438e		yes	0	A critical component of competence in language is being able to identify relevant components of an utterance and reply appropriately. In this paper we examine the extent of such dialogue response sensitivity in pre-trained language models, conducting a series of experiments with a particular focus on sensitivity to dynamics involving phenomena of at-issueness and ellipsis. We find that models show clear sensitivity to a distinctive role of embedded clauses, and a general preference for responses that target main clause content of prior utterances. However, the results indicate mixed and generally weak trends with respect to capturing the full range of dynamics involved in targeting at-issue versus not-at-issue content. Additionally, models show fundamental limitations in grasp of the dynamics governing ellipsis, and response selections show clear interference from superficial factors that outweigh the influence of principled discourse constraints.	a861f5aeef16dd697fed6e9f7e690869aac6438e	@['JournalArticle', 'Conference']{kim-etal-2022-“no,  author = {Sanghee Kim and Lang-Chi Yu and Allyson Ettinger},  booktitle = {International Conference on Computational Linguistics},  journal = {ArXiv},  title = {“No, They Did Not”: Dialogue Response Dynamics in Pre-trained Language Models},  volume = {abs/2210.02526},  year = {2022} }
“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses	2021	http://www.semanticscholar.org/paper/80e015b4edbe72cb10af0bd2cb065bba163d6e0d	Responses to Twitter topics about marginalized communities versus other topics are compared, and a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated is proposed.	maybe	19	Ad hominem attacks are those that target some feature of a person’s character instead of the position the person is maintaining. These attacks are harmful because they propagate implicit biases and diminish a person’s credibility. Since dialogue systems respond directly to user input, it is important to study ad hominems in dialogue responses. To this end, we propose categories of ad hominems, compose an annotated dataset, and build a classifier to analyze human and dialogue system responses to English Twitter posts. We specifically compare responses to Twitter topics about marginalized communities (#BlackLivesMatter, #MeToo) versus other topics (#Vegan, #WFH), because the abusive language of ad hominems could further amplify the skew of power away from marginalized populations. Furthermore, we propose a constrained decoding technique that uses salient n-gram similarity as a soft constraint for top-k sampling to reduce the amount of ad hominems generated. Our results indicate that 1) responses from both humans and DialoGPT contain more ad hominems for discussions around marginalized communities, 2) different quantities of ad hominems in the training data can influence the likelihood of generating ad hominems, and 3) we can use constrained decoding techniques to reduce ad hominems in generated dialogue responses.	80e015b4edbe72cb10af0bd2cb065bba163d6e0d	@['JournalArticle', 'Conference']{sheng-etal-2021-“nice,  author = {Emily Sheng and Kai-Wei Chang and P. Natarajan and Nanyun Peng},  booktitle = {North American Chapter of the Association for Computational Linguistics},  pages = {750-767},  title = {“Nice Try, Kiddo”: Investigating Ad Hominems in Dialogue Responses},  year = {2021} }
“Average” Approximates “First Principal Component”? An Empirical Analysis on Representations from Neural Language Models	2021	http://www.semanticscholar.org/paper/348d58a56f354c73e196a9d9633c27ba3e2bb4e8	An empirical property ofContextualized representations based on neural language models—”average” approximates “first principal component”—is presented and it is shown that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations.	yes	1	Contextualized representations based on neural language models have furthered the state of the art in various NLP tasks. Despite its great success, the nature of such representations remains a mystery. In this paper, we present an empirical property of these representations—”average” approximates “first principal component”. Specifically, experiments show that the average of these representations shares almost the same direction as the first principal component of the matrix whose columns are these representations. We believe this explains why the average representation is always a simple yet strong baseline. Our further examinations show that this property also holds in more challenging scenarios, for example, when the representations are from a model right after its random initialization. Therefore, we conjecture that this property is intrinsic to the distribution of representations and not necessarily related to the input structure. We realize that these representations empirically follow a normal distribution for each dimension, and by assuming this is true, we demonstrate that the empirical property can be in fact derived mathematically.	348d58a56f354c73e196a9d9633c27ba3e2bb4e8	@['JournalArticle', 'Conference']{wang-etal-2021-“average”,  author = {Zihan Wang and Chengyu Dong and Jingbo Shang},  booktitle = {Conference on Empirical Methods in Natural Language Processing},  journal = {ArXiv},  title = {“Average” Approximates “First Principal Component”? An Empirical Analysis on Representations from Neural Language Models},  volume = {abs/2104.08673},  year = {2021} }